- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:39:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:39:21
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2305.19812] A Survey of Label-Efficient Deep Learning for 3D Point Clouds'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2305.19812] 标签高效深度学习的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.19812](https://ar5iv.labs.arxiv.org/html/2305.19812)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.19812](https://ar5iv.labs.arxiv.org/html/2305.19812)
- en: A Survey of Label-Efficient Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签高效深度学习的综述
- en: for 3D Point Clouds
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 针对 3D 点云
- en: 'Aoran Xiao, Xiaoqin Zhang, Ling Shao , and Shijian Lu Aoran Xiao and Shijian
    Lu are with School of Computer Science and Engineering, Nanyang Technological
    University, Singapore. Xiaoqin Zhang is with Key Laboratory of Intelligent Informatics
    for Safety & Emergency of Zhejiang Province, Wenzhou University, China. Ling Shao
    is with the UCAS-Terminus AI Lab, University of Chinese Academy of Sciences, Beijing,
    China. Corresponding author: Shijian Lu (shijian.lu@ntu.edu.sg)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Aoran Xiao 和 Shijian Lu 供职于新加坡南洋理工大学计算机科学与工程学院。Xiaoqin Zhang 供职于中国温州大学浙江省智能信息安全与应急重点实验室。Ling
    Shao 供职于中国科学院大学终点 AI 实验室。通讯作者：Shijian Lu (shijian.lu@ntu.edu.sg)
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In the past decade, deep neural networks have achieved significant progress
    in point cloud learning. However, collecting large-scale precisely-annotated training
    data is extremely laborious and expensive, which hinders the scalability of existing
    point cloud datasets and poses a bottleneck for efficient exploration of point
    cloud data in various tasks and applications. Label-efficient learning offers
    a promising solution by enabling effective deep network training with much-reduced
    annotation efforts. This paper presents the first comprehensive survey of label-efficient
    learning of point clouds. We address three critical questions in this emerging
    research field: i) the importance and urgency of label-efficient learning in point
    cloud processing, ii) the subfields it encompasses, and iii) the progress achieved
    in this area. To achieve this, we propose a taxonomy that organizes label-efficient
    learning methods based on the data prerequisites provided by different types of
    labels. We categorize four typical label-efficient learning approaches that significantly
    reduce point cloud annotation efforts: data augmentation, domain transfer learning,
    weakly-supervised learning, and pretrained foundation models. For each approach,
    we outline the problem setup and provide an extensive literature review that showcases
    relevant progress and challenges. Finally, we share insights into current research
    challenges and potential future directions. A project associated with this survey
    has been built at [https://github.com/xiaoaoran/3D_label_efficient_learning](https://github.com/xiaoaoran/3D_label_efficient_learning).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，深度神经网络在点云学习方面取得了显著进展。然而，收集大规模精确标注的训练数据非常繁琐且昂贵，这阻碍了现有点云数据集的可扩展性，并且成为在各种任务和应用中高效探索点云数据的瓶颈。标签高效学习提供了一种有前景的解决方案，通过大幅减少标注工作量来实现有效的深度网络训练。本文首次全面综述了点云的标签高效学习。我们在这一新兴研究领域中探讨了三个关键问题：i）标签高效学习在点云处理中的重要性和紧迫性，ii）它涵盖的子领域，以及
    iii）在这一领域取得的进展。为此，我们提出了一种分类法，根据不同类型标签提供的数据前提条件来组织标签高效学习方法。我们将四种典型的标签高效学习方法进行了分类，这些方法显著减少了点云标注工作量：数据增强、领域迁移学习、弱监督学习和预训练基础模型。对于每种方法，我们概述了问题设置，并提供了详尽的文献综述，展示了相关进展和挑战。最后，我们分享了当前研究挑战和潜在未来方向的见解。与本综述相关的项目可以在
    [https://github.com/xiaoaoran/3D_label_efficient_learning](https://github.com/xiaoaoran/3D_label_efficient_learning)
    找到。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Point cloud, label-efficient learning, data augmentation, semi-supervised learning,
    weakly-supervised learning, few-shot learning, domain adaptation, domain generalization,
    self-supervised learning, foundation model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 点云，标签高效学习，数据增强，半监督学习，弱监督学习，少样本学习，领域适应，领域泛化，自监督学习，基础模型。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The acquisition of 3D point clouds has recently become more feasible and cost-effective
    with the wide adoption of various 3D devices, such as RGB-D cameras and LiDAR
    sensors. Meanwhile, remarkable advancements in deep learning have led to significant
    progress in point cloud understanding. The concurrence of the two has witnessed
    increasing demands in utilizing point clouds to capture 3D shape representations
    of objects and scenes, ranging from autonomous navigation and robotics to remote
    sensing applications and beyond.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着各种3D设备（如RGB-D摄像头和LiDAR传感器）的广泛应用，获取3D点云最近变得更加可行和经济。同时，深度学习的显著进步也推动了点云理解的显著发展。两者的结合导致了对利用点云捕捉对象和场景的3D形状表示的需求日益增加，涵盖了从自主导航和机器人技术到遥感应用等多个领域。
- en: Despite the great advancements in deep learning in point cloud understanding,
    most existing work relies heavily on large-scale well-annotated 3D data in network
    training. However, collecting such annotated training data is notoriously laborious
    and time-consuming due to the high complexity of the data, large variation in
    point sparsity, rich noises, and frequent 3D view changes in annotation process.
    Hence, how to learn effective point cloud models from training data of limited
    size and variation has become a grand challenge in point cloud understanding.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在点云理解方面取得了巨大进展，但大多数现有工作仍然依赖于大规模的良好标注3D数据进行网络训练。然而，由于数据的高复杂性、点稀疏性的巨大变化、丰富的噪声以及标注过程中的频繁3D视角变化，收集这样的标注训练数据是极其繁琐且耗时的。因此，如何从有限规模和变化的训练数据中学习有效的点云模型已经成为点云理解中的一个重大挑战。
- en: To address the heavy burden in point cloud annotation, a promising solution
    is label-efficient learning, a machine learning paradigm that prioritizes model
    training with minimal annotation while still achieving desired accuracy. Due to
    its importance and high practical values, label-efficient point cloud learning
    has recently emerged as a thriving research field with numerous studies for learning
    effective models from limited point annotations. Various approaches have been
    explored with different data requirements and application scenarios. To this end,
    a systematic survey is urgently needed to provide a comprehensive overview of
    this field, covering multiple learning approaches and setups over various tasks
    in an organized manner.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决点云标注中的巨大负担，一种有前景的解决方案是标签高效学习，这是一种机器学习范式，优先考虑在最小标注的情况下进行模型训练，同时仍然实现所需的准确性。由于其重要性和高实际价值，标签高效点云学习最近已成为一个蓬勃发展的研究领域，有大量研究致力于从有限的点标注中学习有效的模型。各种方法已经被探索，具有不同的数据要求和应用场景。为此，迫切需要一个系统的调查来提供这一领域的全面概述，涵盖多种学习方法和设置，系统化地覆盖各类任务。
- en: 'We thus present a comprehensive literature review of recent advancements in
    label-efficient learning of point clouds. Specifically, we review existing studies
    based on task and data prerequisites and categorize them into four distinct approaches:
    1) Data Augmentation, which augments limited labelled training data distribution
    via data augmentation; 2) Domain Transfer, which utilizes labelled data from source
    domain(s) to train robust models for unlabelled target domain(s); 3) Weakly-Supervised
    Learning, which trains robust models with weakly labelled point clouds; and 4)
    Pretrained Foundation Models, which leverages unsupervised or multi-modal pretraining
    to facilitate 3D modelling with less annotations. For each label-efficient learning
    approach, we introduce the problem setup and provide an exhaustive literature
    review, showcasing the progress made in this field and the challenges that remain.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们呈现了一项关于点云标签高效学习的最新进展的综合文献综述。具体而言，我们基于任务和数据要求回顾了现有研究，并将其分为四种不同的方法：1) 数据增强，通过数据增强扩展有限标记训练数据的分布；2)
    域迁移，利用源领域的标记数据训练对未标记目标领域的鲁棒模型；3) 弱监督学习，利用弱标记点云训练鲁棒模型；以及4) 预训练基础模型，利用无监督或多模态预训练来促进3D建模，减少标注数量。对于每种标签高效学习方法，我们介绍了问题设置并提供了详尽的文献综述，展示了该领域取得的进展以及仍然存在的挑战。
- en: '![Refer to caption](img/efc8bbe53b5d9c2815a81284407af132.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/efc8bbe53b5d9c2815a81284407af132.png)'
- en: 'Figure 1: Taxonomy of label-efficient learning of point clouds.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：点云标签高效学习的分类。
- en: To the best of our knowledge, this is the first systematic and comprehensive
    survey that focuses on label-efficient learning of point clouds, providing a detailed
    overview of the progress and challenges in this field. Several relevant surveys
    have been conducted. For example, Guo et al. [[1](#bib.bib1)] reviewed supervised
    deep learning of point clouds, and Xiao et al. [[2](#bib.bib2)] presented a systematic
    review on unsupervised representation learning of point clouds. In addition, several
    studies [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]
    survey label-efficient learning of other data modalities (e.g., 2D images, texts,
    and graphs) such as self-supervised learning [[4](#bib.bib4)], small sample learning [[5](#bib.bib5)],
    and generalizing across domains [[6](#bib.bib6), [7](#bib.bib7)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，这是首个系统化和全面的调查，专注于点云的标签高效学习，提供了该领域进展和挑战的详细概述。已有几项相关的调查。例如，Guo等人[[1](#bib.bib1)]回顾了点云的监督深度学习，而Xiao等人[[2](#bib.bib2)]对点云的无监督表示学习进行了系统的综述。此外，还有几项研究[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]调查了其他数据模态（如2D图像、文本和图表）的标签高效学习，如自监督学习[[4](#bib.bib4)]、小样本学习[[5](#bib.bib5)]和跨领域泛化[[6](#bib.bib6),
    [7](#bib.bib7)]。
- en: The rest of this survey is organized as follows. Section [2](#S2 "2 Background
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") introduces background
    knowledge including key concepts and a brief description of the efforts and difficulty
    of annotating 3D point-cloud data. Sections [3](#S3 "3 Data Augmentation ‣ A Survey
    of Label-Efficient Deep Learning for 3D Point Clouds"),[4](#S4 "4 Domain Transfer
    Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"),[5](#S5
    "5 Weakly-supervised learning ‣ A Survey of Label-Efficient Deep Learning for
    3D Point Clouds"),[6](#S6 "6 Pretrained Foundation Models ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") then provide systematic and extensive literature
    reviews of four representative data-efficient learning approaches, namely, point
    cloud data augmentation, knowledge transfer across domains, weakly supervised
    learning of point clouds, and pretrained foundation models for point cloud learning.
    Finally, we highlight several promising research directions for future label-efficient
    point cloud learning in Section [7](#S7 "7 Future direction ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"). Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") shows a taxonomy
    of existing label-efficient learning methods for 3D point clouds.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的其余部分组织如下。第[2](#S2 "2 Background ‣ A Survey of Label-Efficient Deep Learning
    for 3D Point Clouds")节介绍了背景知识，包括关键概念以及对3D点云数据标注的努力和难度的简要描述。第[3](#S3 "3 Data Augmentation
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")、[4](#S4 "4 Domain
    Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")、[5](#S5
    "5 Weakly-supervised learning ‣ A Survey of Label-Efficient Deep Learning for
    3D Point Clouds")、[6](#S6 "6 Pretrained Foundation Models ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds")节随后提供了四种代表性的高效学习方法的系统和广泛的文献综述，即点云数据增强、跨领域知识迁移、点云的弱监督学习，以及用于点云学习的预训练基础模型。最后，我们在第[7](#S7
    "7 Future direction ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")节中强调了未来标签高效点云学习的几个有前景的研究方向。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds")展示了现有的3D点云标签高效学习方法的分类。
- en: 2 Background
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 Key concepts
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 关键概念
- en: Point cloud. A point cloud is a collection of 3D points, represented by their
    spatial coordinates in x, y, and z. Depending on the type of point clouds, additional
    attributes may also be included, e.g., normal values for object-level point clouds [[8](#bib.bib8)],
    color information for indoor dense point clouds [[9](#bib.bib9)], or intensity
    value for LiDAR point clouds [[10](#bib.bib10)].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 点云。点云是一个由3D点组成的集合，通过它们在x、y和z方向上的空间坐标来表示。根据点云的类型，可能还会包含其他属性，例如，物体级点云的法线值[[8](#bib.bib8)]、室内密集点云的颜色信息[[9](#bib.bib9)]，或者LiDAR点云的强度值[[10](#bib.bib10)]。
- en: Supervised learning optimizes machine learning models under the full supervision
    of labels where models learn to map input data to output label space. The training
    data consists of pairs of input point clouds and corresponding labels, where the
    labels annotated by humans are exactly the ground truth of the models’ output.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习在标签的完全监督下优化机器学习模型，其中模型学习将输入数据映射到输出标签空间。训练数据由输入点云和对应标签的对组成，标签由人工注释，正是模型输出的真实值。
- en: Label-efficient learning focuses on developing methods that can learn from a
    limited amount of labeled data. The goal is to reduce the amount of labeled data
    in deep network training, as labelling data can be time-consuming and expensive.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 标签高效学习专注于开发能够从有限标记数据中学习的方法。其目标是减少深度网络训练中的标记数据量，因为标记数据可能耗时且昂贵。
- en: '3D shape classification aims to identify the category of an object point cloud,
    such as chairs, tables, cars, and buildings. Categorical labels are needed as
    ground truth for training 3D classification models. Accuracy, defined as the ratio
    of correctly classified objects to the total number of objects in the dataset,
    is widely adopted for evaluations. Two types of accuracy are commonly used: overall
    accuracy (OA), which measures the overall performance of the algorithm, and mean
    accuracy (mAcc), which provides a class-specific measure of accuracy. OA is calculated
    as the ratio of the total number of correctly classified objects to the total
    number of objects in the dataset regardless of the class, while mAcc is calculated
    as the ratio of correctly classified objects to the total number of objects for
    each class, and then averaged to give an overall measure of performance.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 形状分类的目标是识别对象点云的类别，如椅子、桌子、汽车和建筑物。分类标签作为地面真实数据用于训练 3D 分类模型。准确率定义为正确分类的对象数量与数据集中对象总数量的比率，被广泛用于评估。常用两种准确率：总体准确率（OA），衡量算法的整体表现，以及平均准确率（mAcc），提供特定类别的准确度。OA
    计算为正确分类对象数量与数据集中对象总数量的比率，不考虑类别，而 mAcc 计算为每个类别的正确分类对象数量与该类别对象总数量的比率，然后取平均值以提供整体性能的衡量。
- en: 3D object detection is the task of recognizing and localizing 3D objects in
    scene-level point clouds, aiming to estimate their precise positions and orientations.
    3D bounding boxes are annotated as ground truth for training 3D detectors. Average
    precision (AP) is a commonly used evaluation metric, calculated based on precision
    and recall for a given set of objects and confidence thresholds. The metric compares
    ground-truth bounding boxes with predicted ones, and is calculated as the area
    under the precision-recall curve. The precision is calculated as the ratio of
    the number of correctly predicted objects to the total number of predicted objects,
    while the recall is calculated as the ratio of the number of correctly predicted
    objects to the total number of ground-truth objects.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 目标检测的任务是识别和定位场景级点云中的3D对象，旨在估计其精确的位置和方向。3D 边界框作为训练 3D 检测器的地面真实数据进行标注。平均精度（AP）是常用的评估指标，基于给定对象和置信度阈值的精度和召回率计算。该指标将真实边界框与预测框进行比较，并计算精度-召回曲线下的面积。精度计算为正确预测的对象数量与总预测对象数量的比率，而召回率计算为正确预测的对象数量与总真实对象数量的比率。
- en: 'TABLE I: A summary of commonly used datasets for point cloud learning.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：点云学习中常用数据集的汇总。
- en: '| Dataset | Year | #Samples | #Classes | Type | Representation | Label |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | 样本数量 | 类别数量 | 类型 | 表示方式 | 标签 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| ModelNet40 [[11](#bib.bib11)] | 2015 | 12,311 objects | 40 | Synthetic object
    | Mesh | Object category label |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| ModelNet40 [[11](#bib.bib11)] | 2015 | 12,311 个对象 | 40 | 合成物体 | 网格 | 对象类别标签
    |'
- en: '| ShapeNet [[8](#bib.bib8)] | 2015 | 51,190 objects | 55 | Synthetic object
    | Mesh | Object/part category label |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| ShapeNet [[8](#bib.bib8)] | 2015 | 51,190 个对象 | 55 | 合成物体 | 网格 | 对象/部分类别标签
    |'
- en: '| ScanObjectNN [[12](#bib.bib12)] | 2019 | 2,902 objects | 15 | Real-world
    object | Points | Object category label |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| ScanObjectNN [[12](#bib.bib12)] | 2019 | 2,902 个对象 | 15 | 真实世界对象 | 点云 | 对象类别标签
    |'
- en: '| SUN RGB-D [[13](#bib.bib13)] | 2015 | 5K frames | 37 | Indoor scene | RGB-D
    | Bounding box |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| SUN RGB-D [[13](#bib.bib13)] | 2015 | 5K 帧 | 37 | 室内场景 | RGB-D | 边界框 |'
- en: '| S3DIS [[14](#bib.bib14)] | 2016 | 272 scans | 13 | Indoor scene | RGB-D |
    Point category label |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| S3DIS [[14](#bib.bib14)] | 2016 | 272 扫描 | 13 | 室内场景 | RGB-D | 点类别标签 |'
- en: '| ScanNet [[9](#bib.bib9)] | 2017 | 1,513 scans | 20 | Indoor scene | RGB-D
    & mesh | Point category label & Bounding box |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| ScanNet [[9](#bib.bib9)] | 2017 | 1,513 扫描 | 20 | 室内场景 | RGB-D & 网格 | 点类别标签
    & 边界框 |'
- en: '| KITTI [[15](#bib.bib15)] | 2013 | 15K frames | 8 | Outdoor driving | RGB
    & LiDAR | Bounding box |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| KITTI [[15](#bib.bib15)] | 2013 | 15K 帧 | 8 | 户外驾驶 | RGB & 激光雷达 | 边界框 |'
- en: '| nuScenes [[16](#bib.bib16)] | 2020 | 40K | 32 | Outdoor driving | RGB & LiDAR
    | Point category label & Bounding box |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| nuScenes [[16](#bib.bib16)] | 2020 | 40K | 32 | 户外驾驶 | RGB & 激光雷达 | 点类别标签
    & 边界框 |'
- en: '| Waymo [[17](#bib.bib17)] | 2020 | 200K | 23 | Outdoor driving | RGB & LiDAR
    | Point category label & Bounding box |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Waymo [[17](#bib.bib17)] | 2020 | 200K | 23 | 室外驾驶 | RGB & LiDAR | 点类别标签
    & 边界框 |'
- en: '| STF [[18](#bib.bib18)] | 2020 | 13.5K | 4 | Outdoor driving | RGB & LiDAR
    & Radar | Bounding box |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| STF [[18](#bib.bib18)] | 2020 | 13.5K | 4 | 室外驾驶 | RGB & LiDAR & 雷达 | 边界框
    |'
- en: '| ONCE [[19](#bib.bib19)] | 2021 | 1M scenes | 5 | Outdoor driving | RGB &
    LiDAR | Bounding box |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| ONCE [[19](#bib.bib19)] | 2021 | 1M 场景 | 5 | 室外驾驶 | RGB & LiDAR | 边界框 |'
- en: '| Semantic3D [[20](#bib.bib20)] | 2017 | 15 dense scenes | 8 | Outdoor TLS
    | Points | Point category label |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Semantic3D [[20](#bib.bib20)] | 2017 | 15 个密集场景 | 8 | 室外 TLS | 点 | 点类别标签
    |'
- en: '| SemanticKITTI [[10](#bib.bib10)] | 2019 | 43,552 scans | 28 | Outdoor driving
    | LiDAR | Point category label |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| SemanticKITTI [[10](#bib.bib10)] | 2019 | 43,552 个扫描 | 28 | 室外驾驶 | LiDAR
    | 点类别标签 |'
- en: '| SensatUrban [[21](#bib.bib21)] | 2020 | 1.2 $\mathrm{km}^{2}$ | 31 | UAV
    Photogrammetry | Points | Point category label |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| SensatUrban [[21](#bib.bib21)] | 2020 | 1.2 $\mathrm{km}^{2}$ | 31 | UAV
    摄影测量 | 点 | 点类别标签 |'
- en: '| SynLiDAR [[22](#bib.bib22)] | 2022 | 198,396 scans | 32 | Outdoor driving
    | Synthetic LiDAR | Point category label |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| SynLiDAR [[22](#bib.bib22)] | 2022 | 198,396 个扫描 | 32 | 室外驾驶 | 合成 LiDAR |
    点类别标签 |'
- en: '| SemanticSTF [[23](#bib.bib23)] | 2023 | 2,086 scans | 21 | Outdoor driving
    | RGB & LiDAR | Point category label |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| SemanticSTF [[23](#bib.bib23)] | 2023 | 2,086 个扫描 | 21 | 室外驾驶 | RGB & LiDAR
    | 点类别标签 |'
- en: 3D semantic segmentation is the task of assigning semantic labels to each point
    in a 3D point cloud. Point-wise categorical annotations are collected as ground
    truth for this task. IoU (Intersection over Union) and mean IoU (mIoU) are commonly
    used metrics for evaluations. IoU measures the overlap between the predicted and
    ground truth segmentations for a given class and is calculated as the ratio of
    the intersection to the union of the two sets. IoU is calculated for each class
    separately. mIoU is the mean of the IoU values across all classes and provides
    an overall measure of the segmentation model’s performance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 语义分割是将语义标签分配给 3D 点云中的每个点的任务。逐点类别标注作为此任务的真实标注。IoU（交并比）和均值 IoU（mIoU）是常用的评估指标。IoU
    测量给定类别的预测分割与真实分割之间的重叠，计算方法是交集与并集的比率。IoU 是对每个类别单独计算的。mIoU 是所有类别的 IoU 值的均值，提供了分割模型性能的总体衡量。
- en: 3D instance segmentation is a task that involves assigning a unique instance
    ID to each object in a point cloud, thereby separating objects belonging to the
    same category and enabling more accurate object recognition and tracking. Point-wise
    instance annotations are needed to train models for this task. The mean average
    precision (mAP) is a popular evaluation metric used in 3D instance segmentation,
    computed as the mean of the average precision (AP, as used in 3D object detection)
    values across all classes. To calculate mAP, the precision-recall curve is computed
    for each class, and the area under the curve (AUC) is calculated. The AP is then
    calculated as the mean of the precision values at a set of predefined recall levels.
    Finally, the mAP is obtained as the mean of the AP values for all classes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 实例分割是一项任务，涉及为点云中的每个对象分配一个唯一的实例 ID，从而分离属于同一类别的对象，并实现更准确的对象识别和跟踪。为此任务训练模型需要逐点实例标注。均值平均精度（mAP）是用于
    3D 实例分割的流行评估指标，计算方法是所有类别的平均精度（AP，类似于 3D 目标检测中使用的 AP）值的均值。计算 mAP 时，需要为每个类别计算精度-召回曲线，并计算曲线下面积（AUC）。然后，AP
    计算为在一组预定义召回水平下的精度值的均值。最后，mAP 通过所有类别的 AP 值的均值获得。
- en: Backbone. A ”backbone” is the essential and fundamental part of a neural network
    architecture that is responsible for extracting high-level features from input
    data. These features are then processed and analyzed by subsequent layers in the
    network. The backbone carries out most of the computation in a neural network
    and is crucial in determining its performance. To ensure fairness in comparing
    the performance of different label-efficient learning algorithms, it is important
    to use the same backbone implementation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Backbone. “Backbone” 是神经网络架构中基本且核心的部分，负责从输入数据中提取高级特征。这些特征随后由网络中的后续层进行处理和分析。Backbone
    执行神经网络中的大部分计算，并在确定网络性能方面至关重要。为了确保在比较不同标签高效学习算法的性能时的公平性，使用相同的 backbone 实现是重要的。
- en: 2.2 Annotation efforts for 3D datasets
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 3D 数据集的标注工作
- en: Annotating point clouds is challenging which usually requires special training
    due to the unique characteristics of point-cloud data. It faces several new challenges
    compared with annotating data of other modalities such as images. First, the display
    of point clouds is often unaligned with human perceptions. Point clouds are often
    incomplete, sparse, and may not contain color information, leading to rich ambiguity
    in point semantics and point geometries. Second, 3D view changes complicate the
    annotation process greatly which even cause motion sickness for annotators. Hence,
    point cloud annotators require good expertise and experience to ensure annotation
    accuracy and consistency while labelling, e.g., 3D bounding boxes and point-wise
    categories for 3D detection and segmentation tasks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注释点云是具有挑战性的，这通常需要特殊的培训，因为点云数据具有独特的特性。与图像等其他模态的数据注释相比，它面临着一些新的挑战。首先，点云的显示通常与人类的感知不对齐。点云通常是不完整的、稀疏的，可能不包含颜色信息，这导致点语义和点几何中存在丰富的模糊性。其次，3D视图的变化大大复杂化了注释过程，甚至可能导致注释员的晕动病。因此，点云注释员需要具备良好的专业知识和经验，以确保注释的准确性和一致性，例如，为3D检测和分割任务标记`3D
    bounding boxes`和逐点类别。
- en: Third, fully automatic point cloud annotation is still infeasible at the current
    stage. Although some tool such as semi-automatic labelling has been explored to
    streamline the process, the annotation accuracy remains low and plenty of manual
    efforts are required to inspect and correct the automatic annotations. Though
    different approaches have been proposed to simplify the manual annotation process,
    most of they do not generalize well with various extra requirements. For instance,
    Behley et al. [[10](#bib.bib10)] superimpose multiple LiDAR scans to formulate
    dense point representations, allowing labelling multiple scans concurrently and
    consistently while collecting SemanticKITTI. However, the superimposing process
    requires accurate and instant localization and pose of LiDAR sensors, and the
    superimposed moving objects are often distorted and indistinguishable. In summary,
    manual approach remains the primary way of point cloud annotation which requires
    vast time and efforts as well as well-trained annotators.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，目前阶段下完全自动化的点云注释仍然不可行。虽然一些工具如半自动标注已经被探索以简化过程，但注释的准确性仍然较低，需要大量人工工作来检查和修正自动注释。虽然已经提出了不同的方法来简化人工注释过程，但大多数方法在面对各种额外要求时不够通用。例如，Behley等人[[10](#bib.bib10)]将多个LiDAR扫描叠加，以形成密集的点表示，在收集SemanticKITTI时允许同时和一致地标注多个扫描。然而，叠加过程需要准确且即时的LiDAR传感器定位和姿态，而叠加的移动物体通常会被扭曲且难以区分。总之，人工方法仍然是点云注释的主要方式，这需要大量的时间和精力以及经过良好培训的注释员。
- en: The labour-intensive nature of point cloud annotation makes the construction
    of large-scale point-cloud datasets extremely time-consuming. This directly leads
    to limited sizes and diversity in public point-cloud datasets as shown in Table [I](#S2.T1
    "TABLE I ‣ 2.1 Key concepts ‣ 2 Background ‣ A Survey of Label-Efficient Deep
    Learning for 3D Point Clouds"), and poses a great challenge while developing generalizable
    point cloud learning algorithms. Studying label-efficient point cloud learning
    has become an urgent need to mitigate the limitation of existing point-cloud data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 点云注释的劳动密集型特性使得构建大规模点云数据集极其耗时。这直接导致公共点云数据集的规模和多样性有限，如表[I](#S2.T1 "TABLE I ‣ 2.1
    Key concepts ‣ 2 Background ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds")所示，并在开发可泛化的点云学习算法时带来了巨大挑战。因此，研究标签高效的点云学习已成为缓解现有点云数据限制的迫切需求。
- en: 3 Data Augmentation
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据增强
- en: Data augmentation (DA) has been widely adopted in deep network training [[24](#bib.bib24)].
    As illustrated in Fig. [2](#S3.F2 "Figure 2 ‣ 3 Data Augmentation ‣ A Survey of
    Label-Efficient Deep Learning for 3D Point Clouds"), it aims to increase the data
    size and diversity by artificially generating new training data from existing
    one. DA is particularly beneficial in scenarios where the available training data
    is limited. It is therefore considered an important label-efficient learning approach
    with widespread applications across various fields.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强（DA）已被广泛应用于深度网络训练[[24](#bib.bib24)]。如图[2](#S3.F2 "Figure 2 ‣ 3 Data Augmentation
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")所示，它旨在通过从现有数据中人工生成新训练数据来增加数据的大小和多样性。DA在可用训练数据有限的情况下尤其有益。因此，它被认为是一种重要的标签高效学习方法，在各个领域具有广泛的应用。
- en: '![Refer to caption](img/460f5f2f5a8b7e006f59f949c563931b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/460f5f2f5a8b7e006f59f949c563931b.png)'
- en: 'Figure 2: Data augmentation in 3D network training.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：3D网络训练中的数据增强。
- en: 'TABLE II: Categorization of data augmentation methods for point cloud learning.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：点云学习的数据增强方法分类。
- en: '| Augmentation type | Approach | References |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 增强类型 | 方法 | 参考文献 |'
- en: '| --- | --- | --- |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Inta-domain augmentation (§[3.1](#S3.SS1 "3.1 Intra-domain augmentation ‣
    3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"))
    | Conventional augmentation techniques (§[3.1.1](#S3.SS1.SSS1 "3.1.1 Conventional
    augmentation techniques ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) | [[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)] |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 内领域增强 (§[3.1](#S3.SS1 "3.1 内领域增强 ‣ 3 数据增强 ‣ 3D点云标签高效深度学习的调查")) | 常规增强技术 (§[3.1.1](#S3.SS1.SSS1
    "3.1.1 常规增强技术 ‣ 3.1 内领域增强 ‣ 3 数据增强 ‣ 3D点云标签高效深度学习的调查")) | [[25](#bib.bib25), [26](#bib.bib26),
    [27](#bib.bib27)] |'
- en: '| Augmentation methods for 3D shape classification. (§[3.1.2](#S3.SS1.SSS2
    "3.1.2 DA for 3D shape classification ‣ 3.1 Intra-domain augmentation ‣ 3 Data
    Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"))
    | [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)] |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 3D形状分类的增强方法 (§[3.1.2](#S3.SS1.SSS2 "3.1.2 3D形状分类的数据增强 ‣ 3.1 内领域增强 ‣ 3 数据增强
    ‣ 3D点云标签高效深度学习的调查")) | [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)] |'
- en: '| Augmentation methods for 3D object detection. (§[3.1.3](#S3.SS1.SSS3 "3.1.3
    DA for 3D object detection ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) | [[35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)]
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 3D目标检测的增强方法 (§[3.1.3](#S3.SS1.SSS3 "3.1.3 3D目标检测的数据增强 ‣ 3.1 内领域增强 ‣ 3 数据增强
    ‣ 3D点云标签高效深度学习的调查")) | [[35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)] |'
- en: '| Augmentation methods for 3D semantic segmentation. (§[3.1.4](#S3.SS1.SSS4
    "3.1.4 DA for 3D semantic segmentation ‣ 3.1 Intra-domain augmentation ‣ 3 Data
    Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"))
    | [[41](#bib.bib41), [42](#bib.bib42)] |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 3D语义分割的增强方法 (§[3.1.4](#S3.SS1.SSS4 "3.1.4 3D语义分割的数据增强 ‣ 3.1 内领域增强 ‣ 3 数据增强
    ‣ 3D点云标签高效深度学习的调查")) | [[41](#bib.bib41), [42](#bib.bib42)] |'
- en: '| Inter-domain augmentation (§[3.2](#S3.SS2 "3.2 Inter-domain augmentation
    ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point
    Clouds")) | Leverages additional data sources, such as synthetic data, cross-modal
    data, etc. | [[43](#bib.bib43), [22](#bib.bib22), [44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52)] |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 跨领域增强 (§[3.2](#S3.SS2 "3.2 跨领域增强 ‣ 3 数据增强 ‣ 3D点云标签高效深度学习的调查")) | 利用额外的数据源，如合成数据、跨模态数据等。
    | [[43](#bib.bib43), [22](#bib.bib22), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52)] |'
- en: 'This section reviews existing DA studies in point cloud network training, which
    can be broadly grouped into two categories: intra-domain augmentation and inter-domain
    augmentation. The former aims to enrich training data by generating new training
    data from the existing as detailed in Section [3.1](#S3.SS1 "3.1 Intra-domain
    augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning
    for 3D Point Clouds"). The latter leverages additional data to enlarge the existing
    training data distribution as detailed in Section [3.2](#S3.SS2 "3.2 Inter-domain
    augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning
    for 3D Point Clouds"). Table [II](#S3.T2 "TABLE II ‣ 3 Data Augmentation ‣ A Survey
    of Label-Efficient Deep Learning for 3D Point Clouds") shows an overview of existing
    DA studies.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了点云网络训练中现有的数据增强研究，这些研究大致可分为两类：内领域增强和跨领域增强。前者旨在通过生成新的训练数据来丰富训练数据，如第[3.1](#S3.SS1
    "3.1 内领域增强 ‣ 3 数据增强 ‣ 3D点云标签高效深度学习的调查")节所述。后者利用额外数据扩大现有训练数据分布，如第[3.2](#S3.SS2
    "3.2 跨领域增强 ‣ 3 数据增强 ‣ 3D点云标签高效深度学习的调查")节所述。表[II](#S3.T2 "表 II ‣ 3 数据增强 ‣ 3D点云标签高效深度学习的调查")展示了现有数据增强研究的概述。
- en: 3.1 Intra-domain augmentation
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 内领域增强
- en: Intra-domain DA aims to maximize the training knowledge by only utilizing the
    limited annotated training data available. Section [3.1.1](#S3.SS1.SSS1 "3.1.1
    Conventional augmentation techniques ‣ 3.1 Intra-domain augmentation ‣ 3 Data
    Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")
    first provides an introduction to conventional DA that is generic and applicable
    in various point cloud tasks. Subsequently, we review DA methods that are designed
    for specific 3D tasks, including 3D shape classification in Section [3.1.2](#S3.SS1.SSS2
    "3.1.2 DA for 3D shape classification ‣ 3.1 Intra-domain augmentation ‣ 3 Data
    Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"),
    3D object detection in Section [3.1.3](#S3.SS1.SSS3 "3.1.3 DA for 3D object detection
    ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"), and 3D semantic segmentation in Section [3.1.4](#S3.SS1.SSS4
    "3.1.4 DA for 3D semantic segmentation ‣ 3.1 Intra-domain augmentation ‣ 3 Data
    Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds").
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 域内 DA 旨在通过仅利用有限的标注训练数据来最大化训练知识。第 [3.1.1](#S3.SS1.SSS1 "3.1.1 传统数据增强技术 ‣ 3.1
    域内增强 ‣ 3 数据增强 ‣ 3D 点云标注高效深度学习综述") 节首先介绍了适用于各种点云任务的通用传统 DA。随后，我们回顾了针对特定 3D 任务设计的数据增强方法，包括第
    [3.1.2](#S3.SS1.SSS2 "3.1.2 3D 形状分类的 DA ‣ 3.1 域内增强 ‣ 3 数据增强 ‣ 3D 点云标注高效深度学习综述")
    节的 3D 形状分类，第 [3.1.3](#S3.SS1.SSS3 "3.1.3 3D 物体检测的 DA ‣ 3.1 域内增强 ‣ 3 数据增强 ‣ 3D
    点云标注高效深度学习综述") 节的 3D 物体检测，以及第 [3.1.4](#S3.SS1.SSS4 "3.1.4 3D 语义分割的 DA ‣ 3.1 域内增强
    ‣ 3 数据增强 ‣ 3D 点云标注高效深度学习综述") 节的 3D 语义分割。
- en: 3.1.1 Conventional augmentation techniques
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 传统数据增强技术
- en: Conventional DA has been extensively explored as a pre-processing operation
    in various 3D tasks [[25](#bib.bib25), [26](#bib.bib26), [53](#bib.bib53), [54](#bib.bib54),
    [55](#bib.bib55), [56](#bib.bib56)]. It adopts different spatial transformations
    to generate diverse views of point clouds that are crucial for learning transformation-invariant
    and generalizable representations. Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional
    augmentation techniques ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") shows a list
    of typical conventional DA techniques together with qualitative illustrations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 传统 DA 已在各种 3D 任务中广泛探索作为预处理操作 [[25](#bib.bib25), [26](#bib.bib26), [53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56)]。它采用不同的空间变换来生成点云的多样视图，这对学习变换不变和具有广泛泛化能力的表示至关重要。图
    [3](#S3.F3 "图 3 ‣ 3.1.1 传统数据增强技术 ‣ 3.1 域内增强 ‣ 3 数据增强 ‣ 3D 点云标注高效深度学习综述") 显示了一些典型的传统
    DA 技术及其定性示意图。
- en: '![Refer to caption](img/d11669fe842b9663028df8ee275469ff.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d11669fe842b9663028df8ee275469ff.png)'
- en: 'Figure 3: Illustration of widely-used conventional augmentation techniques
    for point clouds.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：点云常用传统数据增强技术的示意图。
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Scaling changes the scale of the point cloud by multiplying the coordinates
    with a ratio $s$, where a value of $s<1$ indicates shrinkage and $s>1$ indicates
    enlargement as illustrated in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional augmentation
    techniques ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation ‣ A Survey of
    Label-Efficient Deep Learning for 3D Point Clouds") (b).
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缩放通过将点云的坐标乘以比例 $s$ 来改变点云的尺度，其中 $s<1$ 表示缩小，$s>1$ 表示放大，如图 [3](#S3.F3 "图 3 ‣ 3.1.1
    传统数据增强技术 ‣ 3.1 域内增强 ‣ 3 数据增强 ‣ 3D 点云标注高效深度学习综述") (b) 所示。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Flipping randomly flips points along the x-axis or y-axis, as illustrated in
    Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional augmentation techniques ‣ 3.1 Intra-domain
    augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning
    for 3D Point Clouds") (c).
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 翻转随机沿 x 轴或 y 轴翻转点，如图 [3](#S3.F3 "图 3 ‣ 3.1.1 传统数据增强技术 ‣ 3.1 域内增强 ‣ 3 数据增强 ‣
    3D 点云标注高效深度学习综述") (c) 所示。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Rotation rotates the points around the z-axis with a random angle, as illustrated
    in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional augmentation techniques ‣ 3.1
    Intra-domain augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") (d).
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 旋转绕 z 轴以随机角度旋转点，如图 [3](#S3.F3 "图 3 ‣ 3.1.1 传统数据增强技术 ‣ 3.1 域内增强 ‣ 3 数据增强 ‣ 3D
    点云标注高效深度学习综述") (d) 所示。
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Jittering adds random perturbations to point clouds with Gaussian noise with
    zero mean and a standard deviation of $\beta$[[25](#bib.bib25)], as illustrated
    in Fig.[3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional augmentation techniques ‣ 3.1
    Intra-domain augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") (e).
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 抖动向点云添加了具有零均值和标准差为$\beta$的高斯噪声的随机扰动，如图[3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional
    augmentation techniques ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") (e)所示。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Translation involves shifting all points in the same direction and distance,
    as shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional augmentation techniques
    ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") (f).
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变换涉及将所有点在相同方向和距离上移动，如图[3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional augmentation
    techniques ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation ‣ A Survey of
    Label-Efficient Deep Learning for 3D Point Clouds") (f)所示。
- en: Note conventional DA can be applied to both global point clouds and local point
    patches [[27](#bib.bib27), [57](#bib.bib57), [58](#bib.bib58)].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，传统的DA可以应用于全球点云和局部点 patch [[27](#bib.bib27), [57](#bib.bib57), [58](#bib.bib58)]。
- en: Conventional DA has been widely adopted in various point cloud learning tasks
    due to its simplicity and efficiency. However, it often leads to insufficient
    training due to two major factors. First, the DA process and network training
    are independent with little interaction, where the training outcome provides little
    feedback for DA optimization. Second, the new training samples are augmented from
    individual instead of a combination of multiple existing samples, leading to limited
    training data distribution. Many DA strategies have been designed to address the
    two limitations, which will be reviewed according to point cloud tasks in the
    ensuing subsections.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其简单性和高效性，传统的DA已被广泛应用于各种点云学习任务。然而，它通常由于两个主要因素导致训练不足。首先，DA过程和网络训练是独立的，互动较少，训练结果对DA优化反馈有限。其次，新的训练样本是从个体中增强的，而不是从多个现有样本的组合中增强的，导致训练数据分布有限。许多DA策略已被设计来解决这两个限制，这些策略将根据点云任务在接下来的小节中进行回顾。
- en: 3.1.2 DA for 3D shape classification
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 3D形状分类的DA
- en: Several studies [[28](#bib.bib28), [29](#bib.bib29)] explored adaptive DA for
    3D shape classification. For example, Li et al. [[28](#bib.bib28)] designed Pointaugment
    that generates training samples with shape-wise transformation and point-wise
    displacement. The Pointaugment and object classifier are jointly optimized via
    adversarial learning. Kim et al. [[29](#bib.bib29)] exploited local deformations
    of objects, aiming to generate realistic object samples with more variation, e.g.,
    a person of varying poses. It introduces AugTune which allows adaptively controlling
    the strength of local augmentation while preserving the shape identity.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究[[28](#bib.bib28), [29](#bib.bib29)]探讨了用于3D形状分类的自适应DA。例如，Li等人[[28](#bib.bib28)]设计了Pointaugment，该方法通过形状变换和点位移生成训练样本。Pointaugment和对象分类器通过对抗学习进行联合优化。Kim等人[[29](#bib.bib29)]利用对象的局部变形，旨在生成更多变化的真实对象样本，例如不同姿势的人。它引入了AugTune，允许在保持形状身份的同时自适应地控制局部增强的强度。
- en: '![Refer to caption](img/b85579564b880002a845052b8bed837c.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/b85579564b880002a845052b8bed837c.png)'
- en: 'Figure 4: Illustration of typical mixing DA methods in point cloud classification,
    including PointMixup [[30](#bib.bib30)], RSMix [[31](#bib.bib31)], and SageMix [[34](#bib.bib34)].
    The figure is extracted from [[34](#bib.bib34)].'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：点云分类中的典型混合DA方法的说明，包括PointMixup [[30](#bib.bib30)], RSMix [[31](#bib.bib31)],
    和SageMix [[34](#bib.bib34)]。该图摘自[[34](#bib.bib34)]。
- en: Another line of research generates more diverse training objects by mixing existing
    ones. Inspired by MixUp [[59](#bib.bib59), [60](#bib.bib60)] in 2D image classification,
    Chen et al. [[30](#bib.bib30)] proposed PointMixup that generates object samples
    via shortest path linear interpolation between two objects of different classes.
    However, the interpolated samples may lose structural information of the original
    objects due to geometrical distortion. Several studies attempt to preserve the
    local object structures during mixing as illustrated in Fig. [4](#S3.F4 "Figure
    4 ‣ 3.1.2 DA for 3D shape classification ‣ 3.1 Intra-domain augmentation ‣ 3 Data
    Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds").
    For example, RSMix [[31](#bib.bib31)] mixes and generates new training samples
    by extracting rigid subsets from different point cloud objects. PointCutMix [[32](#bib.bib32)]
    cuts and replaces local object parts with the optimally assigned pair from other
    objects. SageMix [[34](#bib.bib34)] leverages saliency guidance to preserve local
    object structures in mixing. Point-MixSwap [[33](#bib.bib33)] mixes objects of
    the same categories to enrich the geometric variation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类研究通过混合现有对象来生成更多样化的训练对象。受到 2D 图像分类中 MixUp [[59](#bib.bib59), [60](#bib.bib60)]
    的启发，Chen 等人 [[30](#bib.bib30)] 提出了 PointMixup，通过对两个不同类别的对象进行最短路径线性插值来生成对象样本。然而，由于几何扭曲，插值样本可能会丧失原始对象的结构信息。一些研究尝试在混合过程中保留局部对象结构，如图
    [4](#S3.F4 "Figure 4 ‣ 3.1.2 DA for 3D shape classification ‣ 3.1 Intra-domain
    augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning
    for 3D Point Clouds") 所示。例如，RSMix [[31](#bib.bib31)] 通过从不同点云对象中提取刚性子集来混合和生成新的训练样本。PointCutMix
    [[32](#bib.bib32)] 切割并用其他对象中的最佳配对替换局部对象部分。SageMix [[34](#bib.bib34)] 利用显著性引导来保留混合中的局部对象结构。Point-MixSwap
    [[33](#bib.bib33)] 混合相同类别的对象以丰富几何变化。
- en: 3.1.3 DA for 3D object detection
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 3D 对象检测的 DA
- en: 3D object detection works with scene-level point clouds that are very different
    from object-level point clouds. Specifically, scene-level point clouds have much
    more points, more diverse surroundings, larger density variation, more noises
    or outliers, which pose both chances and challenges for DA. For example, Cheng
    et al. [[35](#bib.bib35)] proposed Progressive Population-Based Augmentation that
    searches for optimal DA strategies across point cloud datasets. Chen et al. [[36](#bib.bib36)]
    proposed Azimuth-Normalization to address the significant variation of LiDAR point
    clouds along the azimuth direction. Leng et al. [[37](#bib.bib37)] exploited pseudo
    labels of unlabelled data for DA in point cloud learning.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 对象检测处理的场景级点云与对象级点云有很大不同。具体来说，场景级点云有更多的点、更复杂的环境、更大的密度变化、更多的噪声或离群点，这些都为 DA
    带来了机遇和挑战。例如，Cheng 等人 [[35](#bib.bib35)] 提出了渐进式基于人群的增强方法，在点云数据集中寻找最佳 DA 策略。Chen
    等人 [[36](#bib.bib36)] 提出了方位角归一化，以解决 LiDAR 点云在方位角方向上的显著变化。Leng 等人 [[37](#bib.bib37)]
    利用未标记数据的伪标签进行点云学习中的 DA。
- en: The mixing idea has also been explored for 3D object detection. For instance,
    Yan et al.[[38](#bib.bib38)] proposed GT-Aug to enrich foreground instances by
    copying objects from other LiDAR frames and randomly pasting them into the current
    frame. However, GT-Aug does not consider the relationships between objects in
    real-world scenarios during the pasting. To address this limitation, Sun et al.[[39](#bib.bib39)]
    performed object pasting by utilizing a correlation energy field to represent
    the functional relationship between objects. In addition, Wu et al. [[40](#bib.bib40)]
    fused multiple LiDAR frames to generate denser point clouds and then use them
    as references to enhance object detection in single-frame scenarios.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 混合思想也被应用于 3D 对象检测。例如，Yan 等人 [[38](#bib.bib38)] 提出了 GT-Aug，通过从其他 LiDAR 帧中复制对象并随机粘贴到当前帧中来丰富前景实例。然而，GT-Aug
    在粘贴过程中没有考虑现实世界场景中对象之间的关系。为了解决这个限制，Sun 等人 [[39](#bib.bib39)] 通过利用相关能量场来表示对象之间的功能关系进行了对象粘贴。此外，Wu
    等人 [[40](#bib.bib40)] 融合了多个 LiDAR 帧以生成更密集的点云，然后将其作为参考来增强单帧场景中的对象检测。
- en: 3.1.4 DA for 3D semantic segmentation
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 3D 语义分割的 DA
- en: Mixing-based DA has shown impressive performance gains in point cloud segmentation.
    For example, Nekrasov et al.[[41](#bib.bib41)] proposed Mix3D that directly concatenates
    two point clouds and their labels for out-of-context augmentation. Xiao et al.[[42](#bib.bib42)]
    proposed PolarMix that mixes LiDAR frames in the polar coordinate system to preserve
    unique properties of LiDAR point clouds such as partial visibility and density
    variation. They designed scene-level swapping and instance-level rotate-pasting
    that achieve consistent augmentation effects across multiple LiDAR segmentation
    and detection benchmarks. Fig. [5](#S3.F5 "Figure 5 ‣ 3.2 Inter-domain augmentation
    ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point
    Clouds") shows qualitative illustrations of the two mixing-based DA methods.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于混合的DA在点云分割中展示了令人印象深刻的性能提升。例如，Nekrasov等人[[41](#bib.bib41)]提出了Mix3D，直接将两个点云及其标签连接起来进行上下文外增强。Xiao等人[[42](#bib.bib42)]提出了PolarMix，通过在极坐标系统中混合LiDAR帧来保持LiDAR点云的独特属性，如部分可见性和密度变化。他们设计了场景级别的交换和实例级别的旋转粘贴，在多个LiDAR分割和检测基准上实现了一致的增强效果。图[5](#S3.F5
    "Figure 5 ‣ 3.2 Inter-domain augmentation ‣ 3 Data Augmentation ‣ A Survey of
    Label-Efficient Deep Learning for 3D Point Clouds")展示了这两种基于混合的DA方法的定性插图。
- en: 3.2 Inter-domain augmentation
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 跨领域增强
- en: 'Inter-domain DA utilizes extra data to enhance network training. It can be
    broadly grouped into two categories depending on the types of data used: synthetic
    data and cross-modality data.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 跨领域DA利用额外的数据来增强网络训练。根据使用的数据类型，它可以大致分为两类：合成数据和跨模态数据。
- en: Synthetic data. Several studies explored synthetic point clouds to augment real
    ones to improve point cloud network training [[43](#bib.bib43), [22](#bib.bib22)].
    For example, Fang et al.[[43](#bib.bib43)] designed LiDAR-Aug that inserts CAD
    objects such as pedestrians into point clouds of road scenes for generating training
    LiDAR scans with richer objects and training better 3D detectors. Xiao et al.[[22](#bib.bib22)]
    collected self-annotated LiDAR point clouds from game engines and combined them
    with real point clouds to train 3D segmentation networks. Though synthetic data
    provide a promising solution to mitigate the data constraint, they have clear
    domain gap [[22](#bib.bib22)] with real point clouds which often limits their
    effectiveness.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据。一些研究探索了合成点云以增强真实点云，从而改善点云网络训练[[43](#bib.bib43), [22](#bib.bib22)]。例如，Fang等人[[43](#bib.bib43)]设计了LiDAR-Aug，将CAD对象如行人插入到道路场景的点云中，以生成带有更多对象的训练LiDAR扫描，从而训练更好的3D检测器。Xiao等人[[22](#bib.bib22)]从游戏引擎中收集了自我标注的LiDAR点云，并将其与真实点云结合起来，以训练3D分割网络。尽管合成数据提供了减轻数据限制的有希望的解决方案，但它们与真实点云存在明显的领域差距[[22](#bib.bib22)]，这往往限制了它们的有效性。
- en: '![Refer to caption](img/c4dea30d077d30c850f0eee3c6c8209d.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/c4dea30d077d30c850f0eee3c6c8209d.png)'
- en: 'Figure 5: Mixing-based DA on point cloud semantic segmentation: Mix3D [[41](#bib.bib41)]
    performs out-of-context mixing, while PolarMix [[42](#bib.bib42)] applies in-context
    mixing. The two graphs are extracted from [[41](#bib.bib41)] and [[42](#bib.bib42)].'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：点云语义分割中的基于混合的DA：Mix3D[[41](#bib.bib41)]进行上下文外混合，而PolarMix[[42](#bib.bib42)]应用上下文内混合。这两幅图从[[41](#bib.bib41)]和[[42](#bib.bib42)]中提取。
- en: Cross-modality data. Several studies fuse point clouds with data of other modalities
    for alleviating the inherent limitations of 3D sensors. For example, RGB images
    are widely adopted to improve network training for 3D object detection [[44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)]
    and 3D semantic segmentation [[50](#bib.bib50)]. Recently, several studies [[51](#bib.bib51),
    [52](#bib.bib52)] fused radar point clouds and LiDAR point clouds for learning
    more robust and generalizable point cloud models.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 跨模态数据。一些研究将点云与其他模态的数据融合，以减轻3D传感器的固有局限性。例如，RGB图像被广泛应用于提高3D物体检测[[44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)]和3D语义分割[[50](#bib.bib50)]的网络训练。最近，一些研究[[51](#bib.bib51),
    [52](#bib.bib52)]将雷达点云与LiDAR点云融合，以学习更强健和更具普遍性的点云模型。
- en: 3.3 Summary
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 总结
- en: Supervised learning of point clouds has achieved great success though most efforts
    focus on collecting large-scale datasets or developing novel network architectures [[1](#bib.bib1)].
    Recent studies have shown that DA can reduce data collection and annotation endeavours
    and achieve comparable performance as by new network architectures, indicating
    the great potential of this research direction. However, DA for point cloud learning
    is still far under-explored especially compared with 2D image processing [[24](#bib.bib24)]
    and natural language processing (NLP) [[61](#bib.bib61)], and more efforts are
    needed to advance this meaningful research field.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管监督学习点云已取得了巨大成功，但大多数努力集中在收集大规模数据集或开发新型网络架构上[[1](#bib.bib1)]。最近的研究表明，DA（领域自适应）可以减少数据收集和注释的工作量，并且能达到与新网络架构相媲美的性能，显示了这一研究方向的巨大潜力。然而，与
    2D 图像处理[[24](#bib.bib24)]和自然语言处理（NLP）[[61](#bib.bib61)]相比，点云学习的 DA 仍然远未被充分探索，需要更多的努力来推动这一有意义的研究领域。
- en: 'TABLE III: Categorization of domain transfer learning methods for point clouds.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：点云领域迁移学习方法的分类。
- en: '| Fields | Illustrations | Tasks | References |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 说明 | 任务 | 参考文献 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Domain adaptation (§[4.1](#S4.SS1 "4.1 Domain adaptation ‣ 4 Domain Transfer
    Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) |
    Adapting a machine learning model trained on one domain to perform well on another
    specific domain by minimizing the distribution shift between the domains. | Unsupervised
    domain adaptive 3D shape classification (§[4.1.2](#S4.SS1.SSS2 "4.1.2 Domain adaptation
    for 3D shape classification ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) | [[62](#bib.bib62),
    [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68)] |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 领域自适应 (§[4.1](#S4.SS1 "4.1 领域自适应 ‣ 4 领域迁移学习 ‣ 标签高效的 3D 点云深度学习综述")) | 通过最小化领域之间的分布偏移，将在一个领域上训练的机器学习模型适应到另一个特定领域。
    | 无监督领域自适应 3D 形状分类 (§[4.1.2](#S4.SS1.SSS2 "4.1.2 3D 形状分类的领域自适应 ‣ 4.1 领域自适应 ‣ 4
    领域迁移学习 ‣ 标签高效的 3D 点云深度学习综述")) | [[62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68)] |'
- en: '| Unsupervised domain adaptive 3D Object detection (§[4.1.3](#S4.SS1.SSS3 "4.1.3
    Domain adaptation for 3D object detection ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer
    Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) |
    [[69](#bib.bib69), [70](#bib.bib70), [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73),
    [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)] |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 无监督领域自适应 3D 物体检测 (§[4.1.3](#S4.SS1.SSS3 "4.1.3 3D 物体检测的领域自适应 ‣ 4.1 领域自适应
    ‣ 4 领域迁移学习 ‣ 标签高效的 3D 点云深度学习综述")) | [[69](#bib.bib69), [70](#bib.bib70), [71](#bib.bib71),
    [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)]
    |'
- en: '| Unsupervised domain adaptive 3D Semantic segmentation (§[4.1.4](#S4.SS1.SSS4
    "4.1.4 Domain adaptation for 3D semantic segmentation ‣ 4.1 Domain adaptation
    ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds")) | [[82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85),
    [86](#bib.bib86), [22](#bib.bib22), [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89),
    [90](#bib.bib90), [91](#bib.bib91), [92](#bib.bib92)] |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 无监督领域自适应 3D 语义分割 (§[4.1.4](#S4.SS1.SSS4 "4.1.4 3D 语义分割的领域自适应 ‣ 4.1 领域自适应
    ‣ 4 领域迁移学习 ‣ 标签高效的 3D 点云深度学习综述")) | [[82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84),
    [85](#bib.bib85), [86](#bib.bib86), [22](#bib.bib22), [87](#bib.bib87), [88](#bib.bib88),
    [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91), [92](#bib.bib92)] |'
- en: '| Other types of 3D domain adaptation (§[4.1.5](#S4.SS1.SSS5 "4.1.5 Extension
    ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds")) | [[88](#bib.bib88), [93](#bib.bib93)] |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 其他类型的 3D 领域自适应 (§[4.1.5](#S4.SS1.SSS5 "4.1.5 扩展 ‣ 4.1 领域自适应 ‣ 4 领域迁移学习 ‣
    标签高效的 3D 点云深度学习综述")) | [[88](#bib.bib88), [93](#bib.bib93)] |'
- en: '| Domain generalization (§[4.2](#S4.SS2 "4.2 Domain generalization ‣ 4 Domain
    Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"))
    | Building a machine learning model that can perform well on new, previously unseen
    domain(s) by learning invariant features that are common across different domain(s).
    | Domain generalized 3D shape classification (§[4.2.2](#S4.SS2.SSS2 "4.2.2 Domain
    generalization for 3D shape classification ‣ 4.2 Domain generalization ‣ 4 Domain
    Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"))
    | [[94](#bib.bib94), [95](#bib.bib95)] |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 域泛化 (§[4.2](#S4.SS2 "4.2 域泛化 ‣ 4 域迁移学习 ‣ 3D 点云标签高效深度学习综述")) | 构建一种机器学习模型，通过学习在不同域间普遍存在的不变特征，使其在新的、先前未见过的域上表现良好。
    | 域泛化的3D形状分类 (§[4.2.2](#S4.SS2.SSS2 "4.2.2 域泛化在3D形状分类中的应用 ‣ 4.2 域泛化 ‣ 4 域迁移学习
    ‣ 3D 点云标签高效深度学习综述")) | [[94](#bib.bib94), [95](#bib.bib95)] |'
- en: '| Domain generalized 3D Object detection (§[4.2.3](#S4.SS2.SSS3 "4.2.3 Domain
    generalization for 3D object detection ‣ 4.2 Domain generalization ‣ 4 Domain
    Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"))
    | [[96](#bib.bib96), [97](#bib.bib97)] |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 域泛化的3D对象检测 (§[4.2.3](#S4.SS2.SSS3 "4.2.3 域泛化在3D对象检测中的应用 ‣ 4.2 域泛化 ‣ 4 域迁移学习
    ‣ 3D 点云标签高效深度学习综述")) | [[96](#bib.bib96), [97](#bib.bib97)] |'
- en: '| Domain generalized 3D Semantic segmentation (§[4.2.4](#S4.SS2.SSS4 "4.2.4
    Domain generalization for 3D semantic segmentation ‣ 4.2 Domain generalization
    ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds")) | [[23](#bib.bib23), [98](#bib.bib98), [99](#bib.bib99)] |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 域泛化的3D语义分割 (§[4.2.4](#S4.SS2.SSS4 "4.2.4 域泛化在3D语义分割中的应用 ‣ 4.2 域泛化 ‣ 4 域迁移学习
    ‣ 3D 点云标签高效深度学习综述")) | [[23](#bib.bib23), [98](#bib.bib98), [99](#bib.bib99)]
    |'
- en: 4 Domain Transfer Learning
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 域迁移学习
- en: 'Domain transfer learning aims to exploit knowledge in previously collected
    and annotated data for handling various new data, hence reducing the labelling
    efforts of the new data significantly. However, transferring knowledge across
    data of different domains often faces domain discrepancy [[100](#bib.bib100),
    [101](#bib.bib101)], the distributional bias/shift across data of different domains.
    Consequently, models trained with source-domain data often experience clear performance
    drops when tested on data of target domains. The domain discrepancy problem has
    greatly hindered the deployment of point cloud models in various tasks. It has
    been studied in two typical approaches: domain adaptation and domain generalization.
    While both approaches aim to learn robust models from source data that can perform
    well on target data, domain adaptation permits access to target data in training
    while domain generalization does not. Table [III](#S3.T3 "TABLE III ‣ 3.3 Summary
    ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point
    Clouds") shows an overview of existing domain transfer learning studies.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 域迁移学习旨在利用之前收集和标注的数据中的知识来处理各种新的数据，从而显著减少新数据的标注工作。然而，将知识迁移到不同域的数据上往往会面临域间差异[[100](#bib.bib100),
    [101](#bib.bib101)]，即不同域数据间的分布偏差/偏移。因此，使用源域数据训练的模型在目标域数据上的测试时通常会出现明显的性能下降。域间差异问题极大地阻碍了点云模型在各种任务中的部署。这个问题通常通过两种典型方法进行研究：域适应和域泛化。虽然这两种方法都旨在从源数据中学习到在目标数据上表现良好的鲁棒模型，但域适应允许在训练中访问目标数据，而域泛化则不允许。表[III](#S3.T3
    "表 III ‣ 3.3 总结 ‣ 3 数据增强 ‣ 3D 点云标签高效深度学习综述") 展示了现有的域迁移学习研究概览。
- en: 4.1 Domain adaptation
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 域适应
- en: Domain adaptation aims to adapt a model trained on a source domain to a specific
    target domain. It provides an economical solution for utilizing existing annotated
    training data with the same label space for fine-tuning models from a source domain
    to a target domain. For point clouds, domain adaptation studies have different
    setups depending on data prerequisites and application scenarios. Specifically,
    most existing studies focus on unsupervised domain adaptation (UDA) that learns
    from labeled source point clouds and unlabeled target point clouds. This section
    presents the problem setup of UDA in subsection [4.1.1](#S4.SS1.SSS1 "4.1.1 Problem
    setup ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"), UDA for 3D shape classification in subsection [4.1.2](#S4.SS1.SSS2
    "4.1.2 Domain adaptation for 3D shape classification ‣ 4.1 Domain adaptation ‣
    4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds"), UDA for 3D object detection in Section [4.1.3](#S4.SS1.SSS3 "4.1.3
    Domain adaptation for 3D object detection ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer
    Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"), UDA
    for 3D semantic segmentation in Section [4.1.4](#S4.SS1.SSS4 "4.1.4 Domain adaptation
    for 3D semantic segmentation ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"), and other types
    of domain adaptation for point clouds in subsection [4.1.5](#S4.SS1.SSS5 "4.1.5
    Extension ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds").
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适配旨在将一个在源领域训练的模型适配到特定的目标领域。它提供了一种经济的解决方案，可以利用现有的带注释的训练数据，通过从源领域到目标领域的微调模型来实现。对于点云，领域适配的研究有不同的设置，取决于数据要求和应用场景。具体来说，大多数现有研究集中在无监督领域适配（UDA）上，该方法从带标签的源点云和未标记的目标点云中进行学习。本节介绍了UDA在子节[4.1.1](#S4.SS1.SSS1
    "4.1.1 问题设置 ‣ 4.1 领域适配 ‣ 4 领域迁移学习 ‣ 一项关于3D点云标签高效深度学习的调查")中的问题设置，子节[4.1.2](#S4.SS1.SSS2
    "4.1.2 3D形状分类的领域适配 ‣ 4.1 领域适配 ‣ 4 领域迁移学习 ‣ 一项关于3D点云标签高效深度学习的调查")中的3D形状分类的UDA，章节[4.1.3](#S4.SS1.SSS3
    "4.1.3 3D目标检测的领域适配 ‣ 4.1 领域适配 ‣ 4 领域迁移学习 ‣ 一项关于3D点云标签高效深度学习的调查")中的3D目标检测的UDA，章节[4.1.4](#S4.SS1.SSS4
    "4.1.4 3D语义分割的领域适配 ‣ 4.1 领域适配 ‣ 4 领域迁移学习 ‣ 一项关于3D点云标签高效深度学习的调查")中的3D语义分割的UDA，以及子节[4.1.5](#S4.SS1.SSS5
    "4.1.5 扩展 ‣ 4.1 领域适配 ‣ 4 领域迁移学习 ‣ 一项关于3D点云标签高效深度学习的调查")中的其他类型的点云领域适配。
- en: 4.1.1 Problem setup
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 问题设置
- en: 'Given source-domain point clouds $X^{S}$ with the corresponding labels $Y^{S}$
    and target-domain point clouds $X^{T}$ without labels, the goal of point cloud
    adaptation is to learn a model $F$ that can produce accurate predictions $\hat{Y}^{T}$
    for unseen target data. The network training in UDA consists of two typical learning
    tasks, i.e., supervised learning from the labelled source data and unsupervised
    adaptation toward unlabelled target data, as shown in Fig. [6](#S4.F6 "Figure
    6 ‣ 4.1.1 Problem setup ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning ‣
    A Survey of Label-Efficient Deep Learning for 3D Point Clouds"). Adaptation is
    usually achieved via four learning approaches: adversarial training, self-training,
    self-supervised learning, and style transfer.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 给定源领域点云$X^{S}$及其对应标签$Y^{S}$，以及没有标签的目标领域点云$X^{T}$，点云适配的目标是学习一个模型$F$，以便对未见的目标数据生成准确的预测$\hat{Y}^{T}$。UDA中的网络训练包括两个典型的学习任务，即从标记的源数据进行监督学习和对未标记的目标数据进行无监督适配，如图[6](#S4.F6
    "图 6 ‣ 4.1.1 问题设置 ‣ 4.1 领域适配 ‣ 4 领域迁移学习 ‣ 一项关于3D点云标签高效深度学习的调查")所示。适配通常通过四种学习方法实现：对抗训练、自训练、自监督学习和风格迁移。
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversarial training [[102](#bib.bib102), [103](#bib.bib103)] aims to learn
    domain-invariant features. It is achieved by training the model to extract features
    (from source and target samples) that are indistinguishable by a domain discriminator.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗训练[[102](#bib.bib102), [103](#bib.bib103)]旨在学习领域不变特征。通过训练模型提取源样本和目标样本中的特征，这些特征无法被领域判别器区分来实现这一目标。
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Self-training [[104](#bib.bib104), [105](#bib.bib105)] employs a source-trained
    model to pseudo-label target data and adopts confident target predictions to retrain
    the model iteratively. It assumes that the confident target predictions have correct
    labels.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自训练[[104](#bib.bib104), [105](#bib.bib105)]利用源训练模型为目标数据生成伪标签，并采用可信的目标预测来迭代地重新训练模型。它假设可信的目标预测具有正确的标签。
- en: •
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Self-supervised learning (SSL) [[2](#bib.bib2)] aims to learn useful representations
    from unlabelled target data without any explicit supervision. It is domain-agnostic
    and exploits the inherent data structure or patterns to define a task that can
    be solved without human annotations. With SSL over target data, the network can
    learn features that are tolerant to domain shifts, hence improving the model generalization
    on target data.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自监督学习（SSL）[[2](#bib.bib2)]旨在从未标记的目标数据中学习有用的表征，而无需任何明确的监督。它与领域无关，利用固有的数据结构或模式来定义一个可以在没有人工标注的情况下解决的任务。通过在目标数据上进行SSL，网络可以学习对领域变化具有容忍度的特征，从而提高模型在目标数据上的泛化能力。
- en: •
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Style transfer [[106](#bib.bib106), [22](#bib.bib22)] aims to translate source
    data to be similar to the target data for training. It works by learning a mapping
    function that transforms the source data to have similar styles as the target
    data. Models trained with the transferred data usually perform better on target
    data due to the reduced domain discrepancy.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 风格迁移[[106](#bib.bib106), [22](#bib.bib22)]旨在将源数据转换为与目标数据相似的风格进行训练。它通过学习一个映射函数，将源数据转换为与目标数据具有相似风格的数据。使用迁移数据训练的模型通常在目标数据上表现更好，因为领域差异减少了。
- en: The following subsections review domain adaptive point cloud learning for various
    3D tasks.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节回顾了针对各种3D任务的领域自适应点云学习。
- en: '![Refer to caption](img/cd4986fb3f9228299988993fbb53d02c.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cd4986fb3f9228299988993fbb53d02c.png)'
- en: 'Figure 6: Typical UDA pipeline for 3D network training'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：3D网络训练的典型UDA管道
- en: 4.1.2 Domain adaptation for 3D shape classification
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 3D形状分类的领域自适应
- en: Object-level point clouds are often collected from various sources such as synthetic
    CAD models [[11](#bib.bib11), [8](#bib.bib8)] and real 3D scans [[107](#bib.bib107),
    [9](#bib.bib9)]. Due to differences in acquisition techniques and object characteristics,
    the collected point clouds may exhibit clear geometric discrepancies as illustrated
    in Fig. [7](#S4.F7 "Figure 7 ‣ 4.1.2 Domain adaptation for 3D shape classification
    ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"). Several studies have recently explored UDA
    for 3D shape classification across different 3D object datasets.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 物体级点云通常来自于合成CAD模型[[11](#bib.bib11), [8](#bib.bib8)]和真实3D扫描[[107](#bib.bib107),
    [9](#bib.bib9)]等各种来源。由于采集技术和物体特征的差异，采集的点云可能会出现明显的几何差异，如图[7](#S4.F7 "图7 ‣ 4.1.2
    3D形状分类的领域自适应 ‣ 4.1 领域自适应 ‣ 4 领域迁移学习 ‣ 3D点云的标签高效深度学习综述")所示。最近有若干研究探索了针对不同3D物体数据集的UDA在3D形状分类中的应用。
- en: '![Refer to caption](img/8f1c017a9f87cb41b5944176c0a68afd.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8f1c017a9f87cb41b5944176c0a68afd.png)'
- en: 'Figure 7: Examples of object-level point clouds in datasets ModelNet [[11](#bib.bib11)],
    ShapeNet [[8](#bib.bib8)], and ScanNet [[9](#bib.bib9)]. The figure is reproduced
    based on [[62](#bib.bib62)].'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：数据集ModelNet [[11](#bib.bib11)]、ShapeNet [[8](#bib.bib8)]和ScanNet [[9](#bib.bib9)]中的物体级点云示例。该图基于[[62](#bib.bib62)]进行了复现。
- en: Specifically, Qin et al. [[62](#bib.bib62)] explored adversarial training and
    designed PointDAN that utilizes the Maximum Classifier Discrepancy[[102](#bib.bib102)]
    to align features across domains. Several subsequent work [[63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65)] explored self-paced self-training for domain adaptive 3D shape
    classification, where the confidence threshold gradually lowers while selecting
    pseudo labels. Fan et al.[[66](#bib.bib66)] designed a voting strategy that pseudo-labels
    target samples by searching for the nearest source neighbours in a shared feature
    space. Chen et al.[[67](#bib.bib67)] proposed quasi-balanced self-training to
    address the class imbalance in pseudo-labelling. Cardace et al. [[68](#bib.bib68)]
    proposed to refine noisy pseudo-labels by matching shape descriptors that are
    learned by the unsupervised task of shape reconstruction on both domains.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，Qin 等人 [[62](#bib.bib62)] 探索了对抗训练，并设计了 PointDAN，它利用最大分类器差异 [[102](#bib.bib102)]
    来对齐跨领域特征。随后的几项工作 [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)] 探索了领域自适应
    3D 形状分类的自适应自训练，其中信心阈值逐渐降低，同时选择伪标签。Fan 等人 [[66](#bib.bib66)] 设计了一种投票策略，通过在共享特征空间中搜索最近的源邻居来伪标记目标样本。Chen
    等人 [[67](#bib.bib67)] 提出了准平衡自训练，以解决伪标签中的类别不平衡问题。Cardace 等人 [[68](#bib.bib68)]
    提出了通过匹配在两个领域中通过形状重建无监督任务学习的形状描述符来细化嘈杂的伪标签。
- en: Several studies designed SSL tasks to encourage networks to learn domain-invariant
    features from unlabelled point cloud objects. For example, Zou et al. [[63](#bib.bib63)]
    introduced a joint task that predicts rotation angles and distortion locations.
    Fan et al.[[66](#bib.bib66)] reconstructed the squeezed 2D projections of objects
    back to 3D space. Shen et al. [[64](#bib.bib64)] learned unsupervised features
    by computing approximations of unsigned distance fields.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究设计了 SSL 任务，旨在鼓励网络从未标记的点云对象中学习领域不变的特征。例如，Zou 等人 [[63](#bib.bib63)] 引入了一个联合任务，预测旋转角度和变形位置。Fan
    等人 [[66](#bib.bib66)] 将压缩的 2D 投影重建到 3D 空间。Shen 等人 [[64](#bib.bib64)] 通过计算无符号距离场的近似值来学习无监督特征。
- en: 4.1.3 Domain adaptation for 3D object detection
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 3D 物体检测的领域适应
- en: Due to differences in physical environments, sensor configurations, weather
    conditions, etc., scene-level point clouds are subject to more geometry shifts
    than object-level point clouds in term of point density and occlusion ratios.
    Domain adaptation across scene-level point clouds is thus even more challenging
    and it has recently attracted increasing attention thanks to the great values
    of scene-level 3D tasks such as 3D object detection and 3D semantic segmentation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 由于物理环境、传感器配置、天气条件等方面的差异，场景级点云在点密度和遮挡比方面受到的几何变化比物体级点云更大。因此，场景级点云的领域适应更加具有挑战性，最近由于
    3D 物体检测和 3D 语义分割等场景级 3D 任务的重要性而受到越来越多的关注。
- en: Domain adaptive 3D object detection has been studied extensively over the past
    few years. For example, Wang et al. [[69](#bib.bib69)] noticed that car size plays
    a crucial role in 3D object detection while adapting across data of different
    countries. They designed a simple normalization strategy for car size, which achieves
    superb adaptation performance. Later, adversarial training was explored for domain
    adaptive 3D object detection. For example, Su et al. [[70](#bib.bib70)] observed
    that semantic features contain both domain-specific attributes and other features
    that may mislead the discriminator. They thus disentangle the domain-specific
    attributes from the semantic features of LiDAR for better adversarial learning.
    Zhang et al. [[71](#bib.bib71)] recognized the distinctive geometric properties
    of LiDAR point clouds, i.e., larger and closer objects have more points, and designed
    scale-aware and range-aware domain alignment strategies for better adversarial
    training of 3D detectors.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 领域自适应 3D 物体检测在过去几年中得到了广泛研究。例如，Wang 等人 [[69](#bib.bib69)] 发现汽车尺寸在跨不同国家的数据中进行
    3D 物体检测时发挥了关键作用。他们设计了一种简单的汽车尺寸归一化策略，实现了出色的适应性能。随后，对抗训练被用于领域自适应 3D 物体检测。例如，Su 等人
    [[70](#bib.bib70)] 观察到语义特征包含领域特定属性以及其他可能误导鉴别器的特征。因此，他们将领域特定属性从 LiDAR 的语义特征中解开，以实现更好的对抗学习。Zhang
    等人 [[71](#bib.bib71)] 认识到 LiDAR 点云的独特几何属性，即较大且较近的物体具有更多点，并设计了尺度感知和范围感知的领域对齐策略，以更好地进行
    3D 检测器的对抗训练。
- en: Several methods [[108](#bib.bib108), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)]
    explored self-training for domain adaptive 3D detection. For instance, ST3D [[72](#bib.bib72)]
    updates pseudo labels with a quality-aware triplet memory bank and trains networks
    with curriculum data augmentation. Luo et al. [[73](#bib.bib73)] designed a multi-level
    consistency network that learns with consistency at the levels of points, instances,
    and neural statistics. Some work [[75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77),
    [78](#bib.bib78)] instead explored style transfer. For example, Hahner et al.[[76](#bib.bib76),
    [78](#bib.bib78)] simulated fog and snowfall over authentic point clouds to alleviate
    domain discrepancy across weather. Xu et al. [[77](#bib.bib77)] generated semantic
    points at foreground regions with missing object parts and combine the generated
    points with the original to enhance detection across domains. Further, Yihan et
    al.[[79](#bib.bib79)] proposed a 3D contrastive co-training approach to improve
    the transferability of learned point features. Wei et al.[[80](#bib.bib80)] introduced
    a teacher-student framework that distills knowledge from high-beam LiDAR data
    to low-beam data, aiming to reduce the domain gap caused by different LiDAR beam
    configurations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法[[108](#bib.bib108), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)]探索了自我训练用于领域自适应3D检测。例如，ST3D[[72](#bib.bib72)]使用质量感知三元组记忆库更新伪标签，并通过课程数据增强训练网络。Luo等人[[73](#bib.bib73)]设计了一个多层一致性网络，该网络在点、实例和神经统计层面上进行一致性学习。一些研究[[75](#bib.bib75),
    [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78)]则探讨了风格迁移。例如，Hahner等人[[76](#bib.bib76),
    [78](#bib.bib78)]在真实点云上模拟雾和降雪，以缓解不同天气下的领域差异。Xu等人[[77](#bib.bib77)]在前景区域生成具有缺失物体部件的语义点，并将生成的点与原始点结合，以增强跨领域检测。此外，Yihan等人[[79](#bib.bib79)]提出了一种3D对比共训练方法，以提高学习到的点特征的可迁移性。Wei等人[[80](#bib.bib80)]引入了一个教师-学生框架，将高束LiDAR数据中的知识提炼到低束数据中，以减少不同LiDAR束配置所造成的领域差异。
- en: 4.1.4 Domain adaptation for 3D semantic segmentation
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 3D语义分割的领域自适应
- en: LiDAR point clouds often have significant domain discrepancy due to variations
    in physical environments, sensor configurations, weather conditions, etc. Hence,
    most prior UDA studies [[86](#bib.bib86), [22](#bib.bib22), [87](#bib.bib87),
    [88](#bib.bib88), [109](#bib.bib109), [92](#bib.bib92)] focus on outdoor LiDAR
    point clouds, while just a few [[89](#bib.bib89)] tackle the issue for indoor
    point clouds. Fig. [8](#S4.F8 "Figure 8 ‣ 4.1.4 Domain adaptation for 3D semantic
    segmentation ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning ‣ A Survey of
    Label-Efficient Deep Learning for 3D Point Clouds") shows point-cloud samples
    of different domains that have clear domain discrepancies.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: LiDAR点云通常由于物理环境、传感器配置、天气条件等变化而具有显著的领域差异。因此，大多数先前的UDA研究[[86](#bib.bib86), [22](#bib.bib22),
    [87](#bib.bib87), [88](#bib.bib88), [109](#bib.bib109), [92](#bib.bib92)]集中在户外LiDAR点云上，而只有少数研究[[89](#bib.bib89)]解决了室内点云的问题。图[8](#S4.F8
    "Figure 8 ‣ 4.1.4 Domain adaptation for 3D semantic segmentation ‣ 4.1 Domain
    adaptation ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep Learning
    for 3D Point Clouds")显示了具有明显领域差异的不同领域的点云样本。
- en: Studies on domain adaptive point cloud segmentation can be broadly classified
    into two categories namely, uni-modal UDA that works with point clouds alone [[86](#bib.bib86),
    [22](#bib.bib22), [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89)] and cross-modal
    UDA that employs both point clouds and image data in training [[90](#bib.bib90),
    [91](#bib.bib91), [93](#bib.bib93), [110](#bib.bib110)].
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 关于领域自适应点云分割的研究大致可以分为两类，即仅处理点云的**单模态UDA**[[86](#bib.bib86), [22](#bib.bib22),
    [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89)]和在训练中同时使用点云和图像数据的**跨模态UDA**[[90](#bib.bib90),
    [91](#bib.bib91), [93](#bib.bib93), [110](#bib.bib110)]。
- en: '![Refer to caption](img/eef1b03416c78b1f5be41137cece3757.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eef1b03416c78b1f5be41137cece3757.png)'
- en: 'Figure 8: Example of LiDAR scans of different domains. (a) A real scan of normal
    weather in SemanticKITTI [[10](#bib.bib10)], (b) A real scan of adverse weather
    of snow in SemanticSTF [[23](#bib.bib23)], and (c) A synthetic scan in SynLiDAR [[22](#bib.bib22)].
    Different colors denote different semantic categories as in (d).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：不同领域的LiDAR扫描示例。（a）SemanticKITTI[[10](#bib.bib10)]中的正常天气下的真实扫描，（b）SemanticSTF[[23](#bib.bib23)]中的恶劣天气下的雪的真实扫描，以及（c）SynLiDAR[[22](#bib.bib22)]中的合成扫描。不同颜色表示不同的语义类别，如（d）所示。
- en: For uni-modal UDA, a line of studies [[82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84),
    [85](#bib.bib85), [81](#bib.bib81)] projected point clouds to depth images and
    adopted 2D UDA methods to mitigate domain shifts. For example, Li et al. [[81](#bib.bib81)]
    proposed an adversarial training framework to learn to generate source masks to
    mimic the pattern of irregular target noise, thereby narrowing the domain gap
    from synthetic point clouds to real ones. However, the 3D-to-2D projection loses
    geometric information, and most 2D UDA methods cannot handle the unique geometry
    of point clouds. Moreover, most 2D UDA methods adopt CNN architectures and cannot
    be generalized to point cloud architectures.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单模态UDA，一系列研究 [[82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85),
    [81](#bib.bib81)] 将点云投影到深度图像上，并采用2D UDA方法来减轻领域偏移。例如，Li等 [[81](#bib.bib81)] 提出了一个对抗训练框架，以学习生成源掩模以模拟不规则目标噪声的模式，从而缩小合成点云与真实点云之间的领域差距。然而，3D到2D的投影会丢失几何信息，并且大多数2D
    UDA方法无法处理点云的独特几何结构。此外，大多数2D UDA方法采用CNN架构，无法推广到点云架构。
- en: Another line of methods [[86](#bib.bib86), [22](#bib.bib22), [87](#bib.bib87)]
    performed domain adaptive point cloud segmentation over point clouds directly.
    For example, [[86](#bib.bib86)] tackled domain adaptation by transforming it into
    a 3D surface completion task. [[22](#bib.bib22)] employed GANs to translate synthetic
    point clouds to match the sparsity and appearance of real ones. [[87](#bib.bib87),
    [42](#bib.bib42)] mix point clouds of source and target domains to generate intermediate
    representations with less domain discrepancy. While most studies focus on outdoor
    LiDAR point clouds, [[89](#bib.bib89)] recently explored synthetic-to-real adaptation
    of indoor point clouds.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类方法 [[86](#bib.bib86), [22](#bib.bib22), [87](#bib.bib87)] 直接在点云上进行领域自适应点云分割。例如，[[86](#bib.bib86)]
    通过将领域自适应转化为3D表面补全任务来解决领域适应问题。[[22](#bib.bib22)] 使用GANs将合成点云转换为与真实点云的稀疏性和外观相匹配的点云。[[87](#bib.bib87),
    [42](#bib.bib42)] 混合源和目标领域的点云，以生成具有较少领域差异的中间表示。虽然大多数研究集中在户外LiDAR点云上，但[[89](#bib.bib89)]
    最近探讨了室内点云的合成到真实适应。
- en: For cross-modal UDA, each training sample typically comprises a 2D image and
    a 3D point cloud that are synchronized across LiDAR and camera sensors. Point-wise
    3D annotations are provided for source data. The goal is to learn a robust 3D
    segmentor that can work independently and requires no images for testing. Though
    the paired images can enrich the learned representation, cross-modal UDA is more
    challenging due to the heterogeneity of the input spaces for images and point
    clouds as well as additional domain shifts between source and target images. Jaritz
    et al. [[90](#bib.bib90)] developed xMUDA, the first cross-modal UDA framework
    that adopts a two-stream architecture to address the domain gap of each modality
    individually. Peng et al. [[91](#bib.bib91)] achieved cross-modal UDA with two
    modules, the first employing intra-domain cross-modal learning for cross-modal
    interaction while the second adopting adversarial learning for cross-domain feature
    alignment via inter-domain cross-modal learning.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于跨模态UDA，每个训练样本通常包括一个2D图像和一个与LiDAR和相机传感器同步的3D点云。为源数据提供了逐点的3D注释。目标是学习一个强大的3D分割器，它可以独立工作且测试时不需要图像。尽管配对的图像可以丰富学习到的表示，但由于图像和点云的输入空间的异质性以及源图像和目标图像之间的额外领域偏移，跨模态UDA更具挑战性。Jaritz等
    [[90](#bib.bib90)] 开发了xMUDA，这是第一个跨模态UDA框架，采用双流架构以单独解决每种模态的领域差距。Peng等 [[91](#bib.bib91)]
    通过两个模块实现了跨模态UDA，第一个模块采用领域内跨模态学习进行跨模态交互，第二个模块则通过领域间跨模态学习采用对抗学习进行跨领域特征对齐。
- en: 4.1.5 Extension
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5 扩展
- en: Source-free UDA [[111](#bib.bib111)] is a variant of UDA that aims to adapt
    source-trained models to target distributions without accessing the source data
    in training. It is useful when data privacy and data portability are critical.
    Recently, Saltori et al. [[88](#bib.bib88)] proposed a pioneering study for source-free
    UDA of point clouds. They designed adaptive self-training with a geometric-feature
    propagation for semantic segmentation of LiDAR point cloud of road scenes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 无源UDA [[111](#bib.bib111)] 是UDA的一种变体，旨在使源训练的模型适应目标分布，而无需在训练中访问源数据。当数据隐私和数据可移植性至关重要时，它非常有用。最近，Saltori
    等 [[88](#bib.bib88)] 提出了无源UDA在点云中的开创性研究。他们设计了带有几何特征传播的自适应自我训练，用于对道路场景中LiDAR点云的语义分割。
- en: Test-time domain adaptation (TTA) is a setup where a source-pretrained model
    is adapted using only the unlabelled test data, usually with a single epoch of
    training. Unlike typical UDA, the goal of TTA is to avoid collecting target data
    in advance, where the model is adapted with the test data flow. Though TTA is
    practical in real-world scenarios, it is challenging as the target data is available
    in test-stage only. Recently, Inkyu Shin et al. [[93](#bib.bib93)] proposed the
    first TTA attempt on multi-modal 3D semantic segmentation. They designed a multi-modal
    fusion module to combine multi-modal input data for more accurate segmentation.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 测试时领域适应（TTA）是一种设置，其中一个经过源预训练的模型仅使用未标记的测试数据进行适应，通常只需一次训练周期。与典型的UDA不同，TTA的目标是避免提前收集目标数据，在这种情况下，模型在测试数据流中进行适应。尽管TTA在实际场景中非常实用，但由于目标数据仅在测试阶段可用，它具有挑战性。最近，Inkyu
    Shin等人[[93](#bib.bib93)]提出了第一个多模态3D语义分割的TTA尝试。他们设计了一个多模态融合模块，将多模态输入数据结合起来，以实现更准确的分割。
- en: 4.2 Domain generalization
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 领域泛化
- en: Another research direction in the field of domain transfer learning is domain
    generalization (DG) [[112](#bib.bib112)], which aims to train a model using labelled
    source data that can generalize to any target domains without accessing target
    data in training. DG removes the dependency on target training data, making it
    very useful in many real-world tasks where target data is difficult or expensive
    to obtain before deploying the model. It is also a critical research area for
    point cloud learning, as many point cloud tasks require 3D deep models to be robust
    and generalizable to unseen domains. For instance, autonomous vehicles require
    generalizable 3D perception to operate safely in various unseen places and scenarios.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 领域迁移学习领域的另一个研究方向是领域泛化（DG）[[112](#bib.bib112)]，其目标是使用标记的源数据训练一个可以泛化到任何目标领域的模型，而无需在训练中访问目标数据。领域泛化消除了对目标训练数据的依赖，使其在许多实际任务中非常有用，因为在部署模型之前获取目标数据可能困难或昂贵。这也是点云学习的一个关键研究领域，因为许多点云任务要求3D深度模型对未见过的领域具有鲁棒性和泛化能力。例如，自动驾驶车辆需要具备泛化的3D感知能力，以便在各种未见过的地方和场景中安全运行。
- en: '![Refer to caption](img/6c999c289c301e5018a40935ded69907.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6c999c289c301e5018a40935ded69907.png)'
- en: 'Figure 9: Typical pipeline of domain generalization (DG) including (a) Single-source
    DG; (b) Multi-source DG.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：领域泛化（DG）的典型流程，包括（a）单源领域泛化；（b）多源领域泛化。
- en: 4.2.1 Problem setup
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 问题设置
- en: Given labelled point clouds of $K$ similar but distinct source domains $\mathcal{S}={\{S_{k}=\{(x^{(k)},y^{(k)})\}\}}_{K}^{k=1}$,
    where $x$ denotes a point cloud and $y$ is its labels, DG aims to learn a deep
    model $F$ with the source data only that can perform well in unseen target domain
    $\mathcal{T}$. Similar to 2D DG studies [[112](#bib.bib112)], we review two DG
    settings for 3D point clouds as shown in Fig. [9](#S4.F9 "Figure 9 ‣ 4.2 Domain
    generalization ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep
    Learning for 3D Point Clouds"). The first is multi-source DG which assumes the
    availability of more than one source domain in training, i.e., $K>1$. The motivation
    is to learn domain-invariant features (from multiple similar but distinct source
    domains) that can generalize well to any unseen domains. The second is single-source
    DG which is more challenging as it allows training data from a single source domain
    only. At the other end, single-source DG methods are more generic and can be applied
    to multi-source DG problems by ignoring the domain label.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 给定标记的点云数据来自$K$个相似但不同的源领域$\mathcal{S}={\{S_{k}=\{(x^{(k)},y^{(k)})\}\}}_{K}^{k=1}$，其中$x$表示一个点云，$y$是其标签，领域泛化（DG）旨在仅使用源数据学习一个深度模型$F$，使其在未见过的目标领域$\mathcal{T}$中表现良好。类似于2D领域泛化研究 [[112](#bib.bib112)]，我们回顾了用于3D点云的两种领域泛化设置，如图[9](#S4.F9
    "图 9 ‣ 4.2 领域泛化 ‣ 4 领域迁移学习 ‣ 3D点云的标签有效深度学习概述")所示。第一种是多源领域泛化，它假设训练中可用多个源领域，即$K>1$。其动机是学习领域不变特征（来自多个相似但不同的源领域），这些特征可以很好地推广到任何未见过的领域。第二种是单源领域泛化，它更加具有挑战性，因为它仅允许来自单个源领域的训练数据。在另一方面，单源领域泛化方法更加通用，可以通过忽略领域标签应用于多源领域泛化问题。
- en: 4.2.2 Domain generalization for 3D shape classification
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 3D形状分类的领域泛化
- en: The pioneer work [[94](#bib.bib94)] first explored geometry shifts from simulated
    point clouds of CAD objects (e.g. ModelNet dataset[[11](#bib.bib11)]) to real
    object point clouds (e.g. ScanObjectNN [[107](#bib.bib107)]). It presents a meta-learning
    framework to train generalizable 3D classification models across domains. Later,
    Huang et al.[[95](#bib.bib95)] designed a manifold adversarial training scheme
    that exploits multiple geometric transformations to generate adversarial training
    samples of intermediate domains. Both studies fall under the single-source DG
    setting.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 先驱工作[[94](#bib.bib94)] 首次探讨了从CAD对象的模拟点云（例如ModelNet数据集[[11](#bib.bib11)]）到实际物体点云（例如ScanObjectNN[[107](#bib.bib107)]）的几何位移。它提出了一个元学习框架，用于训练跨领域的可泛化3D分类模型。后来，黄等人[[95](#bib.bib95)]
    设计了一种流形对抗训练方案，该方案利用多种几何变换生成中间领域的对抗训练样本。这两项研究都属于单源领域泛化设置。
- en: 'TABLE IV: Categorization of weakly-supervised learning methods for point clouds.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 点云的弱监督学习方法分类。'
- en: '| Weak supervision | Annotation | References |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 弱监督 | 标注 | 参考文献 |'
- en: '| --- | --- | --- |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Incomplete supervision (§[5.1](#S5.SS1 "5.1 Incomplete supervision ‣ 5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) |
    3D weakly-supervised learning: Sparsely annotating a small number of points in
    a large number of point cloud frames. (§[5.1.1](#S5.SS1.SSS1 "5.1.1 3D weakly-supervised
    learning ‣ 5.1 Incomplete supervision ‣ 5 Weakly-supervised learning ‣ A Survey
    of Label-Efficient Deep Learning for 3D Point Clouds")) | [[113](#bib.bib113),
    [114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117),
    [118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121),
    [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124), [125](#bib.bib125),
    [126](#bib.bib126)] |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 不完全监督 (§[5.1](#S5.SS1 "5.1 不完全监督 ‣ 5 弱监督学习 ‣ 3D 点云的标签高效深度学习调查")) | 3D 弱监督学习：在大量点云帧中稀疏标注少量点。
    (§[5.1.1](#S5.SS1.SSS1 "5.1.1 3D 弱监督学习 ‣ 5.1 不完全监督 ‣ 5 弱监督学习 ‣ 3D 点云的标签高效深度学习调查"))
    | [[113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121), [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124),
    [125](#bib.bib125), [126](#bib.bib126)] |'
- en: '| 3D semi-supervised learning: Intensively annotating a small number of point
    cloud frames with fully labeled points. (§[5.1.2](#S5.SS1.SSS2 "5.1.2 3D semi-supervised
    learning ‣ 5.1 Incomplete supervision ‣ 5 Weakly-supervised learning ‣ A Survey
    of Label-Efficient Deep Learning for 3D Point Clouds")) | [[127](#bib.bib127),
    [128](#bib.bib128), [129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131),
    [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [109](#bib.bib109)]
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 3D 半监督学习：对少量点云帧进行密集标注，所有点都有标签。 (§[5.1.2](#S5.SS1.SSS2 "5.1.2 3D 半监督学习 ‣ 5.1
    不完全监督 ‣ 5 弱监督学习 ‣ 3D 点云的标签高效深度学习调查")) | [[127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [109](#bib.bib109)] |'
- en: '| 3D few-shot learning: Annotating a few samples for novel (unseen) classes
    on top of many labeled samples of base (seen) classes. (§[5.1.3](#S5.SS1.SSS3
    "5.1.3 3D few-shot learning ‣ 5.1 Incomplete supervision ‣ 5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) |
    [[135](#bib.bib135), [136](#bib.bib136), [137](#bib.bib137), [138](#bib.bib138),
    [139](#bib.bib139), [140](#bib.bib140), [141](#bib.bib141), [142](#bib.bib142)]
    |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 3D 小样本学习：在大量标记的基类（已见）样本的基础上，为新类（未见）样本标注少量样本。 (§[5.1.3](#S5.SS1.SSS3 "5.1.3
    3D 小样本学习 ‣ 5.1 不完全监督 ‣ 5 弱监督学习 ‣ 3D 点云的标签高效深度学习调查")) | [[135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [141](#bib.bib141), [142](#bib.bib142)] |'
- en: '| Inexact supervision (§[5.2](#S5.SS2 "5.2 Inexact supervision ‣ 5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) |
    Weak annotations, e.g., cloud-/position-/box-level annotations, scribble, etc.
    | [[143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146),
    [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149), [150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152), [153](#bib.bib153)] |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 不精确监督 (§[5.2](#S5.SS2 "5.2 不精确监督 ‣ 5 弱监督学习 ‣ 3D 点云的标签高效深度学习调查")) | 弱标注，例如：云级/位置级/框级标注、涂鸦等。
    | [[143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146),
    [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149), [150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152), [153](#bib.bib153)] |'
- en: '| Inaccurate supervision (§[5.3](#S5.SS3 "5.3 Inaccurate supervision ‣ 5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) |
    Noisy labels | [[154](#bib.bib154)] |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 不准确的监督 (§[5.3](#S5.SS3 "5.3 不准确的监督 ‣ 5 弱监督学习 ‣ 3D 点云的标签高效深度学习综述")) | 噪声标签
    | [[154](#bib.bib154)] |'
- en: 4.2.3 Domain generalization for 3D object detection
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 3D 物体检测的领域泛化
- en: Improving the generalizability of 3D detectors is essential in 3D vision tasks
    such as autonomous driving where perception algorithms must maintain stable performance
    over unseen domains. However, DG for 3D object detection remains a relatively
    under-explored area. The pioneer study in [[96](#bib.bib96)] presented the first
    attempt at single-source DG for 3D object detection. It presents an adversarial
    augmentation method that learns to deform point clouds in training to enhance
    the generalization of 3D detectors. Recently, [[97](#bib.bib97)] introduced a
    single-source DG approach for multi-view 3D object detection in Bird-Eye-View
    (BEV). It decouples depth estimation from camera parameters, employs dynamic perspective
    augmentation, and adopts multiple pseudo-domains for better generalization toward
    various unseen new domains.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 提高 3D 检测器的泛化能力对 3D 视觉任务（如自动驾驶）至关重要，因为感知算法必须在未见领域中保持稳定的性能。然而，3D 物体检测的 DG 仍然是一个相对未被充分探索的领域。开创性研究[[96](#bib.bib96)]
    提出了针对 3D 物体检测的单源 DG 的首次尝试。它提出了一种对抗性增强方法，学习在训练中变形点云以增强 3D 检测器的泛化能力。最近，[[97](#bib.bib97)]
    引入了一种单源 DG 方法，用于鸟瞰图 (BEV) 中的多视角 3D 物体检测。它将深度估计与相机参数解耦，采用动态视角增强，并采用多个伪领域以实现更好的泛化能力，适应各种未见的新领域。
- en: 4.2.4 Domain generalization for 3D semantic segmentation
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 3D 语义分割的领域泛化
- en: Several studies on domain-generalizable point cloud segmentation have been reported
    recently. Xiao et al.[[23](#bib.bib23)] study outdoor point cloud segmentation
    under adverse weather, where a domain randomization and aggregation learning pipeline
    was designed to enhance the model generalization performance. [[99](#bib.bib99)]
    augments the source domain and introduces constraints in sparsity invariance consistency
    and semantic correlation consistency for learning more generalized 3D LiDAR representations.
    Zhao et al.[[98](#bib.bib98)] instead focus on indoor point clouds and proposed
    clustering augmentation to improve model generalization.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最近有几项关于领域泛化点云分割的研究。Xiao 等人[[23](#bib.bib23)] 研究了恶劣天气下的室外点云分割，设计了一种领域随机化和聚合学习流程以增强模型的泛化性能。[[99](#bib.bib99)]
    增强了源领域，并在稀疏不变性一致性和语义关联一致性方面引入了约束，以学习更泛化的 3D LiDAR 表示。Zhao 等人[[98](#bib.bib98)]
    则专注于室内点云，提出了聚类增强以提高模型的泛化能力。
- en: 4.3 Summary
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 总结
- en: Transferring knowledge across domains is a key strategy for maximizing the use
    of existing annotations. This triggered extensive studies of UDA and DG in the
    field of machine learning in the past years. Despite great advancements in related
    areas such as 2D computer vision and NLP, UDA and DG for point clouds is far under-explored.
    This is evidenced by the smaller number of published papers and low performance
    on various benchmarks as listed in the appendix. Hence, more efforts are urgently
    needed to advance the development of this very promising research area.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 跨领域知识迁移是最大化现有注释使用的关键策略。这促使了过去几年在机器学习领域对 UDA 和 DG 的广泛研究。尽管在 2D 计算机视觉和 NLP 等相关领域取得了巨大进展，但点云的
    UDA 和 DG 仍然相对未被充分探索。这从发布论文数量较少和在各种基准测试上表现较低中得到证明，详细信息见附录。因此，需要更多努力来推动这一非常有前景的研究领域的发展。
- en: 5 Weakly-supervised learning
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 弱监督学习
- en: 'Weakly-supervised learning (WSL), as an alternative to fully-supervised learning,
    leverages weak supervision for network training. Collecting weak annotations often
    reduces the annotation cost and time significantly, making WSL an important branch
    of label-efficient learning. With the WSL definition in [[155](#bib.bib155)],
    we categorize WSL methods on point clouds based on three types of weak supervision:
    incomplete supervision, inexact supervision, and inaccurate supervision. Incomplete
    supervision involves only a small portion of training samples being labelled,
    while inexact supervision provides coarse-grained labels that may not match the
    model output. Inaccurate supervision refers to noisy labels. Table [IV](#S4.T4
    "TABLE IV ‣ 4.2.2 Domain generalization for 3D shape classification ‣ 4.2 Domain
    generalization ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep
    Learning for 3D Point Clouds") shows a summary of representative approaches.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督学习（WSL）作为完全监督学习的替代方案，利用弱监督进行网络训练。收集弱标注通常会显著降低标注成本和时间，使得WSL成为标签高效学习的重要分支。根据[[155](#bib.bib155)]中的WSL定义，我们根据三种弱监督类型对点云上的WSL方法进行分类：不完整监督、不精确监督和不准确监督。不完整监督涉及仅对一小部分训练样本进行标注，而不精确监督提供粗粒度标签，可能与模型输出不匹配。不准确监督指的是噪声标签。表[IV](#S4.T4
    "TABLE IV ‣ 4.2.2 Domain generalization for 3D shape classification ‣ 4.2 Domain
    generalization ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep
    Learning for 3D Point Clouds")展示了代表性方法的总结。
- en: 5.1 Incomplete supervision
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 不完整监督
- en: 'In the context of incomplete supervision, only a subset of training point clouds
    is labeled. Incomplete supervision can be obtained with two labelling strategies:
    1) sparsely labelling a small number of points from many point-cloud frames and
    2) intensively labelling a small number of point cloud frames with more (or fully)
    labelled points. Following conventions in relevant literature, we refer to studies
    with the first strategy by ”3D weakly-supervised learning” and review them in
    Section [5.1.1](#S5.SS1.SSS1 "5.1.1 3D weakly-supervised learning ‣ 5.1 Incomplete
    supervision ‣ 5 Weakly-supervised learning ‣ A Survey of Label-Efficient Deep
    Learning for 3D Point Clouds"). For the second strategy, we refer to it by ”3D
    semi-supervised learning” and review relevant studies in Section [5.1.2](#S5.SS1.SSS2
    "5.1.2 3D semi-supervised learning ‣ 5.1 Incomplete supervision ‣ 5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"). Both
    labelling strategies adopt similar training paradigms with supervised learning
    on limited labelled points and unsupervised learning on massive unlabelled points,
    as shown in Fig. [10](#S5.F10 "Figure 10 ‣ 5.1.1 3D weakly-supervised learning
    ‣ 5.1 Incomplete supervision ‣ 5 Weakly-supervised learning ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"). Additionally, we review ”3D few-shot learning”
    in Section [5.1.3](#S5.SS1.SSS3 "5.1.3 3D few-shot learning ‣ 5.1 Incomplete supervision
    ‣ 5 Weakly-supervised learning ‣ A Survey of Label-Efficient Deep Learning for
    3D Point Clouds") with a few labelled samples of novel classes and many labelled
    samples of base classes, aiming to reduce the labelling of novel classes in network
    training.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在不完整监督的背景下，仅对训练点云的一个子集进行标注。不完整监督可以通过两种标注策略获得：1) 对多个点云帧中的少量点进行稀疏标注，2) 对少量点云帧进行密集标注，带有更多（或完全）标注的点。根据相关文献中的惯例，我们将采用第一种策略的研究称为“3D
    弱监督学习”，并在第[5.1.1](#S5.SS1.SSS1 "5.1.1 3D weakly-supervised learning ‣ 5.1 Incomplete
    supervision ‣ 5 Weakly-supervised learning ‣ A Survey of Label-Efficient Deep
    Learning for 3D Point Clouds")节中进行回顾。对于第二种策略，我们称之为“3D 半监督学习”，并在第[5.1.2](#S5.SS1.SSS2
    "5.1.2 3D semi-supervised learning ‣ 5.1 Incomplete supervision ‣ 5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")节中回顾相关研究。两种标注策略均采用类似的训练范式，即在有限标注点上进行监督学习，在大量未标注点上进行无监督学习，如图[10](#S5.F10
    "Figure 10 ‣ 5.1.1 3D weakly-supervised learning ‣ 5.1 Incomplete supervision
    ‣ 5 Weakly-supervised learning ‣ A Survey of Label-Efficient Deep Learning for
    3D Point Clouds")所示。此外，我们在第[5.1.3](#S5.SS1.SSS3 "5.1.3 3D few-shot learning ‣
    5.1 Incomplete supervision ‣ 5 Weakly-supervised learning ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds")节中回顾了“3D 小样本学习”，旨在减少新类别的标注，同时使用少量标注样本的新类别和大量标注样本的基础类别进行网络训练。
- en: 5.1.1 3D weakly-supervised learning
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 3D 弱监督学习
- en: 3D weakly-supervised learning (3D-WSL) learns with a small number of sparsely
    annotated points in each point cloud frame. It has high research and application
    value since it allows annotating more point-clouds frames with less labelling
    redundancy.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 3D弱监督学习（3D-WSL）在每个点云帧中使用少量稀疏标注的点进行学习。由于它允许用较少的标注冗余标注更多的点云帧，因此具有很高的研究和应用价值。
- en: '![Refer to caption](img/f568d12617aeffaf3fb035afcb573af1.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f568d12617aeffaf3fb035afcb573af1.png)'
- en: 'Figure 10: Typical pipeline of training 3D point cloud networks with incomplete
    supervision.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：使用不完全监督训练3D点云网络的典型流程。
- en: Problem setup. Let $P$ be a point cloud of the training set consisting of labelled
    points $\{(X_{l},Y_{l})\}$ and unlabelled points $\{(X_{u},\varnothing)\}$, where
    $X$ represents point space and $Y$ means label space. 3D-WSL aims to learn a function
    $f:X_{l}\cup X_{u}\mapsto Y$ given a large amount of point clouds including a
    tiny fraction of labelled points (e.g., 5%) as training input.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 问题设置。设$P$为训练集中的一个点云，包括标记点$\{(X_{l},Y_{l})\}$和未标记点$\{(X_{u},\varnothing)\}$，其中$X$表示点空间，$Y$表示标签空间。3D-WSL旨在学习一个函数$f:X_{l}\cup
    X_{u}\mapsto Y$，给定大量的点云，其中包括少量的标记点（例如，5%）作为训练输入。
- en: 3D-WSL for semantic segmentation. This task aims to learn a robust segmentation
    model with a small portion of labelled points in each point cloud. It has been
    studied via consistency-learning [[113](#bib.bib113), [117](#bib.bib117), [121](#bib.bib121),
    [156](#bib.bib156)] that aims to learn generalizable representations by enforcing
    prediction consistency across different augmented views of the same input data.
    For instance, Xu et al.[[113](#bib.bib113)] introduced a Siamese self-supervision
    branch for consistent learning from unlabelled points. Zhang et al.[[117](#bib.bib117)]
    conducted consistency learning between original data points and their perturbed
    versions. Wu et al. [[121](#bib.bib121)] designed a dual adaptive transformation
    that encourages consistent predictions between original points and their transformed
    counterparts.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 3D-WSL用于语义分割。这个任务旨在通过在每个点云中使用少量标记点来学习一个稳健的分割模型。通过一致性学习[[113](#bib.bib113), [117](#bib.bib117),
    [121](#bib.bib121), [156](#bib.bib156)]，旨在通过强制不同增强视图的预测一致性来学习具有泛化能力的表示。例如，Xu等人[[113](#bib.bib113)]引入了一个Siamese自监督分支，用于从未标记点中进行一致性学习。Zhang等人[[117](#bib.bib117)]进行了原始数据点与其扰动版本之间的一致性学习。Wu等人[[121](#bib.bib121)]设计了一种双重自适应变换，鼓励原始点及其变换对应点之间的一致性预测。
- en: Several studies [[122](#bib.bib122), [118](#bib.bib118)] explored contrastive
    learning on unlabelled point clouds. It pulls the features of points toward their
    augmented views while pushing them away from other points, aiming to learn structural
    point representations in an unsupervised manner. For instance, Liu et al.[[122](#bib.bib122)]
    over-segmented point clouds to extract point boundaries for region-level contrastive
    learning. Li et al.[[118](#bib.bib118)] combined consistency learning and contrastive
    learning for learning more comprehensive representations. Recent studies [[114](#bib.bib114),
    [119](#bib.bib119)] also explored self-training for point cloud segmentation.
    For example, Shi et al.[[119](#bib.bib119)] annotated only a small portion of
    points in the first LiDAR frame and selected confident predictions of unlabelled
    points as pseudo labels for network re-training. Hu et al.[[120](#bib.bib120)]
    recently proposed a Semantic Query Network that leverages sparsely labelled points
    and their local neighbours to learn a compact neighbourhood representation, achieving
    very promising segmentation performance with only 0.1% of labelled points.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究[[122](#bib.bib122), [118](#bib.bib118)]探讨了对未标记点云的对比学习。它将点的特征拉向其增强视图，同时将其推离其他点，旨在以无监督的方式学习结构化的点表示。例如，Liu等人[[122](#bib.bib122)]过度分割点云以提取点边界用于区域级对比学习。Li等人[[118](#bib.bib118)]结合了一致性学习和对比学习，以学习更全面的表示。最近的研究[[114](#bib.bib114),
    [119](#bib.bib119)]也探讨了点云分割的自我训练。例如，Shi等人[[119](#bib.bib119)]仅在第一个LiDAR帧中标注了一小部分点，并选择未标记点的可信预测作为伪标签进行网络再训练。Hu等人[[120](#bib.bib120)]最近提出了一种语义查询网络，利用稀疏标记点及其局部邻域学习紧凑的邻域表示，仅使用0.1%的标记点就实现了非常有前景的分割性能。
- en: Instead of randomly selecting points for labelling, active learning selects
    more representative points for labelling. For instance, Wu et al.[[125](#bib.bib125)]
    exploited entropy, color discontinuity, and structural complexity to split point
    clouds into sub-regions and select representative sub-regions for labelling. Hu
    et al.[[126](#bib.bib126)] used prediction inconsistency across LiDAR frames as
    a measure of uncertainty for active sample selection. This approach requires only
    5% or fewer annotations but achieves comparable segmentation with fully supervised
    models. Several recent studies [[123](#bib.bib123), [157](#bib.bib157)] labelled
    segments instead of individual points as local points often share the same semantics
    due to the homogeneity of 3D objects in scenes. For example, Liu et al. [[123](#bib.bib123)]
    pre-segmented LiDAR sequences into connected components for coarse labelling.
    This strategy greatly reduces labelling efforts as compared with point-wise labelling.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机选择点进行标注不同，主动学习选择更具代表性的点进行标注。例如，吴等人[[125](#bib.bib125)]利用熵、颜色不连续性和结构复杂性将点云划分为子区域，并选择代表性子区域进行标注。胡等人[[126](#bib.bib126)]使用LiDAR帧之间的预测不一致性作为主动样本选择的不确定性度量。这种方法仅需要5%或更少的标注，但与完全监督模型的分割效果相当。一些近期研究[[123](#bib.bib123),
    [157](#bib.bib157)]对标注的片段而非单独的点进行标注，因为局部点通常因场景中3D对象的同质性而共享相同的语义。例如，刘等人[[123](#bib.bib123)]将LiDAR序列预分割为连通组件进行粗略标注。这种策略相比点位标注大大减少了标注工作。
- en: 3D-WSL for object detection. 3D-WSL for object detection is a largely under-explored
    research area. Liu et al. [[124](#bib.bib124)] recently conducted a pioneering
    exploration of sparse annotation strategy for 3D object detection. They annotate
    only one 3D object in each scene and then use the prediction confidence to mine
    object instances for network re-training, achieving similar detection performance
    while reducing the annotation effort greatly.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 3D-WSL用于目标检测。3D-WSL用于目标检测是一个尚未充分探索的研究领域。刘等人[[124](#bib.bib124)]最近开展了对稀疏标注策略在3D目标检测中的开创性探索。他们仅在每个场景中标注一个3D对象，然后利用预测置信度挖掘对象实例以进行网络再训练，取得了类似的检测性能，同时大大减少了标注工作。
- en: 3D-WSL for instance segmentation. 3D-WSL has recently been explored for instance
    segmentation by “annotating one point per instance” [[157](#bib.bib157), [158](#bib.bib158)].
    For example, Tao et al. [[157](#bib.bib157)] first over-segmented point clouds
    and then clicked one point per segment to assign its location, category, and instance
    identity. Tang et al. [[158](#bib.bib158)] selected and labelled one informative
    point per instance and achieved competitive performance on several public benchmarks.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 3D-WSL用于实例分割。最近，3D-WSL已被探索用于实例分割，通过“每个实例标注一个点”[[157](#bib.bib157), [158](#bib.bib158)]。例如，陶等人[[157](#bib.bib157)]首先对点云进行过度分割，然后在每个分段中点击一个点以分配其位置、类别和实例身份。唐等人[[158](#bib.bib158)]选择并标注每个实例中的一个信息点，并在几个公开基准测试中取得了具有竞争力的表现。
- en: 5.1.2 3D semi-supervised learning
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 3D半监督学习
- en: 3D semi-supervised learning (3D-SemiSL) works with intensive (full) annotation
    of a small portion of point cloud frames. As studied in [[113](#bib.bib113)],
    the annotation strategy in 3D-SemiSL leads to inferior part segmentation than
    that in 3D-WSL, largely due to its higher annotation redundancy. However, 3D-SemiSL
    is advantageous in requiring less training data collection during annotations.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 3D半监督学习（3D-SemiSL）与少量点云帧的密集（完全）标注相结合。如在[[113](#bib.bib113)]中所研究，3D-SemiSL中的标注策略导致的部分分割效果较3D-WSL差，这主要由于其更高的标注冗余。然而，3D-SemiSL在标注过程中需要较少的训练数据收集。
- en: Problem setup. Given point clouds $\mathbf{X}_{l}\in\mathbb{R}^{N_{l}\times
    3}$ with labels $\mathbf{Y}_{l}$ and unlabeled point clouds $\mathbf{X}_{u}\in\mathbb{R}^{N_{u}\times
    3}$ ($N_{l}$ and $N_{u}$ are point cloud numbers, $N_{l}<N_{u}$), 3D-SemiSL aims
    to learn a point cloud model $F$ from the labeled data and unlabeled data that
    can perform well on unseen point clouds.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 问题设定。给定带标签的点云$\mathbf{X}_{l}\in\mathbb{R}^{N_{l}\times 3}$和未标记的点云$\mathbf{X}_{u}\in\mathbb{R}^{N_{u}\times
    3}$（$N_{l}$和$N_{u}$是点云数量，$N_{l}<N_{u}$），3D-SemiSL旨在从标记数据和未标记数据中学习一个点云模型$F$，以便在未见过的点云上表现良好。
- en: 3D-SemiSL for object detection. Most existing 3D-SemiSL detection studies adopted
    the Mean-Teacher framework [[159](#bib.bib159)] involving a teacher network and
    a student network of the same architecture. The teacher model is a moving average
    of student models, and its predictions guide the student learning. It assumes
    that the teacher model learns more robust representations that can benefit the
    student learning.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 3D-SemiSL用于目标检测。大多数现有的3D-SemiSL检测研究采用了Mean-Teacher框架[[159](#bib.bib159)]，该框架涉及一个教师网络和一个具有相同架构的学生网络。教师模型是学生模型的移动平均，其预测指导学生学习。它假设教师模型学习到更鲁棒的表示，可以促进学生学习。
- en: 3D-SemiSL has been extensively studied for object detection. For example, SESS [[130](#bib.bib130)]
    adopts consistency learning [[160](#bib.bib160)] between the teacher and student,
    aiming for a perturbation invariant output distribution by assuming that decision
    boundaries lie in low-density regions [[161](#bib.bib161)]. 3DIoUMatch [[131](#bib.bib131)]
    selects confident predictions by the teacher model as pseudo labels for network
    re-training, aiming to minimize the entropy of predictions [[162](#bib.bib162)]
    of student models and lower the point density at the decision boundaries [[163](#bib.bib163)].
    Yin et al. [[132](#bib.bib132)] proposed a Proficient Teacher model that introduces
    a spatial-temporal ensemble module and a clustering-based box voting strategy
    to further enhance pseudo-labelling of 3D bounding boxes. Liu et al. [[109](#bib.bib109)]
    introduced a dual-threshold strategy and data augmentation to improve hierarchical
    supervision and feature representation in training the student network.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 3D-SemiSL在目标检测中得到了广泛研究。例如，SESS[[130](#bib.bib130)]采用了教师和学生之间的一致性学习[[160](#bib.bib160)]，通过假设决策边界位于低密度区域[[161](#bib.bib161)]，以实现扰动不变的输出分布。3DIoUMatch[[131](#bib.bib131)]选择教师模型的置信预测作为网络再训练的伪标签，旨在最小化学生模型预测的熵[[162](#bib.bib162)]，并降低决策边界的点密度[[163](#bib.bib163)]。Yin等人[[132](#bib.bib132)]提出了一种Proficient
    Teacher模型，引入了空间-时间集成模块和基于聚类的框投票策略，以进一步增强3D边界框的伪标记。Liu等人[[109](#bib.bib109)]引入了双阈值策略和数据增强，以改善训练学生网络中的层次监督和特征表示。
- en: 3D-SemiSL for segmentation. Compared to bounding box annotations in 3D object
    detection, point-wise labelling in point cloud segmentation is even more laborious
    and time-consuming. 3D-SemiSL for point cloud segmentation has therefore garnered
    even more attention recently [[127](#bib.bib127), [128](#bib.bib128), [164](#bib.bib164),
    [133](#bib.bib133), [129](#bib.bib129)].
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 3D-SemiSL用于分割。与3D目标检测中的边界框标注相比，点云分割中的逐点标记更加费力和耗时。因此，3D-SemiSL在点云分割中的应用最近受到了更多关注[[127](#bib.bib127),
    [128](#bib.bib128), [164](#bib.bib164), [133](#bib.bib133), [129](#bib.bib129)]。
- en: For 3D semantic segmentation, Jiang et al.[[127](#bib.bib127)] proposed a guided
    point contrastive learning framework to increase the generalization of segmentation
    models. Cheng et al. [[128](#bib.bib128)] built superpoint graphs and pseudo-label
    superpoints to train graph neural networks in a semi-supervised manner. Kong et
    al. [[133](#bib.bib133)] mixed laser beams from different LiDAR scans and learned
    more generalizable segmentation models by encouraging the model to make consistent
    predictions before and after mixing. Li et al. [[134](#bib.bib134)] introduced
    Sparse Depthwise Separable Convolution and constructed a light segmentation model
    requiring less training data. For 3D instance segmentation, Chu et al. [[129](#bib.bib129)]
    proposed a two-way inter-label self-training framework that leverages pseudo semantic
    labels and pseudo instance proposals to mutually denoise pseudo signals for better
    semantic-level and instance-level supervision.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于3D语义分割，Jiang等人[[127](#bib.bib127)]提出了一种引导点对比学习框架，以提高分割模型的泛化能力。Cheng等人[[128](#bib.bib128)]构建了超点图和伪标签超点，以半监督的方式训练图神经网络。Kong等人[[133](#bib.bib133)]混合了来自不同LiDAR扫描的激光束，并通过鼓励模型在混合前后做出一致的预测来学习更具泛化性的分割模型。Li等人[[134](#bib.bib134)]引入了稀疏深度可分离卷积，并构建了一个需要更少训练数据的轻量级分割模型。对于3D实例分割，Chu等人[[129](#bib.bib129)]提出了一种双向标签自训练框架，利用伪语义标签和伪实例提议，相互去噪伪信号，以实现更好的语义级别和实例级别监督。
- en: 3D-SemiSL for other point cloud tasks. 3D-SemiSL has also been explored in other
    3D point cloud tasks thanks to its superiority in saving human annotations. For
    instance, Huang et al.[[165](#bib.bib165)] proposed a semi-supervised framework
    for point cloud registration while several studies [[166](#bib.bib166), [167](#bib.bib167)]
    explore semi-supervised 3d hand pose estimation.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 3D-SemiSL 在其他点云任务中的应用。由于在节省人工标注方面的优势，3D-SemiSL 也被探讨应用于其他 3D 点云任务。例如，Huang 等人[[165](#bib.bib165)]
    提出了一个用于点云配准的半监督框架，而几项研究 [[166](#bib.bib166), [167](#bib.bib167)] 探索了半监督 3D 手部姿势估计。
- en: 'Semi-supervised domain adaptation. The combination of semi-supervised learning
    and domain adaptation formulates another way of label-efficient learning, namely,
    semi-supervised domain adaptation (SSDA). SSDA involves three types of data in
    training: labeled source samples, many unlabeled target samples, and a small amount
    of labeled target samples, aiming for a model that performs well in the target
    domain. Several studies explored SSDA for different point cloud learning tasks
    such as semantic segmentation of LiDAR point clouds [[22](#bib.bib22)] and 3D
    object detection [[168](#bib.bib168)].'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督领域自适应。半监督学习和领域自适应的结合形成了一种新的标签高效学习方法，即半监督领域自适应（SSDA）。SSDA 在训练中涉及三种类型的数据：标记的源样本、大量未标记的目标样本以及少量标记的目标样本，目标是获得在目标领域表现良好的模型。几项研究探讨了不同点云学习任务的
    SSDA，例如 LiDAR 点云的语义分割 [[22](#bib.bib22)] 和 3D 目标检测 [[168](#bib.bib168)]。
- en: 5.1.3 3D few-shot learning
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 3D 小样本学习
- en: Fully supervised deep networks learn from a large amount of training samples
    under a “closed set” setup, i.e., the training and testing data have the same
    label space. Such supervised learning is not ideal for quickly learning new concepts
    with limited data, which motivates few-shot learning (FSL) that aims to learn
    a novel class from just a few labelled samples. FSL can be seen as an extension
    of semi-supervised learning in an ”open-set” setup that has only a few labelled
    samples of novel classes, along with many labelled samples of base classes [[5](#bib.bib5)].
    Due to its superb merit in data requirements, FSL has recently attracted increasing
    attention with several ground-breaking studies [[135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [141](#bib.bib141)].
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 完全监督的深度网络在“大闭集”设置下从大量训练样本中学习，即训练数据和测试数据具有相同的标签空间。这种监督学习对于快速学习新概念并且数据有限并不理想，这促使了小样本学习（FSL）的出现，FSL
    旨在从少量标记样本中学习新类别。FSL 可以看作是半监督学习在“开集”设置中的扩展，其中只有少量新类别的标记样本，以及大量基础类别的标记样本 [[5](#bib.bib5)]。由于其在数据需求方面的卓越优点，FSL
    最近吸引了越来越多的关注，并出现了几项突破性的研究 [[135](#bib.bib135), [136](#bib.bib136), [137](#bib.bib137),
    [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140), [141](#bib.bib141)]。
- en: 'Problem setup. There are two typical settings in FSL: The $N$-way-$K$-shot [[169](#bib.bib169)]
    where the training set and testing set are disjoint in terms of classes; The generalized
    FSL [[170](#bib.bib170)] that recognizes both base and novel classes in testing.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 问题设置。在小样本学习（FSL）中有两种典型设置：$N$-way-$K$-shot [[169](#bib.bib169)]，其中训练集和测试集在类别上是不重叠的；广义
    FSL [[170](#bib.bib170)]，它在测试中识别基础类别和新类别。
- en: •
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$N$-way-$K$-shot. Let $(x,y)$ denotes a point cloud $x$ and its label $y$.
    FSL aims to train a model on a group of few-shot tasks sampled from a data set
    with a training class set $C_{\mathrm{train}}$ and test the trained model on another
    group of tasks sampled from a data set with new classes $C_{\mathrm{test}}$, where
    $C_{\mathrm{train}}\cap C_{\mathrm{test}}=\varnothing$. Each few-shot task is
    denoted by an episode, which is instantiated as a $N$-way-$K$-shot task with a
    few query samples and support samples: The query samples form a query set $\mathcal{Q}=\{(x_{i}^{\mathcal{Q}},y_{i}^{\mathcal{Q}}))\}^{N_{q}=N\times
    Q}_{i=1}$ containing $N$ classes in $C_{\mathrm{train}}$ with $Q$ samples of each
    class, and the support samples forms a support set $\mathcal{S}=\{(x_{i}^{\mathcal{S}},y_{i}^{\mathcal{S}}))\}^{N_{s}=N\times
    K}_{i=1}$ containing the same $N$ classes in $C_{\mathrm{train}}$ with $K$ examples
    of each class. The goal of the $N$-way-$K$-shot learning is to train a model $F(x^{\mathcal{Q}},{\mathcal{S}})$
    that predicts the label distribution $H$ for any query point cloud $x^{\mathcal{Q}}$
    based on ${\mathcal{S}}$. In testing, the trained model is tested over the testing
    episodes ${\mathcal{V}}={(S_{j},Q_{j})}^{J}_{j=1}$ for the new classes $C_{\mathrm{test}}$.
    Note the ground-truth labels $y^{\mathcal{Q}}$ of query samples are only available
    during training.'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $N$-way-$K$-shot。设 $(x,y)$ 表示点云 $x$ 及其标签 $y$。FSL 旨在在从具有训练类别集 $C_{\mathrm{train}}$
    的数据集中抽取的一组少样本任务上训练模型，并在从具有新类别 $C_{\mathrm{test}}$ 的数据集中抽取的另一组任务上测试训练后的模型，其中 $C_{\mathrm{train}}\cap
    C_{\mathrm{test}}=\varnothing$。每个少样本任务由一个 episode 表示，实例化为一个 $N$-way-$K$-shot 任务，具有少量查询样本和支持样本：查询样本形成查询集
    $\mathcal{Q}=\{(x_{i}^{\mathcal{Q}},y_{i}^{\mathcal{Q}}))\}^{N_{q}=N\times Q}_{i=1}$，包含
    $C_{\mathrm{train}}$ 中的 $N$ 类，每类有 $Q$ 个样本，支持样本形成支持集 $\mathcal{S}=\{(x_{i}^{\mathcal{S}},y_{i}^{\mathcal{S}}))\}^{N_{s}=N\times
    K}_{i=1}$，包含 $C_{\mathrm{train}}$ 中相同的 $N$ 类，每类有 $K$ 个例子。$N$-way-$K$-shot 学习的目标是训练一个模型
    $F(x^{\mathcal{Q}},{\mathcal{S}})$，该模型基于 ${\mathcal{S}}$ 预测任何查询点云 $x^{\mathcal{Q}}$
    的标签分布 $H$。在测试时，训练后的模型在测试 episode ${\mathcal{V}}={(S_{j},Q_{j})}^{J}_{j=1}$ 上对新类别
    $C_{\mathrm{test}}$ 进行测试。注意，查询样本的真实标签 $y^{\mathcal{Q}}$ 仅在训练期间可用。
- en: •
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Generalized FSL. This is a more challenging FSL setting. It involves training
    data of base classes and novel classes, including abundant labelled data of the
    base classes and few-shot labelled samples of the novel classes. The goal is to
    obtain a few-shot model that can learn to recognize novel objects by leveraging
    knowledge learnt from the base classes.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 泛化 FSL。这是一个更具挑战性的 FSL 设置。它涉及到基础类别和新类别的训练数据，包括基础类别的丰富标记数据和新类别的少量标记样本。目标是获得一个少样本模型，通过利用从基础类别学习到的知识来识别新对象。
- en: 'FSL has been widely studied for 2D images. Most work performs meta-learning
    in three typical approaches: 1) Metric learning [[171](#bib.bib171), [172](#bib.bib172)]
    that measures the support-query similarity and group each query sample into its
    nearest support class in the latent space; 2) Optimization approach [[173](#bib.bib173),
    [174](#bib.bib174)] that differentiates support-set optimization for fast adaptation;
    3) Model-based approach [[175](#bib.bib175), [176](#bib.bib176)] that tailors
    model architectures for fast learning. We review 3D FSL for point clouds, a far
    under-explored area due to many modal-specific challenges such as unordered data
    structures, point sparsity, and large geometric variations. Several pioneering
    studies recently explored different 3D FSL tasks on 3D shape classification [[135](#bib.bib135),
    [136](#bib.bib136)], 3D semantic segmentation [[138](#bib.bib138)], 3D object
    detection [[140](#bib.bib140)], and 3D instance segmentation [[141](#bib.bib141)].'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: FSL 已广泛研究用于 2D 图像。大多数工作采用三种典型方法进行元学习：1) 度量学习 [[171](#bib.bib171), [172](#bib.bib172)]，衡量支持-查询相似度，并将每个查询样本分组到潜在空间中最接近的支持类别中；2)
    优化方法 [[173](#bib.bib173), [174](#bib.bib174)]，区分支持集优化以实现快速适应；3) 基于模型的方法 [[175](#bib.bib175),
    [176](#bib.bib176)]，定制模型架构以实现快速学习。我们回顾了点云的 3D FSL，这是一个研究较少的领域，因其面临诸如无序数据结构、点稀疏性和大几何变异等许多模式特定挑战。最近几项开创性研究探索了不同的
    3D FSL 任务，如 3D 形状分类 [[135](#bib.bib135), [136](#bib.bib136)]，3D 语义分割 [[138](#bib.bib138)]，3D
    对象检测 [[140](#bib.bib140)] 和 3D 实例分割 [[141](#bib.bib141)]。
- en: FSL for 3D shape classification. Ye et al. [[135](#bib.bib135), [136](#bib.bib136)]
    conducted a pioneering study on FSL for 3D shape classification under the $N$-way-$K$-shot
    setting. They extended existing 2D FSL methods for 3D point-cloud data and introduced
    a baseline method to deal with high intra-class variance and subtle inter-class
    differences in point cloud representations. Yang et al. [[137](#bib.bib137)] projected
    point clouds into depth images and explored cross-modal FSL for 3D shape classification.
    In addition, Chowdhury et al. [[142](#bib.bib142)] studied few-shot class-incremental
    learning that incrementally fine-tunes a trained model (on base classes) for novel
    classes with few samples.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 用于3D形状分类的FSL。Ye等人[[135](#bib.bib135), [136](#bib.bib136)] 在$N$-way-$K$-shot设置下对3D形状分类的FSL进行了开创性研究。他们扩展了现有的2D
    FSL方法，以适用于3D点云数据，并引入了一种基线方法来处理点云表示中的高类内差异和细微的类间差异。Yang等人[[137](#bib.bib137)] 将点云投影到深度图像中，并探索了用于3D形状分类的跨模态FSL。此外，Chowdhury等人[[142](#bib.bib142)]
    研究了少样本类别增量学习，该方法逐步微调训练好的模型（基于基础类别），以处理具有少量样本的新类别。
- en: FSL for 3D Segmentation. Zhao et al. [[138](#bib.bib138)] explored FSL for 3D
    semantic segmentation under the $N$-way-$K$-shot setting. They distill discriminative
    knowledge from scarce support that can effectively represent the distributions
    of novel classes and leverage such knowledge for 3D semantic segmentation. Ngo
    et al. [[141](#bib.bib141)] proposed a geodesic-guided transformer for 3D few-shot
    instance segmentation of indoor dense point clouds. They employed a few support
    point cloud scenes and their ground-truth masks to generate discriminative features
    for instance mask prediction, and utilized geodesic distance as guidance with
    improved segmentation.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 用于3D分割的FSL。Zhao等人[[138](#bib.bib138)] 在$N$-way-$K$-shot设置下探索了3D语义分割的FSL。他们从稀缺的支持中提炼出具有区分性的知识，这些知识能够有效地表示新类别的分布，并利用这些知识进行3D语义分割。Ngo等人[[141](#bib.bib141)]
    提出了用于室内稠密点云的3D少样本实例分割的测地线引导变换器。他们利用少量支持点云场景及其真实掩膜生成区分性特征用于实例掩膜预测，并利用测地线距离作为指导来提高分割效果。
- en: FSL for 3D object detection. FSL-based 3D detection is far under-explored. Zhao
    et al. [[140](#bib.bib140)] designed Prototypical VoteNet, the first 3D detector
    for generalized FSL. They introduced a class-agnostic 3D primitive memory bank
    to store geometric prototypes of base classes and designed multi-head cross-attention
    to associate the geometric prototypes with scene points for better feature representations.
    Similar to 3D FSL segmentation, the study only covers indoor dense point clouds
    with dense representations.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 用于3D物体检测的FSL。基于FSL的3D检测尚未得到充分探索。Zhao等人[[140](#bib.bib140)] 设计了Prototypical VoteNet，这是第一个用于广义FSL的3D检测器。他们引入了一个类别无关的3D原型记忆库，用于存储基础类别的几何原型，并设计了多头交叉注意力，将几何原型与场景点关联，以便更好地表示特征。类似于3D
    FSL分割，该研究仅涵盖了具有密集表示的室内稠密点云。
- en: 5.2 Inexact supervision
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 不精确监督
- en: The term “inexact supervision” refers to supervision that is not as precise
    as desired for specific tasks. One example is coarse-grained labels that are much
    easier to collect.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: “不精确监督”一词指的是监督不如期望的那样精确，适用于特定任务。一个例子是粗粒度标签，这些标签更容易收集。
- en: 3D semantic segmentation. Different weak supervision has been explored to save
    expensive point-wise annotation. For instance, Wei et al.[[143](#bib.bib143),
    [177](#bib.bib177)] employed subcloud-level labels for point cloud parsing, where
    classes appearing in the neighbourhood of uniformly sampled seeds are used as
    labels. Unal et al.[[145](#bib.bib145)] used scribbles as labels for LiDAR point
    clouds as shown in Fig. [11](#S5.F11 "Figure 11 ‣ 5.2 Inexact supervision ‣ 5
    Weakly-supervised learning ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds"), greatly facilitating the efficiency of point cloud labelling greatly.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 3D语义分割。不同的弱监督方法已被探索以节省昂贵的点位注释。例如，Wei等人[[143](#bib.bib143), [177](#bib.bib177)]
    采用了子云级标签进行点云解析，其中在均匀采样的种子邻域中出现的类别被用作标签。Unal等人[[145](#bib.bib145)] 使用了草图作为LiDAR点云的标签，如图[11](#S5.F11
    "图11 ‣ 5.2 不精确监督 ‣ 5 弱监督学习 ‣ 关于3D点云的标签高效深度学习的调查")所示，大大提高了点云标注的效率。
- en: '![Refer to caption](img/79afe099a3d57d76afebf25c8664a648.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/79afe099a3d57d76afebf25c8664a648.png)'
- en: 'Figure 11: Example of scribble-annotated LiDAR point cloud scenes (left) and
    superimposed frames (right) in ScribbleKITTI [[145](#bib.bib145)]. The figure
    is extracted from [[145](#bib.bib145)].'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：ScribbleKITTI[[145](#bib.bib145)]中的涂鸦标注 LiDAR 点云场景（左）和叠加帧（右）的示例。图像摘自[[145](#bib.bib145)]。
- en: 3D object detection. Several recent studies used position-level annotations
    instead of 3D bounding boxes for 3D detection. For example, Meng et al. [[146](#bib.bib146)]
    and Xu et al. [[148](#bib.bib148)] used object centers to provide coarse position
    information for training. Ren et al. [[144](#bib.bib144)] employed scene-level
    tags for point cloud segmentation and detection without involving any point-wise
    semantic labels or object locations. Beyond 3D weak annotations, several studies
    exploited 2D image classes [[153](#bib.bib153)] or 2D bounding boxes [[151](#bib.bib151),
    [152](#bib.bib152)] to guide the training of 3D detectors. It reduces the annotation
    cost significantly as 2D annotations are much easier to collect.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 物体检测。最近的几项研究使用了位置级注释，而不是 3D 边界框来进行 3D 检测。例如，孟等人[[146](#bib.bib146)]和徐等人[[148](#bib.bib148)]使用了物体中心来提供粗略的位置信息进行训练。任等人[[144](#bib.bib144)]采用了场景级标签进行点云分割和检测，而没有涉及任何逐点语义标签或物体位置。除了
    3D 弱注释外，几项研究还利用了 2D 图像类别[[153](#bib.bib153)]或 2D 边界框[[151](#bib.bib151), [152](#bib.bib152)]来指导
    3D 检测器的训练。这显著减少了标注成本，因为 2D 注释更容易收集。
- en: 3D instance segmentation. Another line of research [[149](#bib.bib149), [150](#bib.bib150)]
    exploited coarse 3D bounding boxes to train 3D instance segmentation networks
    as illustrated in Fig. [12](#S5.F12 "Figure 12 ‣ 5.2 Inexact supervision ‣ 5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"). To
    address the inaccuracy of 3D bounding boxes, Liao et al. [[149](#bib.bib149)]
    iteratively refined the bounding boxes and performed point-wise instance segmentation
    with the refined bounding boxes. Differently, Chibane et al. [[150](#bib.bib150)]
    introduced Box2Mask that adopts Hough Voting to generate accurate instance segmentation
    masks from 3D bounding boxes. These studies show promising 3D instance segmentation
    performance under weak supervision signals.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 实例分割。另一类研究[[149](#bib.bib149), [150](#bib.bib150)]利用粗略的 3D 边界框来训练 3D 实例分割网络，如图[12](#S5.F12
    "图 12 ‣ 5.2 不精确的监督 ‣ 5 弱监督学习 ‣ 3D 点云标签高效深度学习调研")所示。为了解决 3D 边界框的准确性问题，廖等人[[149](#bib.bib149)]对边界框进行了迭代精细化，并使用精细化的边界框进行逐点实例分割。不同的是，Chibane
    等人[[150](#bib.bib150)]引入了 Box2Mask，该方法采用 Hough 投票从 3D 边界框生成准确的实例分割掩码。这些研究显示了在弱监督信号下的有希望的
    3D 实例分割性能。
- en: '![Refer to caption](img/73d4ba2b53d2a590e6981670484bd1dc.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/73d4ba2b53d2a590e6981670484bd1dc.png)'
- en: 'Figure 12: Typical pipeline of training 3D segmentation networks with inexact
    supervision of 3D bounding boxes.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：训练 3D 分割网络的典型流程图，使用不精确的 3D 边界框监督。
- en: 5.3 Inaccurate supervision
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 不准确的监督
- en: Inaccurate supervision refers to annotations that are noisy with false labels.
    It’s very common as human annotators cannot guarantee 100% accuracy especially
    when only limited time and resources are available. The noisy labels provide wrong
    guidance and often hinder network training. Therefore, the key to learning with
    inaccurate supervision is to refine annotations and improve supervision quality
    as illustrated in Fig. [13](#S5.F13 "Figure 13 ‣ 5.3 Inaccurate supervision ‣
    5 Weakly-supervised learning ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds").
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 不准确的监督指的是标注中存在噪声和错误标签的情况。这很常见，因为人工标注者无法保证 100% 的准确性，特别是在时间和资源有限的情况下。噪声标签提供了错误的指导，并且常常阻碍网络训练。因此，使用不准确监督的关键在于细化标注和提高监督质量，如图[13](#S5.F13
    "图 13 ‣ 5.3 不准确的监督 ‣ 5 弱监督学习 ‣ 3D 点云标签高效深度学习调研")所示。
- en: '![Refer to caption](img/5307bc858f04b22a33c13464b4c95b79.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5307bc858f04b22a33c13464b4c95b79.png)'
- en: 'Figure 13: Typical pipeline for training 3D networks with inaccurate supervision'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：使用不准确监督训练 3D 网络的典型流程图。
- en: Despite the high research and application value, robust point cloud learning
    from inaccurate supervision is largely neglected in the literature. Ye et al. [[154](#bib.bib154),
    [178](#bib.bib178)] designed a 3D semantic segmentation framework that introduces
    point-level confidence mechanism to select reliable labels and a cluster-level
    label correction process to refine training data. More research is needed to advance
    this very useful but far under-explored research area.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管具有很高的研究和应用价值，但从不准确监督中进行的强健点云学习在文献中基本被忽视。Ye 等人 [[154](#bib.bib154), [178](#bib.bib178)]
    设计了一个 3D 语义分割框架，引入了点级置信机制以选择可靠标签，并且通过集群级标签修正过程来优化训练数据。需要更多研究来推进这一非常有用但尚未充分探索的研究领域。
- en: 5.4 Summary
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 总结
- en: Weakly supervised point cloud learning is an emerging area that has attracted
    increasing attention in recent years. It aims to train robust deep network models
    with partially available, noisy, or not exactly desired annotations. Preparing
    precise supervision is very challenging for unordered and unstructured point clouds,
    which has recently triggered great progress in weakly supervised point cloud learning.
    Some work shows that weakly supervised models can even achieve competitive performance
    as fully supervised models. In addition, researchers have explored combining weakly
    supervised learning with other techniques such as transfer learning and self-supervised
    learning for better point cloud modelling. Their studies demonstrate the potential
    of weakly supervised learning for point clouds and a broader field of 3D deep
    learning.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督点云学习是一个新兴领域，近年来引起了越来越多的关注。它旨在训练具有部分可用、噪声或不完全理想注释的强健深度网络模型。为无序和非结构化点云准备精确的监督非常具有挑战性，这最近催生了弱监督点云学习的重大进展。一些研究表明，弱监督模型甚至可以实现与完全监督模型相媲美的性能。此外，研究人员还探索了将弱监督学习与转移学习和自监督学习等其他技术相结合，以更好地进行点云建模。他们的研究展示了弱监督学习在点云及更广泛的
    3D 深度学习领域的潜力。
- en: Overall, the progress in weakly supervised point cloud learning is ongoing,
    and new methods have been proposed and evaluated regularly. This research field
    is expected to continue to grow and evolve in the coming years as researchers
    continue to explore new ways to leverage weakly supervised learning for point
    cloud understanding.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，弱监督点云学习的进展仍在进行中，新的方法也在定期提出和评估。预计这一研究领域在未来几年将继续增长和发展，因为研究人员不断探索利用弱监督学习进行点云理解的新方法。
- en: 6 Pretrained Foundation Models
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 预训练基础模型
- en: 'TABLE V: Categorization of point cloud studies with pretrained foundation models.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：带有预训练基础模型的点云研究分类。
- en: '| Pretraining type | Approach | References |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 预训练类型 | 方法 | 参考文献 |'
- en: '| --- | --- | --- |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Self-supervised pretraining (§[6.1](#S6.SS1 "6.1 Self-supervised pretraining
    ‣ 6 Pretrained Foundation Models ‣ A Survey of Label-Efficient Deep Learning for
    3D Point Clouds")) | Contrastive-learning-based methods | Please refer to [[2](#bib.bib2)]
    for a systematic survey |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 自监督预训练 (§[6.1](#S6.SS1 "6.1 Self-supervised pretraining ‣ 6 Pretrained Foundation
    Models ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) | 对比学习方法
    | 请参考 [[2](#bib.bib2)] 进行系统调查 |'
- en: '| Generation-based methods |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 基于生成的方法 |'
- en: '| Multi-modal pretraining (§[6.2](#S6.SS2 "6.2 Multi-modal pretraining ‣ 6
    Pretrained Foundation Models ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds")) | Transferring knowledge from existing vision-language foundation
    models to point cloud models (§[6.2](#S6.SS2 "6.2 Multi-modal pretraining ‣ 6
    Pretrained Foundation Models ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds")) | [[179](#bib.bib179), [180](#bib.bib180), [181](#bib.bib181),
    [182](#bib.bib182)] |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 多模态预训练 (§[6.2](#S6.SS2 "6.2 Multi-modal pretraining ‣ 6 Pretrained Foundation
    Models ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) | 将现有的视觉-语言基础模型的知识迁移到点云模型
    (§[6.2](#S6.SS2 "6.2 Multi-modal pretraining ‣ 6 Pretrained Foundation Models
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) | [[179](#bib.bib179),
    [180](#bib.bib180), [181](#bib.bib181), [182](#bib.bib182)] |'
- en: '| Extending the image-text pretraining paradigm to point cloud-text pretraining
    (§[6.2.2](#S6.SS2.SSS2 "6.2.2 Language-point cloud pretraining ‣ 6.2 Multi-modal
    pretraining ‣ 6 Pretrained Foundation Models ‣ A Survey of Label-Efficient Deep
    Learning for 3D Point Clouds")) | [[183](#bib.bib183), [184](#bib.bib184), [185](#bib.bib185),
    [186](#bib.bib186), [187](#bib.bib187)] |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 将图像-文本预训练范式扩展到点云-文本预训练 (§[6.2.2](#S6.SS2.SSS2 "6.2.2 语言-点云预训练 ‣ 6.2 多模态预训练
    ‣ 6 预训练基础模型 ‣ 3D 点云的标签高效深度学习综述")) | [[183](#bib.bib183), [184](#bib.bib184), [185](#bib.bib185),
    [186](#bib.bib186), [187](#bib.bib187)] |'
- en: The recent advance of pretrained foundation models (PFMs) has yielded great
    breakthroughs across various AI fields including 2D computer vision, NLP, and
    their intersection (i.e., vision-language foundation models (VLMs) [[188](#bib.bib188),
    [189](#bib.bib189)]). PFMs with Internet-scale data in an unsupervised manner [[190](#bib.bib190),
    [191](#bib.bib191), [192](#bib.bib192)] can be easily adapted to downstream tasks
    by fine-tuning with much fewer task data, enabling fast network convergence and
    learning with small data. In addition, VLMs [[193](#bib.bib193)] trained with
    image-text pairs have demonstrated remarkable zero-shot visual prediction performance,
    being able to recognize objects of novel concepts with impressive accuracy without
    involving any labelled images but only text illustrations in training.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练基础模型（PFMs）的最新进展在包括 2D 计算机视觉、自然语言处理（NLP）及其交集（即视觉-语言基础模型（VLMs）[[188](#bib.bib188),
    [189](#bib.bib189)]）等各种 AI 领域取得了重大突破。具有互联网规模数据的 PFMs 以无监督方式 [[190](#bib.bib190),
    [191](#bib.bib191), [192](#bib.bib192)] 可通过少量任务数据的微调轻松适应下游任务，实现快速网络收敛和小数据学习。此外，使用图像-文本对训练的
    VLMs [[193](#bib.bib193)] 在零样本视觉预测性能上表现出色，能够在没有任何标记图像的情况下，仅凭文本描述准确识别新概念的对象。
- en: The great success of PFMs sheds light on label-efficient learning for point
    clouds. This section reviews related 3D studies with self-supervised pretraining
    in Section [6.1](#S6.SS1 "6.1 Self-supervised pretraining ‣ 6 Pretrained Foundation
    Models ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") and multi-modal
    pretraining in Section [6.2](#S6.SS2 "6.2 Multi-modal pretraining ‣ 6 Pretrained
    Foundation Models ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds").
    Relevant challenges are finally discussed in Section [6.3](#S6.SS3 "6.3 Summary
    and discussion ‣ 6 Pretrained Foundation Models ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"). Table [V](#S6.T5 "TABLE V ‣ 6 Pretrained
    Foundation Models ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")
    shows a summary of representative approaches.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: PFMs 的巨大成功为点云的标签高效学习提供了启示。本节回顾了与 3D 研究相关的自监督预训练（第 [6.1](#S6.SS1 "6.1 自监督预训练
    ‣ 6 预训练基础模型 ‣ 3D 点云的标签高效深度学习综述") 节）和多模态预训练（第 [6.2](#S6.SS2 "6.2 多模态预训练 ‣ 6 预训练基础模型
    ‣ 3D 点云的标签高效深度学习综述") 节）。相关挑战最终在第 [6.3](#S6.SS3 "6.3 总结与讨论 ‣ 6 预训练基础模型 ‣ 3D 点云的标签高效深度学习综述")
    节讨论。表 [V](#S6.T5 "TABLE V ‣ 6 预训练基础模型 ‣ 3D 点云的标签高效深度学习综述") 显示了代表性方法的总结。
- en: 6.1 Self-supervised pretraining
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 自监督预训练
- en: '![Refer to caption](img/bed1882bf349302139a4f4bd5cddcb90.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bed1882bf349302139a4f4bd5cddcb90.png)'
- en: 'Figure 14: Typical pipeline for self-supervised pertaining. The figure is extracted
    from [[2](#bib.bib2)].'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：自监督预训练的典型流程。图示摘自 [[2](#bib.bib2)]。
- en: Self-supervised pretraining learns from large-scale unlabelled point clouds,
    and the learnt parameters can be applied to initialize downstream networks for
    faster convergence and effective learning from small task data, as illustrated
    in Fig. [14](#S6.F14 "Figure 14 ‣ 6.1 Self-supervised pretraining ‣ 6 Pretrained
    Foundation Models ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds").
    It has attracted increasing interest as it can work with no human annotations.
    A milestone is PointContrast [[56](#bib.bib56)] which learns network weights from
    3D scene frames and fine-tunes networks on multiple high-level 3D tasks such as
    semantic segmentation and object detection. However, the performance gains of
    self-supervised pretraining remain limited compared with 2D image and NLP pretraining.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督预训练从大规模未标记的点云中学习，学习到的参数可以用于初始化下游网络，以便在小规模任务数据上更快收敛和有效学习，如图 [14](#S6.F14 "Figure
    14 ‣ 6.1 Self-supervised pretraining ‣ 6 Pretrained Foundation Models ‣ A Survey
    of Label-Efficient Deep Learning for 3D Point Clouds") 所示。由于可以在没有人工注释的情况下工作，它引起了越来越多的关注。一个里程碑是
    PointContrast [[56](#bib.bib56)]，它从 3D 场景帧中学习网络权重，并在多个高层次的 3D 任务上微调网络，如语义分割和目标检测。然而，与
    2D 图像和 NLP 预训练相比，自监督预训练的性能提升仍然有限。
- en: 'Most existing studies tackle point cloud pretraining via two approaches: contrastive
    pretraining and generative pretraining. Contrastive pretraining [[56](#bib.bib56),
    [194](#bib.bib194), [195](#bib.bib195), [196](#bib.bib196)] adopts a discriminative
    approach and it learns by maximizing the similarity of positive pairs (different
    augmentations of the same sample or different views of the same scene) while minimizing
    similarity between negative pairs (different samples). This enhances the network’s
    ability to distinguish between similar and dissimilar examples, leading to improved
    generalization performance. Differently, generative pretraining [[197](#bib.bib197),
    [198](#bib.bib198)] learns to generate new point-cloud samples with similar input
    distribution. The learned model captures representative features of input data
    which can be fine-tuned for downstream tasks. Recently, Xiao et al. [[2](#bib.bib2)]
    performed a comprehensive survey of self-supervised learning for point clouds.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的大多数研究通过两种方法处理点云预训练：对比预训练和生成预训练。对比预训练 [[56](#bib.bib56), [194](#bib.bib194),
    [195](#bib.bib195), [196](#bib.bib196)] 采用了区分性的方法，它通过最大化正样本对的相似性（同一样本的不同增强或同一场景的不同视图），同时最小化负样本对的相似性（不同样本）来进行学习。这增强了网络区分相似和不相似例子的能力，从而提高了泛化性能。不同于此，生成预训练
    [[197](#bib.bib197), [198](#bib.bib198)] 学习生成具有相似输入分布的新点云样本。学习到的模型捕捉了输入数据的代表性特征，可以针对下游任务进行微调。最近，Xiao
    等人 [[2](#bib.bib2)] 对点云自监督学习进行了全面调查。
- en: 6.2 Multi-modal pretraining
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 多模态预训练
- en: 'Unlike self-supervised pretraining that learns from massive unlabeled data,
    VLMs are pre-trained with image-text pairs crawled from the Internet. The objective
    is to train a model to understand the relationships between images and their corresponding
    textual descriptions. Its remarkable zero-/few-shot recognition ability has inspired
    several studies on multi-modal point cloud pretraining. Two typical approaches
    have been explored: (1) transferring knowledge from existing VLMs to point cloud
    models and (2) extending the image-text pretraining paradigm for point cloud-text
    pretraining.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 与从大量未标记数据中学习的自监督预训练不同，VLMs 是通过从互联网上抓取的图像-文本对进行预训练的。目标是训练一个模型来理解图像与其对应文本描述之间的关系。其卓越的零样本/少样本识别能力激发了对多模态点云预训练的若干研究。已经探索了两种典型的方法：（1）将现有
    VLMs 的知识迁移到点云模型上，和（2）将图像-文本预训练范式扩展到点云-文本预训练。
- en: 6.2.1 From language-vision to point cloud
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 从语言视觉到点云
- en: VLMs are trained on billions of images with semantic-rich captions. Though they
    advanced open-vocabulary image understanding tasks greatly, they are not directly
    applicable in the 3D domain due to the lack of large-scale 3D-text pairs. One
    line of research aims to leverage the knowledge in VLMs to aid point cloud learning.
    The primary approach relies on images paired with point clouds as a bridge for
    knowledge distillation across modalities.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: VLMs 在大量的图像和语义丰富的标题上进行训练。尽管它们在开放词汇图像理解任务中取得了显著进展，但由于缺乏大规模的3D文本对，它们在3D领域并不直接适用。一个研究方向是利用
    VLMs 中的知识来辅助点云学习。主要方法依赖于将图像与点云配对，作为跨模态知识蒸馏的桥梁。
- en: Several pioneering studies [[179](#bib.bib179), [180](#bib.bib180), [199](#bib.bib199)]
    transfer CLIP [[193](#bib.bib193)] model for point cloud classification. For instance,
    Zhang et al. [[179](#bib.bib179)] generate scatter depth maps by projecting raw
    points onto pre-defined image planes, feed the depth maps to CLIP’s visual encoder
    to extract multi-view features, and obtain zero-shot predictions with a text-generated
    classifier. Liu et al.[[181](#bib.bib181)] render object-level point clouds into
    multi-view images of predefined camera poses for low-shot object part segmentation.
    The rendered images are fed to the pretrained GLIP[[200](#bib.bib200)] along with
    a text prompt for predicting bounding boxes from the text prompt. Wang et al. [[182](#bib.bib182)]
    exploit visual-linguistic assistance for 3D semantic scene graph prediction. The
    method projects point clouds into images and trains a multi-modal model to capture
    semantics from vision, language, and point clouds, and it adopts CLIP to align
    the visual-linguistic semantics.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 一些开创性的研究 [[179](#bib.bib179), [180](#bib.bib180), [199](#bib.bib199)] 将 CLIP
    [[193](#bib.bib193)] 模型应用于点云分类。例如，张等人 [[179](#bib.bib179)] 通过将原始点投影到预定义的图像平面上生成散点深度图，将深度图输入
    CLIP 的视觉编码器以提取多视角特征，并使用文本生成的分类器获得零-shot 预测。刘等人 [[181](#bib.bib181)] 将对象级点云渲染为预定义相机姿势的多视角图像，用于低-shot
    对象部件分割。渲染图像与文本提示一起输入预训练的 GLIP [[200](#bib.bib200)] 以从文本提示中预测边界框。王等人 [[182](#bib.bib182)]
    利用视觉-语言辅助进行 3D 语义场景图预测。该方法将点云投影到图像中，并训练一个多模态模型来捕捉来自视觉、语言和点云的语义，并采用 CLIP 对齐视觉-语言语义。
- en: 6.2.2 Language-point cloud pretraining
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 语言点云预训练
- en: Inspired by impressive performance of VLMs, researchers are now exploring the
    extension of the vision-language pretraining for point-cloud learning. However,
    collecting Internet-scale point-text samples is extremely difficult. To overcome
    this challenge, recent studies exploit VLMs to generate captions for image data
    that can be easily obtained and aligned with point clouds, producing an abundance
    of point-text pairs for pretraining. This approach allows learning rich and transferrable
    3D visual-semantic representations with little human annotations.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 VLMs（视觉语言模型）出色表现的启发，研究人员现在正在探索将视觉-语言预训练扩展到点云学习。然而，收集互联网规模的点-文本样本极其困难。为了解决这个挑战，最近的研究利用
    VLMs 为容易获得且与点云对齐的图像数据生成标题，从而产生大量的点-文本对用于预训练。这种方法允许在较少的人类注释下学习丰富且可迁移的 3D 视觉-语义表示。
- en: '![Refer to caption](img/e343ad76bdc8870d420af988a5c7cde9.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e343ad76bdc8870d420af988a5c7cde9.png)'
- en: 'Figure 15: One representative pipeline to leverage multi-view images of 3D
    scenes to access knowledge in vision-language foundation models, enabling language
    supervision without human annotation. The figure is modified from [[184](#bib.bib184)].'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：一种利用 3D 场景的多视角图像来访问视觉-语言基础模型中的知识的代表性流程，允许语言监督而无需人工注释。该图修改自 [[184](#bib.bib184)]。
- en: For example, Xue et al. [[183](#bib.bib183)] design cross-modal contrastive
    learning to learn a unified representation of images, texts, and point clouds
    for 3D shape classification. They adopt CLIP to generate training triplets to
    learn a 3D representation space that aligned with the image-text space. Zeng et
    al.[[186](#bib.bib186)] explore contrastive language-image-point pretraining for
    point cloud object recognition. They adopt DetCLIP[[201](#bib.bib201)] to extract
    image proposals given language captions, employ the proposals to parse corresponding
    point cloud instances, and conduct cross-modal contrastive pretraining to learn
    semantic-level language-3D alignment between texts and point clouds as well as
    instance-level image-3D alignment between images and point clouds.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，薛等人 [[183](#bib.bib183)] 设计了跨模态对比学习，以学习图像、文本和点云的统一表示用于 3D 形状分类。他们采用 CLIP
    生成训练三元组，以学习与图像-文本空间对齐的 3D 表示空间。曾等人 [[186](#bib.bib186)] 探索了用于点云对象识别的对比语言-图像-点云预训练。他们采用
    DetCLIP [[201](#bib.bib201)] 根据语言标题提取图像建议，使用这些建议解析相应的点云实例，并进行跨模态对比预训练，以学习文本与点云之间的语义级语言-3D
    对齐，以及图像与点云之间的实例级图像-3D 对齐。
- en: While most studies focus on object-level point cloud understanding, several
    studies [[184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187)] explore open-vocabulary
    scene understanding with VLM knowledge. They tackle different tasks such as 3D
    semantic segmentation, 3D object detection, and 3D instance segmentation, aiming
    to localize and recognize categories that are not present in the annotated label
    space. For example, Ding et al.[[184](#bib.bib184)], as shown in Fig. [15](#S6.F15
    "Figure 15 ‣ 6.2.2 Language-point cloud pretraining ‣ 6.2 Multi-modal pretraining
    ‣ 6 Pretrained Foundation Models ‣ A Survey of Label-Efficient Deep Learning for
    3D Point Clouds"), generate captions for images of 3D indoor scenes to create
    hierarchical point-caption pairs including scene-, view-, and entity-level captions.
    The pairs provide coarse-to-fine supervision signals and help learn appropriate
    3D visual-semantic representations with contrastive learning. With frozen text
    encoder in BERT[[192](#bib.bib192)] or CLIP, category embeddings can be extracted
    as text-embedded semantic classifier for recognition.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数研究集中在对象级点云理解上，但一些研究[[184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187)]
    探讨了使用 VLM 知识进行开放词汇场景理解。它们处理不同的任务，如 3D 语义分割、3D 对象检测和 3D 实例分割，旨在定位和识别在标注标签空间中不存在的类别。例如，丁等人[[184](#bib.bib184)]，如图[15](#S6.F15
    "Figure 15 ‣ 6.2.2 Language-point cloud pretraining ‣ 6.2 Multi-modal pretraining
    ‣ 6 Pretrained Foundation Models ‣ A Survey of Label-Efficient Deep Learning for
    3D Point Clouds")所示，为 3D 室内场景图像生成描述，创建了包括场景级、视图级和实体级描述在内的分层点-描述对。这些对提供了从粗到细的监督信号，并通过对比学习帮助学习适当的
    3D 视觉-语义表示。通过 BERT[[192](#bib.bib192)] 或 CLIP 中冻结的文本编码器，可以提取类别嵌入作为文本嵌入的语义分类器进行识别。
- en: Though [[184](#bib.bib184)] achieves promising results, it is often hindered
    by its coarse image-level inputs. Consequently, it largely identifies sparse and
    salient scene objects only, making it difficult for dense understanding tasks
    such as semantic and instance segmentation. Yang et al. [[185](#bib.bib185)] address
    this issue by introducing dense visual prompts that elicit region-level visual-language
    knowledge via captioning. The method allows creating dense regional point-language
    associations, enabling point-discriminative contrastive learning for point-independent
    learning from captions as well as better open-vocabulary scene understanding.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管[[184](#bib.bib184)]取得了有希望的结果，但它常常受到粗略图像级输入的限制。因此，它主要识别稀疏和显著的场景对象，使得在语义和实例分割等密集理解任务中表现困难。杨等人[[185](#bib.bib185)]
    通过引入密集视觉提示解决了这个问题，这些提示通过描述引出区域级视觉-语言知识。该方法允许创建密集的区域点-语言关联，实现点-独立的对比学习以及更好的开放词汇场景理解。
- en: 6.3 Summary and discussion
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 总结与讨论
- en: PFMs have shown great potential in aiding point cloud learning with much less
    human annotations. Despite several groundbreaking studies, this field of research
    remains under-unexplored, leaving plenty of opportunities for further investigation.
    Specifically, though self-supervised pretraining has shown great effectiveness
    in 2D computer vision and NLP, its effects on point cloud learning lag far behind
    with random initialization still dominating the field. This is largely attributed
    to the scarcity of large-scale point cloud data, and the absence of unified and
    generalizable point cloud backbone models also affects [[2](#bib.bib2)].
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: PFMs 在减少人工标注的情况下，已显示出在点云学习中的巨大潜力。尽管有几个突破性的研究，但这一研究领域仍然探索不足，留有大量进一步研究的机会。具体而言，尽管自监督预训练在
    2D 计算机视觉和 NLP 中显示出了极大的效果，但在点云学习中的效果仍远远落后，随机初始化仍主导该领域。这在很大程度上归因于大规模点云数据的稀缺，以及统一和通用的点云骨干模型的缺失也影响了[[2](#bib.bib2)]。
- en: In addition, though VLMs offer a viable solution for point cloud pretraining,
    the potential of pre-training language-point cloud foundation models remains largely
    untapped. One major challenge is the construction of internet-scale point-text
    pairs for pretraining, which is a daunting task as we cannot crawl infinite 3D
    data from the internet, not to mention point-text pairs. Leveraging existing VLMs
    helps a lot, but it still requires collecting massive pairs of point clouds and
    images with the latter serving as the bridge for knowledge transfer. Despite these
    challenges, this research direction is very promising, and more research is expected
    to fully explore its potential.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管VLMs为点云预训练提供了可行的解决方案，但预训练语言-点云基础模型的潜力仍然未得到充分挖掘。一个主要挑战是构建互联网规模的点-文本对用于预训练，这是一个艰巨的任务，因为我们无法从互联网爬取无限的3D数据，更不用说点-文本对了。利用现有的VLMs有很大帮助，但仍需要收集大量的点云和图像对，其中后者作为知识转移的桥梁。尽管面临这些挑战，这一研究方向非常有前景，预计会有更多研究充分探索其潜力。
- en: 7 Future direction
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 未来方向
- en: In this section, we will discuss several open problems and future directions
    in label-efficient learning for point clouds.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论点云标签高效学习中的几个开放问题和未来方向。
- en: 7.1 Data Challenges
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 数据挑战
- en: Efficient labelling pipelines and tools. Point-wise annotation is far more laborious
    than annotating images which largely explains the scarcity of large-scale point
    cloud datasets. Efficient annotation tools and automatic/semi-automatic annotation
    techniques have been attempted but their performance cannot meet the increasing
    demand of large-scale point cloud data. More efficient annotation tools and techniques
    are urgently needed for better exploitation of the very useful point cloud data.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 高效标注管道和工具。点位注释比图像注释费时得多，这在很大程度上解释了大规模点云数据集的稀缺。尽管已经尝试了高效的注释工具和自动/半自动注释技术，但它们的性能仍无法满足大规模点云数据日益增长的需求。需要更高效的注释工具和技术，以更好地利用非常有用的点云数据。
- en: Next-generation datasets. Most existing point cloud datasets including object
    datasets (e.g., ModelNet, ShapeNet, and ScanObjectNN), indoor datasets (e.g.,
    S3DIS, ScanNet, and SUN RGB-D), and outdoor datasets (e.g., KITTI, nuScenes, and
    SemanticKITTI) have very limited size and plateaued performance with prevalent
    deep networks. They also have limited diversity for robustness and generalization
    assessment. Hence, it is critical to introduce much larger and more diverse datasets
    to advance point cloud study further. This is well aligned with the recent advance
    in PFMs. Self-supervised pretraining requires a huge amount of point clouds to
    achieve desired pretraining effects. Similarly, learning multi-modal PFMs also
    requires large-scale multi-modal data to learn meaningful associations between
    point clouds and other data modalities.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 下一代数据集。大多数现有的点云数据集，包括对象数据集（例如ModelNet、ShapeNet和ScanObjectNN）、室内数据集（例如S3DIS、ScanNet和SUN
    RGB-D）以及室外数据集（例如KITTI、nuScenes和SemanticKITTI）都具有非常有限的规模，并且在流行深度网络下表现趋于平稳。它们在鲁棒性和泛化评估方面也缺乏多样性。因此，引入更大规模和更多样化的数据集对于进一步推进点云研究至关重要。这与PFMs的最新进展非常契合。自监督预训练需要大量的点云数据以达到期望的预训练效果。类似地，学习多模态PFMs也需要大规模的多模态数据，以学习点云和其他数据模态之间的有意义的关联。
- en: 7.2 Model Architecture
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 模型架构
- en: Uniformed architectures. Unified backbone structures facilitate knowledge transfer
    across datasets and tasks which are instrumental to the success of prior deep
    learning research [[202](#bib.bib202), [203](#bib.bib203)]. However, existing
    work on label-efficient point cloud learning employs very different deep architectures,
    which poses great challenges for benchmarking and integration as well as benefiting
    from PFMs. Designing highly efficient and unified deep architectures is a pressing
    issue with immense value for point cloud learning.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 统一架构。统一的骨干结构促进了数据集和任务之间的知识转移，这对先前深度学习研究的成功至关重要[[202](#bib.bib202), [203](#bib.bib203)]。然而，现有的标签高效点云学习工作采用了非常不同的深度架构，这给基准测试和集成带来了很大挑战，也不利于PFMs的利用。设计高效且统一的深度架构是一个紧迫的问题，对点云学习具有巨大价值。
- en: Label-efficient architectures. Another interesting research direction is label-efficient
    deep architectures that can achieve competitive performance with much fewer annotations.
    Several pioneering studies have been conducted, e.g.,  [[204](#bib.bib204)] for
    constructing light architectures via neural architecture search,  [[120](#bib.bib120)]
    for saving annotations by fully exploiting strong local semantic homogeneity of
    point neighbours, etc.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 标签高效的架构。另一个有趣的研究方向是标签高效的深度架构，这些架构可以在使用更少的注释情况下实现具有竞争力的性能。一些开创性的研究已经开展，例如，[[204](#bib.bib204)]
    通过神经架构搜索构建轻量级架构，[[120](#bib.bib120)] 通过充分利用点邻域的强局部语义一致性来节省注释等。
- en: 7.3 Label-efficient Learning Algorithms
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 标签高效学习算法
- en: Label-efficient learning has been rapidly evolving along different direction
    in data augmentation in Section [3](#S3 "3 Data Augmentation ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"), domain transfer learning in Section [4](#S4
    "4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds"), weakly-supervised learning in Section [5](#S5 "5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"), and
    pretrained foundation models in Section [6](#S6 "6 Pretrained Foundation Models
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"). Nevertheless,
    research in these areas remains limited as witnessed by the very few papers as
    reviewed in this survey as well as the very low performance in various benchmarks
    as listed in the appendix. At the other end, these gaps also pose great opportunities
    for future exploration, by either adapting relevant methods for other modalities
    or developing novel techniques tailored for point clouds. We expect more related
    studies in this emerging and very promising research field.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 标签高效学习在数据增强（见第[3](#S3 "3 Data Augmentation ‣ A Survey of Label-Efficient Deep
    Learning for 3D Point Clouds)））、领域迁移学习（见第[4](#S4 "4 Domain Transfer Learning ‣
    A Survey of Label-Efficient Deep Learning for 3D Point Clouds)））、弱监督学习（见第[5](#S5
    "5 Weakly-supervised learning ‣ A Survey of Label-Efficient Deep Learning for
    3D Point Clouds)））以及预训练基础模型（见第[6](#S6 "6 Pretrained Foundation Models ‣ A Survey
    of Label-Efficient Deep Learning for 3D Point Clouds)））等方向上快速发展。然而，正如本调查中审阅的少数论文以及附录中列出的各种基准测试的低性能所见，这些领域的研究仍然有限。另一方面，这些空白也为未来的探索提供了巨大的机会，无论是通过将相关方法适应于其他模态，还是开发专门针对点云的新技术。我们期望在这一新兴且极具前景的研究领域中会有更多相关研究。
- en: 8 Conclusion
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: Label-efficient learning of point clouds has been investigated extensively over
    the past decade, leading to plenty of work across different tasks. This survey
    presents three key points that are critical to research in this field. Firstly,
    we share the importance and urgency of label-efficient learning in point cloud
    processing, especially under the context of big data and resource constraints.
    Secondly, we review four representative label-efficient learning approaches, including
    data augmentation, domain transfer learning, weakly-supervised learning, and pretrained
    foundation models, as well as related studies that have achieved very promising
    outcomes but still have vast space for improvements. Lastly, we comprehensively
    discuss the progress made in this field and share the challenges and promising
    future research directions. We expect that this timely and up-to-date survey will
    inspire more useful studies to further advance this very meaningful research field.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 点云的标签高效学习在过去十年中得到了广泛研究，涉及了各种任务的大量工作。本调查提出了三个对这一领域研究至关重要的关键点。首先，我们分享了在大数据和资源限制背景下点云处理中的标签高效学习的重要性和紧迫性。其次，我们回顾了四种具有代表性的标签高效学习方法，包括数据增强、领域迁移学习、弱监督学习和预训练基础模型，以及相关研究，这些研究取得了非常有前景的成果，但仍有广阔的改进空间。最后，我们全面讨论了这一领域的进展，分享了挑战和有前途的未来研究方向。我们期望这一及时且最新的调查将激发更多有用的研究，进一步推动这一极具意义的研究领域。
- en: '[Additional Benchmark Performances]'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[附加基准性能]'
- en: This section presents the benchmarking of representative methods on various
    label-efficient learning tasks. To ensure evaluation fairness, we selected the
    widely adopted benchmarks for various 3D tasks and extracted the performance of
    compared methods directly from the corresponding papers.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了在各种标签高效学习任务上的代表性方法的基准测试。为了确保评估的公平性，我们选择了各种3D任务中广泛采用的基准，并直接从相关论文中提取了比较方法的性能。
- en: 'Data augmentation for 3D shape classification. Table [VI](#A0.T6 "TABLE VI
    ‣ 8 Conclusion ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")
    provides a summary of the DA effects of existing methods for 3D shape classification
    on both synthetic object dataset ModelNet40 [[11](#bib.bib11)] and real object
    dataset ScanObjectNN [[12](#bib.bib12)]. All methods used the same DGCNN [[205](#bib.bib205)]
    backbone. The results indicate that different DA methods have led to continuous
    improvements in overall accuracy for 3D shape classification. However, the augmentation
    effects are still limited. From these observations, two conclusions can be drawn:
    firstly, there is a need for further relevant studies on point cloud augmentation;
    and secondly, the performance of current benchmarks is saturated, indicating the
    need for larger datasets to benchmark more powerful DA methods in the future.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 形状分类的数据增强。表格 [VI](#A0.T6 "TABLE VI ‣ 8 Conclusion ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") 总结了现有方法在合成对象数据集 ModelNet40 [[11](#bib.bib11)]
    和真实对象数据集 ScanObjectNN [[12](#bib.bib12)] 上的 DA 效果。所有方法使用相同的 DGCNN [[205](#bib.bib205)]
    主干网。结果表明，不同的 DA 方法在 3D 形状分类的总体准确率上持续改进。然而，增强效果仍然有限。从这些观察结果中，可以得出两个结论：首先，需要进一步研究点云增强；其次，目前基准的性能已经饱和，表明未来需要更大的数据集来评估更强大的
    DA 方法。
- en: 'TABLE VI: Domain adaptive hape classification on ModelNet40 [[11](#bib.bib11)]
    and “OBJ_ONLY” split of ScanObjectNN [[12](#bib.bib12)]. All methods use the same
    DGCNN as backbone [[205](#bib.bib205)].'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：在 ModelNet40 [[11](#bib.bib11)] 和 ScanObjectNN [[12](#bib.bib12)] 的“OBJ_ONLY”划分上进行的领域自适应形状分类。所有方法使用相同的
    DGCNN 作为主干网 [[205](#bib.bib205)]。
- en: '| Method | Publication | ModelNet40 | ScanObjectNN |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 出版 | ModelNet40 | ScanObjectNN |'
- en: '| --- | --- | --- | --- |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| baseline [[205](#bib.bib205)] | TOG2019 | 92.2 | 86.2 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 基线 [[205](#bib.bib205)] | TOG2019 | 92.2 | 86.2 |'
- en: '| PointAugment [[28](#bib.bib28)] | CVPR2020 | 93.4 (+1.2) | 86.9 (+0.7) |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| PointAugment [[28](#bib.bib28)] | CVPR2020 | 93.4 (+1.2) | 86.9 (+0.7) |'
- en: '| PointMixup [[30](#bib.bib30)] | ECCV2020 | 93.1 (+0.2) | - |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| PointMixup [[30](#bib.bib30)] | ECCV2020 | 93.1 (+0.2) | - |'
- en: '| RSMix [[31](#bib.bib31)] | CVPR2021 | 93.5 (+0.7) | 86.6 (+0.4) |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| RSMix [[31](#bib.bib31)] | CVPR2021 | 93.5 (+0.7) | 86.6 (+0.4) |'
- en: '| PointWOLF [[29](#bib.bib29)] | ICCV2021 | 93.2 (+1.0) | 88.8 (+2.6) |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| PointWOLF [[29](#bib.bib29)] | ICCV2021 | 93.2 (+1.0) | 88.8 (+2.6) |'
- en: '| Point MixSwap [[33](#bib.bib33)] | ECCV2022 | 93.5 (+1.3) | - |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| Point MixSwap [[33](#bib.bib33)] | ECCV2022 | 93.5 (+1.3) | - |'
- en: '| SageMix [[34](#bib.bib34)] | NeurIPS2022 | 93.6 (+1.4) | 88.0 (+1.8) |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| SageMix [[34](#bib.bib34)] | NeurIPS2022 | 93.6 (+1.4) | 88.0 (+1.8) |'
- en: Data augmentation for 3D semantic segmentation. Table [VII](#A0.T7 "TABLE VII
    ‣ 8 Conclusion ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")
    shows state-of-the-art DA methods on 3D semantic segmentation on indoor dataset
    ScanNet [[9](#bib.bib9)] and outdoor LiDAR dataset SemanticKITTI [[10](#bib.bib10)].
    All compared methods use the same MinkowskiNet [[206](#bib.bib206)] backbone.
    We can see that different DA methods improve 3D semantic segmentation clearly
    and consistently. However, the improvements are still limited, indicating the
    urgent need for further studies on point cloud augmentation.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 语义分割的数据增强。表格 [VII](#A0.T7 "TABLE VII ‣ 8 Conclusion ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") 显示了在室内数据集 ScanNet [[9](#bib.bib9)] 和户外 LiDAR
    数据集 SemanticKITTI [[10](#bib.bib10)] 上的最新 DA 方法。所有对比方法使用相同的 MinkowskiNet [[206](#bib.bib206)]
    主干网。可以看到，不同的 DA 方法明显且一致地提高了 3D 语义分割的效果。然而，改进仍然有限，表明急需进一步研究点云增强。
- en: 'TABLE VII: Data augmentation effects for 3D semantic segmentation over indoor
    dataset ScanNet [[9](#bib.bib9)] and outdoor LiDAR dataset SemanticKITTI [[10](#bib.bib10)].'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：在室内数据集 ScanNet [[9](#bib.bib9)] 和户外 LiDAR 数据集 SemanticKITTI [[10](#bib.bib10)]
    上的 3D 语义分割数据增强效果。
- en: '| Method | Publication | ScanNet | SemanticKITTI |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 出版 | ScanNet | SemanticKITTI |'
- en: '| --- | --- | --- | --- |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Baseline [[206](#bib.bib206)] | - | 72.4 | 58.9 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 基线 [[206](#bib.bib206)] | - | 72.4 | 58.9 |'
- en: '| Mix3D [[41](#bib.bib41)] | 3DV 2021 | 73.6 (+1.2) | 62.4 (+3.5) |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| Mix3D [[41](#bib.bib41)] | 3DV 2021 | 73.6 (+1.2) | 62.4 (+3.5) |'
- en: '| PolarMix [[42](#bib.bib42)] | NeurIPS 2022 | - | 65.0 (+6.1) |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| PolarMix [[42](#bib.bib42)] | NeurIPS 2022 | - | 65.0 (+6.1) |'
- en: 'Domain adaptive 3D shape classification. PointDA-10 [[62](#bib.bib62)] is the
    most widely adopted dataset for benchmarking UDA studies on 3D shape classification.
    It was generated by selecting object samples from 10 classes that overlap across
    three point cloud datasets: ModelNet [[11](#bib.bib11)], ShapeNet [[8](#bib.bib8)],
    and ScanNet [[9](#bib.bib9)]. Table [VIII](#A0.T8 "TABLE VIII ‣ 8 Conclusion ‣
    A Survey of Label-Efficient Deep Learning for 3D Point Clouds") presents the performance
    of different UDA methods on this task. We can observe that due to geometric domain
    discrepancies, source-only models exhibit a significant drop in cross-domain classification
    accuracy across all datasets, as compared to ”Oracle” (i.e., supervised performance
    on the target domain). Moreover, larger performance drops can be seen when transitioning
    from synthetic point clouds (ModelNet or ShapeNet) to real point clouds (ScanNet),
    and vice versa. As a result, most methods focus on mitigating the domain discrepancies
    between synthetic and real domains, leading to a narrowing of the performance
    gaps.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 域自适应 3D 形状分类。PointDA-10 [[62](#bib.bib62)] 是用于 3D 形状分类 UDA 研究的最广泛采用的数据集。它通过从
    10 个类别中选择对象样本生成，这些类别在三个点云数据集中交叉重叠：ModelNet [[11](#bib.bib11)]、ShapeNet [[8](#bib.bib8)]
    和 ScanNet [[9](#bib.bib9)]。表[VIII](#A0.T8 "TABLE VIII ‣ 8 Conclusion ‣ A Survey
    of Label-Efficient Deep Learning for 3D Point Clouds")展示了不同 UDA 方法在该任务上的性能。我们可以观察到，由于几何领域差异，仅源模型在所有数据集上的跨域分类准确率显著下降，相比于
    “Oracle”（即目标领域的监督性能）。此外，从合成点云（ModelNet 或 ShapeNet）过渡到真实点云（ScanNet）时，以及反之亦然，性能下降更为明显。因此，大多数方法专注于减少合成域和真实域之间的领域差异，从而缩小性能差距。
- en: 'TABLE VIII: Domain adaptive 3D shape classification on dataset PointDA-10 [[62](#bib.bib62)]
    . ’S’: ShapeNet-10; S*: ScanNet-10; M: ModelNet-10\. All methods use the same
    backbone DGCNN [[205](#bib.bib205)].'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VIII: 数据集 PointDA-10 [[62](#bib.bib62)] 上的域自适应 3D 形状分类。’S’: ShapeNet-10;
    S*: ScanNet-10; M: ModelNet-10。所有方法使用相同的骨干网络 DGCNN [[205](#bib.bib205)]。'
- en: '| Method | M→S | M→S* | S→M | S→S* | S*→M | S*→S | Avg. |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | M→S | M→S* | S→M | S→S* | S*→M | S*→S | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Oracle | 93.9±0.2 | 78.4±0.6 | 96.2±0.1 | 78.4±0.6 | 96.2±0.1 | 93.9±0.2
    | 89.5 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Oracle | 93.9±0.2 | 78.4±0.6 | 96.2±0.1 | 78.4±0.6 | 96.2±0.1 | 93.9±0.2
    | 89.5 |'
- en: '| Source-only | 83.3±0.7 | 43.8±2.3 | 75.5±1.8 | 42.5±1.4 | 63.8±3.9 | 64.2±0.8
    | 62.2 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 仅源模型 | 83.3±0.7 | 43.8±2.3 | 75.5±1.8 | 42.5±1.4 | 63.8±3.9 | 64.2±0.8 |
    62.2 |'
- en: '| DANN [[207](#bib.bib207)] | 74.8±2.8 | 42.1±0.6 | 57.5±0.4 | 50.9±1.0 | 43.7±2.9
    | 71.6±1.0 | 56.8 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| DANN [[207](#bib.bib207)] | 74.8±2.8 | 42.1±0.6 | 57.5±0.4 | 50.9±1.0 | 43.7±2.9
    | 71.6±1.0 | 56.8 |'
- en: '| PointDAN[[62](#bib.bib62)] | 83.9±0.3 | 44.8±1.4 | 63.3±1.1 | 45.7±0.7 |
    43.6±2.0 | 56.4±1.5 | 56.3 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| PointDAN[[62](#bib.bib62)] | 83.9±0.3 | 44.8±1.4 | 63.3±1.1 | 45.7±0.7 |
    43.6±2.0 | 56.4±1.5 | 56.3 |'
- en: '| RS [[208](#bib.bib208)] | 79.9±0.8 | 46.7±4.8 | 75.2±2.0 | 51.4±3.9 | 71.8±2.3
    | 71.2±2.8 | 66.0 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| RS [[208](#bib.bib208)] | 79.9±0.8 | 46.7±4.8 | 75.2±2.0 | 51.4±3.9 | 71.8±2.3
    | 71.2±2.8 | 66.0 |'
- en: '| DefRec[[209](#bib.bib209)] | 81.7±0.6 | 51.8±0.3 | 78.6±0.7 | 54.5±0.3 |
    73.7±1.6 | 71.1±1.4 | 68.6 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| DefRec[[209](#bib.bib209)] | 81.7±0.6 | 51.8±0.3 | 78.6±0.7 | 54.5±0.3 |
    73.7±1.6 | 71.1±1.4 | 68.6 |'
- en: '| GAST [[63](#bib.bib63)] | 84.8±0.1 | 59.8±0.2 | 80.8±0.6 | 56.7±0.2 | 81.1±0.8
    | 74.9±0.5 | 73.0 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| GAST [[63](#bib.bib63)] | 84.8±0.1 | 59.8±0.2 | 80.8±0.6 | 56.7±0.2 | 81.1±0.8
    | 74.9±0.5 | 73.0 |'
- en: '| GLRV [[66](#bib.bib66)] | 85.4±0.4 | 60.4±0.4 | 78.8±0.6 | 57.7±0.4 | 77.8±1.1
    | 76.2±0.6 | 72.7 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| GLRV [[66](#bib.bib66)] | 85.4±0.4 | 60.4±0.4 | 78.8±0.6 | 57.7±0.4 | 77.8±1.1
    | 76.2±0.6 | 72.7 |'
- en: '| IPCDA [[64](#bib.bib64)] | 86.2±0.2 | 58.6±0.1 | 81.4±0.4 | 56.9±0.2 | 81.5±0.5
    | 74.4±0.6 | 73.2 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| IPCDA [[64](#bib.bib64)] | 86.2±0.2 | 58.6±0.1 | 81.4±0.4 | 56.9±0.2 | 81.5±0.5
    | 74.4±0.6 | 73.2 |'
- en: '| MLSP [[65](#bib.bib65)] | 86.2±0.8 | 59.1±0.9 | 83.5±0.4 | 57.6±0.6 | 81.2±0.4
    | 76.4±0.3 | 74.0 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| MLSP [[65](#bib.bib65)] | 86.2±0.8 | 59.1±0.9 | 83.5±0.4 | 57.6±0.6 | 81.2±0.4
    | 76.4±0.3 | 74.0 |'
- en: 'Domain adaptive 3D object detection. Table [IX](#A0.T9 "TABLE IX ‣ 8 Conclusion
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") summarizes the
    performance of different UDA methods for 3D object detection adapting from Waymo [[210](#bib.bib210)]
    to nuScenes [[16](#bib.bib16)]. Specifically, the performance metric is evaluated
    in 3D and the bird’s-eye view (BEV) focusing on the Car category and average precision
    (AP) with the IoU thresholds at 0.7 is reported: a car is correctly detected if
    the intersection over union (IoU) with the predicted 3D box is larger than 0.7\.
    AP for the 3D and BEV tasks are denoted as $\mathrm{AP_{3D}}$ and $\mathrm{AP_{BEV}}$,
    respectively.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 域自适应 3D 物体检测。表 [IX](#A0.T9 "TABLE IX ‣ 8 Conclusion ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") 总结了不同 UDA 方法在从 Waymo [[210](#bib.bib210)]
    适应到 nuScenes [[16](#bib.bib16)] 的 3D 物体检测中的性能。具体来说，性能指标在 3D 和鸟瞰图（BEV）上进行评估，重点关注汽车类别和平均精度（AP），IoU
    阈值为 0.7：如果与预测的 3D 盒子的交并比（IoU）大于 0.7，则可以正确检测到一辆车。3D 和 BEV 任务的 AP 分别表示为 $\mathrm{AP_{3D}}$
    和 $\mathrm{AP_{BEV}}$。
- en: We can see that various source-only trained 3D detectors, including SECOND [[38](#bib.bib38)],
    PV-RCNN [[211](#bib.bib211)], and PointPillars [[212](#bib.bib212)], achieve relatively
    low detection performance over target point clouds, showing the large domain discrepancy
    of the two datasets that are captured in different scenarios with different LiDAR
    sensors. Recent domain adaptation studies keep improving the target performance
    but the state-of-the-art performance is still unsaturated, indicating the great
    opportunities in this field. In addition, adapting BEV achieves clearly better
    adaptation effects than adapting 3D point clouds, which further shows the unique
    challenges and value of learning and adaptation for point clouds.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，各种仅源训练的 3D 检测器，包括 SECOND [[38](#bib.bib38)]、PV-RCNN [[211](#bib.bib211)]
    和 PointPillars [[212](#bib.bib212)]，在目标点云上的检测性能相对较低，显示了这两个数据集在不同场景和不同 LiDAR 传感器下捕获的领域差异。近期的领域适应研究不断提高目标性能，但最先进的性能仍未饱和，表明这个领域有很大的机会。此外，适应
    BEV 显然比适应 3D 点云取得了更好的适应效果，这进一步显示了点云学习和适应的独特挑战和价值。
- en: 'TABLE IX: Domain adaptive 3D object detection from Waymo [[210](#bib.bib210)]
    to nuScenes [[16](#bib.bib16)]. We report APBEV and AP3D over 40 recall positions
    of the car category at IoU = 0.7.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IX：从 Waymo [[210](#bib.bib210)] 到 nuScenes [[16](#bib.bib16)] 的领域自适应 3D 物体检测。我们报告了
    IoU = 0.7 时汽车类别的 APBEV 和 AP3D 在 40 个召回位置上的表现。
- en: '| Method | Year | SECOND [[38](#bib.bib38)] | PV-RCNN [[211](#bib.bib211)]
    | PointPillars [[212](#bib.bib212)] |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | SECOND [[38](#bib.bib38)] | PV-RCNN [[211](#bib.bib211)] | PointPillars [[212](#bib.bib212)]
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| $\mathrm{AP_{BEV}}$ | $\mathrm{AP_{3D}}$ | ${\mathrm{AP_{BEV}}}$ | $\mathrm{AP_{3D}}$
    | ${\mathrm{AP_{BEV}}}$ | ${\mathrm{AP_{3D}}}$ |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| $\mathrm{AP_{BEV}}$ | $\mathrm{AP_{3D}}$ | ${\mathrm{AP_{BEV}}}$ | $\mathrm{AP_{3D}}$
    | ${\mathrm{AP_{BEV}}}$ | ${\mathrm{AP_{3D}}}$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Source-only | - | 32.9 | 17.2 | 34.5 | 21.5 | 27.8 | 12.1 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 仅源 | - | 32.9 | 17.2 | 34.5 | 21.5 | 27.8 | 12.1 |'
- en: '| SN[[69](#bib.bib69)] | 2020 | 33.2 | 18.6 | 34.2 | 22.3 | 28.3 | 13.0 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| SN[[69](#bib.bib69)] | 2020 | 33.2 | 18.6 | 34.2 | 22.3 | 28.3 | 13.0 |'
- en: '| ST3D[[72](#bib.bib72)] | 2021 | 35.9 | 20.2 | 36.4 | 23.0 | 30.6 | 15.6 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| ST3D[[72](#bib.bib72)] | 2021 | 35.9 | 20.2 | 36.4 | 23.0 | 30.6 | 15.6 |'
- en: '| 3D-CoCo[[79](#bib.bib79)] | 2021 | - | - | - | - | 33.1 | 20.7 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 3D-CoCo[[79](#bib.bib79)] | 2021 | - | - | - | - | 33.1 | 20.7 |'
- en: '| LiDARDistill[[80](#bib.bib80)] | 2022 | 42.0 | 24.5 | 44.1 | 26.4 | 40.8
    | 21.0 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| LiDARDistill[[80](#bib.bib80)] | 2022 | 42.0 | 24.5 | 44.1 | 26.4 | 40.8
    | 21.0 |'
- en: '| DTS [[213](#bib.bib213)] | 2023 | 41.2 | 23.0 | 44.0 | 26.2 | 42.2 | 21.5
    |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| DTS [[213](#bib.bib213)] | 2023 | 41.2 | 23.0 | 44.0 | 26.2 | 42.2 | 21.5
    |'
- en: Domain adaptive 3D semantic segmentation. Table [X](#A0.T10 "TABLE X ‣ 8 Conclusion
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") shows the performance
    of different UDA methods on SynLiDAR [[22](#bib.bib22)] $\rightarrow$ SemanticKITTI [[10](#bib.bib10)].
    We can see that the synthetic and real point clouds have clear domain discrepancies,
    as indicated by the very low performance of the source-only model. The recent
    mixing-based PolarMix and CosMix [[42](#bib.bib42), [87](#bib.bib87)] show very
    promising results, but the state-of-the-art is still far lower than the oracle’s
    performance, showing the great opportunity in this research direction.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 领域自适应 3D 语义分割。表 [X](#A0.T10 "TABLE X ‣ 8 Conclusion ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") 显示了不同 UDA 方法在 SynLiDAR [[22](#bib.bib22)]
    $\rightarrow$ SemanticKITTI [[10](#bib.bib10)] 上的性能。我们可以看到，合成和真实点云之间存在明显的领域差异，源模型的性能非常低。最近的基于混合的方法
    PolarMix 和 CosMix [[42](#bib.bib42), [87](#bib.bib87)] 显示了非常有希望的结果，但最先进的技术仍远低于理论最佳性能，显示了该研究方向的巨大潜力。
- en: 'TABLE X: Point cloud semantic segmentation on UDA task SynLiDAR [[22](#bib.bib22)]
    $\rightarrow$ SemanticKITTI [[10](#bib.bib10)]. All methods use MinkowskiNet [[206](#bib.bib206)]
    as the backbone segmentation model.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 表 X：点云语义分割在 UDA 任务 SynLiDAR [[22](#bib.bib22)] $\rightarrow$ SemanticKITTI [[10](#bib.bib10)]
    上。所有方法使用 MinkowskiNet [[206](#bib.bib206)] 作为骨干分割模型。
- en: '| Method | Publication | mIoU |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发表 | mIoU |'
- en: '| --- | --- | --- |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Source-only [[22](#bib.bib22)] | - | 20.4 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 仅源模型 [[22](#bib.bib22)] | - | 20.4 |'
- en: '| ADDA [[214](#bib.bib214)] | CVPR2017 | 23.0 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| ADDA [[214](#bib.bib214)] | CVPR2017 | 23.0 |'
- en: '| Ent-Min [[215](#bib.bib215)] | CVPR2019 | 25.8 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| Ent-Min [[215](#bib.bib215)] | CVPR2019 | 25.8 |'
- en: '| ST [[104](#bib.bib104)] | ECCV2018 | 26.5 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| ST [[104](#bib.bib104)] | ECCV2018 | 26.5 |'
- en: '| PCT [[22](#bib.bib22)] | AAAI2022 | 23.9 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| PCT [[22](#bib.bib22)] | AAAI2022 | 23.9 |'
- en: '| ST-PCT [[22](#bib.bib22)] | AAAI2022 | 28.9 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| ST-PCT [[22](#bib.bib22)] | AAAI2022 | 28.9 |'
- en: '| PolarMix [[42](#bib.bib42)] | NeurIPS2022 | 31.0 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| PolarMix [[42](#bib.bib42)] | NeurIPS2022 | 31.0 |'
- en: '| CoSMix [[87](#bib.bib87)] | ECCV2022 | 32.2 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| CoSMix [[87](#bib.bib87)] | ECCV2022 | 32.2 |'
- en: '| Oracle [[42](#bib.bib42)] | - | 65.0 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Oracle [[42](#bib.bib42)] | - | 65.0 |'
- en: Domain generalized point cloud classification. Table [XI](#A0.T11 "TABLE XI
    ‣ 8 Conclusion ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")
    summarizes the performance of state-of-the-art domain generalized classification
    methods from synthetic to real point clouds, including ModelNet40 [[11](#bib.bib11)]
    → ScanObjectNN [[107](#bib.bib107)] and ShapeNet [[8](#bib.bib8)] → ScanObjectNN [[107](#bib.bib107)].
    Due to large domain discrepancies between synthetic and real point clouds, typical
    point cloud networks PointNet [[25](#bib.bib25)] and DGCNN [[205](#bib.bib205)]
    perform worse for cross-domain classification. Recent DG methods [[94](#bib.bib94),
    [95](#bib.bib95)] achieve clear performance gains but there is still large room
    for further research.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 领域泛化点云分类。表 [XI](#A0.T11 "TABLE XI ‣ 8 Conclusion ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") 总结了从合成到真实点云的最先进领域泛化分类方法的表现，包括 ModelNet40 [[11](#bib.bib11)]
    → ScanObjectNN [[107](#bib.bib107)] 和 ShapeNet [[8](#bib.bib8)] → ScanObjectNN [[107](#bib.bib107)]。由于合成和真实点云之间存在较大的领域差异，典型的点云网络
    PointNet [[25](#bib.bib25)] 和 DGCNN [[205](#bib.bib205)] 在跨领域分类上表现较差。最近的 DG 方法 [[94](#bib.bib94),
    [95](#bib.bib95)] 取得了明显的性能提升，但仍有很大的进一步研究空间。
- en: 'TABLE XI: Domain generalized 3D shape classification accuracy (%) over the:
    ModelNet40 [[11](#bib.bib11)] → ScanObjectNN [[107](#bib.bib107)] and ShapeNet [[8](#bib.bib8)]
    → ScanObjectNN [[107](#bib.bib107)]. ¹: with PointNet as backbone; ²: with DGCNN
    as backbone. “(Bg.)”: with background noise. “MN40”: ModelNet40; “SONN”: ScanObjectNN;
    “SHN”: ShapeNet.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 表 XI：领域泛化 3D 形状分类准确率（%）在：ModelNet40 [[11](#bib.bib11)] → ScanObjectNN [[107](#bib.bib107)]
    和 ShapeNet [[8](#bib.bib8)] → ScanObjectNN [[107](#bib.bib107)]。¹：以 PointNet 作为骨干网络；²：以
    DGCNN 作为骨干网络。“（Bg.）”：有背景噪声。“MN40”：ModelNet40；“SONN”：ScanObjectNN；“SHN”：ShapeNet。
- en: '| Method | MN40→SONN | SHN→SONN |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | MN40→SONN | SHN→SONN |'
- en: '| Object | Object(Bg.) | Object | Object(Bg.) |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Object | Object(Bg.) | Object | Object(Bg.) |'
- en: '| PointNet [[25](#bib.bib25)] | 55.9±1.5 | 49.5±2.3 | 54.0±0.3 | 45.5±1.0 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| PointNet [[25](#bib.bib25)] | 55.9±1.5 | 49.5±2.3 | 54.0±0.3 | 45.5±1.0 |'
- en: '| DGCNN [[205](#bib.bib205)] | 61.68±1.26 | 57.61±0.44 | 57.42±1.01 | 54.42±0.80
    |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| DGCNN [[205](#bib.bib205)] | 61.68±1.26 | 57.61±0.44 | 57.42±1.01 | 54.42±0.80
    |'
- en: '| ¹PointDAN [[62](#bib.bib62)] | 63.3±0.9 | 55.1±1.0 | 55.0±0.9 | 43.0±1.0
    |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| ¹PointDAN [[62](#bib.bib62)] | 63.3±0.9 | 55.1±1.0 | 55.0±0.9 | 43.0±1.0
    |'
- en: '| ¹MetaSets [[94](#bib.bib94)] | 68.3±0.8 | 57.2±1.2 | 55.3±0.4 | 49.5±0.4
    |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| ¹MetaSets [[94](#bib.bib94)] | 68.3±0.8 | 57.2±1.2 | 55.3±0.4 | 49.5±0.4
    |'
- en: '| ²MetaSets [[94](#bib.bib94)] | 72.4±0.2 | 65.7±1.1 | 60.9±0.8 | 59.1±1.0
    |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| ²MetaSets [[94](#bib.bib94)] | 72.4±0.2 | 65.7±1.1 | 60.9±0.8 | 59.1±1.0
    |'
- en: '| ¹MAL [[95](#bib.bib95)] | 69.8±0.6 | 58.4±0.9 | 57.0±0.5 | 51.4±0.5 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| ¹MAL [[95](#bib.bib95)] | 69.8±0.6 | 58.4±0.9 | 57.0±0.5 | 51.4±0.5 |'
- en: '| ²MAL [[95](#bib.bib95)] | 73.8±0.5 | 66.7±1.0 | 62.2±0.5 | 61.1±0.9 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| ²MAL [[95](#bib.bib95)] | 73.8±0.5 | 66.7±1.0 | 62.2±0.5 | 61.1±0.9 |'
- en: Domain generalized 3D semantic segmentation. Though domain adaptation benchmarks
    can be used for domain generalization evaluation, we adopt an alternative setup
    that generalizes from normal weather to adverse weather. This setup has great
    value as LiDAR point clouds are susceptible to weather conditions. Table [XII](#A0.T12
    "TABLE XII ‣ 8 Conclusion ‣ A Survey of Label-Efficient Deep Learning for 3D Point
    Clouds") shows the performance of two generalization tasks, one trained on SemanticKITTI
    and tested on SemanticSTF [[23](#bib.bib23)], and another trained on SynLiDAR
    and tested on SemanticSTF. We can see that segmentation models trained on normal
    weather data (both real and synthetic) perform poorly under adverse weather conditions.
    The state-of-the-art PointDR [[23](#bib.bib23)] shows improved generalization,
    but there is still significant room for improvement in this area.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 域泛化 3D 语义分割。尽管可以使用领域适配基准来评估领域泛化，我们采用了一种替代设置，即从正常天气到恶劣天气进行泛化。这种设置具有很大的价值，因为 LiDAR
    点云容易受到天气条件的影响。表[XII](#A0.T12 "TABLE XII ‣ 8 Conclusion ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") 显示了两个泛化任务的性能，一个是在 SemanticKITTI 上训练并在 SemanticSTF
    [[23](#bib.bib23)] 上测试，另一个是在 SynLiDAR 上训练并在 SemanticSTF 上测试。我们可以看到，在恶劣天气条件下，基于正常天气数据（包括真实和合成）的分割模型表现较差。最先进的
    PointDR [[23](#bib.bib23)] 显示了改进的泛化能力，但在这一领域仍有显著的改进空间。
- en: 'TABLE XII: Domain generalization benchmarking on point clouds of adverse weathers
    in SemanticSTF. “DF”: dense-fog; “LF”: light-fog. “SKT”: SemanticKITTI [[10](#bib.bib10)];
    “SSTF: SemanticSTF [[23](#bib.bib23)]; “Syn”: SynLiDAR [[22](#bib.bib22)].'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XII: 在 SemanticSTF 中对恶劣天气的点云进行领域泛化基准测试。“DF”：密雾；“LF”：轻雾。“SKT”：SemanticKITTI
    [[10](#bib.bib10)]; “SSTF”：SemanticSTF [[23](#bib.bib23)]; “Syn”：SynLiDAR [[22](#bib.bib22)]。'
- en: '|  | SKT$\rightarrow$SSTF | Syn$\rightarrow$SSTF |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  | SKT$\rightarrow$SSTF | Syn$\rightarrow$SSTF |'
- en: '| --- | --- | --- |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Model | DF | LF | Rain | Snow | All | DF | LF | Rain | Snow | All |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | DF | LF | 雨 | 雪 | 全部 | DF | LF | 雨 | 雪 | 全部 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Baseline | 29.5 | 26.0 | 28.4 | 21.4 | 24.4 | 16.9 | 17.2 | 17.2 | 11.9 |
    15.0 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 29.5 | 26.0 | 28.4 | 21.4 | 24.4 | 16.9 | 17.2 | 17.2 | 11.9 | 15.0
    |'
- en: '| Dropout [[216](#bib.bib216)] | 29.3 | 25.6 | 29.4 | 24.8 | 25.7 | 15.3 |
    16.6 | 20.4 | 14.0 | 15.2 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| Dropout [[216](#bib.bib216)] | 29.3 | 25.6 | 29.4 | 24.8 | 25.7 | 15.3 |
    16.6 | 20.4 | 14.0 | 15.2 |'
- en: '| Perturbation | 26.3 | 27.8 | 30.0 | 24.5 | 25.9 | 16.3 | 16.7 | 19.3 | 13.4
    | 15.2 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 扰动 | 26.3 | 27.8 | 30.0 | 24.5 | 25.9 | 16.3 | 16.7 | 19.3 | 13.4 | 15.2
    |'
- en: '| PolarMix [[42](#bib.bib42)] | 29.7 | 25.0 | 28.6 | 25.6 | 26.0 | 16.1 | 15.5
    | 19.2 | 15.6 | 15.7 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| PolarMix [[42](#bib.bib42)] | 29.7 | 25.0 | 28.6 | 25.6 | 26.0 | 16.1 | 15.5
    | 19.2 | 15.6 | 15.7 |'
- en: '| MMD [[217](#bib.bib217)] | 30.4 | 28.1 | 32.8 | 25.2 | 26.9 | 17.3 | 16.3
    | 20.0 | 12.7 | 15.1 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| MMD [[217](#bib.bib217)] | 30.4 | 28.1 | 32.8 | 25.2 | 26.9 | 17.3 | 16.3
    | 20.0 | 12.7 | 15.1 |'
- en: '| PCL [[218](#bib.bib218)] | 28.9 | 27.6 | 30.1 | 24.6 | 26.4 | 17.8 | 16.7
    | 19.3 | 14.1 | 15.5 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| PCL [[218](#bib.bib218)] | 28.9 | 27.6 | 30.1 | 24.6 | 26.4 | 17.8 | 16.7
    | 19.3 | 14.1 | 15.5 |'
- en: '| PointDR [[23](#bib.bib23)] | 31.3 | 29.7 | 31.9 | 26.2 | 28.6 | 18.0 | 17.1
    | 19.9 | 15.0 | 16.2 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| PointDR [[23](#bib.bib23)] | 31.3 | 29.7 | 31.9 | 26.2 | 28.6 | 18.0 | 17.1
    | 19.9 | 15.0 | 16.2 |'
- en: 3D weakly-supervised semantic segmentation. Table [XIII](#A0.T13 "TABLE XIII
    ‣ 8 Conclusion ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")
    summarizes the performance of various 3D weakly-supervised learning methods on
    SemanticKITTI. It shows that the state-of-the-art achieves impressive segmentation
    with minimal annotations, approaching or even surpassing the baseline model that
    relies on full annotations. For instance, LESS [[123](#bib.bib123)] outperforms
    the baseline with only 0.1% annotations, highlighting the redundancy in point
    cloud annotations as well as enormous potential of this research direction.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 弱监督语义分割。表[XIII](#A0.T13 "TABLE XIII ‣ 8 Conclusion ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") 总结了各种 3D 弱监督学习方法在 SemanticKITTI 上的性能。它显示，最先进的技术在最小的标注下取得了令人印象深刻的分割效果，接近甚至超过了依赖全标注的基准模型。例如，LESS
    [[123](#bib.bib123)] 在仅使用 0.1% 标注的情况下优于基准，突显了点云标注的冗余性以及这一研究方向的巨大潜力。
- en: 'TABLE XIII: Comparison of different 3D weakly-supervised learning methods on
    the SemanticKITTI validation set with the same backbone Cylinder3D [[219](#bib.bib219)].'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XIII: 在 SemanticKITTI 验证集上使用相同骨干网络 Cylinder3D [[219](#bib.bib219)] 比较不同的
    3D 弱监督学习方法。'
- en: '| Method | Publication | Annotation | mIoU |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| Method | Publication | Annotation | mIoU |'
- en: '| --- | --- | --- | --- |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Baseline [[219](#bib.bib219)] | CVPR2021 | 100% | 65.9 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| Baseline [[219](#bib.bib219)] | CVPR2021 | 100% | 65.9 |'
- en: '| ReDAL [[125](#bib.bib125)] | ICCV2021 | 5% | 59.8 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| ReDAL [[125](#bib.bib125)] | ICCV2021 | 5% | 59.8 |'
- en: '| OneThingOneClick [[114](#bib.bib114)] | CVPR2021 | 0.1% | 26.0 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| OneThingOneClick [[114](#bib.bib114)] | CVPR2021 | 0.1% | 26.0 |'
- en: '| ContrastiveSC [[115](#bib.bib115)] | CVPR2021 | 0.1% | 46.0 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| ContrastiveSC [[115](#bib.bib115)] | CVPR2021 | 0.1% | 46.0 |'
- en: '| SQN [[120](#bib.bib120)] | ECCV2022 | 0.1% | 52.0 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| SQN [[120](#bib.bib120)] | ECCV2022 | 0.1% | 52.0 |'
- en: '| LESS [[123](#bib.bib123)] | ECCV2022 | 0.1% | 66.0 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| LESS [[123](#bib.bib123)] | ECCV2022 | 0.1% | 66.0 |'
- en: '| SQN [[120](#bib.bib120)] | ECCV2022 | 0.01% | 38.3 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| SQN [[120](#bib.bib120)] | ECCV2022 | 0.01% | 38.3 |'
- en: '| LESS [[123](#bib.bib123)] | ECCV2022 | 0.01% | 61.0 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| LESS [[123](#bib.bib123)] | ECCV2022 | 0.01% | 61.0 |'
- en: Semi-supervised 3D object detection. Table [XIV](#A0.T14 "TABLE XIV ‣ 8 Conclusion
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") presents a summary
    of the performance of recent advanced semi-supervised methods for 3D object detection.
    These methods were evaluated on indoor datasets, specifically ScanNet-V2 [[9](#bib.bib9)]
    and SUN RGB-D [[13](#bib.bib13)], where different proportions of annotations were
    used for training the same 3D detector VoteNet [[53](#bib.bib53)]. The results
    show that the use of fewer annotations during training results in a continuous
    performance decrease. However, state-of-the-art 3D semi-supervised methods achieve
    clear performance gains by exploiting unlabeled training point clouds, emphasizing
    the significance of 3D SemiSL in label-efficient learning.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督3D物体检测。表格 [XIV](#A0.T14 "TABLE XIV ‣ 8 Conclusion ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") 总结了近期先进半监督方法在3D物体检测中的表现。这些方法在室内数据集上进行评估，特别是ScanNet-V2 [[9](#bib.bib9)]
    和SUN RGB-D [[13](#bib.bib13)]，不同比例的标注被用于训练相同的3D检测器VoteNet [[53](#bib.bib53)]。结果显示，训练时使用较少的标注会导致性能持续下降。然而，最先进的3D半监督方法通过利用未标记的训练点云取得了明显的性能提升，强调了3D
    SemiSL在标签高效学习中的重要性。
- en: 'TABLE XIV: Comparing state-of-the-art semi-supervised methods for 3D object
    detection on ScanNet-V2 [[9](#bib.bib9)] and SUN RGB-D [[13](#bib.bib13)] val
    sets under varying ratios of labeled data. mAP@0.25 by mean±standard deviation
    across 3 runs of different random data splits are reported.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 XIV：比较了在ScanNet-V2 [[9](#bib.bib9)] 和SUN RGB-D [[13](#bib.bib13)] 验证集上，使用不同标记数据比例的最先进半监督方法。报告了不同随机数据拆分的3次运行中，mAP@0.25的均值±标准差。
- en: '| Datasets | Model | 5% | 10% | 20% |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 模型 | 5% | 10% | 20% |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SUNRGB-D | VoteNet (baseline) | 29.9±1.5 | 34.4±1.1 | 41.1±0.4 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| SUNRGB-D | VoteNet (baseline) | 29.9±1.5 | 34.4±1.1 | 41.1±0.4 |'
- en: '| SESS [[130](#bib.bib130)] | 34.2±2.0 | 42.9±1.0 | 47.9±0.5 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| SESS [[130](#bib.bib130)] | 34.2±2.0 | 42.9±1.0 | 47.9±0.5 |'
- en: '| 3DIoUMatch [[131](#bib.bib131)] | 39.0±1.9 | 45.5±1.5 | 49.7±0.4 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 3DIoUMatch [[131](#bib.bib131)] | 39.0±1.9 | 45.5±1.5 | 49.7±0.4 |'
- en: '| SPD [[220](#bib.bib220)] | - | 46.0±1.0 | 49.6±0.4 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| SPD [[220](#bib.bib220)] | - | 46.0±1.0 | 49.6±0.4 |'
- en: '| ScanNet-V2 | VoteNet (baseline) | 27.9±0.5 | 31.0±0.8 | 41.6±0.5 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| ScanNet-V2 | VoteNet (baseline) | 27.9±0.5 | 31.0±0.8 | 41.6±0.5 |'
- en: '| SESS [[130](#bib.bib130)] | 32.0±0.7 | 39.7±0.9 | 47.9±0.4 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| SESS [[130](#bib.bib130)] | 32.0±0.7 | 39.7±0.9 | 47.9±0.4 |'
- en: '| 3DIoUMatch [[131](#bib.bib131)] | 40.0±0.9 | 47.2±0.4 | 52.8±1.2 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 3DIoUMatch [[131](#bib.bib131)] | 40.0±0.9 | 47.2±0.4 | 52.8±1.2 |'
- en: '| SPD [[220](#bib.bib220)] | - | 43.2±1.2 | 51.9±0.4 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| SPD [[220](#bib.bib220)] | - | 43.2±1.2 | 51.9±0.4 |'
- en: Pretrained foundation models. Table [XV](#A0.T15 "TABLE XV ‣ 8 Conclusion ‣
    A Survey of Label-Efficient Deep Learning for 3D Point Clouds") validates the
    open-world capability of various methods with different numbers of annotated categories,
    including base-annotated open world with a portion of annotated categories and
    annotation-free open world with no category annotated. ”B15/N4” represents 15
    base classes and 4 novel classes as defined in [[184](#bib.bib184), [185](#bib.bib185)].
    Evaluation metrics $\mathrm{mIoU}^{\mathcal{B}}$, $\mathrm{mIoU}^{\mathcal{N}}$,
    and harmonic mean IoU (hIoU) are used to assess the performance on base categories,
    novel categories, and their harmonic mean, respectively.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练基础模型。表格 [XV](#A0.T15 "TABLE XV ‣ 8 Conclusion ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") 验证了不同标注类别数量的各种方法的开放世界能力，包括基础标注开放世界（部分标注类别）和无标注类别的开放世界。“B15/N4”代表15个基础类和4个新颖类，如[[184](#bib.bib184),
    [185](#bib.bib185)]中定义。评估指标$\mathrm{mIoU}^{\mathcal{B}}$、$\mathrm{mIoU}^{\mathcal{N}}$和调和平均IoU（hIoU）用于评估基础类别、新颖类别及其调和平均的表现。
- en: We can see that recent methods such as PLA [[184](#bib.bib184)] and RegionPLC [[185](#bib.bib185)]
    leveraging 2D foundation models achieve very impressive zero-shot performance
    compared to traditional zero-shot methods [[221](#bib.bib221), [222](#bib.bib222)].
    This suggests a promising direction of collecting pair-wise point cloud-image
    data and utilizing pre-trained language-vision foundation models for reducing
    point cloud annotations. However, it also shows that fewer base classes lead to
    lower few-shot performance, indicating the necessity of point annotations in semantic
    representation learning and highlighting the need for further research in this
    direction.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，最近的方法如 PLA [[184](#bib.bib184)] 和 RegionPLC [[185](#bib.bib185)] 利用 2D
    基础模型，相较于传统的零样本方法 [[221](#bib.bib221), [222](#bib.bib222)]，取得了非常令人印象深刻的零样本性能。这表明收集成对点云-图像数据并利用预训练的语言-视觉基础模型以减少点云标注是一个有前景的方向。然而，这也表明基础类别较少会导致较低的少样本性能，指出了在语义表示学习中点注释的必要性，并强调了在这一方向上需要进一步研究。
- en: 'TABLE XV: Open-world 3D semantic segmentation on ScanNet and nuScenes in hIoU/$\mathrm{mIoU}^{\mathcal{B}}$/$\mathrm{mIoU}^{\mathcal{N}}$.
    PLA w/o t means training without language supervision [[184](#bib.bib184)].'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 表 XV：在 ScanNet 和 nuScenes 上的开放世界 3D 语义分割，以 hIoU/$\mathrm{mIoU}^{\mathcal{B}}$/$\mathrm{mIoU}^{\mathcal{N}}$
    表示。PLA 无 t 意味着没有语言监督的训练 [[184](#bib.bib184)]。
- en: '| Method | ScanNet |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ScanNet |'
- en: '| B15/N4 | B12/N7 | B10/N9 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| B15/N4 | B12/N7 | B10/N9 |'
- en: '| 3DGenZ [[221](#bib.bib221)] | 20.6/56.0/12.6 | 19.8/35.5/13.3 | 12.0/63.6/06.6
    |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 3DGenZ [[221](#bib.bib221)] | 20.6/56.0/12.6 | 19.8/35.5/13.3 | 12.0/63.6/06.6
    |'
- en: '| 3DTZSL [[222](#bib.bib222)] | 10.5/36.7/06.1 | 03.8/36.6/02.0 | 07.8/55.5/04.2
    |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 3DTZSL [[222](#bib.bib222)] | 10.5/36.7/06.1 | 03.8/36.6/02.0 | 07.8/55.5/04.2
    |'
- en: '| LSeg-3D [[184](#bib.bib184)] | 00.0/64.4/00.0 | 00.9/55.7/00.1 | 01.8/68.4/00.9
    |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| LSeg-3D [[184](#bib.bib184)] | 00.0/64.4/00.0 | 00.9/55.7/00.1 | 01.8/68.4/00.9
    |'
- en: '| PLA w/o t [[184](#bib.bib184)] | 39.7/68.3/28.0 | 24.5/70.0/14.8 | 25.7/75.6/15.5
    |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| PLA 无 t [[184](#bib.bib184)] | 39.7/68.3/28.0 | 24.5/70.0/14.8 | 25.7/75.6/15.5
    |'
- en: '| PLA [[184](#bib.bib184)] | 65.3/68.3/62.4 | 55.3/69.5/45.9 | 53.1/76.2/40.8
    |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| PLA [[184](#bib.bib184)] | 65.3/68.3/62.4 | 55.3/69.5/45.9 | 53.1/76.2/40.8
    |'
- en: '| RegionPLC[[185](#bib.bib185)] | 69.9/68.4/71.5 | 65.1/69.6/61.1 | 58.8/76.6/47.7
    |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| RegionPLC [[185](#bib.bib185)] | 69.9/68.4/71.5 | 65.1/69.6/61.1 | 58.8/76.6/47.7
    |'
- en: '| Fully-Sup. | 73.3/68.4/79.1 | 70.6/70.0/71.8 | 69.9/75.8/64.9 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 完全监督 | 73.3/68.4/79.1 | 70.6/70.0/71.8 | 69.9/75.8/64.9 |'
- en: '| Method | nuScenes |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | nuScenes |'
- en: '| B12/N3 | B10/N5 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| B12/N3 | B10/N5 |'
- en: '| 3DGenZ [[221](#bib.bib221)] | 01.6/53.3/00.8 | 01.9/44.6/01.0 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 3DGenZ [[221](#bib.bib221)] | 01.6/53.3/00.8 | 01.9/44.6/01.0 |'
- en: '| 3DTZSL [[222](#bib.bib222)] | 01.2/21.0/00.6 | 06.4/17.1/03.9 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 3DTZSL [[222](#bib.bib222)] | 01.2/21.0/00.6 | 06.4/17.1/03.9 |'
- en: '| LSeg-3D [[184](#bib.bib184)] | 00.6/74.4/00.3 | 0.00/71.5/0.00 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| LSeg-3D [[184](#bib.bib184)] | 00.6/74.4/00.3 | 0.00/71.5/0.00 |'
- en: '| PLA w/o t [[184](#bib.bib184)] | 25.5/75.8/15.4 | 10.7/76.0/05.7 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| PLA 无 t [[184](#bib.bib184)] | 25.5/75.8/15.4 | 10.7/76.0/05.7 |'
- en: '| PLA [[184](#bib.bib184)] | 47.7/73.4/35.4 | 24.3/73.1/14.5 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| PLA [[184](#bib.bib184)] | 47.7/73.4/35.4 | 24.3/73.1/14.5 |'
- en: '| RegionPLC[[185](#bib.bib185)] | 62.0/75.8/52.4 | 36.6/76.7/24.1 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| RegionPLC [[185](#bib.bib185)] | 62.0/75.8/52.4 | 36.6/76.7/24.1 |'
- en: '| Fully-Sup. | 73.7/76.6/71.1 | 74.8/76.8/72.8 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 完全监督 | 73.7/76.6/71.1 | 74.8/76.8/72.8 |'
- en: References
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun, “Deep learning
    for 3d point clouds: A survey,” *IEEE transactions on pattern analysis and machine
    intelligence*, 2020.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, 和 M. Bennamoun, “深度学习用于 3D 点云：综述,”
    *IEEE 计算机视觉与模式分析汇刊*, 2020年。'
- en: '[2] A. Xiao, J. Huang, D. Guan, X. Zhang, S. Lu, and L. Shao, “Unsupervised
    point cloud representation learning with deep neural networks: A survey,” *IEEE
    Transactions on Pattern Analysis and Machine Intelligence*, pp. 1–20, 2023.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Xiao, J. Huang, D. Guan, X. Zhang, S. Lu, 和 L. Shao, “无监督点云表示学习与深度神经网络：综述,”
    *IEEE 计算机视觉与模式分析汇刊*, 页1–20, 2023年。'
- en: '[3] W. Shen, Z. Peng, X. Wang, H. Wang, J. Cen, D. Jiang, L. Xie, X. Yang,
    and Q. Tian, “A survey on label-efficient deep image segmentation: Bridging the
    gap between weak supervision and dense prediction,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, 2023.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] W. Shen, Z. Peng, X. Wang, H. Wang, J. Cen, D. Jiang, L. Xie, X. Yang,
    和 Q. Tian, “基于标签效率的深度图像分割调查：缩小弱监督与密集预测之间的差距,” *IEEE 计算机视觉与模式分析汇刊*, 2023年。'
- en: '[4] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
    neural networks: A survey,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 43, no. 11, pp. 4037–4058, 2020.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] L. Jing 和 Y. Tian, “自监督视觉特征学习与深度神经网络：综述,” *IEEE 计算机视觉与模式分析汇刊*, 第43卷，第11期，页4037–4058，2020年。'
- en: '[5] G.-J. Qi and J. Luo, “Small data challenges in big data era: A survey of
    recent progress on unsupervised and semi-supervised methods,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, vol. 44, no. 4, pp. 2168–2187,
    2020.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] G.-J. Qi 和 J. Luo, “大数据时代的小数据挑战：无监督和半监督方法的最新进展综述，” *IEEE模式分析与机器智能汇刊*，第44卷，第4期，第2168–2187页，2020年。'
- en: '[6] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, and
    P. Yu, “Generalizing to unseen domains: A survey on domain generalization,” *IEEE
    Transactions on Knowledge and Data Engineering*, 2022.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, 和
    P. Yu, “泛化到未见领域：领域泛化的综述，” *IEEE知识与数据工程汇刊*，2022年。'
- en: '[7] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy, “Domain generalization
    in vision: A survey,” *arXiv preprint arXiv:2103.02503*, 2021.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, 和 C. C. Loy, “视觉中的领域泛化：综述，” *arXiv预印本arXiv:2103.02503*，2021年。'
- en: '[8] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese,
    M. Savva, S. Song, H. Su *et al.*, “Shapenet: An information-rich 3d model repository,”
    *arXiv preprint arXiv:1512.03012*, 2015.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S.
    Savarese, M. Savva, S. Song, H. Su *等*， “Shapenet: 一个信息丰富的3D模型库，” *arXiv预印本arXiv:1512.03012*，2015年。'
- en: '[9] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
    “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2017, pp.
    5828–5839.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, 和 M. Nießner,
    “ScanNet: 丰富注释的室内场景3D重建数据集，” 载于 *IEEE计算机视觉与模式识别会议论文集*，2017年，第5828–5839页。'
- en: '[10] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,
    and J. Gall, “Semantickitti: A dataset for semantic scene understanding of lidar
    sequences,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, 2019, pp. 9297–9307.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,
    和 J. Gall, “SemanticKITTI: 用于激光雷达序列的语义场景理解数据集，” 载于 *IEEE/CVF国际计算机视觉大会论文集*，2019年，第9297–9307页。'
- en: '[11] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3d
    shapenets: A deep representation for volumetric shapes,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2015, pp. 1912–1920.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, 和 J. Xiao, “3D Shapenets:
    一种体积形状的深度表示，” 载于 *IEEE计算机视觉与模式识别会议论文集*，2015年，第1912–1920页。'
- en: '[12] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, and S.-K. Yeung, “Revisiting
    point cloud classification: A new benchmark dataset and classification model on
    real-world data,” in *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2019, pp. 1588–1597.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, 和 S.-K. Yeung, “重访点云分类：一个新的基准数据集和在实际数据上的分类模型，”
    载于 *IEEE/CVF国际计算机视觉大会论文集*，2019年，第1588–1597页。'
- en: '[13] S. Song, S. P. Lichtenberg, and J. Xiao, “Sun rgb-d: A rgb-d scene understanding
    benchmark suite,” in *Proceedings of the IEEE conference on computer vision and
    pattern recognition*, 2015, pp. 567–576.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. Song, S. P. Lichtenberg, 和 J. Xiao, “SUN RGB-D: 一个RGB-D场景理解基准套件，” 载于
    *IEEE计算机视觉与模式识别会议论文集*，2015年，第567–576页。'
- en: '[14] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and
    S. Savarese, “3d semantic parsing of large-scale indoor spaces,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2016, pp.
    1534–1543.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, 和
    S. Savarese, “大规模室内空间的3D语义解析，” 载于 *IEEE计算机视觉与模式识别会议论文集*，2016年，第1534–1543页。'
- en: '[15] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
    The kitti dataset,” *The International Journal of Robotics Research*, vol. 32,
    no. 11, pp. 1231–1237, 2013.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] A. Geiger, P. Lenz, C. Stiller, 和 R. Urtasun, “视觉遇见机器人：KITTI数据集，” *国际机器人研究杂志*，第32卷，第11期，第1231–1237页，2013年。'
- en: '[16] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous
    driving,” in *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*, 2020, pp. 11 621–11 631.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, 和 O. Beijbom, “nuscenes: 一个用于自动驾驶的多模态数据集，” 载于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第11,621–11,631页。'
- en: '[17] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
    J. Guo, Y. Zhou, Y. Chai, B. Caine *et al.*, “Scalability in perception for autonomous
    driving: Waymo open dataset,” in *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition*, 2020, pp. 2446–2454.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
    J. Guo, Y. Zhou, Y. Chai, B. Caine *等*，“自动驾驶感知的可扩展性：Waymo 开放数据集”，发表于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2020年，页 2446–2454。'
- en: '[18] M. Bijelic, T. Gruber, F. Mannan, F. Kraus, W. Ritter, K. Dietmayer, and
    F. Heide, “Seeing through fog without seeing fog: Deep multimodal sensor fusion
    in unseen adverse weather,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 11 682–11 692.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] M. Bijelic, T. Gruber, F. Mannan, F. Kraus, W. Ritter, K. Dietmayer 和
    F. Heide，“透过雾而不见雾：在未知恶劣天气中的深度多模态传感器融合”，发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2020年，页
    11,682–11,692。'
- en: '[19] J. Mao, M. Niu, C. Jiang, H. Liang, J. Chen, X. Liang, Y. Li, C. Ye, W. Zhang,
    Z. Li *et al.*, “One million scenes for autonomous driving: Once dataset,” *arXiv
    preprint arXiv:2106.11037*, 2021.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Mao, M. Niu, C. Jiang, H. Liang, J. Chen, X. Liang, Y. Li, C. Ye, W.
    Zhang, Z. Li *等*，“百万场景自动驾驶：一次数据集”，*arXiv 预印本 arXiv:2106.11037*，2021年。'
- en: '[20] T. Hackel, N. Savinov, J. D. Wegner, K. Schindler, M. Pollefeys *et al.*,
    “Semantic3d. net: A new large-scale point cloud classification benchmark,” in
    *ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences*,
    vol. 4.   ISPRS Foundation, 2017, pp. 91–98.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] T. Hackel, N. Savinov, J. D. Wegner, K. Schindler, M. Pollefeys *等*，“Semantic3d.
    net：一个新的大规模点云分类基准”，发表于 *ISPRS 测绘、遥感与空间信息科学年鉴*，第 4 卷，ISPRS 基金会，2017年，页 91–98。'
- en: '[21] Q. Hu, B. Yang, S. Khalid, W. Xiao, N. Trigoni, and A. Markham, “Towards
    semantic segmentation of urban-scale 3d point clouds: A dataset, benchmarks and
    challenges,” in *Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition*, 2021, pp. 4977–4987.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Q. Hu, B. Yang, S. Khalid, W. Xiao, N. Trigoni 和 A. Markham，“面向城市规模 3D
    点云的语义分割：数据集、基准和挑战”，发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2021年，页 4977–4987。'
- en: '[22] A. Xiao, J. Huang, D. Guan, F. Zhan, and S. Lu, “Transfer learning from
    synthetic to real lidar point cloud for semantic segmentation,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 36, no. 3, 2022, pp.
    2795–2803.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Xiao, J. Huang, D. Guan, F. Zhan 和 S. Lu，“从合成到真实激光雷达点云的迁移学习用于语义分割”，发表于
    *AAAI 人工智能会议论文集*，第 36 卷，第 3 期，2022年，页 2795–2803。'
- en: '[23] A. Xiao, J. Huang, W. Xuan, R. Ren, K. Liu, D. Guan, A. El Saddik, S. Lu,
    and E. P. Xing, “3d semantic segmentation in the wild: Learning generalized models
    for adverse-condition point clouds,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp. 9382–9392.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. Xiao, J. Huang, W. Xuan, R. Ren, K. Liu, D. Guan, A. El Saddik, S.
    Lu 和 E. P. Xing，“野外 3D 语义分割：为不良条件点云学习通用模型”，发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2023年6月，页
    9382–9392。'
- en: '[24] C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmentation
    for deep learning,” *Journal of big data*, vol. 6, no. 1, pp. 1–48, 2019.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] C. Shorten 和 T. M. Khoshgoftaar，“深度学习图像数据增强的综述”，*大数据期刊*，第 6 卷，第 1 期，页
    1–48，2019年。'
- en: '[25] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 652–660.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] C. R. Qi, H. Su, K. Mo 和 L. J. Guibas，“Pointnet：用于 3D 分类和分割的点集深度学习”，发表于
    *IEEE 计算机视觉与模式识别会议论文集*，2017年，页 652–660。'
- en: '[26] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
    feature learning on point sets in a metric space,” *Advances in neural information
    processing systems*, vol. 30, 2017.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] C. R. Qi, L. Yi, H. Su 和 L. J. Guibas，“Pointnet++：在度量空间中对点集进行深度层次特征学习”，*神经信息处理系统进展*，第
    30 卷，2017年。'
- en: '[27] M. Hahner, D. Dai, A. Liniger, and L. Van Gool, “Quantifying data augmentation
    for lidar based 3d object detection,” *arXiv preprint arXiv:2004.01643*, 2020.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] M. Hahner, D. Dai, A. Liniger 和 L. Van Gool，“量化激光雷达基础 3D 目标检测的数据增强”，*arXiv
    预印本 arXiv:2004.01643*，2020年。'
- en: '[28] R. Li, X. Li, P.-A. Heng, and C.-W. Fu, “Pointaugment: an auto-augmentation
    framework for point cloud classification,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 6378–6387.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] R. Li, X. Li, P.-A. Heng 和 C.-W. Fu，“Pointaugment：一种用于点云分类的自动增强框架”，发表于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，2020年，页 6378–6387。'
- en: '[29] S. Kim, S. Lee, D. Hwang, J. Lee, S. J. Hwang, and H. J. Kim, “Point cloud
    augmentation with weighted local transformations,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 548–557.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. Kim, S. Lee, D. Hwang, J. Lee, S. J. Hwang, 和 H. J. Kim, “具有加权局部变换的点云数据增强，”发表于*IEEE/CVF国际计算机视觉会议论文集*，2021年，第548–557页。'
- en: '[30] Y. Chen, V. T. Hu, E. Gavves, T. Mensink, P. Mettes, P. Yang, and C. G.
    Snoek, “Pointmixup: Augmentation for point clouds,” in *European Conference on
    Computer Vision*.   Springer, 2020, pp. 330–345.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. Chen, V. T. Hu, E. Gavves, T. Mensink, P. Mettes, P. Yang, 和 C. G.
    Snoek, “Pointmixup: 点云数据增强，”发表于*欧洲计算机视觉会议*，Springer，2020年，第330–345页。'
- en: '[31] D. Lee, J. Lee, J. Lee, H. Lee, M. Lee, S. Woo, and S. Lee, “Regularization
    strategy for point cloud via rigidly mixed sample,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 15 900–15 909.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] D. Lee, J. Lee, J. Lee, H. Lee, M. Lee, S. Woo, 和 S. Lee, “通过刚性混合样本进行点云正则化策略，”发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第15 900–15 909页。'
- en: '[32] J. Zhang, L. Chen, B. Ouyang, B. Liu, J. Zhu, Y. Chen, Y. Meng, and D. Wu,
    “Pointcutmix: Regularization strategy for point cloud classification,” *Neurocomputing*,
    vol. 505, pp. 58–67, 2022.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Zhang, L. Chen, B. Ouyang, B. Liu, J. Zhu, Y. Chen, Y. Meng, 和 D. Wu,
    “Pointcutmix: 点云分类的正则化策略，”*Neurocomputing*，第505卷，第58–67页，2022年。'
- en: '[33] A. Umam, C.-K. Yang, Y.-Y. Chuang, J.-H. Chuang, and Y.-Y. Lin, “Point
    mixswap: Attentional point cloud mixing via swapping matched structural divisions,”
    in *European Conference on Computer Vision*.   Springer, 2022, pp. 596–611.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] A. Umam, C.-K. Yang, Y.-Y. Chuang, J.-H. Chuang, 和 Y.-Y. Lin, “Point mixswap:
    通过交换匹配的结构划分进行注意力点云混合，”发表于*欧洲计算机视觉会议*，Springer，2022年，第596–611页。'
- en: '[34] S. Lee, M. Jeon, I. Kim, Y. Xiong, and H. J. Kim, “Sagemix: Saliency-guided
    mixup for point clouds,” *arXiv preprint arXiv:2210.06944*, 2022.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Lee, M. Jeon, I. Kim, Y. Xiong, 和 H. J. Kim, “Sagemix: 基于显著性的点云混合方法，”*arXiv预印本
    arXiv:2210.06944*，2022年。'
- en: '[35] S. Cheng, Z. Leng, E. D. Cubuk, B. Zoph, C. Bai, J. Ngiam, Y. Song, B. Caine,
    V. Vasudevan, C. Li *et al.*, “Improving 3d object detection through progressive
    population based augmentation,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 279–294.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] S. Cheng, Z. Leng, E. D. Cubuk, B. Zoph, C. Bai, J. Ngiam, Y. Song, B. Caine,
    V. Vasudevan, C. Li *等*，“通过渐进式群体增强提高3D目标检测，”发表于*欧洲计算机视觉会议*，Springer，2020年，第279–294页。'
- en: '[36] S. Chen, X. Wang, T. Cheng, W. Zhang, Q. Zhang, C. Huang, and W. Liu,
    “Azinorm: Exploiting the radial symmetry of point cloud for azimuth-normalized
    3d perception,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 6387–6396.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Chen, X. Wang, T. Cheng, W. Zhang, Q. Zhang, C. Huang, 和 W. Liu, “Azinorm:
    利用点云的径向对称性进行方位归一化3D感知，”发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，第6387–6396页。'
- en: '[37] Z. Leng, S. Cheng, B. Caine, W. Wang, X. Zhang, J. Shlens, M. Tan, and
    D. Anguelov, “Pseudoaugment: Learning to use unlabeled data for data augmentation
    in point clouds,” in *European Conference on Computer Vision*.   Springer, 2022,
    pp. 555–572.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Z. Leng, S. Cheng, B. Caine, W. Wang, X. Zhang, J. Shlens, M. Tan, 和 D. Anguelov,
    “Pseudoaugment: 学习利用未标记数据进行点云数据增强，”发表于*欧洲计算机视觉会议*，Springer，2022年，第555–572页。'
- en: '[38] Y. Yan, Y. Mao, and B. Li, “Second: Sparsely embedded convolutional detection,”
    *Sensors*, vol. 18, no. 10, p. 3337, 2018.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Y. Yan, Y. Mao, 和 B. Li, “Second: 稀疏嵌入卷积检测，”*传感器*，第18卷，第10期，第3337页，2018年。'
- en: '[39] J. Sun, H.-S. Fang, X. Zhu, J. Li, and C. Lu, “Correlation field for boosting
    3d object detection in structured scenes,” *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 36, no. 2, pp. 2298–2306, Jun. 2022.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J. Sun, H.-S. Fang, X. Zhu, J. Li, 和 C. Lu, “用于提升结构化场景中的3D目标检测的相关场，”*AAAI人工智能会议论文集*，第36卷，第2期，第2298–2306页，2022年6月。'
- en: '[40] W. Zheng, L. Jiang, F. Lu, Y. Ye, and C.-W. Fu, “Boosting single-frame
    3d object detection by simulating multi-frame point clouds,” in *Proceedings of
    the 30th ACM International Conference on Multimedia*, 2022, pp. 4848–4856.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] W. Zheng, L. Jiang, F. Lu, Y. Ye, 和 C.-W. Fu, “通过模拟多帧点云提升单帧3D目标检测，”发表于*第30届ACM国际多媒体会议论文集*，2022年，第4848–4856页。'
- en: '[41] A. Nekrasov, J. Schult, O. Litany, B. Leibe, and F. Engelmann, “Mix3d:
    Out-of-context data augmentation for 3d scenes,” in *2021 International Conference
    on 3D Vision (3DV)*.   IEEE, 2021, pp. 116–125.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] A. Nekrasov, J. Schult, O. Litany, B. Leibe, 和 F. Engelmann, “Mix3d: 用于3D场景的上下文外数据增强，”发表于*2021国际3D视觉会议（3DV）*，IEEE，2021年，第116–125页。'
- en: '[42] A. Xiao, J. Huang, D. Guan, K. Cui, S. Lu, and L. Shao, “Polarmix: A general
    data augmentation technique for lidar point clouds,” in *Advances in Neural Information
    Processing Systems*, 2022.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] A. Xiao, J. Huang, D. Guan, K. Cui, S. Lu, 和 L. Shao, “Polarmix: 一种用于激光雷达点云的通用数据增强技术”，发表于
    *神经信息处理系统进展*，2022年。'
- en: '[43] J. Fang, X. Zuo, D. Zhou, S. Jin, S. Wang, and L. Zhang, “Lidar-aug: A
    general rendering-based augmentation framework for 3d object detection,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 4710–4720.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] J. Fang, X. Zuo, D. Zhou, S. Jin, S. Wang, 和 L. Zhang, “Lidar-aug: 一种用于3D目标检测的通用渲染增强框架”，发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第4710–4720页。'
- en: '[44] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object detection
    network for autonomous driving,” in *Proceedings of the IEEE conference on Computer
    Vision and Pattern Recognition*, 2017, pp. 1907–1915.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] X. Chen, H. Ma, J. Wan, B. Li, 和 T. Xia, “用于自动驾驶的多视角3D目标检测网络”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2017年，第1907–1915页。'
- en: '[45] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets for
    3d object detection from rgb-d data,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2018, pp. 918–927.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] C. R. Qi, W. Liu, C. Wu, H. Su, 和 L. J. Guibas, “用于RGB-D数据的3D目标检测的Frustum
    Pointnets”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2018年，第918–927页。'
- en: '[46] M. Liang, B. Yang, S. Wang, and R. Urtasun, “Deep continuous fusion for
    multi-sensor 3d object detection,” in *Proceedings of the European conference
    on computer vision (ECCV)*, 2018, pp. 641–656.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] M. Liang, B. Yang, S. Wang, 和 R. Urtasun, “用于多传感器3D目标检测的深度连续融合”，发表于 *欧洲计算机视觉会议论文集（ECCV）*，2018年，第641–656页。'
- en: '[47] T. Huang, Z. Liu, X. Chen, and X. Bai, “Epnet: Enhancing point features
    with image semantics for 3d object detection,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 35–52.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] T. Huang, Z. Liu, X. Chen, 和 X. Bai, “Epnet: 通过图像语义增强点特征以进行3D目标检测”，发表于
    *欧洲计算机视觉会议*。 Springer，2020年，第35–52页。'
- en: '[48] C. Wang, C. Ma, M. Zhu, and X. Yang, “Pointaugmenting: Cross-modal augmentation
    for 3d object detection,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2021, pp. 11 794–11 803.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] C. Wang, C. Ma, M. Zhu, 和 X. Yang, “Pointaugmenting: 用于3D目标检测的跨模态增强”，发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第11 794–11 803页。'
- en: '[49] S. Vora, A. H. Lang, B. Helou, and O. Beijbom, “Pointpainting: Sequential
    fusion for 3d object detection,” in *Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition*, 2020, pp. 4604–4612.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] S. Vora, A. H. Lang, B. Helou, 和 O. Beijbom, “Pointpainting: 用于3D目标检测的序列融合”，发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第4604–4612页。'
- en: '[50] X. Yan, J. Gao, C. Zheng, C. Zheng, R. Zhang, S. Cui, and Z. Li, “2dpass:
    2d priors assisted semantic segmentation on lidar point clouds,” in *European
    Conference on Computer Vision*.   Springer, 2022, pp. 677–695.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] X. Yan, J. Gao, C. Zheng, C. Zheng, R. Zhang, S. Cui, 和 Z. Li, “2dpass:
    基于2D先验的激光雷达点云语义分割”，发表于 *欧洲计算机视觉会议*。 Springer，2022年，第677–695页。'
- en: '[51] B. Yang, R. Guo, M. Liang, S. Casas, and R. Urtasun, “Radarnet: Exploiting
    radar for robust perception of dynamic objects,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 496–512.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] B. Yang, R. Guo, M. Liang, S. Casas, 和 R. Urtasun, “Radarnet: 利用雷达进行动态目标的稳健感知”，发表于
    *欧洲计算机视觉会议*。 Springer，2020年，第496–512页。'
- en: '[52] K. Qian, S. Zhu, X. Zhang, and L. E. Li, “Robust multimodal vehicle detection
    in foggy weather using complementary lidar and radar signals,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 444–453.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] K. Qian, S. Zhu, X. Zhang, 和 L. E. Li, “在雾天使用互补的激光雷达和雷达信号进行稳健的多模态车辆检测”，发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第444–453页。'
- en: '[53] C. R. Qi, O. Litany, K. He, and L. J. Guibas, “Deep hough voting for 3d
    object detection in point clouds,” in *proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 9277–9286.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] C. R. Qi, O. Litany, K. He, 和 L. J. Guibas, “点云中的3D目标检测的深度霍夫投票”，发表于 *IEEE/CVF国际计算机视觉会议论文集*，2019年，第9277–9286页。'
- en: '[54] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, and A. Markham,
    “Randla-net: Efficient semantic segmentation of large-scale point clouds,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 11 108–11 117.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, 和 A. Markham,
    “Randla-net: 高效的大规模点云语义分割”，发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第11 108–11 117页。'
- en: '[55] A. Xiao, X. Yang, S. Lu, D. Guan, and J. Huang, “Fps-net: A convolutional
    fusion network for large-scale lidar point cloud segmentation,” *ISPRS Journal
    of Photogrammetry and Remote Sensing*, vol. 176, pp. 237–249, 2021.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] A. Xiao, X. Yang, S. Lu, D. Guan, 和 J. Huang，“Fps-net: 大规模激光雷达点云分割的卷积融合网络，”
    *ISPRS摄影测量与遥感杂志*，第176卷，pp. 237–249，2021。'
- en: '[56] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, and O. Litany, “Pointcontrast:
    Unsupervised pre-training for 3d point cloud understanding,” in *European conference
    on computer vision*.   Springer, 2020, pp. 574–591.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, 和 O. Litany，“Pointcontrast:
    用于3D点云理解的无监督预训练，”在*欧洲计算机视觉会议*上。 Springer, 2020, pp. 574–591。'
- en: '[57] S. V. Sheshappanavar, V. V. Singh, and C. Kambhamettu, “Patchaugment:
    Local neighborhood augmentation in point cloud classification,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 2118–2127.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] S. V. Sheshappanavar, V. V. Singh, 和 C. Kambhamettu，“Patchaugment: 点云分类中的局部邻域增强，”在*IEEE/CVF国际计算机视觉会议论文集*上，2021,
    pp. 2118–2127。'
- en: '[58] J. Choi, Y. Song, and N. Kwak, “Part-aware data augmentation for 3d object
    detection in point cloud,” in *2021 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS)*.   IEEE, 2021, pp. 3391–3397.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] J. Choi, Y. Song, 和 N. Kwak，“用于点云中3D对象检测的部分感知数据增强，”在*2021 IEEE/RSJ智能机器人与系统国际会议
    (IROS)*上。 IEEE, 2021, pp. 3391–3397。'
- en: '[59] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical
    risk minimization,” *arXiv preprint arXiv:1710.09412*, 2017.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] H. Zhang, M. Cisse, Y. N. Dauphin, 和 D. Lopez-Paz，“mixup: 超越经验风险最小化，”
    *arXiv 预印本 arXiv:1710.09412*，2017。'
- en: '[60] V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-Paz,
    and Y. Bengio, “Manifold mixup: Better representations by interpolating hidden
    states,” in *International Conference on Machine Learning*.   PMLR, 2019, pp.
    6438–6447.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-Paz,
    和 Y. Bengio，“流形mixup: 通过插值隐藏状态获得更好的表示，”在*国际机器学习会议*上。 PMLR, 2019, pp. 6438–6447。'
- en: '[61] S. Y. Feng, V. Gangal, J. Wei, S. Chandar, S. Vosoughi, T. Mitamura, and
    E. Hovy, “A survey of data augmentation approaches for nlp,” *arXiv preprint arXiv:2105.03075*,
    2021.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] S. Y. Feng, V. Gangal, J. Wei, S. Chandar, S. Vosoughi, T. Mitamura, 和
    E. Hovy，“NLP的数据增强方法综述，” *arXiv 预印本 arXiv:2105.03075*，2021。'
- en: '[62] C. Qin, H. You, L. Wang, C.-C. J. Kuo, and Y. Fu, “Pointdan: A multi-scale
    3d domain adaption network for point cloud representation,” *Advances in Neural
    Information Processing Systems*, vol. 32, 2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] C. Qin, H. You, L. Wang, C.-C. J. Kuo, 和 Y. Fu，“Pointdan: 用于点云表示的多尺度3D领域自适应网络，”
    *神经信息处理系统进展*，第32卷，2019。'
- en: '[63] L. Zou, H. Tang, K. Chen, and K. Jia, “Geometry-aware self-training for
    unsupervised domain adaptation on object point clouds,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2021, pp. 6403–6412.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] L. Zou, H. Tang, K. Chen, 和 K. Jia，“基于几何的自训练用于对象点云的无监督领域自适应，”在*IEEE/CVF国际计算机视觉会议论文集*上，2021,
    pp. 6403–6412。'
- en: '[64] Y. Shen, Y. Yang, M. Yan, H. Wang, Y. Zheng, and L. J. Guibas, “Domain
    adaptation on point clouds via geometry-aware implicits,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp. 7223–7232.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Y. Shen, Y. Yang, M. Yan, H. Wang, Y. Zheng, 和 L. J. Guibas，“通过几何感知隐式的点云领域自适应，”在*IEEE/CVF计算机视觉与模式识别会议论文集*上，2022,
    pp. 7223–7232。'
- en: '[65] H. Liang, H. Fan, Z. Fan, Y. Wang, T. Chen, Y. Cheng, and Z. Wang, “Point
    cloud domain adaptation via masked local 3d structure prediction,” in *European
    Conference on Computer Vision*.   Springer, 2022, pp. 156–172.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] H. Liang, H. Fan, Z. Fan, Y. Wang, T. Chen, Y. Cheng, 和 Z. Wang，“通过掩蔽局部3D结构预测的点云领域自适应，”在*欧洲计算机视觉会议*上。
    Springer, 2022, pp. 156–172。'
- en: '[66] H. Fan, X. Chang, W. Zhang, Y. Cheng, Y. Sun, and M. Kankanhalli, “Self-supervised
    global-local structure modeling for point cloud domain adaptation with reliable
    voted pseudo labels,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 6377–6386.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] H. Fan, X. Chang, W. Zhang, Y. Cheng, Y. Sun, 和 M. Kankanhalli，“用于点云领域自适应的自监督全球-局部结构建模及可靠投票伪标签，”在*IEEE/CVF计算机视觉与模式识别会议论文集*上，2022,
    pp. 6377–6386。'
- en: '[67] Y. Chen, Z. Wang, L. Zou, K. Chen, and K. Jia, “Quasi-balanced self-training
    on noise-aware synthesis of object point clouds for closing domain gap,” in *European
    Conference on Computer Vision*.   Springer, 2022, pp. 728–745.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Y. Chen, Z. Wang, L. Zou, K. Chen, 和 K. Jia，“在噪声感知的对象点云合成上的准平衡自训练以缩小领域差距，”在*欧洲计算机视觉会议*上。
    Springer, 2022, pp. 728–745。'
- en: '[68] A. Cardace, R. Spezialetti, P. Z. Ramirez, S. Salti, and L. Di Stefano,
    “Refrec: Pseudo-labels refinement via shape reconstruction for unsupervised 3d
    domain adaptation,” in *2021 International Conference on 3D Vision (3DV)*.   IEEE,
    2021, pp. 331–341.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] A. Cardace, R. Spezialetti, P. Z. Ramirez, S. Salti, 和 L. Di Stefano，"Refrec:
    通过形状重建进行伪标签细化以进行无监督 3D 领域适应"，发表于 *2021年国际3D视觉大会 (3DV)*。 IEEE，2021年，第331–341页。'
- en: '[69] Y. Wang, X. Chen, Y. You, L. E. Li, B. Hariharan, M. Campbell, K. Q. Weinberger,
    and W.-L. Chao, “Train in germany, test in the usa: Making 3d object detectors
    generalize,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2020, pp. 11 713–11 723.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Y. Wang, X. Chen, Y. You, L. E. Li, B. Hariharan, M. Campbell, K. Q. Weinberger,
    和 W.-L. Chao，"在德国训练，在美国测试：让 3D 目标检测器具有通用性"，发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第11 713–11 723页。'
- en: '[70] P. Su, K. Wang, X. Zeng, S. Tang, D. Chen, D. Qiu, and X. Wang, “Adapting
    object detectors with conditional domain normalization,” in *European Conference
    on Computer Vision*.   Springer, 2020, pp. 403–419.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] P. Su, K. Wang, X. Zeng, S. Tang, D. Chen, D. Qiu, 和 X. Wang，"通过条件领域归一化来适应目标检测器"，发表于
    *欧洲计算机视觉大会*。 Springer，2020年，第403–419页。'
- en: '[71] W. Zhang, W. Li, and D. Xu, “Srdan: Scale-aware and range-aware domain
    adaptation network for cross-dataset 3d object detection,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp.
    6769–6779.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] W. Zhang, W. Li, 和 D. Xu，"Srdan: 针对跨数据集 3D 目标检测的尺度感知与范围感知领域适应网络"，发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第6769–6779页。'
- en: '[72] J. Yang, S. Shi, Z. Wang, H. Li, and X. Qi, “St3d: Self-training for unsupervised
    domain adaptation on 3d object detection,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 10 368–10 378.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] J. Yang, S. Shi, Z. Wang, H. Li, 和 X. Qi，"St3d: 自我训练以进行 3D 目标检测的无监督领域适应"，发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第10 368–10 378页。'
- en: '[73] Z. Luo, Z. Cai, C. Zhou, G. Zhang, H. Zhao, S. Yi, S. Lu, H. Li, S. Zhang,
    and Z. Liu, “Unsupervised domain adaptive 3d detection with multi-level consistency,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2021, pp. 8866–8875.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Z. Luo, Z. Cai, C. Zhou, G. Zhang, H. Zhao, S. Yi, S. Lu, H. Li, S. Zhang,
    和 Z. Liu，"具有多级一致性的无监督领域自适应 3D 检测"，发表于 *IEEE/CVF国际计算机视觉大会论文集*，2021年，第8866–8875页。'
- en: '[74] J. Yang, S. Shi, Z. Wang, H. Li, and X. Qi, “St3d++: Denoised self-training
    for unsupervised domain adaptation on 3d object detection,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2022.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] J. Yang, S. Shi, Z. Wang, H. Li, 和 X. Qi，"St3d++: 去噪自我训练以进行 3D 目标检测的无监督领域适应"，*IEEE模式分析与机器智能汇刊*，2022年。'
- en: '[75] K. Saleh, A. Abobakr, M. Attia, J. Iskander, D. Nahavandi, M. Hossny,
    and S. Nahvandi, “Domain adaptation for vehicle detection from bird’s eye view
    lidar point cloud data,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision Workshops*, 2019, pp. 0–0.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] K. Saleh, A. Abobakr, M. Attia, J. Iskander, D. Nahavandi, M. Hossny,
    和 S. Nahvandi，"从鸟瞰视角激光雷达点云数据中进行车辆检测的领域适应"，发表于 *IEEE/CVF国际计算机视觉大会研讨会论文集*，2019年，第0–0页。'
- en: '[76] M. Hahner, C. Sakaridis, D. Dai, and L. Van Gool, “Fog simulation on real
    lidar point clouds for 3d object detection in adverse weather,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 15 283–15 292.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] M. Hahner, C. Sakaridis, D. Dai, 和 L. Van Gool，"在真实激光雷达点云上进行雾模拟以应对恶劣天气下的
    3D 目标检测"，发表于 *IEEE/CVF国际计算机视觉大会论文集*，2021年，第15 283–15 292页。'
- en: '[77] Q. Xu, Y. Zhou, W. Wang, C. R. Qi, and D. Anguelov, “Spg: Unsupervised
    domain adaptation for 3d object detection via semantic point generation,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 15 446–15 456.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Q. Xu, Y. Zhou, W. Wang, C. R. Qi, 和 D. Anguelov，"Spg: 通过语义点生成进行 3D 目标检测的无监督领域适应"，发表于
    *IEEE/CVF国际计算机视觉大会论文集*，2021年，第15 446–15 456页。'
- en: '[78] M. Hahner, C. Sakaridis, M. Bijelic, F. Heide, F. Yu, D. Dai, and L. Van Gool,
    “Lidar snowfall simulation for robust 3d object detection,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp.
    16 364–16 374.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. Hahner, C. Sakaridis, M. Bijelic, F. Heide, F. Yu, D. Dai, 和 L. Van
    Gool，"激光雷达降雪模拟用于鲁棒的 3D 目标检测"，发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，第16 364–16 374页。'
- en: '[79] Z. Yihan, C. Wang, Y. Wang, H. Xu, C. Ye, Z. Yang, and C. Ma, “Learning
    transferable features for point cloud detection via 3d contrastive co-training,”
    *Advances in Neural Information Processing Systems*, vol. 34, pp. 21 493–21 504,
    2021.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Z. Yihan, C. Wang, Y. Wang, H. Xu, C. Ye, Z. Yang, 和 C. Ma, “通过3D对比协同训练学习可转移特征用于点云检测”，*神经信息处理系统进展*，第34卷，页21,493–21,504，2021年。'
- en: '[80] Y. Wei, Z. Wei, Y. Rao, J. Li, J. Zhou, and J. Lu, “Lidar distillation:
    Bridging the beam-induced domain gap for 3d object detection,” *ECCV*, 2022.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. Wei, Z. Wei, Y. Rao, J. Li, J. Zhou, 和 J. Lu, “激光雷达蒸馏: 缩小由束缚引起的领域差距以进行3D目标检测”，*ECCV*，2022年。'
- en: '[81] G. Li, G. Kang, X. Wang, Y. Wei, and Y. Yang, “Adversarially masking synthetic
    to mimic real: Adaptive noise injection for point cloud segmentation adaptation,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2023, pp. 20 464–20 474.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] G. Li, G. Kang, X. Wang, Y. Wei, 和 Y. Yang, “对抗性掩蔽合成数据以模拟真实: 点云分割适应的自适应噪声注入”，见于*IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*，2023年6月，页20,464–20,474。'
- en: '[82] B. Wu, X. Zhou, S. Zhao, X. Yue, and K. Keutzer, “Squeezesegv2: Improved
    model structure and unsupervised domain adaptation for road-object segmentation
    from a lidar point cloud,” in *2019 International Conference on Robotics and Automation
    (ICRA)*.   IEEE, 2019, pp. 4376–4382.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] B. Wu, X. Zhou, S. Zhao, X. Yue, 和 K. Keutzer, “Squeezesegv2: 改进的模型结构和无监督领域自适应用于从激光雷达点云中进行道路物体分割”，见于*2019国际机器人与自动化会议（ICRA）*。IEEE,
    2019, 页4376–4382。'
- en: '[83] F. Langer, A. Milioto, A. Haag, J. Behley, and C. Stachniss, “Domain transfer
    for semantic segmentation of lidar data using deep neural networks,” in *2020
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2020, pp. 8263–8270.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] F. Langer, A. Milioto, A. Haag, J. Behley, 和 C. Stachniss, “使用深度神经网络的激光雷达数据语义分割的领域迁移”，见于*2020
    IEEE/RSJ智能机器人与系统国际会议（IROS）*。IEEE, 2020, 页8263–8270。'
- en: '[84] S. Zhao, Y. Wang, B. Li, B. Wu, Y. Gao, P. Xu, T. Darrell, and K. Keutzer,
    “epointda: An end-to-end simulation-to-real domain adaptation framework for lidar
    point cloud segmentation,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 35, no. 4, 2021, pp. 3500–3509.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] S. Zhao, Y. Wang, B. Li, B. Wu, Y. Gao, P. Xu, T. Darrell, 和 K. Keutzer,
    “epointda: 一种端到端的仿真到现实领域自适应框架用于激光雷达点云分割”，见于*人工智能AAAI会议论文集*，第35卷，第4期，2021年，页3500–3509。'
- en: '[85] P. Jiang and S. Saripalli, “Lidarnet: A boundary-aware domain adaptation
    model for point cloud semantic segmentation,” in *2021 IEEE International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2021, pp. 2457–2464.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] P. Jiang 和 S. Saripalli, “Lidarnet: 一种边界感知的领域自适应模型用于点云语义分割”，见于*2021 IEEE国际机器人与自动化会议（ICRA）*。IEEE,
    2021, 页2457–2464。'
- en: '[86] L. Yi, B. Gong, and T. Funkhouser, “Complete & label: A domain adaptation
    approach to semantic segmentation of lidar point clouds,” in *Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition*, 2021, pp. 15 363–15 373.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] L. Yi, B. Gong, 和 T. Funkhouser, “Complete & label: 一种用于激光雷达点云语义分割的领域自适应方法”，见于*IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，页15,363–15,373。'
- en: '[87] C. Saltori, F. Galasso, G. Fiameni, N. Sebe, E. Ricci, and F. Poiesi,
    “Cosmix: Compositional semantic mix for domain adaptation in 3d lidar segmentation,”
    in *European Conference on Computer Vision*.   Springer, 2022, pp. 586–602.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] C. Saltori, F. Galasso, G. Fiameni, N. Sebe, E. Ricci, 和 F. Poiesi, “Cosmix:
    3D激光雷达分割领域自适应的组合语义混合”，见于*欧洲计算机视觉会议*。Springer, 2022, 页586–602。'
- en: '[88] C. Saltori, E. Krivosheev, S. Lathuiliére, N. Sebe, F. Galasso, G. Fiameni,
    E. Ricci, and F. Poiesi, “Gipso: Geometrically informed propagation for online
    adaptation in 3d lidar segmentation,” in *European Conference on Computer Vision*.   Springer,
    2022, pp. 567–585.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] C. Saltori, E. Krivosheev, S. Lathuiliére, N. Sebe, F. Galasso, G. Fiameni,
    E. Ricci, 和 F. Poiesi, “Gipso: 几何信息驱动的传播用于3D激光雷达分割中的在线自适应”，见于*欧洲计算机视觉会议*。Springer,
    2022, 页567–585。'
- en: '[89] R. Ding, J. Yang, L. Jiang, and X. Qi, “Doda: Data-oriented sim-to-real
    domain adaptation for 3d indoor semantic segmentation,” *arXiv preprint arXiv:2204.01599*,
    2022.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] R. Ding, J. Yang, L. Jiang, 和 X. Qi, “Doda: 数据导向的仿真到现实领域自适应用于3D室内语义分割”，*arXiv预印本
    arXiv:2204.01599*，2022年。'
- en: '[90] M. Jaritz, T.-H. Vu, R. d. Charette, E. Wirbel, and P. Pérez, “xmuda:
    Cross-modal unsupervised domain adaptation for 3d semantic segmentation,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2020,
    pp. 12 605–12 614.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] M. Jaritz, T.-H. Vu, R. d. Charette, E. Wirbel, 和 P. Pérez, “xmuda: 跨模态无监督领域自适应用于3D语义分割”，见于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，页12,605–12,614。'
- en: '[91] D. Peng, Y. Lei, W. Li, P. Zhang, and Y. Guo, “Sparse-to-dense feature
    matching: Intra and inter domain cross-modal learning in domain adaptation for
    3d semantic segmentation,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2021, pp. 7108–7117.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] D. Peng, Y. Lei, W. Li, P. Zhang, 和 Y. Guo, “稀疏到密集特征匹配：领域适应中的内域和跨域交叉模态学习”，在
    *IEEE/CVF 国际计算机视觉会议论文集* 中，2021, pp. 7108–7117。'
- en: '[92] K. Ryu, S. Hwang, and J. Park, “Instant domain augmentation for lidar
    semantic segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, June 2023, pp. 9350–9360.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] K. Ryu, S. Hwang, 和 J. Park, “Lidar 语义分割的即时领域增强”，在 *IEEE/CVF 计算机视觉与模式识别会议论文集
    (CVPR)* 中，2023年6月，pp. 9350–9360。'
- en: '[93] I. Shin, Y.-H. Tsai, B. Zhuang, S. Schulter, B. Liu, S. Garg, I. S. Kweon,
    and K.-J. Yoon, “Mm-tta: Multi-modal test-time adaptation for 3d semantic segmentation,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 16 928–16 937.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] I. Shin, Y.-H. Tsai, B. Zhuang, S. Schulter, B. Liu, S. Garg, I. S. Kweon,
    和 K.-J. Yoon, “Mm-tta: 多模态测试时适应用于 3D 语义分割”，在 *IEEE/CVF 计算机视觉与模式识别会议论文集* 中，2022,
    pp. 16 928–16 937。'
- en: '[94] C. Huang, Z. Cao, Y. Wang, J. Wang, and M. Long, “Metasets: Meta-learning
    on point sets for generalizable representations,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 8863–8872.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] C. Huang, Z. Cao, Y. Wang, J. Wang, 和 M. Long, “Metasets: 针对可泛化表示的点集元学习”，在
    *IEEE/CVF 计算机视觉与模式识别会议论文集* 中，2021, pp. 8863–8872。'
- en: '[95] H. Huang, C. Chen, and Y. Fang, “Manifold adversarial learning for cross-domain
    3d shape representation,” in *European Conference on Computer Vision*.   Springer,
    2022, pp. 272–289.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] H. Huang, C. Chen, 和 Y. Fang, “跨领域 3D 形状表示的流形对抗学习”，在 *欧洲计算机视觉会议* 上。Springer,
    2022, pp. 272–289。'
- en: '[96] A. Lehner, S. Gasperini, A. Marcos-Ramiro, M. Schmidt, M.-A. N. Mahani,
    N. Navab, B. Busam, and F. Tombari, “3d-vfield: Adversarial augmentation of point
    clouds for domain generalization in 3d object detection,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp. 17 295–17 304.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] A. Lehner, S. Gasperini, A. Marcos-Ramiro, M. Schmidt, M.-A. N. Mahani,
    N. Navab, B. Busam, 和 F. Tombari, “3d-vfield: 用于 3D 物体检测的点云对抗增强与领域泛化”，在 *IEEE/CVF
    计算机视觉与模式识别会议论文集* 中，2022, pp. 17 295–17 304。'
- en: '[97] S. Wang, X. Zhao, H.-M. Xu, Z. Chen, D. Yu, J. Chang, Z. Yang, and F. Zhao,
    “Towards domain generalization for multi-view 3d object detection in bird-eye-view,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2023, pp. 13 333–13 342.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] S. Wang, X. Zhao, H.-M. Xu, Z. Chen, D. Yu, J. Chang, Z. Yang, 和 F. Zhao,
    “针对鸟瞰视角的多视图 3D 物体检测的领域泛化”，在 *IEEE/CVF 计算机视觉与模式识别会议论文集* 中，2023, pp. 13 333–13 342。'
- en: '[98] Y. Zhao, N. Zhao, and G. H. Lee, “Synthetic-to-real domain generalized
    semantic segmentation for 3d indoor point clouds,” *arXiv preprint arXiv:2212.04668*,
    2022.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Y. Zhao, N. Zhao, 和 G. H. Lee, “合成到真实领域的 3D 室内点云语义分割”，*arXiv 预印本 arXiv:2212.04668*，2022。'
- en: '[99] H. Kim, Y. Kang, C. Oh, and K.-J. Yoon, “Single domain generalization
    for lidar semantic segmentation,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp. 17 587–17 598.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] H. Kim, Y. Kang, C. Oh, 和 K.-J. Yoon, “Lidar 语义分割的单领域泛化”，在 *IEEE/CVF 计算机视觉与模式识别会议论文集
    (CVPR)* 中，2023年6月，pp. 17 587–17 598。'
- en: '[100] J. G. Moreno-Torres, T. Raeder, R. Alaiz-Rodríguez, N. V. Chawla, and
    F. Herrera, “A unifying view on dataset shift in classification,” *Pattern recognition*,
    vol. 45, no. 1, pp. 521–530, 2012.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] J. G. Moreno-Torres, T. Raeder, R. Alaiz-Rodríguez, N. V. Chawla, 和 F.
    Herrera, “分类中数据集偏移的统一视角”，*模式识别*，第 45 卷，第 1 期，pp. 521–530, 2012。'
- en: '[101] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W.
    Vaughan, “A theory of learning from different domains,” *Machine learning*, vol. 79,
    no. 1, pp. 151–175, 2010.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, 和 J. W.
    Vaughan, “来自不同领域的学习理论”，*机器学习*，第 79 卷，第 1 期，pp. 151–175, 2010。'
- en: '[102] K. Saito, K. Watanabe, Y. Ushiku, and T. Harada, “Maximum classifier
    discrepancy for unsupervised domain adaptation,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2018, pp. 3723–3732.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] K. Saito, K. Watanabe, Y. Ushiku, 和 T. Harada, “用于无监督领域适应的最大分类器差异”，在
    *IEEE 计算机视觉与模式识别会议论文集* 中，2018, pp. 3723–3732。'
- en: '[103] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative
    domain adaptation,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2017, pp. 7167–7176.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] E. Tzeng, J. Hoffman, K. Saenko, 和 T. Darrell, “对抗性判别领域适应，” 收录于 *IEEE
    计算机视觉与模式识别会议论文集*，2017 年，第 7167–7176 页。'
- en: '[104] Y. Zou, Z. Yu, B. Kumar, and J. Wang, “Unsupervised domain adaptation
    for semantic segmentation via class-balanced self-training,” in *Proceedings of
    the European conference on computer vision (ECCV)*, 2018, pp. 289–305.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Y. Zou, Z. Yu, B. Kumar, 和 J. Wang, “通过类平衡自训练进行语义分割的无监督领域适应，” 收录于 *欧洲计算机视觉会议
    (ECCV) 论文集*，2018 年，第 289–305 页。'
- en: '[105] Y. Zou, Z. Yu, X. Liu, B. Kumar, and J. Wang, “Confidence regularized
    self-training,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, 2019, pp. 5982–5991.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Y. Zou, Z. Yu, X. Liu, B. Kumar, 和 J. Wang, “置信度正则化自训练，” 收录于 *IEEE/CVF
    国际计算机视觉会议论文集*，2019 年，第 5982–5991 页。'
- en: '[106] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros,
    and T. Darrell, “Cycada: Cycle-consistent adversarial domain adaptation,” in *International
    conference on machine learning*.   Pmlr, 2018, pp. 1989–1998.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros,
    和 T. Darrell, “Cycada：循环一致对抗领域适应，” 收录于 *国际机器学习会议*。Pmlr，2018 年，第 1989–1998 页。'
- en: '[107] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, and S.-K. Yeung, “Revisiting
    point cloud classification: A new benchmark dataset and classification model on
    real-world data,” in *Proceedings of the IEEE/CVF international conference on
    computer vision*, 2019, pp. 1588–1597.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, 和 S.-K. Yeung, “重新审视点云分类：一个新的基准数据集和基于真实数据的分类模型，”
    收录于 *IEEE/CVF 国际计算机视觉会议论文集*，2019 年，第 1588–1597 页。'
- en: '[108] B. Caine, R. Roelofs, V. Vasudevan, J. Ngiam, Y. Chai, Z. Chen, and J. Shlens,
    “Pseudo-labeling for scalable 3d object detection,” *arXiv preprint arXiv:2103.02093*,
    2021.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] B. Caine, R. Roelofs, V. Vasudevan, J. Ngiam, Y. Chai, Z. Chen, 和 J.
    Shlens, “用于可扩展的 3d 目标检测的伪标签方法，” *arXiv 预印本 arXiv:2103.02093*，2021 年。'
- en: '[109] C. Liu, C. Gao, F. Liu, P. Li, D. Meng, and X. Gao, “Hierarchical supervision
    and shuffle data augmentation for 3d semi-supervised object detection,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    June 2023, pp. 23 819–23 828.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] C. Liu, C. Gao, F. Liu, P. Li, D. Meng, 和 X. Gao, “用于 3d 半监督目标检测的层次监督和数据增强，”
    收录于 *IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集*，2023 年 6 月，第 23 819–23 828 页。'
- en: '[110] M. Jaritz, T.-H. Vu, R. De Charette, É. Wirbel, and P. Pérez, “Cross-modal
    learning for domain adaptation in 3d semantic segmentation,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2022.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] M. Jaritz, T.-H. Vu, R. De Charette, É. Wirbel, 和 P. Pérez, “用于 3d 语义分割领域适应的跨模态学习，”
    *IEEE 模式分析与机器智能学报*，2022 年。'
- en: '[111] J. Huang, D. Guan, A. Xiao, and S. Lu, “Model adaptation: Historical
    contrastive learning for unsupervised domain adaptation without source data,”
    *Advances in Neural Information Processing Systems*, vol. 34, pp. 3635–3649, 2021.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] J. Huang, D. Guan, A. Xiao, 和 S. Lu, “模型适应：用于无源数据领域适应的历史对比学习，” *神经信息处理系统进展*，第
    34 卷，第 3635–3649 页，2021 年。'
- en: '[112] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy, “Domain generalization:
    A survey,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2022.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, 和 C. C. Loy, “领域泛化：综述，” *IEEE 模式分析与机器智能学报*，2022
    年。'
- en: '[113] X. Xu and G. H. Lee, “Weakly supervised semantic point cloud segmentation:
    Towards 10x fewer labels,” in *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition*, 2020, pp. 13 706–13 715.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] X. Xu 和 G. H. Lee, “弱监督语义点云分割：目标是减少 10 倍标签，” 收录于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2020
    年，第 13 706–13 715 页。'
- en: '[114] Z. Liu, X. Qi, and C.-W. Fu, “One thing one click: A self-training approach
    for weakly supervised 3d semantic segmentation,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 1726–1736.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Z. Liu, X. Qi, 和 C.-W. Fu, “一物一点击：一种用于弱监督 3d 语义分割的自训练方法，” 收录于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2021 年，第 1726–1736 页。'
- en: '[115] J. Hou, B. Graham, M. Nießner, and S. Xie, “Exploring data-efficient
    3d scene understanding with contrastive scene contexts,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 15 587–15 597.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] J. Hou, B. Graham, M. Nießner, 和 S. Xie, “通过对比场景上下文探索数据高效的 3d 场景理解，”
    收录于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2021 年，第 15 587–15 597 页。'
- en: '[116] Y. Zhang, Z. Li, Y. Xie, Y. Qu, C. Li, and T. Mei, “Weakly supervised
    semantic segmentation for large-scale point cloud,” in *Proceedings of the AAAI
    Conference on Artificial Intelligence*, vol. 35, no. 4, 2021, pp. 3421–3429.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Y. Zhang, Z. Li, Y. Xie, Y. Qu, C. Li, 和 T. Mei, “用于大规模点云的弱监督语义分割，”发表于*AAAI人工智能会议论文集*，vol.
    35, no. 4, 2021, pp. 3421–3429。'
- en: '[117] Y. Zhang, Y. Qu, Y. Xie, Z. Li, S. Zheng, and C. Li, “Perturbed self-distillation:
    Weakly supervised large-scale point cloud semantic segmentation,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 15 520–15 528.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Y. Zhang, Y. Qu, Y. Xie, Z. Li, S. Zheng, 和 C. Li, “扰动自蒸馏：弱监督的大规模点云语义分割，”发表于*IEEE/CVF国际计算机视觉会议论文集*，2021,
    pp. 15 520–15 528。'
- en: '[118] M. Li, Y. Xie, Y. Shen, B. Ke, R. Qiao, B. Ren, S. Lin, and L. Ma, “Hybridcr:
    Weakly-supervised 3d point cloud semantic segmentation via hybrid contrastive
    regularization,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 14 930–14 939.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] M. Li, Y. Xie, Y. Shen, B. Ke, R. Qiao, B. Ren, S. Lin, 和 L. Ma, “Hybridcr:
    通过混合对比正则化的弱监督3D点云语义分割，”发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2022, pp. 14 930–14 939。'
- en: '[119] H. Shi, J. Wei, R. Li, F. Liu, and G. Lin, “Weakly supervised segmentation
    on outdoor 4d point clouds with temporal matching and spatial graph propagation,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 11 840–11 849.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] H. Shi, J. Wei, R. Li, F. Liu, 和 G. Lin, “具有时间匹配和空间图传播的户外4D点云弱监督分割，”发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2022,
    pp. 11 840–11 849。'
- en: '[120] Q. Hu, B. Yang, G. Fang, Y. Guo, A. Leonardis, N. Trigoni, and A. Markham,
    “Sqn: Weakly-supervised semantic segmentation of large-scale 3d point clouds,”
    in *European Conference on Computer Vision*.   Springer, 2022, pp. 600–619.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Q. Hu, B. Yang, G. Fang, Y. Guo, A. Leonardis, N. Trigoni, 和 A. Markham,
    “Sqn: 大规模3D点云的弱监督语义分割，”发表于*欧洲计算机视觉会议*。Springer, 2022, pp. 600–619。'
- en: '[121] Z. Wu, Y. Wu, G. Lin, J. Cai, and C. Qian, “Dual adaptive transformations
    for weakly supervised point cloud segmentation,” in *European Conference on Computer
    Vision*.   Springer, 2022, pp. 78–96.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Z. Wu, Y. Wu, G. Lin, J. Cai, 和 C. Qian, “用于弱监督点云分割的双重自适应变换，”发表于*欧洲计算机视觉会议*。Springer,
    2022, pp. 78–96。'
- en: '[122] K. Liu, Y. Zhao, Q. Nie, Z. Gao, and B. M. Chen, “Weakly supervised 3d
    scene segmentation with region-level boundary awareness and instance discrimination,”
    in *European conference on computer vision*.   Springer, 2022, pp. 37–55.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] K. Liu, Y. Zhao, Q. Nie, Z. Gao, 和 B. M. Chen, “具有区域级边界意识和实例区分的弱监督3D场景分割，”发表于*欧洲计算机视觉会议*。Springer,
    2022, pp. 37–55。'
- en: '[123] M. Liu, Y. Zhou, C. R. Qi, B. Gong, H. Su, and D. Anguelov, “Less: Label-efficient
    semantic segmentation for lidar point clouds,” in *European Conference on Computer
    Vision*.   Springer, 2022, pp. 70–89.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] M. Liu, Y. Zhou, C. R. Qi, B. Gong, H. Su, 和 D. Anguelov, “Less: 激光雷达点云的标签高效语义分割，”发表于*欧洲计算机视觉会议*。Springer,
    2022, pp. 70–89。'
- en: '[124] C. Liu, C. Gao, F. Liu, J. Liu, D. Meng, and X. Gao, “Ss3d: Sparsely-supervised
    3d object detection from point cloud,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 8428–8437.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] C. Liu, C. Gao, F. Liu, J. Liu, D. Meng, 和 X. Gao, “Ss3d: 基于点云的稀疏监督3D目标检测，”发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2022,
    pp. 8428–8437。'
- en: '[125] T.-H. Wu, Y.-C. Liu, Y.-K. Huang, H.-Y. Lee, H.-T. Su, P.-C. Huang, and
    W. H. Hsu, “Redal: Region-based and diversity-aware active learning for point
    cloud semantic segmentation,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2021, pp. 15 510–15 519.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] T.-H. Wu, Y.-C. Liu, Y.-K. Huang, H.-Y. Lee, H.-T. Su, P.-C. Huang, 和
    W. H. Hsu, “Redal: 基于区域和多样性意识的点云语义分割主动学习，”发表于*IEEE/CVF国际计算机视觉会议论文集*，2021, pp.
    15 510–15 519。'
- en: '[126] Z. Hu, X. Bai, R. Zhang, X. Wang, G. Sun, H. Fu, and C.-L. Tai, “Lidal:
    Inter-frame uncertainty based active learning for 3d lidar semantic segmentation,”
    in *European Conference on Computer Vision*.   Springer, 2022, pp. 248–265.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Z. Hu, X. Bai, R. Zhang, X. Wang, G. Sun, H. Fu, 和 C.-L. Tai, “Lidal:
    基于帧间不确定性的主动学习用于3D激光雷达语义分割，”发表于*欧洲计算机视觉会议*。Springer, 2022, pp. 248–265。'
- en: '[127] L. Jiang, S. Shi, Z. Tian, X. Lai, S. Liu, C.-W. Fu, and J. Jia, “Guided
    point contrastive learning for semi-supervised point cloud semantic segmentation,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2021, pp. 6423–6432.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] L. Jiang, S. Shi, Z. Tian, X. Lai, S. Liu, C.-W. Fu, 和 J. Jia, “用于半监督点云语义分割的引导点对比学习，”发表于*IEEE/CVF国际计算机视觉会议论文集*，2021,
    pp. 6423–6432。'
- en: '[128] M. Cheng, L. Hui, J. Xie, and J. Yang, “Sspc-net: Semi-supervised semantic
    3d point cloud segmentation network,” in *Proceedings of the AAAI Conference on
    Artificial Intelligence*, vol. 35, no. 2, 2021, pp. 1140–1147.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] M. Cheng, L. Hui, J. Xie, 和 J. Yang，“SSPC-Net：半监督语义 3D 点云分割网络”，在 *AAAI
    人工智能会议论文集*，第 35 卷，第 2 期，2021 年，第 1140–1147 页。'
- en: '[129] R. Chu, X. Ye, Z. Liu, X. Tan, X. Qi, C.-W. Fu, and J. Jia, “Twist: Two-way
    inter-label self-training for semi-supervised 3d instance segmentation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022,
    pp. 1100–1109.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] R. Chu, X. Ye, Z. Liu, X. Tan, X. Qi, C.-W. Fu, 和 J. Jia，“Twist：用于半监督
    3D 实例分割的双向标签自训练”，在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2022 年，第 1100–1109 页。'
- en: '[130] N. Zhao, T.-S. Chua, and G. H. Lee, “Sess: Self-ensembling semi-supervised
    3d object detection,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2020, pp. 11 079–11 087.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] N. Zhao, T.-S. Chua, 和 G. H. Lee，“Sess：自我集成的半监督 3D 对象检测”，在 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2020 年，第 11 079–11 087 页。'
- en: '[131] H. Wang, Y. Cong, O. Litany, Y. Gao, and L. J. Guibas, “3dioumatch: Leveraging
    iou prediction for semi-supervised 3d object detection,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 14 615–14 624.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] H. Wang, Y. Cong, O. Litany, Y. Gao, 和 L. J. Guibas，“3dioumatch：利用 IoU
    预测进行半监督 3D 对象检测”，在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2021 年，第 14 615–14 624 页。'
- en: '[132] J. Yin, J. Fang, D. Zhou, L. Zhang, C.-Z. Xu, J. Shen, and W. Wang, “Semi-supervised
    3d object detection with proficient teachers,” in *European Conference on Computer
    Vision*.   Springer, 2022, pp. 727–743.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] J. Yin, J. Fang, D. Zhou, L. Zhang, C.-Z. Xu, J. Shen, 和 W. Wang，“利用熟练教师的半监督
    3D 对象检测”，在 *欧洲计算机视觉会议*。  Springer，2022 年，第 727–743 页。'
- en: '[133] L. Kong, J. Ren, L. Pan, and Z. Liu, “Lasermix for semi-supervised lidar
    semantic segmentation,” *arXiv preprint arXiv:2207.00026*, 2022.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] L. Kong, J. Ren, L. Pan, 和 Z. Liu，“用于半监督激光雷达语义分割的 LaserMix”，*arXiv 预印本
    arXiv:2207.00026*，2022 年。'
- en: '[134] L. Li, H. P. H. Shum, and T. P. Breckon, “Less is more: Reducing task
    and model complexity for 3d point cloud semantic segmentation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    June 2023, pp. 9361–9371.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] L. Li, H. P. H. Shum, 和 T. P. Breckon，“少即是多：减少 3D 点云语义分割的任务和模型复杂性”，在
    *IEEE/CVF 计算机视觉与模式识别会议（CVPR）*，2023 年 6 月，第 9361–9371 页。'
- en: '[135] C. Ye, H. Zhu, Y. Liao, Y. Zhang, T. Chen, and J. Fan, “What makes for
    effective few-shot point cloud classification?” in *Proceedings of the IEEE/CVF
    winter conference on applications of computer vision*, 2022, pp. 1829–1838.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] C. Ye, H. Zhu, Y. Liao, Y. Zhang, T. Chen, 和 J. Fan，“什么因素使少样本点云分类有效？”在
    *IEEE/CVF 计算机视觉应用冬季会议论文集*，2022 年，第 1829–1838 页。'
- en: '[136] C. Ye, H. Zhu, B. Zhang, and T. Chen, “A closer look at few-shot 3d point
    cloud classification,” *International Journal of Computer Vision*, pp. 1–24, 2022.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] C. Ye, H. Zhu, B. Zhang, 和 T. Chen，“深入探讨少样本 3D 点云分类”，*国际计算机视觉杂志*，第 1–24
    页，2022 年。'
- en: '[137] M. Yang, J. Chen, and S. Velipasalar, “Cross-modality feature fusion
    network for few-shot 3d point cloud classification,” in *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, 2023, pp. 653–662.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] M. Yang, J. Chen, 和 S. Velipasalar，“用于少样本 3D 点云分类的跨模态特征融合网络”，在 *IEEE/CVF
    计算机视觉应用冬季会议论文集*，2023 年，第 653–662 页。'
- en: '[138] N. Zhao, T.-S. Chua, and G. H. Lee, “Few-shot 3d point cloud semantic
    segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2021, pp. 8873–8882.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] N. Zhao, T.-S. Chua, 和 G. H. Lee，“少样本 3D 点云语义分割”，在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2021
    年，第 8873–8882 页。'
- en: '[139] Z. Zhao, Z. Wu, X. Wu, C. Zhang, and S. Wang, “Crossmodal few-shot 3d
    point cloud semantic segmentation,” in *Proceedings of the 30th ACM International
    Conference on Multimedia*, 2022, pp. 4760–4768.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Z. Zhao, Z. Wu, X. Wu, C. Zhang, 和 S. Wang，“跨模态少样本 3D 点云语义分割”，在 *第 30
    届 ACM 国际多媒体会议论文集*，2022 年，第 4760–4768 页。'
- en: '[140] S. Zhao and X. QI, “Prototypical votenet for few-shot 3d point cloud
    object detection,” in *Advances in Neural Information Processing Systems*, 2022.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] S. Zhao 和 X. QI，“用于少样本 3D 点云对象检测的原型投票网络”，在 *神经信息处理系统进展*，2022 年。'
- en: '[141] T. Ngo and K. Nguyen, “Geodesic-former: A geodesic-guided few-shot 3d
    point cloud instance segmenter,” in *Computer Vision–ECCV 2022: 17th European
    Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIX*.   Springer,
    2022, pp. 561–578.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] T. Ngo 和 K. Nguyen，“Geodesic-former：一种地质导向的少样本 3D 点云实例分割器”，见于 *计算机视觉–ECCV
    2022：第 17 届欧洲会议，特拉维夫，以色列，2022 年 10 月 23–27 日，会议论文集，第 XXIX 部分*。   Springer，2022年，第561–578页。'
- en: '[142] T. Chowdhury, A. Cheraghian, S. Ramasinghe, S. Ahmadi, M. Saberi, and
    S. Rahman, “Few-shot class-incremental learning for 3d point cloud objects,” in
    *Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October
    23–27, 2022, Proceedings, Part XX*.   Springer, 2022, pp. 204–220.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] T. Chowdhury, A. Cheraghian, S. Ramasinghe, S. Ahmadi, M. Saberi, 和 S.
    Rahman，“面向 3D 点云对象的少样本类别增量学习”，见于 *计算机视觉–ECCV 2022：第 17 届欧洲会议，特拉维夫，以色列，2022 年 10
    月 23–27 日，会议论文集，第 XX 部分*。   Springer，2022年，第204–220页。'
- en: '[143] J. Wei, G. Lin, K.-H. Yap, T.-Y. Hung, and L. Xie, “Multi-path region
    mining for weakly supervised 3d semantic segmentation on point clouds,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2020,
    pp. 4384–4393.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] J. Wei, G. Lin, K.-H. Yap, T.-Y. Hung, 和 L. Xie，“针对点云的弱监督 3D 语义分割的多路径区域挖掘”，见于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，2020年，第4384–4393页。'
- en: '[144] Z. Ren, I. Misra, A. G. Schwing, and R. Girdhar, “3d spatial recognition
    without spatially labeled 3d,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2021, pp. 13 204–13 213.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Z. Ren, I. Misra, A. G. Schwing, 和 R. Girdhar，“无需空间标记的 3D 空间识别”，见于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2021年，第13 204–13 213页。'
- en: '[145] O. Unal, D. Dai, and L. Van Gool, “Scribble-supervised lidar semantic
    segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2022, pp. 2697–2707.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] O. Unal, D. Dai, 和 L. Van Gool，“涂鸦监督的激光雷达语义分割”，见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2022年，第2697–2707页。'
- en: '[146] Q. Meng, W. Wang, T. Zhou, J. Shen, L. V. Gool, and D. Dai, “Weakly supervised
    3d object detection from lidar point cloud,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 515–531.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Q. Meng, W. Wang, T. Zhou, J. Shen, L. V. Gool, 和 D. Dai，“从激光雷达点云中进行弱监督
    3D 对象检测”，见于 *欧洲计算机视觉会议*。   Springer，2020年，第515–531页。'
- en: '[147] Q. Meng, W. Wang, T. Zhou, J. Shen, Y. Jia, and L. Van Gool, “Towards
    a weakly supervised framework for 3d point cloud object detection and annotation,”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2021.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Q. Meng, W. Wang, T. Zhou, J. Shen, Y. Jia, 和 L. Van Gool，“面向 3D 点云对象检测和标注的弱监督框架”，*IEEE
    计算机视觉与模式识别杂志*，2021年。'
- en: '[148] X. Xu, Y. Wang, Y. Zheng, Y. Rao, J. Zhou, and J. Lu, “Back to reality:
    Weakly-supervised 3d object detection with shape-guided label enhancement,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 8438–8447.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] X. Xu, Y. Wang, Y. Zheng, Y. Rao, J. Zhou, 和 J. Lu，“回到现实：带有形状引导标签增强的弱监督
    3D 对象检测”，见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2022年，第8438–8447页。'
- en: '[149] Y. Liao, H. Zhu, Y. Zhang, C. Ye, T. Chen, and J. Fan, “Point cloud instance
    segmentation with semi-supervised bounding-box mining,” *IEEE Transactions on
    Pattern Analysis and Machine Intelligence*, vol. 44, no. 12, pp. 10 159–10 170,
    2021.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Y. Liao, H. Zhu, Y. Zhang, C. Ye, T. Chen, 和 J. Fan，“带有半监督边界框挖掘的点云实例分割”，*IEEE
    计算机视觉与模式识别杂志*，第44卷，第12期，第10 159–10 170页，2021年。'
- en: '[150] J. Chibane, F. Engelmann, T. Anh Tran, and G. Pons-Moll, “Box2mask: Weakly
    supervised 3d semantic instance segmentation using bounding boxes,” in *Computer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part XXXI*.   Springer, 2022, pp. 681–699.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] J. Chibane, F. Engelmann, T. Anh Tran, 和 G. Pons-Moll，“Box2mask：使用边界框的弱监督
    3D 语义实例分割”，见于 *计算机视觉–ECCV 2022：第 17 届欧洲会议，特拉维夫，以色列，2022 年 10 月 23–27 日，会议论文集，第
    XXXI 部分*。   Springer，2022年，第681–699页。'
- en: '[151] Y. Wei, S. Su, J. Lu, and J. Zhou, “Fgr: Frustum-aware geometric reasoning
    for weakly supervised 3d vehicle detection,” in *2021 IEEE International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2021, pp. 4348–4354.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Y. Wei, S. Su, J. Lu, 和 J. Zhou，“Fgr：基于视锥的几何推理用于弱监督 3D 车辆检测”，见于 *2021
    IEEE 国际机器人与自动化会议（ICRA）*。   IEEE，2021年，第4348–4354页。'
- en: '[152] Z. Qin, J. Wang, and Y. Lu, “Weakly supervised 3d object detection from
    point clouds,” in *Proceedings of the 28th ACM International Conference on Multimedia*,
    2020, pp. 4144–4152.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Z. Qin, J. Wang, 和 Y. Lu，“从点云中进行弱监督 3D 对象检测”，见于 *第 28 届 ACM 国际多媒体会议论文集*，2020年，第4144–4152页。'
- en: '[153] H. Liu, H. Ma, Y. Wang, B. Zou, T. Hu, R. Wang, and J. Chen, “Eliminating
    spatial ambiguity for weakly supervised 3d object detection without spatial labels,”
    in *Proceedings of the 30th ACM International Conference on Multimedia*, 2022,
    pp. 3511–3520.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] H. Liu, H. Ma, Y. Wang, B. Zou, T. Hu, R. Wang, 和 J. Chen, “消除弱监督3D目标检测中的空间歧义，无需空间标签，”
    在 *第30届ACM国际多媒体会议论文集*，2022年，第3511–3520页。'
- en: '[154] S. Ye, D. Chen, S. Han, and J. Liao, “Learning with noisy labels for
    robust point cloud segmentation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 6443–6452.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] S. Ye, D. Chen, S. Han, 和 J. Liao, “利用噪声标签进行鲁棒的点云分割，” 在 *IEEE/CVF国际计算机视觉会议论文集*，2021年，第6443–6452页。'
- en: '[155] Z.-H. Zhou, “A brief introduction to weakly supervised learning,” *National
    science review*, vol. 5, no. 1, pp. 44–53, 2018.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Z.-H. Zhou, “弱监督学习简要介绍，” *国家科学评论*，第5卷，第1期，第44–53页，2018年。'
- en: '[156] P. Wang and W. Yao, “A new weakly supervised approach for als point cloud
    semantic segmentation,” *ISPRS Journal of Photogrammetry and Remote Sensing*,
    vol. 188, pp. 237–254, 2022.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] P. Wang 和 W. Yao, “一种新的弱监督ALS点云语义分割方法，” *ISPRS摄影测量与遥感杂志*，第188卷，第237–254页，2022年。'
- en: '[157] A. Tao, Y. Duan, Y. Wei, J. Lu, and J. Zhou, “Seggroup: Seg-level supervision
    for 3d instance and semantic segmentation,” *IEEE Transactions on Image Processing*,
    vol. 31, pp. 4952–4965, 2022.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] A. Tao, Y. Duan, Y. Wei, J. Lu, 和 J. Zhou, “Seggroup: 基于分割级别监督的3D实例和语义分割，”
    *IEEE图像处理汇刊*，第31卷，第4952–4965页，2022年。'
- en: '[158] L. Tang, L. Hui, and J. Xie, “Learning inter-superpoint affinity for
    weakly supervised 3d instance segmentation,” in *Proceedings of the Asian Conference
    on Computer Vision*, 2022, pp. 1282–1297.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] L. Tang, L. Hui, 和 J. Xie, “为弱监督3D实例分割学习超点之间的亲和性，” 在 *亚洲计算机视觉大会论文集*，2022年，第1282–1297页。'
- en: '[159] A. Tarvainen and H. Valpola, “Mean teachers are better role models: Weight-averaged
    consistency targets improve semi-supervised deep learning results,” *Advances
    in neural information processing systems*, vol. 30, 2017.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] A. Tarvainen 和 H. Valpola, “均值教师是更好的角色模型：权重平均一致性目标改善半监督深度学习结果，” *神经信息处理系统进展*，第30卷，2017年。'
- en: '[160] V. Verma, K. Kawaguchi, A. Lamb, J. Kannala, Y. Bengio, and D. Lopez-Paz,
    “Interpolation consistency training for semi-supervised learning,” *arXiv preprint
    arXiv:1903.03825*, 2019.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] V. Verma, K. Kawaguchi, A. Lamb, J. Kannala, Y. Bengio, 和 D. Lopez-Paz,
    “半监督学习的插值一致性训练，” *arXiv预印本arXiv:1903.03825*，2019年。'
- en: '[161] O. Chapelle and A. Zien, “Semi-supervised classification by low density
    separation,” in *International workshop on artificial intelligence and statistics*.   PMLR,
    2005, pp. 57–64.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] O. Chapelle 和 A. Zien, “通过低密度分离进行半监督分类，” 在 *人工智能与统计国际研讨会*。 PMLR，2005年，第57–64页。'
- en: '[162] Y. Grandvalet and Y. Bengio, “Semi-supervised learning by entropy minimization,”
    *Advances in neural information processing systems*, vol. 17, 2004.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Y. Grandvalet 和 Y. Bengio, “通过熵最小化进行半监督学习，” *神经信息处理系统进展*，第17卷，2004年。'
- en: '[163] D.-H. Lee *et al.*, “Pseudo-label: The simple and efficient semi-supervised
    learning method for deep neural networks,” in *Workshop on challenges in representation
    learning, ICML*, vol. 3, no. 2, 2013, p. 896.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] D.-H. Lee *等*，“伪标签：一种简单高效的深度神经网络半监督学习方法，” 在 *ICML表示学习挑战研讨会*，第3卷，第2期，2013年，第896页。'
- en: '[164] S. Deng, Q. Dong, B. Liu, and Z. Hu, “Superpoint-guided semi-supervised
    semantic segmentation of 3d point clouds,” in *2022 International Conference on
    Robotics and Automation (ICRA)*.   IEEE, 2022, pp. 9214–9220.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] S. Deng, Q. Dong, B. Liu, 和 Z. Hu, “基于超级点引导的半监督3D点云语义分割，” 在 *2022年国际机器人与自动化大会（ICRA）*
    中。 IEEE，2022年，第9214–9220页。'
- en: '[165] X. Huang, G. Mei, and J. Zhang, “Feature-metric registration: A fast
    semi-supervised approach for robust point cloud registration without correspondences,”
    in *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    2020, pp. 11 366–11 374.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] X. Huang, G. Mei, 和 J. Zhang, “特征度量配准：一种快速的半监督方法，用于在没有对应关系的情况下进行稳健的点云配准，”
    在 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第11 366–11 374页。'
- en: '[166] Y. Chen, Z. Tu, L. Ge, D. Zhang, R. Chen, and J. Yuan, “So-handnet: Self-organizing
    network for 3d hand pose estimation with semi-supervised learning,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2019, pp. 6961–6970.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Y. Chen, Z. Tu, L. Ge, D. Zhang, R. Chen, 和 J. Yuan, “So-handnet: 一种用于3D手部姿态估计的自组织网络，采用半监督学习，”
    在 *IEEE/CVF国际计算机视觉会议论文集*，2019年，第6961–6970页。'
- en: '[167] T. H. E. Tse, Z. Zhang, K. I. Kim, A. Leonardis, F. Zheng, and H. J.
    Chang, “S 2 contact: Graph-based network for 3d hand-object contact estimation
    with semi-supervised learning,” in *Computer Vision–ECCV 2022: 17th European Conference,
    Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part I*.   Springer, 2022,
    pp. 568–584.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] T. H. E. Tse, Z. Zhang, K. I. Kim, A. Leonardis, F. Zheng, 和 H. J. Chang，“S
    2 contact：基于图的网络用于 3D 手部-物体接触估计与半监督学习”，发表于 *计算机视觉–ECCV 2022：第 17 届欧洲会议，特拉维夫，以色列，2022年10月23–27日，会议论文集，第
    I 部分*。   Springer，2022年，第 568–584 页。'
- en: '[168] Y. Wang, J. Yin, W. Li, P. Frossard, R. Yang, and J. Shen, “Ssda3d: Semi-supervised
    domain adaptation for 3d object detection from point cloud,” *Proceedings of the
    AAAI Conference on Artificial Intelligence*, 2023.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Y. Wang, J. Yin, W. Li, P. Frossard, R. Yang, 和 J. Shen，“Ssda3d：从点云中进行3D目标检测的半监督领域适应”，*AAAI
    人工智能会议论文集*，2023年。'
- en: '[169] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B. Huang, “A closer
    look at few-shot classification,” in *International Conference on Learning Representations*.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, 和 J.-B. Huang，“深入探讨少样本分类”，发表于
    *国际学习表示会议*。'
- en: '[170] B. Kang, Z. Liu, X. Wang, F. Yu, J. Feng, and T. Darrell, “Few-shot object
    detection via feature reweighting,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 8420–8429.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] B. Kang, Z. Liu, X. Wang, F. Yu, J. Feng, 和 T. Darrell，“通过特征重新加权进行小样本目标检测”，发表于
    *IEEE/CVF 国际计算机视觉会议论文集*，2019年，第 8420–8429 页。'
- en: '[171] G. Koch, R. Zemel, R. Salakhutdinov *et al.*, “Siamese neural networks
    for one-shot image recognition,” in *ICML deep learning workshop*, vol. 2, no. 1.   Lille,
    2015.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] G. Koch, R. Zemel, R. Salakhutdinov *等*，“用于一次性图像识别的孪生神经网络”，发表于 *ICML
    深度学习研讨会*，第 2 卷，第 1 期。   Lille, 2015。'
- en: '[172] M. Ren, E. Triantafillou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum,
    H. Larochelle, and R. S. Zemel, “Meta-learning for semi-supervised few-shot classification,”
    in *International Conference on Learning Representations*, 2018.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] M. Ren, E. Triantafillou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum,
    H. Larochelle, 和 R. S. Zemel，“用于半监督少样本分类的元学习”，发表于 *国际学习表示会议*，2018年。'
- en: '[173] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” in *International conference on machine learning*.   PMLR,
    2017, pp. 1126–1135.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] C. Finn, P. Abbeel, 和 S. Levine，“针对深度网络的模型无关元学习”，发表于 *国际机器学习会议*。   PMLR，2017年，第
    1126–1135 页。'
- en: '[174] S. Ravi and H. Larochelle, “Optimization as a model for few-shot learning,”
    in *International conference on learning representations*, 2017.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] S. Ravi 和 H. Larochelle，“将优化作为少样本学习模型”，发表于 *国际学习表示会议*，2017年。'
- en: '[175] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap,
    “Meta-learning with memory-augmented neural networks,” in *International conference
    on machine learning*.   PMLR, 2016, pp. 1842–1850.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, 和 T. Lillicrap，“具有记忆增强神经网络的元学习”，发表于
    *国际机器学习会议*。   PMLR，2016年，第 1842–1850 页。'
- en: '[176] Q. Cai, Y. Pan, T. Yao, C. Yan, and T. Mei, “Memory matching networks
    for one-shot image recognition,” in *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2018, pp. 4080–4088.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Q. Cai, Y. Pan, T. Yao, C. Yan, 和 T. Mei，“用于一次性图像识别的记忆匹配网络”，发表于 *IEEE
    计算机视觉与模式识别会议论文集*，2018年，第 4080–4088 页。'
- en: '[177] Y. Lin, G. Vosselman, and M. Y. Yang, “Weakly supervised semantic segmentation
    of airborne laser scanning point clouds,” *ISPRS journal of photogrammetry and
    remote sensing*, vol. 187, pp. 79–100, 2022.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] Y. Lin, G. Vosselman, 和 M. Y. Yang，“空中激光扫描点云的弱监督语义分割”，*ISPRS 摄影测量与遥感杂志*，第
    187 卷，第 79–100 页，2022年。'
- en: '[178] S. Ye, D. Chen, S. Han, and J. Liao, “Robust point cloud segmentation
    with noisy annotations,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2022.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] S. Ye, D. Chen, S. Han, 和 J. Liao，“具有噪声标注的鲁棒点云分割”，*IEEE 计算机视觉与模式识别杂志*，2022年。'
- en: '[179] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao,
    and H. Li, “Pointclip: Point cloud understanding by clip,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp.
    8552–8562.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao,
    和 H. Li，“Pointclip：通过剪辑进行点云理解”，发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2022年，第 8552–8562
    页。'
- en: '[180] T. Huang, B. Dong, Y. Yang, X. Huang, R. W. Lau, W. Ouyang, and W. Zuo,
    “Clip2point: Transfer clip to point cloud classification with image-depth pre-training,”
    *arXiv preprint arXiv:2210.01055*, 2022.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] T. Huang, B. Dong, Y. Yang, X. Huang, R. W. Lau, W. Ouyang, 和 W. Zuo，“Clip2point：通过图像-深度预训练将剪辑转移到点云分类”，*arXiv
    预印本 arXiv:2210.01055*，2022年。'
- en: '[181] M. Liu, Y. Zhu, H. Cai, S. Han, Z. Ling, F. Porikli, and H. Su, “Partslip:
    Low-shot part segmentation for 3d point clouds via pretrained image-language models,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2023, pp. 21 736–21 746.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] M. Liu, Y. Zhu, H. Cai, S. Han, Z. Ling, F. Porikli, 和 H. Su，“Partslip：通过预训练的图像-语言模型进行低样本部件分割，”发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集（CVPR）*，2023年6月，页码21,736–21,746。'
- en: '[182] Z. Wang, B. Cheng, L. Zhao, D. Xu, Y. Tang, and L. Sheng, “Vl-sat: Visual-linguistic
    semantics assisted training for 3d semantic scene graph prediction in point cloud,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2023, pp. 21 560–21 569.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Z. Wang, B. Cheng, L. Zhao, D. Xu, Y. Tang, 和 L. Sheng，“Vl-sat：用于点云中3D语义场景图预测的视觉-语言语义辅助训练，”发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集（CVPR）*，2023年6月，页码21,560–21,569。'
- en: '[183] M. Gao, C. Xing, R. Martín-Martín, J. Wu, C. Xiong, L. Xue, R. Xu, J. C.
    Niebles, and S. Savarese, “Ulip: Learning a unified representation of language,
    images, and point clouds for 3d understanding,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp.
    1179–1189.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] M. Gao, C. Xing, R. Martín-Martín, J. Wu, C. Xiong, L. Xue, R. Xu, J.
    C. Niebles, 和 S. Savarese，“Ulip：学习语言、图像和点云的统一表示以进行3D理解，”发表于 *IEEE/CVF计算机视觉与模式识别会议论文集（CVPR）*，2023年6月，页码1179–1189。'
- en: '[184] R. Ding, J. Yang, C. Xue, W. Zhang, S. Bai, and X. Qi, “Pla: Language-driven
    open-vocabulary 3d scene understanding,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp. 7010–7019.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] R. Ding, J. Yang, C. Xue, W. Zhang, S. Bai, 和 X. Qi，“Pla：语言驱动的开放词汇3D场景理解，”发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集（CVPR）*，2023年6月，页码7010–7019。'
- en: '[185] J. Yang, R. Ding, Z. Wang, and X. Qi, “Regionplc: Regional point-language
    contrastive learning for open-world 3d scene understanding,” *arXiv preprint arXiv:2304.00962*,
    2023.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] J. Yang, R. Ding, Z. Wang, 和 X. Qi，“Regionplc：用于开放世界3D场景理解的区域点-语言对比学习，”
    *arXiv预印本 arXiv:2304.00962*，2023年。'
- en: '[186] Y. Zeng, C. Jiang, J. Mao, J. Han, C. Ye, Q. Huang, D.-Y. Yeung, Z. Yang,
    X. Liang, and H. Xu, “Clip2: Contrastive language-image-point pretraining from
    real-world point cloud data,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, June 2023, pp. 15 244–15 253.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Y. Zeng, C. Jiang, J. Mao, J. Han, C. Ye, Q. Huang, D.-Y. Yeung, Z. Yang,
    X. Liang, 和 H. Xu，“Clip2：从现实世界点云数据中进行对比语言-图像-点预训练，”发表于 *IEEE/CVF计算机视觉与模式识别会议论文集（CVPR）*，2023年6月，页码15,244–15,253。'
- en: '[187] Y. Lu, C. Xu, X. Wei, X. Xie, M. Tomizuka, K. Keutzer, and S. Zhang,
    “Open-vocabulary point-cloud object detection without 3d annotation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023,
    pp. 1190–1199.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Y. Lu, C. Xu, X. Wei, X. Xie, M. Tomizuka, K. Keutzer, 和 S. Zhang，“无3D标注的开放词汇点云物体检测，”发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2023年，页码1190–1199。'
- en: '[188] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,
    L. He *et al.*, “A comprehensive survey on pretrained foundation models: A history
    from bert to chatgpt,” *arXiv preprint arXiv:2302.09419*, 2023.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,
    L. He *等*，“预训练基础模型的全面综述：从bert到chatgpt的历史，” *arXiv预印本 arXiv:2302.09419*，2023年。'
- en: '[189] J. Zhang, J. Huang, S. Jin, and S. Lu, “Vision-language models for vision
    tasks: A survey,” *arXiv preprint arXiv:2304.00685*, 2023.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] J. Zhang, J. Huang, S. Jin, 和 S. Lu，“视觉-语言模型在视觉任务中的应用：综述，” *arXiv预印本
    arXiv:2304.00685*，2023年。'
- en: '[190] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for
    unsupervised visual representation learning,” in *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, 2020, pp. 9729–9738.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] K. He, H. Fan, Y. Wu, S. Xie, 和 R. Girshick，“动量对比用于无监督视觉表示学习，”发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，页码9729–9738。'
- en: '[191] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders
    are scalable vision learners,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022, pp. 16 000–16 009.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, 和 R. Girshick，“掩码自编码器是可扩展的视觉学习器，”发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，页码16,000–16,009。'
- en: '[192] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” *arXiv preprint
    arXiv:1810.04805*, 2018.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova，“Bert：用于语言理解的深度双向变换器预训练，”
    *arXiv预印本 arXiv:1810.04805*，2018年。'
- en: '[193] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *et al.*, “Learning transferable visual models
    from natural language supervision,” in *International conference on machine learning*.   PMLR,
    2021, pp. 8748–8763.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G.
    Sastry, A. Askell, P. Mishkin, J. Clark *等*，“从自然语言监督中学习可迁移的视觉模型，” 收录于*国际机器学习会议*。PMLR，2021年，第8748–8763页。'
- en: '[194] S. Huang, Y. Xie, S.-C. Zhu, and Y. Zhu, “Spatio-temporal self-supervised
    representation learning for 3d point clouds,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 6535–6545.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] S. Huang, Y. Xie, S.-C. Zhu, 和 Y. Zhu, “用于3D点云的时空自监督表示学习，” 收录于*IEEE/CVF国际计算机视觉会议论文集*，2021年，第6535–6545页。'
- en: '[195] Y. Chen, M. Nießner, and A. Dai, “4dcontrast: Contrastive learning with
    dynamic correspondences for 3d scene understanding,” in *Computer Vision–ECCV
    2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part XXXII*.   Springer, 2022, pp. 543–560.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Y. Chen, M. Nießner, 和 A. Dai, “4DContrast: 动态对应的对比学习用于3D场景理解，” 收录于*计算机视觉–ECCV
    2022: 第17届欧洲会议，以色列特拉维夫，2022年10月23–27日，论文集，第XXXII部分*。Springer，2022年，第543–560页。'
- en: '[196] K. Liu, A. Xiao, X. Zhang, S. Lu, and L. Shao, “Fac: 3d representation
    learning via foreground aware feature contrast,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp.
    9476–9485.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] K. Liu, A. Xiao, X. Zhang, S. Lu, 和 L. Shao, “Fac: 通过前景感知特征对比进行3D表示学习，”
    收录于*IEEE/CVF计算机视觉与模式识别会议论文集 (CVPR)*，2023年6月，第9476–9485页。'
- en: '[197] R. Xu, T. Wang, W. Zhang, R. Chen, J. Cao, J. Pang, and D. Lin, “Mv-jar:
    Masked voxel jigsaw and reconstruction for lidar-based self-supervised pre-training,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2023, pp. 13 445–13 454.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] R. Xu, T. Wang, W. Zhang, R. Chen, J. Cao, J. Pang, 和 D. Lin, “MV-JAR:
    基于掩码的体素拼图和重建用于激光雷达自监督预训练，” 收录于*IEEE/CVF计算机视觉与模式识别会议论文集 (CVPR)*，2023年6月，第13,445–13,454页。'
- en: '[198] H. Yang, T. He, J. Liu, H. Chen, B. Wu, B. Lin, X. He, and W. Ouyang,
    “Gd-mae: Generative decoder for mae pre-training on lidar point clouds,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    June 2023, pp. 9403–9414.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] H. Yang, T. He, J. Liu, H. Chen, B. Wu, B. Lin, X. He, 和 W. Ouyang, “GD-MAE:
    用于MAE预训练的生成解码器在激光雷达点云上的应用，” 收录于*IEEE/CVF计算机视觉与模式识别会议论文集 (CVPR)*，2023年6月，第9403–9414页。'
- en: '[199] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, and P. Gao, “Pointclip v2:
    Adapting clip for powerful 3d open-world learning,” *arXiv preprint arXiv:2211.11682*,
    2022.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, 和 P. Gao, “Pointclip v2:
    为强大的3D开放世界学习调整clip，” *arXiv预印本 arXiv:2211.11682*，2022年。'
- en: '[200] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan,
    L. Zhang, J.-N. Hwang *et al.*, “Grounded language-image pre-training,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022,
    pp. 10 965–10 975.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan,
    L. Zhang, J.-N. Hwang *等*，“基础语言-图像预训练，” 收录于*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，第10,965–10,975页。'
- en: '[201] L. Yao, J. Han, Y. Wen, X. Liang, D. Xu, W. Zhang, Z. Li, C. Xu, and
    H. Xu, “Detclip: Dictionary-enriched visual-concept paralleled pre-training for
    open-world detection,” in *Advances in Neural Information Processing Systems*,
    2022.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] L. Yao, J. Han, Y. Wen, X. Liang, D. Xu, W. Zhang, Z. Li, C. Xu, 和 H.
    Xu, “Detclip: 字典增强的视觉概念并行预训练用于开放世界检测，” 收录于*神经信息处理系统进展*，2022年。'
- en: '[202] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] K. He, X. Zhang, S. Ren, 和 J. Sun, “深度残差学习用于图像识别，” 收录于*IEEE计算机视觉与模式识别会议论文集*，2016年，第770–778页。'
- en: '[203] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin, “注意力机制就是你所需要的，” *神经信息处理系统进展*，第30卷，2017年。'
- en: '[204] H. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang, and S. Han, “Searching
    efficient 3d architectures with sparse point-voxel convolution,” in *Computer
    Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part XXVIII*.   Springer, 2020, pp. 685–702.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] H. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang, 和 S. Han, “通过稀疏点体素卷积搜索高效3D架构，”
    收录于*计算机视觉–ECCV 2020: 第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第XXVIII部分*。Springer，2020年，第685–702页。'
- en: '[205] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon,
    “Dynamic graph cnn for learning on point clouds,” *Acm Transactions On Graphics
    (tog)*, vol. 38, no. 5, pp. 1–12, 2019.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein 和 J. M. Solomon，"用于点云学习的动态图卷积神经网络"，*ACM图形学汇刊
    (TOG)*，第38卷，第5期，页1–12，2019年。'
- en: '[206] C. Choy, J. Gwak, and S. Savarese, “4d spatio-temporal convnets: Minkowski
    convolutional neural networks,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 3075–3084.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] C. Choy, J. Gwak 和 S. Savarese，"4D时空卷积网络：明可夫斯基卷积神经网络"，载于 *IEEE计算机视觉与模式识别会议论文集*，2019年，页3075–3084。'
- en: '[207] Y. Ganin and V. Lempitsky, “Unsupervised domain adaptation by backpropagation,”
    in *International conference on machine learning*.   PMLR, 2015, pp. 1180–1189.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Y. Ganin 和 V. Lempitsky，"通过反向传播进行无监督领域自适应"，载于 *国际机器学习会议*，PMLR，2015年，页1180–1189。'
- en: '[208] J. Sauder and B. Sievers, “Self-supervised deep learning on point clouds
    by reconstructing space,” *Advances in Neural Information Processing Systems*,
    vol. 32, 2019.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] J. Sauder 和 B. Sievers，"通过重建空间进行点云的自监督深度学习"，*神经信息处理系统进展*，第32卷，2019年。'
- en: '[209] I. Achituve, H. Maron, and G. Chechik, “Self-supervised learning for
    domain adaptation on point clouds,” in *Proceedings of the IEEE/CVF winter conference
    on applications of computer vision*, 2021, pp. 123–133.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] I. Achituve, H. Maron 和 G. Chechik，"点云领域自适应的自监督学习"，载于 *IEEE/CVF冬季计算机视觉应用会议论文集*，2021年，页123–133。'
- en: '[210] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
    J. Guo, Y. Zhou, Y. Chai, B. Caine *et al.*, “Scalability in perception for autonomous
    driving: Waymo open dataset,” in *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition*, 2020, pp. 2446–2454.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
    J. Guo, Y. Zhou, Y. Chai, B. Caine *等*，"自动驾驶中的感知可扩展性：Waymo开放数据集"，载于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，页2446–2454。'
- en: '[211] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, “Pv-rcnn:
    Point-voxel feature set abstraction for 3d object detection,” in *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2020.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang 和 H. Li，"PV-RCNN：用于3D物体检测的点-体素特征集抽象"，载于
    *IEEE/CVF计算机视觉与模式识别会议 (CVPR)*，2020年6月。'
- en: '[212] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, “Pointpillars:
    Fast encoders for object detection from point clouds,” in *Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition*, 2019, pp. 12 697–12 705.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang 和 O. Beijbom，"Pointpillars：点云物体检测的快速编码器"，载于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，页12,697–12,705。'
- en: '[213] Q. Hu, D. Liu, and W. Hu, “Density-insensitive unsupervised domain adaption
    on 3d object detection,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, June 2023, pp. 17 556–17 566.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] Q. Hu, D. Liu 和 W. Hu，"针对3D物体检测的密度不敏感无监督领域自适应"，载于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2023年6月，页17,556–17,566。'
- en: '[214] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative
    domain adaptation,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2017, pp. 7167–7176.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] E. Tzeng, J. Hoffman, K. Saenko, 和 T. Darrell，"对抗性判别领域自适应"，载于 *IEEE计算机视觉与模式识别会议论文集*，2017年，页7167–7176。'
- en: '[215] T.-H. Vu, H. Jain, M. Bucher, M. Cord, and P. Pérez, “Advent: Adversarial
    entropy minimization for domain adaptation in semantic segmentation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 2517–2526.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] T.-H. Vu, H. Jain, M. Bucher, M. Cord 和 P. Pérez，"Advent：用于语义分割领域自适应的对抗性熵最小化"，载于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，页2517–2526。'
- en: '[216] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    “Dropout: a simple way to prevent neural networks from overfitting,” *The journal
    of machine learning research*, vol. 15, no. 1, pp. 1929–1958, 2014.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever 和 R. Salakhutdinov，"Dropout：一种防止神经网络过拟合的简单方法"，*机器学习研究期刊*，第15卷，第1期，页1929–1958，2014年。'
- en: '[217] H. Li, S. J. Pan, S. Wang, and A. C. Kot, “Domain generalization with
    adversarial feature learning,” in *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2018, pp. 5400–5409.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] H. Li, S. J. Pan, S. Wang 和 A. C. Kot，"通过对抗特征学习进行领域泛化"，载于 *IEEE计算机视觉与模式识别会议论文集*，2018年，页5400–5409。'
- en: '[218] X. Yao, Y. Bai, X. Zhang, Y. Zhang, Q. Sun, R. Chen, R. Li, and B. Yu,
    “Pcl: Proxy-based contrastive learning for domain generalization,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022,
    pp. 7097–7107.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] X. Yao, Y. Bai, X. Zhang, Y. Zhang, Q. Sun, R. Chen, R. Li, 和 B. Yu，“Pcl：基于代理的对比学习用于领域泛化”，发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，页码7097–7107。'
- en: '[219] X. Zhu, H. Zhou, T. Wang, F. Hong, Y. Ma, W. Li, H. Li, and D. Lin, “Cylindrical
    and asymmetrical 3d convolution networks for lidar segmentation,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2021,
    pp. 9939–9948.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] X. Zhu, H. Zhou, T. Wang, F. Hong, Y. Ma, W. Li, H. Li, 和 D. Lin，“用于激光雷达分割的圆柱形和非对称3D卷积网络”，发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，页码9939–9948。'
- en: '[220] B. Xie, Z. Yang, L. Yang, R. Luo, J. Lu, A. Wei, X. Weng, and B. Li,
    “Spd: Semi-supervised learning and progressive distillation for 3-d detection,”
    *IEEE Transactions on Neural Networks and Learning Systems*, 2022.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] B. Xie, Z. Yang, L. Yang, R. Luo, J. Lu, A. Wei, X. Weng, 和 B. Li，“Spd：用于3D检测的半监督学习和渐进蒸馏”，*IEEE神经网络与学习系统汇刊*，2022年。'
- en: '[221] A. Cheraghian, S. Rahman, D. Campbell, and L. Petersson, “Transductive
    zero-shot learning for 3d point cloud classification,” in *Proceedings of the
    IEEE/CVF winter conference on applications of computer vision*, 2020, pp. 923–933.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] A. Cheraghian, S. Rahman, D. Campbell, 和 L. Petersson，“用于3D点云分类的传导性零样本学习”，发表于*IEEE/CVF冬季计算机视觉应用会议论文集*，2020年，页码923–933。'
- en: '[222] B. Michele, A. Boulch, G. Puy, M. Bucher, and R. Marlet, “Generative
    zero-shot learning for semantic segmentation of 3d point clouds,” in *2021 International
    Conference on 3D Vision (3DV)*.   IEEE, 2021, pp. 992–1002.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] B. Michele, A. Boulch, G. Puy, M. Bucher, 和 R. Marlet，“用于3D点云语义分割的生成式零样本学习”，发表于*2021年国际3D视觉会议（3DV）*，IEEE，2021年，页码992–1002。'
- en: '| ![[Uncaptioned image]](img/fbdd70644cdcc453159c1fabc0adbf3d.png) | Aoran
    Xiao received his B.Sc. and M.Sc. degrees from Wuhan University, China in 2016
    and 2019, respectively. He is currently pursuing a Ph.D. degree with the School
    of Computer Science and Engineering at Nanyang Technological University, Singapore.
    His research interests lie in point cloud processing, computer vision, and remote
    sensing. |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/fbdd70644cdcc453159c1fabc0adbf3d.png) | 肖璠于2016年和2019年分别获得中国武汉大学的学士和硕士学位。他目前正在新加坡南洋理工大学计算机科学与工程学院攻读博士学位。他的研究兴趣包括点云处理、计算机视觉和遥感。'
- en: '| ![[Uncaptioned image]](img/072afd33b2af50d70d908abe3394f437.png) | Xiaoqin
    Zhang is a senior member of the IEEE. He received the B.Sc. degree in electronic
    information science and technology from Central South University, China, in 2005,
    and the Ph.D. degree in pattern recognition and intelligent system from the National
    Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of
    Sciences, China, in 2010\. He is currently a Professor at Wenzhou University,
    China. He has published more than 100 papers in international and national journals,
    and international conferences, including IEEE T-PAMI, IJCV, IEEE T-IP, IEEE T-NNLS,
    IEEE T-C, ICCV, CVPR, NIPS, IJCAI, AAAI, and among others. His research interests
    include in pattern recognition, computer vision, and machine learning. |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/072afd33b2af50d70d908abe3394f437.png) | 张小琴是IEEE的高级会员。他于2005年获得中国中南大学电子信息科学与技术学士学位，并于2010年获得中国科学院自动化研究所模式识别国家实验室模式识别与智能系统博士学位。他目前是中国温州大学的教授。他在国际和国内期刊以及国际会议上发表了100多篇论文，包括IEEE
    T-PAMI、IJCV、IEEE T-IP、IEEE T-NNLS、IEEE T-C、ICCV、CVPR、NIPS、IJCAI、AAAI等。他的研究兴趣包括模式识别、计算机视觉和机器学习。'
- en: '| ![[Uncaptioned image]](img/8075ca924af941f747fb2564c4c6023b.png) | Ling Shao
    is a Distinguished Professor with the UCAS-Terminus AI Lab, University of Chinese
    Academy of Sciences, Beijing, China. He was the founding CEO and Chief Scientist
    of the Inception Institute of Artificial Intelligence, Abu Dhabi, UAE. His research
    interests include computer vision, deep learning, medical imaging and vision and
    language. He is a fellow of the IEEE, the IAPR, the BCS and the IET. |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/8075ca924af941f747fb2564c4c6023b.png) | 邵岭是中国科学院大学UCAS-Terminus
    AI实验室的杰出教授。他曾是阿布扎比Inception人工智能研究所的创始首席执行官兼首席科学家。他的研究兴趣包括计算机视觉、深度学习、医学成像以及视觉与语言。他是IEEE、IAPR、BCS和IET的会士。'
- en: '| ![[Uncaptioned image]](img/211914497bc1a5f03bf5ca0980b7b28c.png) | Shijian
    Lu is an Associate Professor with the School of Computer Science and Engineering
    at the Nanyang Technological University, Singapore. He received his PhD in electrical
    and computer engineering from the National University of Singapore. His major
    research interests include image and video analytics, visual intelligence, and
    machine learning. |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/211914497bc1a5f03bf5ca0980b7b28c.png) | Shijian Lu 是新加坡南洋理工大学计算机科学与工程学院的副教授。他在新加坡国立大学获得了电气与计算机工程博士学位。他的主要研究兴趣包括图像与视频分析、视觉智能以及机器学习。
    |'
