- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:01:09'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2005.11691] How to Build a Graph-Based Deep Learning Architecture in Traffic
    Domain: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2005.11691](https://ar5iv.labs.arxiv.org/html/2005.11691)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Jiexia Ye, Juanjuan Zhao*, Kejiang Ye, IEEE Member,  Chengzhong Xu, IEEE Fellow
    *Corresponding author: Juanjuan Zhao Jiexia Ye, Juanjuan Zhao, Kejiang Ye are
    with Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,
    China (E-mail: {jx.ye, jj.zhao, kj.ye}@siat.ac.cn). Chengzhong Xu is with State
    Key Lab of IOTSC, Department of Computer Science, University of Macau, Macau SAR,
    China (E-mail: czxu@um.edu.mo).'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In recent years, various deep learning architectures have been proposed to solve
    complex challenges (e.g. spatial dependency, temporal dependency) in traffic domain,
    which have achieved satisfactory performance. These architectures are composed
    of multiple deep learning techniques in order to tackle various challenges in
    traffic tasks. Traditionally, convolution neural networks (CNNs) are utilized
    to model spatial dependency by decomposing the traffic network as grids. However,
    many traffic networks are graph-structured in nature. In order to utilize such
    spatial information fully, it’s more appropriate to formulate traffic networks
    as graphs mathematically. Recently, various novel deep learning techniques have
    been developed to process graph data, called graph neural networks (GNNs). More
    and more works combine GNNs with other deep learning techniques to construct an
    architecture dealing with various challenges in a complex traffic task, where
    GNNs are responsible for extracting spatial correlations in traffic network. These
    graph-based architectures have achieved state-of-the-art performance. To provide
    a comprehensive and clear picture of such emerging trend, this survey carefully
    examines various graph-based deep learning architectures in many traffic applications.
    We first give guidelines to formulate a traffic problem based on graph and construct
    graphs from various kinds of traffic datasets. Then we decompose these graph-based
    architectures to discuss their shared deep learning techniques, clarifying the
    utilization of each technique in traffic tasks. What’s more, we summarize some
    common traffic challenges and the corresponding graph-based deep learning solutions
    to each challenge. Finally, we provide benchmark datasets, open source codes and
    future research directions in this rapidly growing field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Graph Neural Networks, GNNs, Graph Convolution Network, GCN, Graph, Deep Learning,
    Traffic Forecasting, Traffic Domain, ITS
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Along with the acceleration of urbanization process, mass population is quickly
    gathering together towards cities. In many cities, especially cities in developing
    countries, the rapidly increasing number of private vehicles and growing demand
    of public transport services are putting great pressure on their current transportation
    systems. The traffic problems such as frequent traffic jams, serious traffic accidents
    and long commute have seriously decreased the operation efficiency of cities and
    degraded the travel experience of passengers. To address these challenges, many
    cities are committed to develop an Intelligent Transportation System (ITS) which
    can provide efficient traffic management, accurate traffic resources allocation
    and high-quality transportation service. Such a system can reduce traffic accidents,
    relieve traffic congestion and ensure public traffic safety.
  prefs: []
  type: TYPE_NORMAL
- en: To construct an Intelligent Transportation System which makes cities smart,
    there are mainly two indispensable components, i.e. intelligent infrastructures
    and advanced algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: On one hand, with the increasing investment in transportation infrastructures,
    there are more and more traffic equipments and systems, including loop detectors,
    probes, cameras on road networks, GPS in taxis or buses, smart cards on subways
    and buses, automatic fare collection system and online ride-hailing system. These
    infrastructures produce traffic data around-the-clock, which are heterogeneous
    data, including numeric data (e.g. GPS trajectories, traffic measurements), image/video
    data (e.g. vehicle images) and textual data (e.g. incident reports). These transportation
    data are enormous in volume and complicated in structure, containing complex traffic
    patterns (e.g. spatiotemporal dependency, highly nonlinearity, complex dynamics).
    There is an urgent need to utilize more intelligent and powerful approaches to
    process such traffic data.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, in transportation domain, researchers have witnessed the
    algorithms evolving from statistical methods, to machine learning models and recently
    to deep learning approaches. In the early stage, statistic methods including ARIMA
    and its variants [[1](#bib.bib1)],[[2](#bib.bib2)], VAR[[3](#bib.bib3)], Kalman
    filtering [[4](#bib.bib4)] were prevalent, as they have solid and widely accepted
    mathematical foundations. However, the linear and stationarity assumptions of
    these methods are violated by the highly non-linearity and dynamics in traffic
    data, resulting in poor performance in practice. Traditional machine learning
    approaches such as Support Vector Machine [[5](#bib.bib5)], K-Nearest Neighbors[[6](#bib.bib6)]
    can model non-linearity and extract more complex correlations in traffic data.
    However, the shallow architecture, manual feature selection and separated learning
    in these models are considered to be unsatisfactory in big data scenarios [[7](#bib.bib7)].
  prefs: []
  type: TYPE_NORMAL
- en: The breakthrough of deep learning in many domains, including computer vision,
    natural language processing has attracted attention from transportation industry
    and research community. Deep learning techniques overcome the handcrafted feature
    engineering by providing an end-to-end learning from raw traffic data. The powerful
    capacities of deep learning techniques to approximate any complex functions in
    theory can model more complicated patterns in various traffic tasks. In recent
    years, due to the increasing computing power (e.g. GPU) and sufficient traffic
    data [[7](#bib.bib7)], deep learning based techniques have been widely employed
    and achieved state-of-the-art performance in various traffic applications. The
    Recurrent neural networks (RNNs) and Convolutional neural networks (CNNs) based
    architectures used to be popular in extracting spatiotemporal dependencies. In
    these architectures, RNN or its variants are employed to extract the temporal
    correlations in traffic data [[8](#bib.bib8)]. CNNs are used to capture the spatial
    correlations in grid-based traffic network [[9](#bib.bib9)]. However, many traffic
    networks are graph-structured in nature, e.g. road network [[10](#bib.bib10)]
    and subway network. The spatial features learned in CNN are not optimal for representing
    the graph-based traffic network. Although some previous works have analyzed traffic
    problems in a graph view [[11](#bib.bib11)],[[12](#bib.bib12)], these traditional
    approaches are not powerful enough to process big data and tackle complicated
    correlations in traffic network.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, many researchers have extended deep learning approaches on graph data
    to exploit graph structure information [[13](#bib.bib13)] and proposed a new group
    of neural networks called graph neural networks (GNNs)[[14](#bib.bib14)],[[15](#bib.bib15)],[[16](#bib.bib16)],
    which aims to address graph-related applications. GNNs have become the state-of-the-art
    approaches in many domains, including computer vision [[17](#bib.bib17)], natural
    language processing [[18](#bib.bib18)], biology [[19](#bib.bib19)], recommendation
    system [[20](#bib.bib20)]. Since many traffic data are graph-structured, many
    existing works incorporate GNNs into a deep learning architecture to capture the
    spatial dependency. Recent works have shown that such GNNs-based architectures
    can achieve better performance than CNNs-based architectures, for that most traffic
    networks are graph-structured naturally and GNNs can extract the spatial dependency
    more accurately. In addition, some tasks inherently require researchers to conduct
    prediction based on a graph, e.g. prediction in traffic network with irregular
    shapes. Many related works have been produced during the last couple of years
    and more are on the road. Under this circumstance, a comprehensive literature
    review on these graph-based deep learning architectures in transportation domain
    would be very timely, which is exactly our work.
  prefs: []
  type: TYPE_NORMAL
- en: To our best knowledge, we are the first to provide a comprehensive survey on
    graph-based deep learning works in traffic domain. Note that some works we review
    actually work on similar traffic problems with similar techniques. Our work can
    help the upcoming researchers avoid repetitive works and focus on new solutions.
    What’s more, the practical and clear guidance in this survey enables participators
    to apply these new emerging approaches in real-world traffic tasks quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, the main contributions of this paper are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We systematically outline traffic problems, related research directions, challenges
    and techniques in traffic domain, which can help related researchers to locate
    or expand their researches.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We summarize a general formulation about various traffic problems and provide
    a specific guidance to construct graphs from several typical kinds of raw traffic
    datasets. Such thorough summarization is quite practical and can accelerate the
    applications of graph-based approaches in traffic domain.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide a comprehensive review over typical deep learning techniques widely
    used in graph-based traffic works. We elaborate their theoretical aspects, advantages,
    limitations and variants in specific traffic tasks, hoping to inspire the followers
    to develop more novel models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discuss some challenges shared by most graph-based traffic tasks. For each
    challenge, we conclude multiple deep learning-based solutions and make necessary
    comparison, providing useful suggestions for model selection in traffic tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We collect benchmark datasets, open-source codes in related papers to facilitate
    baseline experiments in traffic domain. Finally, we propose some future research
    directions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of the paper is organized as follows. Section [II](#S2 "II Related
    Research Surveys ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic
    Domain: A Survey") presents some surveys in traffic domain and some reviews about
    graph neural networks. Section [III](#S3 "III Problems, Research Directions and
    Challenges ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic
    Domain: A Survey") briefly outlines several traffic problems and the corresponding
    research directions, challenges and solutions. Section [IV](#S4 "IV Problem Formulation
    and Graph Construction ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey") summarizes a general formulation about traffic problems
    and the graph construction from traffic datasets. Section [V](#S5 "V Deep Learning
    Techniques Perspective ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey") analyzes the functionality, advantages and defects
    of GNNs and other deep learning techniques, as well as examining the tricks to
    create novel variants of these techniques in specific traffic tasks. Section [VI](#S6
    "VI Challenges Perspective ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey") discusses common challenges in traffic domain and
    the corresponding multiple solutions. Section [VII](#S7 "VII Public Datasets and
    Open Source Codes ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic
    Domain: A Survey") provides hyperlinks of datasets and open codes in papers we
    investigate. Section [VIII](#S8 "VIII Future Directions ‣ How to Build a Graph-Based
    Deep Learning Architecture in Traffic Domain: A Survey") presents future directions.
    Section [IX](#S9 "IX Conclusion ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey") concludes the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: II Related Research Surveys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There have been some surveys summarizing the development process of algorithms
    in traffic tasks from different perspectives. Karlaftis et al. [[21](#bib.bib21)]
    discussed differences and similarities between statistical methods and neural
    networks to promote the comprehension between these two communities. Vlahogianni
    et al. [[22](#bib.bib22)] reviewed ten challenges on short-term traffic forecasting,
    which stemmed from the changing needs of ITS applications. Xie et al. [[23](#bib.bib23)]
    conducted a comprehensive overview of approaches in urban flow forecasting. Liu
    et al. [[7](#bib.bib7)] classified deep learning based urban big data fusion methods
    into three categories, i.e. DL-output-based fusion, DL-input-based fusion and
    DL-double-stage-based fusion. Deep learning approaches for popular topics including
    traffic network representation, traffic flow forecasting, traffic signal control,
    automatic vehicle detection are discussed in [[24](#bib.bib24)], [[25](#bib.bib25)].
    Veres et al. [[26](#bib.bib26)] and Chen et al.[[27](#bib.bib27)] gave a similar
    but more elaborate analysis on new emerging deep learning models in various transportation
    topics. Wang et al. [[28](#bib.bib28)] provided a spatial-temporal perspective
    to summarize deep learning techniques in traffic domain and other domains. However,
    all these surveys do not take graph neural networks (GNNs) related literatures
    into consideration, except that Wang et al. [[28](#bib.bib28)] mentioned GNNs
    but in a very short subsection.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, in recent years, there are several reviews summarizing literatures
    about GNNs in different aspects. Bronstein et al. [[29](#bib.bib29)] is the first
    to overview deep learning techniques on processing data in non-Euclidean space
    (e.g. graph data). Zhou et al. [[30](#bib.bib30)] categorized GNNs into graph
    types, propagation types and training types. In addition, they divided related
    applications into structural scenarios, non-structural scenarios, and other scenarios.
    Zhang et al.[[31](#bib.bib31)] introduced GNNs on small graphs and giant graphs
    respectively. Quan et al. [[32](#bib.bib32)] and Zhang et al. [[33](#bib.bib33)]
    focused on reviewing works in a specific branch of GNNs, i.e. graph convolutional
    network (GCN). However, they seldom introduce GNNs works in traffic scenarios.
    Wu et.al proposed [[34](#bib.bib34)] the only survey spending a paragraph to describe
    GNNs in traffic domain, which is obviously not enough for anyone desiring to explore
    this field.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, there still lacks a systematic and elaborated survey to explore
    the rapidly developed graph-based deep learning techniques in traffic domain recently.
    Our work aims to fill this gap and promote understanding of the new emerging techniques
    in transportation community.
  prefs: []
  type: TYPE_NORMAL
- en: III Problems, Research Directions and Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b4882e1dfd04f5f90bba98791b446b60.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Typical traffic problems and the corresponding research directions'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we introduce background knowledge in traffic domain briefly,
    including some important traffic problems and the corresponding research directions
    (as shown in Figure [1](#S3.F1 "Figure 1 ‣ III Problems, Research Directions and
    Challenges ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic
    Domain: A Survey")), as well as common challenges and techniques under these problems.
    On one hand, we believe that such a concise but systematic introduction can help
    readers understand this domain quickly. On the other hand, our survey shows that
    existing works related with graph-based deep learning techniques only cover some
    research directions, which inspires successors to transfer similar techniques
    to remaining directions.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Traffic Problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goals the transportation community aims to achieve include relieving traffic
    congestion, satisfying travel demand, enhancing traffic management, ensuring transportation
    safety and realizing automatic driving. Each problem under the corresponding traffic
    goal can be partitioned into several research directions and each direction can
    serve more than one problem.
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 Traffic Congestion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Traffic congestion [[35](#bib.bib35)] is one of the most important and urgent
    problems in modern cities in terms of significant time loss, air pollution and
    energy waste. The congestion can be solved by increasing the traffic efficiency
    [[36](#bib.bib36)], [[37](#bib.bib37)], alleviating the traffic congestion on
    road network [[38](#bib.bib38)], [[39](#bib.bib39)], [[40](#bib.bib40)], controlling
    the road conditions by traffic state prediction[[41](#bib.bib41)],[[42](#bib.bib42)],
    optimizing vehicle flow by controlling traffic signals [[43](#bib.bib43)],[[44](#bib.bib44)],
    optimizing passenger flow by predicting passenger demand in public transportation
    systems [[45](#bib.bib45)].
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Travel Demand
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The travel demand prediction refers to the demand of traffic services, such
    as taxi, bike, metro and bus in a crowd perspective. With the emerging of online
    ride-hailing platforms (e.g. Uber, DiDi) and rapid development of public transportation
    systems (e.g. metro system and bus system), travel demand prediction has become
    more and more important for transport authorities, business sectors and individuals.
    For related authorities, it can help to better allocate resources, e.g. increase
    metro frequency at rush hours, add more buses to service hotspots. For business
    sector, it enables them to better manage taxi-hiring [[46](#bib.bib46)], carpooling
    [[47](#bib.bib47)], bike-sharing services [[48](#bib.bib48)],[[49](#bib.bib49)],
    and maximize their revenues. For individuals, it encourages users to consider
    various forms of transportation to decrease their commuting time and improve travel
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: III-A3 Transportation Safety
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Transportation safety is an indispensable part of public safety. Traffic accidents
    can not only cause damage to victims, vehicles and road infrastructures, but also
    lead to traffic congestion and reduce efficiency of road network. Therefore, monitoring
    the traffic accidents is essential to avoid property loss and save life. Many
    researchers focus on directions such as detecting traffic incidents [[50](#bib.bib50)],
    predicting traffic accidents from social media data [[51](#bib.bib51)], predicting
    the injury severity of traffic accidents [[52](#bib.bib52)], [[53](#bib.bib53)],
    predicting prevention of accidents [[54](#bib.bib54)], [[55](#bib.bib55)], [[56](#bib.bib56)].
  prefs: []
  type: TYPE_NORMAL
- en: III-A4 Traffic Surveillance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Nowadays, surveillance cameras have been widely deployed in city roads, generating
    numerous images and videos [[27](#bib.bib27)]. Such development has enhanced traffic
    surveillance, which includes traffic law enforcement, automatic toll collection
    [[57](#bib.bib57)] and traffic monitoring systems. The research directions of
    traffic surveillance include license plate detection[[58](#bib.bib58)], automatic
    vehicle detection [[59](#bib.bib59)], pedestrian detection [[60](#bib.bib60)].
  prefs: []
  type: TYPE_NORMAL
- en: III-A5 Autonomous Driving
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, automatic driving vehicle has become a hot spot of research in transportation
    domain. Many tasks are related with visual recognition. The research directions
    of autonomous driving include lane/vehicle detection [[61](#bib.bib61)], pedestrian
    detection [[62](#bib.bib62)], traffic sign detection [[63](#bib.bib63)] and human/vehicle
    trajectory prediction [[64](#bib.bib64)].
  prefs: []
  type: TYPE_NORMAL
- en: III-B Research Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our survey of graph-based deep learning in traffic domain shows that existing
    works focus mainly on traffic state prediction, travel demand prediction, trajectory
    prediction. A few works focus on vehicle behavior classification [[65](#bib.bib65)],
    optimal dynamic electronic toll collection (DETC) scheme [[57](#bib.bib57)], path
    availability [[66](#bib.bib66)], traffic signal control [[67](#bib.bib67)]. To
    our best knowledge, traffic incident detection and vehicle detection have not
    been explored based in a graph view yet.
  prefs: []
  type: TYPE_NORMAL
- en: III-B1 Traffic State Prediction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Traffic state in literatures refers to traffic flow, traffic speed, travel time,
    traffic density and so on. Traffic Flow Prediction (TFP) [[68](#bib.bib68)],[[69](#bib.bib69)],
    Traffic Speed Prediction (TSP) [[70](#bib.bib70)], [[71](#bib.bib71)], Travel
    Time Prediction (TTP) [[72](#bib.bib72)],[[73](#bib.bib73)], [[74](#bib.bib74)]
    are hot branches of traffic state prediction and have attracted intensive studies.
  prefs: []
  type: TYPE_NORMAL
- en: III-B2 Travel Demand Prediction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Travel demand prediction aims to estimate the future number of users who require
    traffic services. It can be categorized into two kinds, i.e. zone-level demand
    prediction and origin-destination travel demand prediction. The former one aims
    to predict the future travel demand in each region of a city, for example, to
    predict future taxi request in each area of a city [[75](#bib.bib75)],[[76](#bib.bib76)],
    or to predict the station-level passenger demand in subway system [[77](#bib.bib77)],
    [[78](#bib.bib78)], [[79](#bib.bib79)], [[45](#bib.bib45)] or to predict the bike
    hiring demand in each region of a city [[48](#bib.bib48)],[[49](#bib.bib49)].
    The latter one aims to predict the number of travel demand from one region to
    another, which can provide richer information than the zone-level demand prediction
    and is a more challenging issue worth exploration. Up to now, there are only a
    few studies [[80](#bib.bib80)], [[81](#bib.bib81)], [[82](#bib.bib82)] directed
    towards the origin-destination based travel demand prediction, which is a promising
    research direction.
  prefs: []
  type: TYPE_NORMAL
- en: III-B3 Traffic Signal Control
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The traffic signal control aims to properly control the traffic lights so as
    to reduce vehicle staying time at the road intersections in the long run [[25](#bib.bib25)].
    Traffic signal control [[67](#bib.bib67)] can optimize the traffic flow and reduce
    traffic congestion and vehicle emission.
  prefs: []
  type: TYPE_NORMAL
- en: III-B4 Traffic Incident Detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Major incidents can cause fatal injuries to travelers and long delays on a road
    network. Therefore, understanding the main cause of incidents and the impact of
    incidents on a traffic network is crucial for a modern transportation management
    system [[50](#bib.bib50)],[[52](#bib.bib52)], [[53](#bib.bib53)].
  prefs: []
  type: TYPE_NORMAL
- en: III-B5 Human/Vehicle Trajectory Prediction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Trajectory Prediction [[64](#bib.bib64)], [[83](#bib.bib83)], [[84](#bib.bib84)]
    aims to forecast future positions of dynamic agents in a scene. Accurate human/vehicle
    trajectories prediction is of great importance for downstream tasks including
    autonomous driving and traffic surveillance [[85](#bib.bib85)]. For instance,
    an accurate pedestrian trajectory prediction can help controller to control the
    vehicle ahead in a dangerous environment [[86](#bib.bib86)]. It can also enable
    transportation surveillance system to identify suspicious activities [[87](#bib.bib87)].
  prefs: []
  type: TYPE_NORMAL
- en: III-C Challenges and Techniques Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d68339e180bc804472ff4d6cf39b0e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Traffic challenges and the corresponding deep learning techniques.
    SGCN refers spectral graph convolution network, DGCN refers diffusion graph convolution
    network, GAT refers graph attention network, TCN refers temporal convolution network,
    RNN refers recurrent neural network, GRU refers gated recurrent unit, LSTM refers
    long short term memory network, MLP refers multi-layer perceptron, Seq2Seq refers
    sequence to sequence model, GAN refers generative adversarial network.'
  prefs: []
  type: TYPE_NORMAL
- en: Although traffic problems and the related research directions are different,
    most of them share the same challenges, e.g. spatial dependency, temporal dependency,
    external factors.
  prefs: []
  type: TYPE_NORMAL
- en: III-C1 Spatiotemporal Dependency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are complex spatiotemporal dependency in traffic data which can affect
    the prediction in traffic tasks. For instance, to predict a traffic congestion
    in a region, its previous traffic conditions and the traffic conditions of its
    surrounding regions are important factors for prediction[[35](#bib.bib35)],[[38](#bib.bib38)],[[39](#bib.bib39)].
    In vehicle trajectory prediction, the stochastic behaviors of surrounding vehicles
    and the historical information of self-trajectory influence the prediction performance
    [[88](#bib.bib88)]. When it comes to predict the ride-hailing demand in a region,
    its previous orders as well as orders in other regions with similar functionality
    are critical for prediction[[89](#bib.bib89)]. To predict the traffic signal,
    the geometric features of multiple intersections are taken into consideration,
    as well as the previous traffic flow around [[67](#bib.bib67)].
  prefs: []
  type: TYPE_NORMAL
- en: III-C2 External Factors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Except the spatiotemporal data, some types of data play an important role in
    traffic tasks, referred as external factors, such as holidays, weather conditions
    (e.g. rainfall, temperature, air quality), extreme events [[90](#bib.bib90)] and
    traffic incidents (e.g. incident time, incident type) [[91](#bib.bib91)]. The
    influence of external factors on traffic conditions can be observed in daily life.
    A rainstorm is likely to affect the traffic volume. A large-scale concert or football
    match results in traffic congregation, affecting traffic conditions around.
  prefs: []
  type: TYPE_NORMAL
- en: To tackle challenges above, various deep learning techniques have been proposed.
    In this paper, we focus on graph-based deep learning architectures in traffic
    domain. Among these graph-based deep learning frameworks, graph neural networks
    (GNNs) are usually employed to model the spatial dependency in traffic network.
    Recurrent neural networks (RNNs) and temporal convolution network (TCN) are generally
    adopted to model the temporal dependency in traffic data. RNNs and Multi-layer
    Perceptrons (MLPs) are typically employed to process external factors. Sequence
    to Sequence (Seq2Seq) model is usually utilized to make multi-step traffic prediction.
    These techniques along with other tricks (e.g. gated mechanism, attention mechanism)
    are combined organically to improve the prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we aim to provide readers guidance about how to build a graph-based
    deep learning architecture and we have investigated enormous existing traffic
    works adopting graph-based deep learning solutions. In the following sections,
    we first introduce a common way to formulate the traffic problem and give detailed
    guidelines to build traffic graphs from various kinds of traffic data. Then we
    clarify the correlations between challenges and techniques (as shown in Figure
    [2](#S3.F2 "Figure 2 ‣ III-C Challenges and Techniques Overview ‣ III Problems,
    Research Directions and Challenges ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey")) in two perspectives, i.e. the techniques
    perspective and the challenges perspective. In the perspective of techniques,
    we introduce several common techniques and interpret the way they tackle challenges
    in traffic tasks. In the perspective of challenges, we elaborate each challenge
    and summarize the techniques which can tackle this challenge. In a word, we hope
    to provide insights into solving traffic challenges with various deep learning
    techniques based on a graph view.'
  prefs: []
  type: TYPE_NORMAL
- en: IV Problem Formulation and Graph Construction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Among the graph-based deep learning traffic literatures we investigate, the
    majority of tasks (more than 80%) belong to spatiotemporal forecasting problems,
    especially traffic state prediction and travel demand prediction. In this section,
    we first list commonly used notations. Then we summarize a general formulation
    of graph-based spatiotemporal prediction in traffic domain. We provide details
    to construct graphs from various traffic datasets. We also discuss multiple definitions
    of adjacency matrix, which represents the topology of graph-based traffic network
    and is the key element of graph-based solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Notations In This Paper'
  prefs: []
  type: TYPE_NORMAL
- en: '| Graph related elements |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{G}$ | Graph |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{E}$ | Edges of graph $\mathbf{G}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{V}$ | Vertices of graph $\mathbf{G}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{A}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | Adjacency matrix
    of graph $\mathbf{G}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{A}^{T}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | The transpose
    matrix of $\mathbf{A}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{\tilde{A}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | Equal to
    $\mathbf{A}+\mathbf{I_{N}}$, a self-looped $\mathbf{A}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{D}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | The degree matrix
    of adjacency matrix $\mathbf{A}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{D_{I}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | The in-degree
    matrix of adjacency matrix $\mathbf{A}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{D_{O}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | The out-degree
    matrix of adjacency matrix $\mathbf{A}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{L}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | Laplacian matrix
    of graph $\mathbf{G}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{U}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | The eigenvectors
    matrix of $\mathbf{L}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{\Lambda}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | The diagonal
    eigenvalues matrix of $\mathbf{L}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\boldsymbol{\lambda}_{max}$ | The max eigenvalue of $\mathbf{L}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{I_{N}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | An identity
    matrix |'
  prefs: []
  type: TYPE_TB
- en: '| Hyper parameters |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{N}$ | The number of nodes in graph $\mathbf{G}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{F_{I}}$ | The number of input features |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{F_{H}}$ | The number of hidden features |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{F_{O}}$ | The number of output features |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{P}$ | The number of past time slices |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{Q}$ | The number of future time slices |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{d}$ | The dilation rate |'
  prefs: []
  type: TYPE_TB
- en: '| Trainable parameters |'
  prefs: []
  type: TYPE_TB
- en: '| $W,b,\theta,\phi$ | The trainable parameters |'
  prefs: []
  type: TYPE_TB
- en: '| $\Theta$ | The kernel |'
  prefs: []
  type: TYPE_TB
- en: '| Activation functions |'
  prefs: []
  type: TYPE_TB
- en: '| $\boldsymbol{\rho}(\boldsymbol{\cdot})$ | The activation function, e.g. tanh,
    sigmoid, ReLU |'
  prefs: []
  type: TYPE_TB
- en: '| $\boldsymbol{\sigma}(\boldsymbol{\cdot})\in[0,1]$ | The sigmoid function
    |'
  prefs: []
  type: TYPE_TB
- en: '| $\boldsymbol{tanh}(\boldsymbol{\cdot})\in[-1,1]$ | The hyperbolic tangent
    function |'
  prefs: []
  type: TYPE_TB
- en: '| $\boldsymbol{ReLU}(\boldsymbol{\cdot})\in[0,x]$ | The ReLU function |'
  prefs: []
  type: TYPE_TB
- en: '| Operations |'
  prefs: []
  type: TYPE_TB
- en: '| $\boldsymbol{*_{\mathcal{G}}}$ | The convolution operator on graph |'
  prefs: []
  type: TYPE_TB
- en: '| $\boldsymbol{\odot}$ | Element-wise multiplication |'
  prefs: []
  type: TYPE_TB
- en: '| $\boldsymbol{\cdot}$ | Matrix multiplication |'
  prefs: []
  type: TYPE_TB
- en: '| Spatial variables |'
  prefs: []
  type: TYPE_TB
- en: '| $X\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{I}}}$ | An input graph composed
    of $\mathbf{N}$ nodes with $\mathbf{F_{I}}$ features |'
  prefs: []
  type: TYPE_TB
- en: '| $X_{j}\in\mathbb{R}^{\mathbf{N}}$ | The $j^{th}$ feature of an input graph
    |'
  prefs: []
  type: TYPE_TB
- en: '| $X^{i}\in\mathbb{R}^{\mathbf{F_{I}}}$ | Node $i$ in an input graph |'
  prefs: []
  type: TYPE_TB
- en: '| $x\in\mathbb{R}^{\mathbf{N}}$ | A simply input graph |'
  prefs: []
  type: TYPE_TB
- en: '| $Y\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{O}}}$ | An output graph composed
    of $\mathbf{N}$ nodes with $\mathbf{\mathbf{F_{O}}}$ features |'
  prefs: []
  type: TYPE_TB
- en: '| $Y_{j}\in\mathbb{R}^{\mathbf{N}}$ | The $j^{th}$ feature of an output graph
    |'
  prefs: []
  type: TYPE_TB
- en: '| $Y^{i}\in\mathbb{R}^{\mathbf{F_{O}}}$ | Node $i$ in an output graph |'
  prefs: []
  type: TYPE_TB
- en: '| $y\in\mathbb{R}^{\mathbf{N}}$ | A simply output graph |'
  prefs: []
  type: TYPE_TB
- en: '| Temporal variables |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{X}\in\mathbb{R}^{\mathbf{P}\times\mathbf{F_{I}}}$ | A sequential
    input with $\mathbf{F_{I}}$ features over $\mathbf{P}$ time slices |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{X}_{t}\in\mathbb{R}^{\mathbf{F_{I}}}$ | The element of sequential
    input at time $t$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{x}\in\mathbb{R}^{\mathbf{P}}$ | A simply sequential input over $\mathbf{P}$
    time slices |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{x}_{t}\in\mathbb{R}$ | The element of simply sequential input at
    time $t$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{H}_{t}\in\mathbb{R}^{\mathbf{F_{H}}}$ | A hidden state with $\mathbf{\mathbf{F}_{H}}$
    features at time $t$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{Y}\in\mathbb{R}^{\mathbf{P}\times\mathbf{F_{O}}}$ | A sequential
    output with $\mathbf{F_{O}}$ features over $\mathbf{P}$ time slices |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{Y}_{t}\in\mathbb{R}^{\mathbf{F_{O}}}$ | The element of sequential
    output at time $t$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{y}\in\mathbb{R}^{\mathbf{P}}$ | A simply sequential output over
    $\mathbf{P}$ time slices |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{y}_{t}\in\mathbb{R}$ | The element of simply sequential output at
    time $t$ |'
  prefs: []
  type: TYPE_TB
- en: '| Spatiotemporal variables |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{X}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{I}}}$
    | A series of input graphs composed of $\mathbf{N}$ nodes with $\mathbf{F_{I}}$
    features over $\mathbf{P}$ time slices |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{X}_{t}\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{I}}}$ | An input
    graph at time $t$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{X}^{i}_{t}\in\mathbb{R}^{\mathbf{F_{I}}}$ | node $i$ in an input
    graph at time $t$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{X}_{t,j}\in\mathbb{R}^{\mathbf{N}}$ | the $j^{th}$ feature of an
    input graph at time $t$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{X}^{i}_{t,j}\in\mathbb{R}$ | the $j^{th}$ feature of node $i$ in
    an input graph at time $t$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{Y}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{O}}}$
    | A series of output graphs composed of $\mathbf{N}$ nodes with $\mathbf{F_{O}}$
    features over $\mathbf{P}$ time slices |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{Y}_{t}\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{O}}}$ | An output
    graph at time $t$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{Y}^{i}_{t}\in\mathbb{R}^{\mathbf{F_{O}}}$ | node $i$ in an output
    graph at time $t$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{Y}_{t,j}\in\mathbb{R}^{\mathbf{N}}$ | the $j^{th}$ feature of an
    output graph at time $t$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{Y}^{i}_{t,j}\in\mathbb{R}$ | the $j^{th}$ feature of node $i$ in
    an output graph at time $t$ |'
  prefs: []
  type: TYPE_TB
- en: IV-A Notations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we have denoted some commonly used notations, including graph
    related elements, variables, parameters (hyper or trainable), activation functions,
    and operations. The variables are comprised of input variables {$x$, $X$, $\mathbf{x}$,
    $\mathbf{X}$, $\mathcal{X}$} and output variables {$y$, $Y$, $\mathbf{y}$, $\mathbf{Y}$,
    $\mathcal{Y}$}. These variables can divided into spatial variables, temporal variables,
    spatiotemporal variables. The spatial variables are only related with spatial
    attributes and the temporal variables are only related with temporal attributes.
    The spatiotemporal variables are related with both spatial and temporal attributes.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Graph-based Spatio-Temporal Forecasting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To our best knowledge, most existing graph-based deep learning traffic works
    can be categorized into spatial-temporal forecasting due to that most traffic
    datasets have both spatial attributes and temporal attributes. They formalize
    their prediction problems in a very similar way despite different mathematical
    notations and representations. We summarize their works to provide a general formulation
    for graph-based spatial-temporal problems in traffic domain.
  prefs: []
  type: TYPE_NORMAL
- en: The traffic network is represented as a graph $\mathbf{G}=(\mathbf{V},\mathbf{E},\mathbf{A})$,
    which can be weighted [[92](#bib.bib92)],[[72](#bib.bib72)],[[68](#bib.bib68)]
    or unweighted [[66](#bib.bib66)],[[93](#bib.bib93)],[[94](#bib.bib94)], directed
    [[66](#bib.bib66)],[[95](#bib.bib95)],[[96](#bib.bib96)] or undirected [[69](#bib.bib69)],[[97](#bib.bib97)],
    depending on specific tasks. $\mathbf{V}$ is a set of nodes and $|\mathbf{V}|=\mathbf{N}$
    refers $\mathbf{N}$ nodes in the graph. Each node represents a traffic object,
    which can be a sensor [[70](#bib.bib70)],[[69](#bib.bib69)],[[98](#bib.bib98)],
    a road segment [[92](#bib.bib92)],[[99](#bib.bib99)],[[100](#bib.bib100)], a road
    intersection [[72](#bib.bib72)],[[95](#bib.bib95)], [[68](#bib.bib68)]. $\mathbf{E}$
    is a set of edges referring the connectivity between nodes.
  prefs: []
  type: TYPE_NORMAL
- en: $\mathbf{A}=(\mathbf{a}_{ij})_{\mathbf{N}\times\mathbf{N}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$
    is the adjacency matrix containing the topology information of the traffic network,
    which is valuable for traffic prediction. The entry $\mathbf{a}_{ij}$ in matrix
    $\mathbf{A}$ represents the node proximity and is different in various applications.
    It can be a binary value $0$ or $1$ [[69](#bib.bib69)],[[93](#bib.bib93)],[[94](#bib.bib94)].
    Specifically, $0$ indicates no edge between node $i$ and node $j$ while $1$ indicates
    an edge between these two nodes. It can also be a float value representing some
    kind of relationship between nodes [[92](#bib.bib92)],[[101](#bib.bib101)], e.g.
    the road distance between two sensors [[70](#bib.bib70)],[[102](#bib.bib102)],[[96](#bib.bib96)].
  prefs: []
  type: TYPE_NORMAL
- en: $\mathcal{X}_{t}=[\mathcal{X}_{t}^{1},\cdots,\mathcal{X}_{t}^{i},\cdots,\mathcal{X}_{t}^{\mathbf{N}}]\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{I}}}$
    is a feature matrix of the whole graph at time $t$. $\mathcal{X}^{i}_{t}\in\mathbb{R}^{\mathbf{F_{I}}}$
    represents node $i$ with $\mathbf{F_{I}}$ features at time $t$. The features are
    usually traffic indicators, such as traffic flow [[97](#bib.bib97)],[[96](#bib.bib96)],
    traffic speed [[70](#bib.bib70)],[[99](#bib.bib99)],[[95](#bib.bib95)], or rail-hail
    orders [[89](#bib.bib89)],[[101](#bib.bib101)], passenger flow [[77](#bib.bib77)],[[78](#bib.bib78)].
    Usually, continuous indicators are normalized during data preprocessing phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given historical indicators of the whole traffic network over past $\mathbf{P}$
    time slices, denoted as $\mathcal{X}=[\mathcal{X}_{1},\cdots,\mathcal{X}_{t},\cdots,\mathcal{X}_{\mathbf{P}}]\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{I}}}$,
    the spatial-temporal forecasting problem in traffic domain aims to predict the
    future traffic indicators over the next $\mathbf{Q}$ time slices, denoted as $\mathcal{Y}=[\mathcal{Y}_{1},\cdots,\mathcal{Y}_{t},\cdots,\mathcal{Y}_{\mathbf{Q}}]\in\mathbb{R}^{\mathbf{Q}\times\mathbf{N}\times\mathbf{F_{O}}}$,
    where $\mathcal{Y}_{t}\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{O}}}$ represents
    output graph with $\mathbf{F_{O}}$ features at time $t$. The problem (as shown
    in Figure [3](#S4.F3 "Figure 3 ‣ IV-B Graph-based Spatio-Temporal Forecasting
    ‣ IV Problem Formulation and Graph Construction ‣ How to Build a Graph-Based Deep
    Learning Architecture in Traffic Domain: A Survey")) can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{Y}=f(\mathcal{X};\mathbf{G})$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/0af64006d3a74e004443f8d9429b10ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The graph-based spatial-temporal problem formulation in traffic domain'
  prefs: []
  type: TYPE_NORMAL
- en: Some works predict multiple traffic indicators in the future (i.e. $\mathbf{F_{O}}>1$)
    while other works predict one traffic indicator (i.e. $\mathbf{F_{O}}=1$), such
    as traffic speed [[99](#bib.bib99)], [[95](#bib.bib95)], rail-hide orders [[89](#bib.bib89)],[[101](#bib.bib101)].
    Some works only consider one-step prediction [[103](#bib.bib103)],[[75](#bib.bib75)],[[57](#bib.bib57)],
    i.e. forecasting traffic conditions in the next time step and $\mathbf{Q}=1$.
    But models designed for one-step prediction can’t be directly applied to predict
    multiple steps, because they are optimized by reducing error during the training
    stage for the next-step instead of the subsequent time steps [[76](#bib.bib76)].
    Many works focus on multi-step forecasting (i.e. $\mathbf{Q}>1$) [[104](#bib.bib104)],[[42](#bib.bib42)],[[105](#bib.bib105)].
    According to our survey, there are mainly three kinds of techniques to generate
    a multi-step output, i.e. FC layer, Seq2Seq, dilation technique. Fully connected
    (FC) layer is the simplest technique as being the output layer to obtain a desired
    output shape [[70](#bib.bib70)], [[69](#bib.bib69)], [[106](#bib.bib106)], [[93](#bib.bib93)],
    [[91](#bib.bib91)], [[107](#bib.bib107)]. Some works adopt the Sequence to Sequence
    (Seq2Seq) architecture with a RNNs-based decoder to generate output recursively
    through multiple steps [[108](#bib.bib108)],[[98](#bib.bib98)],[[109](#bib.bib109)],[[104](#bib.bib104)],[[110](#bib.bib110)],[[96](#bib.bib96)].
    Dilation technique is adopted to get a desired output length [[102](#bib.bib102)],
    [[105](#bib.bib105)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b2b985f7fa0050c683c29a5f25dacf96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Graph construction from various traffic datasets: a) In a sensor
    graph, sensor represents node and there is an edge between adjacent sensors on
    the same side of a road. b) In a road segment graph, road segment represents node
    and two connected segments have an edge. c) In a road intersection graph, road
    intersection represents node and there is an edge between two road intersections
    connected by a road segment. Most works consider the edge direction being the
    traffic flow direction[[70](#bib.bib70)],[[98](#bib.bib98)],[[66](#bib.bib66)],[[96](#bib.bib96)],[[68](#bib.bib68)],[[111](#bib.bib111)],
    while some works ignore the direction and construct an undirected graph [[69](#bib.bib69)],[[102](#bib.bib102)],[[94](#bib.bib94)][[100](#bib.bib100)],[[95](#bib.bib95)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, some works not only consider traffic indicators, but also take
    external factors (e.g. time attributes, weather) [[70](#bib.bib70)],[[112](#bib.bib112)],[[91](#bib.bib91)],[[113](#bib.bib113)]
    into consideration. Therefore, the problem formulation becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{Y}=f(\mathcal{X},\mathcal{E};\mathbf{G})$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{E}$ is the external factors.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Graph Construction from Traffic Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To model a traffic network as a graph is vital for any works that intend to
    utilize graph-based deep learning architectures to solve traffic problems. A traffic
    graph $\mathbf{G}$ for prediction is generally composed of four parts, i.e. nodes
    $\mathbf{V}$, node features (feature matrix $\mathcal{X}_{t}$), edges $\mathbf{E}$,
    edge weight $\mathbf{a}_{ij}$. Note that edges and edge weight can be represented
    by adjacency matrix $\mathbf{A}=(\mathbf{a}_{ij})_{\mathbf{N}\times\mathbf{N}}$.
    Nodes and node features can be constructed from traffic datasets. The construction
    of adjacency matrix not only depends on traffic datasets but also depends on the
    assumption of node relationship, which can be static or dynamic. We first introduce
    how to construct node and node features from various kinds of traffic datasets
    and then we give a systematic introduction to the popular adjacency matrices.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C1 Nodes and Node Features Construction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many works are different in graph construction due to the different traffic
    datasets they collect. We divide these datasets into four categories according
    to the traffic infrastructures: data collected by the sensors deployed on road
    network [[70](#bib.bib70)],[[69](#bib.bib69)],[[71](#bib.bib71)], vehicle GPS
    trajectories [[68](#bib.bib68)],[[111](#bib.bib111)],[[95](#bib.bib95)], orders
    of rail-hailing system [[101](#bib.bib101)],[[76](#bib.bib76)],[[113](#bib.bib113)],
    transaction records of subway system [[77](#bib.bib77)],[[78](#bib.bib78)] or
    bus system [[111](#bib.bib111)]. For each category, we describe the datasets and
    explain the construction of nodes $\mathbf{V}$, feature matrix $\mathcal{X}_{t}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Sensors Datasets Traffic measurements (e.g. traffic speed) are generally collected
    during a short time interval by the sensors (e.g. loop detectors, probes) on a
    road network in metropolises like Beijing [[92](#bib.bib92)], California [[71](#bib.bib71)],
    Los Angeles [[70](#bib.bib70)], New York [[99](#bib.bib99)], Philadelphia [[106](#bib.bib106)],
    Seattle [[94](#bib.bib94)], Xiamen [[98](#bib.bib98)], and Washington [[106](#bib.bib106)].
    Sensor datasets are the most prevalent datasets in existing works, especially
    PEMS dataset from California. Generally, a road network contains traffic objects
    such as sensors, road segments.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sensor graph (as shown in Figure [4](#S4.F4 "Figure 4 ‣ IV-B Graph-based
    Spatio-Temporal Forecasting ‣ IV Problem Formulation and Graph Construction ‣
    How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey"))
    is constructed in [[70](#bib.bib70)],[[69](#bib.bib69)],[[96](#bib.bib96)] where
    a sensor represents a node and features of this node are traffic measurements
    collected by its corresponding sensor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A road segment graph (as shown in Figure [4](#S4.F4 "Figure 4 ‣ IV-B Graph-based
    Spatio-Temporal Forecasting ‣ IV Problem Formulation and Graph Construction ‣
    How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey"))
    is constructed in [[92](#bib.bib92)],[[99](#bib.bib99)],[[106](#bib.bib106)] where
    a road segment represents a node and features of this node are average traffic
    measurements (e.g. traffic speed) recorded by all the sensors on its corresponding
    road segment.'
  prefs: []
  type: TYPE_NORMAL
- en: GPS Datasets GPS trajectories datasets are usually generated by numbers of taxis
    over some period of time in a city, e.g. Beijing [[68](#bib.bib68)], Chengdu [[68](#bib.bib68)],
    Shenzhen [[93](#bib.bib93)], Cologne [[95](#bib.bib95)], and Chicago [[100](#bib.bib100)].
    Each taxi produces substantial GPS records with time, location, speed information
    every day. Every GPS record is fitted to its nearest road on the city road map.
    All roads are divided into multiple road segments through road intersections.
  prefs: []
  type: TYPE_NORMAL
- en: 'A road segment graph (as shown in Figure [4](#S4.F4 "Figure 4 ‣ IV-B Graph-based
    Spatio-Temporal Forecasting ‣ IV Problem Formulation and Graph Construction ‣
    How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey"))
    is constructed in [[100](#bib.bib100)], [[93](#bib.bib93)] where a road segment
    represents a node and features of this node are average traffic measurements recorded
    by all the GPS points on its corresponding road segment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A road intersection graph (as shown in Figure [4](#S4.F4 "Figure 4 ‣ IV-B Graph-based
    Spatio-Temporal Forecasting ‣ IV Problem Formulation and Graph Construction ‣
    How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey"))
    is constructed in [[72](#bib.bib72)],[[68](#bib.bib68)],[[95](#bib.bib95)] where
    a road intersection represents a node and features of this node are sum-up of
    the traffic measurements through it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rail-hailing Datasets These datasets record car/taxi/bicycle demand orders
    over a period of time in cities like Beijing [[89](#bib.bib89)],[[101](#bib.bib101)],
    Chengdu [[101](#bib.bib101)], and Shanghai [[89](#bib.bib89)], Manhattan, New
    York [[99](#bib.bib99)]. The target city with an OpenStreetMap is divided into
    equal-size grid-based regions (as shown in Figure [5](#S4.F5 "Figure 5 ‣ IV-C1
    Nodes and Node Features Construction ‣ IV-C Graph Construction from Traffic Datasets
    ‣ IV Problem Formulation and Graph Construction ‣ How to Build a Graph-Based Deep
    Learning Architecture in Traffic Domain: A Survey")). Each region is defined as
    a node in a graph. The feature of each node is the number of orders in the corresponding
    region during a given interval.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0e06f62f993496e47efadcc8c998b317.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Multi-relationships: a) A spatial locality graph: This graph is based
    on spatial proximity and it constructs edges between a region and its 8 adjacent
    regions in a 3 x 3 grid. b) A functional similarity graph: This graph assumes
    that regions sharing similar functionality might have similar demand patterns.
    Edges are constructed between regions with similar surrounding POIs (Point of
    Interests). c) A transportation connectivity graph: This graph assumes that regions
    which are geographically distant from the target region but conveniently reachable
    by transportation (e.g. motorway, highway or subway) have strong correlations
    with the target region. There should be edges between them.'
  prefs: []
  type: TYPE_NORMAL
- en: Transactions Datasets These datasets are collected by automatic fare collection
    (AFC) system deployed in public transit network, such as subway network and bus
    network. A subway graph is constructed in [[77](#bib.bib77)],[[78](#bib.bib78)],[[111](#bib.bib111)].
    Each station in the subway system is treated as a node. The features of a station
    usually contain the number of passengers departing at the station and the number
    of passengers arriving at the station during a given time interval based on transaction
    records collected by subwayAFC systems, which log when each passenger enters and
    leaves a metro system.
  prefs: []
  type: TYPE_NORMAL
- en: A bus graph is constructed in [[111](#bib.bib111)]. Each bus stop is treated
    as a node. The features of a bus stop usually contain the number of departing
    passengers at the station during a given time interval, but not the number of
    arriving passengers, since most bus AFC systems only log the boarding record of
    each passenger.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C2 Adjacency Matrix Construction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The adjacency matrix $\mathbf{A}=(\mathbf{a}_{ij})_{\mathbf{N}\times\mathbf{N}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$
    is the key to capture spatial dependency which is valuable for prediction. Element
    $\mathbf{a}_{ij}$ (unweighted or weighted) represents heterogeneous pairwise relationship
    between nodes. However, there are different assumptions of node relationships
    in different traffic scenarios, based on which the adjacency matrix can be designed
    differently, e.g. fixed matrix, dynamic matrix, evolving matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Fixed Matrix Many works assume that the correlations between nodes are fixed
    and do not change over time. Therefore, a fixed matrix is designed and unchanged
    during the whole experiment. Researchers have designed various fixed adjacency
    matrices to capture various kinds of pre-defined correlations between nodes in
    a traffic graph, like function similarity and transportation connectivity [[89](#bib.bib89)],
    semantic connection [[101](#bib.bib101)], temporal similarity [[71](#bib.bib71)].
    Here, we introduce several popular adjacency matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Connection matrix measures the connectivity between nodes. The entry value in
    the matrix is defined as $1$ (connection) or $0$ (disconnection) [[69](#bib.bib69)],[[106](#bib.bib106)],[[93](#bib.bib93)],[[94](#bib.bib94)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Distance matrix measures the closeness between nodes in terms of geometrical
    distance. The entry value is defined as a function of distance between nodes [[85](#bib.bib85)].
    For example, some works [[72](#bib.bib72)],[[68](#bib.bib68)],[[100](#bib.bib100)],[[97](#bib.bib97)],[[76](#bib.bib76)],[[95](#bib.bib95)]
    used threshold Gaussian Kernel to define $\mathbf{a}_{ij}$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{a}_{ij}=\left\{\begin{array}[]{l}\exp\left(-\frac{\mathbf{d}_{ij}^{2}}{\sigma^{2}}\right),i\neq
    j\text{ and }\mathbf{d}_{ij}\geq\epsilon\\ 0\quad,i=j\text{ or }\mathbf{d}_{ij}<\epsilon\end{array}\right.$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{d}_{ij}$ is the distance between node $i$ and node $j$. Hyper
    parameters $\sigma^{2}$ and $\epsilon$ are thresholds to control the distribution
    and sparsity of matrix $\mathbf{A}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Functional similarity matrix measures whether two nodes are similar in terms
    of functionality (e.g. both of them are business zones). The corresponding functional
    similarity graph is shown in Figure [5](#S4.F5 "Figure 5 ‣ IV-C1 Nodes and Node
    Features Construction ‣ IV-C Graph Construction from Traffic Datasets ‣ IV Problem
    Formulation and Graph Construction ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey"). It assumes that regions sharing similar
    functionality might have similar demand patterns [[89](#bib.bib89)]. Edges are
    constructed between regions with similar surrounding POIs (Point of Interests).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transportation connectivity matrix measures the correlation between regions
    that are geographically distant but conveniently reachable by motorway, highway
    or subway. The corresponding transportation connectivity graph is shown in Figure
    [5](#S4.F5 "Figure 5 ‣ IV-C1 Nodes and Node Features Construction ‣ IV-C Graph
    Construction from Traffic Datasets ‣ IV Problem Formulation and Graph Construction
    ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey").
    There should be edges between them [[89](#bib.bib89)].'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Matrix Some works argue that the pre-defined matrix does not necessarily
    reflect the true dependency among nodes due to the defective prior knowledge or
    incomplete data [[72](#bib.bib72)]. A novel adaptive matrix is proposed and learned
    through data. Experiments in [[102](#bib.bib102)],[[72](#bib.bib72)],[[99](#bib.bib99)]
    have proven that adaptive matrix can precisely capture the hidden spatial dependency
    more precisely in some traffic tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Evolving Matrix In some scenarios, the graph structure can evolve over time
    as some edges may become unavailable, like road congestion or closure, and become
    available again after alleviating congestion. An evolving topological structure
    [[66](#bib.bib66)], [[114](#bib.bib114)] is incorporated into the model to capture
    such dynamic spatial change.
  prefs: []
  type: TYPE_NORMAL
- en: V Deep Learning Techniques Perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE II: The decomposition of graph-based deep learning architectures investigated
    in this paper'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Year | Directions | Models | Modules |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[83](#bib.bib83)] | 2019 | Human/Vehicle Trajectory Prediction | SAGCN |
    SGCN, TCN, Attention |'
  prefs: []
  type: TYPE_TB
- en: '| [[87](#bib.bib87)] | 2019 | Human/Vehicle Trajectory Prediction | Social-BiGAT
    | GAT, LSTM, GAN |'
  prefs: []
  type: TYPE_TB
- en: '| [[85](#bib.bib85)] | 2020 | Human/Vehicle Trajectory Prediction | Social-STGCNN
    | SGCN, TCN |'
  prefs: []
  type: TYPE_TB
- en: '| [[64](#bib.bib64)] | 2020 | Human/Vehicle Trajectory Prediction | Social-WaGDAT
    | GAT, Seq2Seq, MLP |'
  prefs: []
  type: TYPE_TB
- en: '| [[88](#bib.bib88)] | 2020 | Human/Vehicle Trajectory Prediction |  | SGCN,
    LSTM |'
  prefs: []
  type: TYPE_TB
- en: '| [[57](#bib.bib57)] | 2019 | Optimal DETC Scheme |  | SGCN |'
  prefs: []
  type: TYPE_TB
- en: '| [[65](#bib.bib65)] | 2020 | Vehicle Behaviour Classification | MR-GCN | SGCN,
    LSTM |'
  prefs: []
  type: TYPE_TB
- en: '| [[67](#bib.bib67)] | 2018 | Traffic Signal Control |  | SGCN, Reinforcement
    Learning |'
  prefs: []
  type: TYPE_TB
- en: '| [[66](#bib.bib66)] | 2019 | Path Availability | LRGCN-SAPE | SGCN, LSTM |'
  prefs: []
  type: TYPE_TB
- en: '| [[72](#bib.bib72)] | 2019 | Travel Time Prediction |  | SGCN |'
  prefs: []
  type: TYPE_TB
- en: '| [[68](#bib.bib68)] | 2018 | Traffic Flow Prediction | KW-GCN | SGCN, LCN
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[97](#bib.bib97)] | 2018 | Traffic Flow Prediction | Graph-CNN | CNN, Graph
    Matrix |'
  prefs: []
  type: TYPE_TB
- en: '| [[115](#bib.bib115)] | 2018 | Traffic Flow Prediction | DST-GCNN | SGCN |'
  prefs: []
  type: TYPE_TB
- en: '| [[69](#bib.bib69)] | 2019 | Traffic Flow Prediction |  | SGCN, CNN, Attention
    Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[111](#bib.bib111)] | 2019 | Traffic Flow Prediction |  | SGCN, TCN, Residual
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[109](#bib.bib109)] | 2019 | Traffic Flow Prediction | GHCRNN | SGCN, GRU,
    Seq2Seq |'
  prefs: []
  type: TYPE_TB
- en: '| [[104](#bib.bib104)] | 2019 | Traffic Flow Prediction | STGSA | GAT, GRU,
    Seq2Seq |'
  prefs: []
  type: TYPE_TB
- en: '| [[96](#bib.bib96)] | 2019 | Traffic Flow Prediction | DCRNN-RIL | DGCN, GRU,
    Seq2Seq |'
  prefs: []
  type: TYPE_TB
- en: '| [[116](#bib.bib116)] | 2019 | Traffic Flow Prediction | MVGCN | SGCN, FNN,
    Gate Mechanism, Residual |'
  prefs: []
  type: TYPE_TB
- en: '| [[117](#bib.bib117)] | 2019 | Traffic Flow Prediction | STGI- ResNet | SGCN,
    Residual |'
  prefs: []
  type: TYPE_TB
- en: '| [[118](#bib.bib118)] | 2020 | Traffic Flow Prediction | FlowConvGRU | DGCN,
    GRU |'
  prefs: []
  type: TYPE_TB
- en: '| [[45](#bib.bib45)] | 2020 | Traffic Flow Prediction | Multi-STGCnet | SGCN,
    LSTM |'
  prefs: []
  type: TYPE_TB
- en: '| [[119](#bib.bib119)] | 2018 | Traffic Speed Prediction |  | GAT, GRU, Gate
    Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[70](#bib.bib70)] | 2019 | Traffic Speed Prediction | GTCN | SGCN, TCN,
    Residual |'
  prefs: []
  type: TYPE_TB
- en: '| [[71](#bib.bib71)] | 2019 | Traffic Speed Prediction | 3D-TGCN | SGCN, Gate
    Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[91](#bib.bib91)] | 2019 | Traffic Speed Prediction | DIGC-Net | SGCN, LSTM
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[120](#bib.bib120)] | 2019 | Traffic Speed Prediction | MW-TGC | SGCN, LSTM
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[110](#bib.bib110)] | 2019 | Traffic Speed Prediction | AGC-Seq2Seq | SGCN,
    GRU, Seq2Seq, Attention Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[95](#bib.bib95)] | 2019 | Traffic Speed Prediction | GCGA | SGCN, GAN |'
  prefs: []
  type: TYPE_TB
- en: '| [[107](#bib.bib107)] | 2019 | Traffic Speed Prediction | ST-GAT | GAT, LSTM
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[92](#bib.bib92)] | 2018 | Traffic State Prediction | STGCN | SGCN, TCN,
    Gate Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[108](#bib.bib108)] | 2018 | Traffic State Prediction | DCRNN | DGCN, GRU,
    Seq2Seq |'
  prefs: []
  type: TYPE_TB
- en: '| [[99](#bib.bib99)] | 2019 | Traffic State Prediction |  | SGCN, CNN, Gate
    Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[112](#bib.bib112)] | 2019 | Traffic State Prediction | MRes-RGNN | DGCN,
    GRU, Residual, Gate Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[100](#bib.bib100)] | 2019 | Traffic State Prediction | GCGAN | DGCN, LSTM,
    GAN, Seq2Seq, Attention Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[102](#bib.bib102)] | 2019 | Traffic State Prediction | Graph WaveNet |
    DGCN, TCN, Residual, Gate Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[93](#bib.bib93)] | 2019 | Traffic State Prediction | T-GCN | SGCN, GRU
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[94](#bib.bib94)] | 2019 | Traffic State Prediction | TGC-LSTM | SGCN, LSTM
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[41](#bib.bib41)] | 2019 | Traffic State Prediction | DualGraph | Seq2Seq,
    MLP, Graph Matirx |'
  prefs: []
  type: TYPE_TB
- en: '| [[105](#bib.bib105)] | 2019 | Traffic State Prediction | ST-UNet | SGCN,
    GRU |'
  prefs: []
  type: TYPE_TB
- en: '| [[98](#bib.bib98)] | 2020 | Traffic State Prediction | GMAN | GAT, Gate Mechanism,
    Seq2Seq, Attention Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[106](#bib.bib106)] | 2020 | Traffic State Prediction | OGCRNN | SGCN, GRU,
    Attention Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[42](#bib.bib42)] | 2020 | Traffic State Prediction | MRA-BGCN | SGCN, GRU,
    Seq2Seq, Attention Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[48](#bib.bib48)] | 2018 | Travel Demand-Bike |  | SGCN, LSTM, Seq2Seq |'
  prefs: []
  type: TYPE_TB
- en: '| [[49](#bib.bib49)] | 2018 | Travel Demand-Bike | GCNN-DDGF | SGCN, LSTM |'
  prefs: []
  type: TYPE_TB
- en: '| [[77](#bib.bib77)] | 2020 | Travel Demand-Subway | PVCGN | SGCN, GRU, Seq2Seq,
    Attention Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[78](#bib.bib78)] | 2019 | Travel Demand-Subway | WDGTC | Tensor Completion,
    Graph Matrix |'
  prefs: []
  type: TYPE_TB
- en: '| [[89](#bib.bib89)] | 2019 | Travel Demand-Taxi | CGRNN | SGCN, RNN, Attention
    Mechanism, Gate Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| [[101](#bib.bib101)] | 2019 | Travel Demand-Taxi | GEML | SGCN, LSTM |'
  prefs: []
  type: TYPE_TB
- en: '| [[75](#bib.bib75)] | 2019 | Travel Demand-Taxi | MGCN | SGCN |'
  prefs: []
  type: TYPE_TB
- en: '| [[76](#bib.bib76)] | 2019 | Travel Demand-Taxi | STG2Seq | SGCN, Seq2Seq,
    Attention Mechanism, Gate Mechanism, Residual |'
  prefs: []
  type: TYPE_TB
- en: '| [[113](#bib.bib113)] | 2019 | Travel Demand-Taxi |  | SGCN, LSTM, Seq2Seq
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[103](#bib.bib103)] | 2019 | Travel Demand-Taxi | ST-ED-RMGC | SGCN, LSTM,
    Seq2Seq, Residual |'
  prefs: []
  type: TYPE_TB
- en: 'We summarize the graph-based deep learning architectures in existing traffic
    literatures and find that most of them are composed of graph neural networks (GNNs)
    and other modules, such as recurrent neural networks (RNNs), temporal convolution
    network (TCN), Sequence to Sequence (Seq2Seq) model, generative adversarial network
    (GAN) (as shown in Table [II](#S5.T2 "TABLE II ‣ V Deep Learning Techniques Perspective
    ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")).
    It is the cooperation of GNNs and other deep learning techniques that achieves
    state-of-the-art performance in many traffic scenarios. This section aims to introduce
    the functionality, advantages, defects and variants of these techniques in traffic
    tasks, which can help participators understand how to utilize these deep learning
    techniques in traffic domain.'
  prefs: []
  type: TYPE_NORMAL
- en: V-A GNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad1d77a3d24e51a148d2049a2244ed57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The structure of Graph Neural Network is generally composed of two
    kind of layers: 1) Aggregation layer: In each feature dimension, the features
    of adjacent nodes are aggregated to the central node. Mathematically, the output
    of aggregation layer is the product of adjacency matrix and features matrix. 2)
    Non-linear transformation layer: All the aggregated features of each node are
    fed into the non-linear transformation layer to create higher level feature representation.
    All nodes share the same transformation kernel. $\{1,2,3,4\}$ are node indexes.'
  prefs: []
  type: TYPE_NORMAL
- en: In the last couple of years, motivated by the huge success of deep learning
    approaches (e.g. CNNs, RNNs), there is an increasing interest in generalizing
    neural networks to arbitrarily structured graphs and such networks are classified
    as graph neural networks (GNNs). In the early stage, the studies about GNNs can
    be categorized into recurrent graph neural networks (RecGNNs) which are inspired
    by RNNs [[34](#bib.bib34)]. Subsequently, inspired by the huge success of CNNs,
    many works focus on extending the convolution of CNN on graph data and these works
    can be categorized into convolutional graph neural networks (ConvGNNs) [[34](#bib.bib34)].
    There are also other branches of GNNs developed in recent years, e.g. graph auto-encoders
    (GAEs) [[121](#bib.bib121)] and graph attention networks (GATs) [[122](#bib.bib122)].
    According to our investigation, most traffic works focus on ConvGNNs and there
    are only a few studies [[119](#bib.bib119)] employing other branches of GNNs up
    to now. Further, ConvGNNs can be divided into two main streams, i.e. the spectral-based
    approaches which develop graph convolutions based on the spectral theory and the
    spatial-based approaches which define graph convolutions based on spatial relations
    between nodes[[123](#bib.bib123)]. Recently, many novel spatial-based convolutions
    have emerged, among which diffusion convolution is a popular spatial-based graph
    convolution which regards graph convolution as a diffusion process.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to our survey, most existing traffic works utilize either spectral
    graph convolution or diffusion graph convolution. There are also other novel convolutions
    [[68](#bib.bib68)] but their applications in traffic domain are relatively few.
    Therefore, in this section, we focus on introducing spectral graph convolution
    (SGC) and diffusion graph convolution (DGC) in traffic domain. In this paper,
    we refer the graph neural network with spectral graph convolution as SGCN and
    that with diffusion graph convolution as DGCN. Note that SGC is for undirected
    graph while DGC can be applied in both directed graph and undirected graph. In
    addition, both SGC and DGC aim to generate new feature representations for each
    node in a graph through feature aggregation and non-linear transformation (as
    shown in Figure [6](#S5.F6 "Figure 6 ‣ V-A GNNs ‣ V Deep Learning Techniques Perspective
    ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: V-A1 Spectral Graph Convolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the spectral theory, a graph is represented by its corresponding normalized
    Laplacian matrix $\mathbf{L}=\mathbf{I_{N}}-\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$.
    The real symmetric matrix $\mathbf{L}$ can be diagonalized via eigendecomposition
    as $\mathbf{L}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{T}$ where $\mathbf{U}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$
    is the eigenvectors matrix and $\mathbf{\Lambda}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$
    is the diagonal eigenvalues matrix. Since $\mathbf{U}$ is also an orthogonal matrix,
    Shuman et al. [[124](#bib.bib124)] adopted it as a graph Fourier basis, defining
    graph Fourier transform of a graph signal $x\in\mathbb{R}^{\mathbf{N}}$ as $\hat{x}=\mathbf{U}^{T}x$,
    and its inverse as $x=\mathbf{U}\hat{x}$.
  prefs: []
  type: TYPE_NORMAL
- en: Bruna et al. [[125](#bib.bib125)] tried to build an analogue of CNN convolution
    in spectral domain and defined the spectral convolution as $y=\Theta\boldsymbol{*_{\mathcal{G}}}x=\mathbf{U}\Theta\mathbf{U}^{T}x$,
    i.e. transforming $x$ into spectral domain, adjusting its amplitude by a diagonal
    kernel $\Theta=\operatorname{diag}(\theta_{0},\ldots,\theta_{\mathbf{N}-1})\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$,
    and doing inverse Fourier transform to get the final result $y$ in spatial domain.
    Although such convolution is theoretically guaranteed, it is computationally expensive
    as multiplication with $\mathbf{U}$ is $\mathcal{O}(\mathbf{N}^{2})$ and the eigendecomposition
    of $\mathbf{L}$ is intolerable for large scale graphs. In addition, it considers
    all nodes by the kernel $\Theta$ with $\mathbf{N}$ parameters and can’t extract
    spatial localization.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid such limitations, Defferrard et al. [[126](#bib.bib126)] localized
    the convolution and reduced its parameters by restricting the kernel $\Theta$
    to be a polynomial of eigenvalues matrix $\mathbf{\Lambda}$ as $\Theta=\sum_{k=0}^{\mathbf{K}-1}\theta_{k}\mathbf{\Lambda}^{k}$
    and $\mathbf{K}$ determines the maximum radius of the convolution from a central
    node. Thus, the convolution can be rewritten as $\Theta\boldsymbol{*_{\mathcal{G}}}x=\sum_{k=0}^{\mathbf{K}-1}\theta_{k}\mathbf{U}\mathbf{\Lambda}^{k}\mathbf{U}^{T}x=\sum_{k=0}^{\mathbf{K}-1}\theta_{k}\mathbf{L}^{k}x$.
    Further more, Defferrard et al. [[126](#bib.bib126)] adopted the Chebyshev polynomials
    $T_{k}(x)$ to approximate $\mathbf{L}^{k}$, resulting in $\Theta\boldsymbol{*_{\mathcal{G}}}x\approx\sum_{k=0}^{\mathbf{K}-1}\theta_{k}T_{k}(\tilde{\mathbf{L}})x$
    with a rescaled $\tilde{\mathbf{L}}=\frac{2}{\boldsymbol{\lambda}_{\max}}\mathbf{L}-\mathbf{I_{N}}$
    where $\boldsymbol{\lambda}_{\max}$ is the largest eigenvalue of $\mathbf{L}$
    and $T_{k}(x)=2xT_{k-1}(x)-T_{k-2}(x)$, $T_{0}(x)=1$, $T_{1}(x)=x$ [[127](#bib.bib127)].
    By recursively computing $T_{k}(x)$, the complexity of this $\mathbf{K}$-localized
    convolution can be reduced to $\mathcal{O}(\mathbf{K}|\mathbf{E}|)$ with $|\mathbf{E}|$
    being the number of edges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on [[126](#bib.bib126)], Kipf et al. [[128](#bib.bib128)] simplified
    the spectral graph convolution by limiting $\mathbf{K}=2$ and with $T_{0}(\tilde{\mathbf{L}})=1$,
    $T_{1}(\tilde{\mathbf{L}})=\tilde{\mathbf{L}}$. They got $\Theta\boldsymbol{*_{\mathcal{G}}}x\approx\theta_{0}T_{0}(\tilde{\mathbf{L}})x+\theta_{1}T_{1}(\tilde{\mathbf{L}})x=\theta_{0}x+\theta_{1}\tilde{\mathbf{L}}x$.
    Noticing that $\tilde{\mathbf{L}}=\frac{2}{\lambda_{\max}}\mathbf{L}-\mathbf{I_{N}}$,
    they set $\boldsymbol{\lambda}_{\max}=2$, resulting in $\Theta\boldsymbol{*_{\mathcal{G}}}x\approx\theta_{0}x+\theta_{1}(\mathbf{L}-\mathbf{I_{N}})x$.
    For that $\mathbf{L}=\mathbf{I_{N}}-\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}$
    and $\mathbf{L}-\mathbf{I_{N}}=-\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}$,
    they got $\Theta\boldsymbol{*_{\mathcal{G}}}x\approx\theta_{0}x-\theta_{1}(\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}})x$.
    Further, they reduced the number of parameters by setting $\theta=\theta_{0}=-\theta_{1}$
    to address overfitting and got $\Theta\boldsymbol{*_{\mathcal{G}}}x\approx\theta(\mathbf{I_{N}}+\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}})x$.
    They defined $\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I_{N}}$ and adopted a renormalization
    trick to get $y=\Theta\boldsymbol{\boldsymbol{*_{\mathcal{G}}}}x\approx\theta\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}x$,
    where $\tilde{\mathbf{D}}$ is the degree matrix of $\tilde{\mathbf{A}}$. Finally,
    Kipf et al.[[128](#bib.bib128)] proposed a spectral graph convolution layer as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}Y_{j}&amp;=\boldsymbol{\rho}(\Theta_{j}\boldsymbol{*_{\mathcal{G}}}X)=\boldsymbol{\rho}(\sum_{i=1}^{\mathbf{F_{I}}}\theta_{i,j}\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}X_{i}),1\leq
    j\leq\mathbf{F_{O}}\\ Y&amp;=\boldsymbol{\rho}(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}XW)\end{split}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: here, $X\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{I}}}$ is the layer input with
    $\mathbf{F_{I}}$ features, $X_{i}\in\mathbb{R}^{\mathbf{N}}$ is its $i^{th}$ feature.
    $Y\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{O}}}$ is the layer output with $\mathbf{F_{O}}$
    features, $Y_{j}\in\mathbb{R}^{\mathbf{N}}$ is its $j^{th}$ feature. $W\in\mathbb{R}^{\mathbf{F_{I}}\times\mathbf{F_{O}}}$
    is a trainable parameter. $\boldsymbol{\rho}(\boldsymbol{\cdot})$ is the activation
    function. Such layer can aggregate information of 1-hop neighbors. The receptive
    field of neighborhood can be expanded by stacking multiple graph convolution layers
    [[42](#bib.bib42)].
  prefs: []
  type: TYPE_NORMAL
- en: V-A2 Diffusion Graph Convolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Spectral graph convolution requires a symmetric Laplacian matrix to implement
    eigendecomposition. It becomes invalid for a directed graph with an asymmetric
    Laplacian matrix. Diffusion convolution origins from graph diffusion and has no
    constraint on graph. Graph diffusion [[129](#bib.bib129)], [[130](#bib.bib130)]
    can be represented as a transition matrix power series giving the probability
    of jumping from one node to another node at each step. After many steps, such
    Markov process converges to a stationary distribution $\mathcal{P}=\sum_{k=0}^{\infty}\alpha(1-\alpha)^{k}(\mathbf{D_{O}}^{-1}\mathbf{A})^{k}$,
    where $\mathbf{D_{O}}^{-1}\mathbf{A}$ is the transition matrix, $\alpha\in[0,1]$
    is the restart probability and $k$ is the diffusion step. In practice, a finite
    $\mathbf{K}$-step truncation of the diffusion process is adopted and each step
    is assigned a trainable weight $\theta$. Based on the $\mathbf{K}$-step diffusion
    process, Li et al. [[108](#bib.bib108)] defined diffusion graph convolution as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y=\Theta\boldsymbol{*_{\mathcal{G}}}x=\sum_{k=0}^{\mathbf{K}-1}(\theta_{k,1}(\mathbf{D_{O}}^{-1}\mathbf{A})^{k}+\theta_{k,2}(\mathbf{D_{I}}^{-1}\mathbf{A}^{T})^{k})x$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'here, $\mathbf{D_{O}}^{-1}\mathbf{A}$ represents the transition matrix and
    $\mathbf{D_{I}}^{-1}\mathbf{A}^{T}$ is its transpose. Such bidirectional diffusion
    enables the operation to capture the spatial correlation on a directed graph[[108](#bib.bib108)].
    Similar to spectral graph convolution layer, a diffusion graph convolutional layer
    is built as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}Y_{j}&amp;=\boldsymbol{\rho}(\sum_{k=0}^{\mathbf{K}-1}\sum_{i=1}^{\mathbf{F_{I}}}(\theta_{k,1,i,j}(\mathbf{D_{O}}^{-1}\mathbf{A})^{k}+\theta_{k,2,i,j}(\mathbf{D_{I}}^{-1}\mathbf{A}^{T})^{k})X_{i})\\
    Y&amp;=\boldsymbol{\rho}(\sum_{k=0}^{\mathbf{K}-1}(\mathbf{D_{O}}^{-1}\mathbf{A})^{k}XW_{k1}+(\mathbf{D_{I}}^{-1}\mathbf{A}^{T})^{k}XW_{k2})\end{split}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $1\leq j\leq\mathbf{F_{O}}$, parameters $W_{k1},W_{k2}\in\mathbb{R}^{\mathbf{F_{I}}\times\mathbf{F_{O}}}$
    are trainable.
  prefs: []
  type: TYPE_NORMAL
- en: V-A3 GNNs in Traffic Domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many traffic works, such as subway network and road network, are graph structure
    naturally (See Section [IV](#S4 "IV Problem Formulation and Graph Construction
    ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")).
    Compared with previous works modeling traffic network as grids [[131](#bib.bib131)],[[132](#bib.bib132)],
    the works modeling traffic network as graph can fully utilize spatial information.'
  prefs: []
  type: TYPE_NORMAL
- en: By now, many works employ convolution operation directly on traffic graph to
    capture the complex spatial dependency of traffic data. Most of them adopt spectral
    graph convolution (SGC) while some employ diffusion graph convolution (DGC) [[112](#bib.bib112)],
    [[108](#bib.bib108)], [[100](#bib.bib100)], [[102](#bib.bib102)], [[96](#bib.bib96)],[[118](#bib.bib118)].
    There are also some other graph based deep learning techniques such as graph attention
    network (GAT) [[119](#bib.bib119)], [[107](#bib.bib107)], [[98](#bib.bib98)],[[104](#bib.bib104)],
    tensor decomposition and completion on graph [[78](#bib.bib78)], but their related
    works are few, which might be a future research direction.
  prefs: []
  type: TYPE_NORMAL
- en: The key difference between SGC and DGC lies in their matrices which represent
    different assumptions on the spatial correlations in traffic network. The adjacency
    matrix in SGC infers that a central node in a graph has stronger correlation with
    its adjacent nodes than other distant ones [[89](#bib.bib89)],[[70](#bib.bib70)].
    The state transition matrix in DGC indicates that the spatial dependency is stochastic
    depending on the restart probability and dynamic instead of being fixed. The traffic
    flow is related to a diffusion process on a traffic graph to model its dynamic
    spatial correlations. In addition, the bidirectional diffusion in DGC offers the
    model more flexibility to capture the influence from both upstream and downstream
    traffic [[108](#bib.bib108)]. In a word, DGC is more complicated than SGC. DGC
    can be adopted in both symmetric or asymmetric traffic network graph while SGC
    can be only utilized to process symmetric traffic graph.
  prefs: []
  type: TYPE_NORMAL
- en: Existing graph convolution theories are mainly applied on 2-D signal $X\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{I}}}$.
    However, the traffic data with both spatial and temporal attributes are usually
    3-D signal $\mathcal{X}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{I}}}$.
    The convolution operations need to be further generalized to 3-D signal. Equal
    convolution operation (e.g. SGC, DGC) with the same kernel is imposed on each
    time step of 3-D signal $\mathcal{X}$ in parallel [[92](#bib.bib92)], [[70](#bib.bib70)],
    [[111](#bib.bib111)],[[115](#bib.bib115)].
  prefs: []
  type: TYPE_NORMAL
- en: In order to enhance the performance of graph convolution in traffic tasks, many
    works develop various variants of SGC.
  prefs: []
  type: TYPE_NORMAL
- en: 'Guo et al. [[69](#bib.bib69)] redefined SGC with attention mechanism to adaptively
    capture the dynamic correlations in traffic network: $\Theta\boldsymbol{*_{\mathcal{G}}}x\approx\sum_{k=0}^{\mathbf{K}-1}\theta_{k}(T_{k}(\tilde{\mathbf{L}})\boldsymbol{\odot}\mathbf{S})x$
    , where $\mathbf{S}=W_{1}\boldsymbol{\odot}\boldsymbol{\rho}((XW_{2})W_{3}(W_{4}X)^{T}+b)\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$
    is the spatial attention.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yu et al. [[71](#bib.bib71)] generalized SGC on both spatial and temporal dimensions
    by scanning $\mathbf{K}$ order neighbors on graph and $\mathbf{K}_{t}$ neighbors
    on time-axis without padding. The equation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{Y}_{t,j}=\boldsymbol{\rho}(\sum_{t^{\prime}=0}^{\mathbf{K}_{t}-1}\sum_{k=0}^{\mathbf{K}-1}\sum_{i=1}^{\mathbf{F_{I}}}\theta_{j,t^{\prime},k,i}\tilde{\mathbf{L}}^{k}\mathcal{X}_{t-t^{\prime},i})$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{X}_{t-t^{\prime},i}\in\mathbb{R}^{\mathbf{N}}$ is the $i^{th}$
    feature of input $\mathcal{X}$ at time $t-t^{\prime}$ , $\mathcal{Y}_{t,j}\in\mathbb{R}^{\mathbf{N}}$
    is the $j^{th}$ feature of output $\mathcal{Y}$ at time $t$.
  prefs: []
  type: TYPE_NORMAL
- en: Zhao et al. [[94](#bib.bib94)] changed SGC as $\Theta\boldsymbol{*_{\mathcal{G}}}x=(W\boldsymbol{\odot}\tilde{\mathbf{A}}^{\mathbf{K}}\boldsymbol{\odot}\mathcal{FFR})x$
    , where $\tilde{\mathbf{A}}^{\mathbf{K}}$ is the $\mathbf{K}$-hop neighborhood
    matrix and $\mathcal{FFR}$ is a matrix representing physical properties of road
    network. Some researchers [[120](#bib.bib120)],[[110](#bib.bib110)] followed this
    work and redefined $\Theta\boldsymbol{*_{\mathcal{G}}}x=(W\boldsymbol{\odot}Bi(\mathbf{A}^{\mathbf{K}}+\mathbf{I_{N}}))x$,
    where $Bi(.)$ is a function clipping each nonzero element in matrix to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Sun et al. [[116](#bib.bib116)] modified adjacency matrix $\mathbf{A}$ in SGC
    as $\mathbf{S}=\mathbf{A}\boldsymbol{\odot}\omega$ to integrate the geospatial
    positions information into the model and $\omega$ is a matrix calculated via a
    thresholded Gaussian kernel weighting function. The layer is built as $Y=\boldsymbol{\rho}(\tilde{\mathbf{Q}}^{-\frac{1}{2}}\tilde{\mathbf{S}}\tilde{\mathbf{Q}}^{-\frac{1}{2}}XW)$,
    where $\tilde{\mathbf{Q}}$ is the degree matrix of $\tilde{\mathbf{S}}=\mathbf{S}+\mathbf{I_{N}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Qiu et al. [[57](#bib.bib57)] designed a novel edge-based SGC on road network
    to extract the spatiotemporal correlations of the edge features. Both the feature
    matrix $X$ and adjacency matrix $\mathbf{A}$ are defined on edges instead of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: V-B RNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNNs) are a type of neural network architecture which
    is mainly used to detect patterns in sequential data [[133](#bib.bib133)]. The
    traffic data collected in many traffic tasks are time series data, thus RNNs are
    commonly utilized in these traffic tasks to capture the temporal dependency in
    traffic data. In this subsection, we introduce three classical models of RNNs
    (i.e. RNN, LSTM, GRU) and the correlations among them, providing theoretical evidence
    for participators to choose appropriate models for specific traffic problems.
  prefs: []
  type: TYPE_NORMAL
- en: V-B1 RNN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b80c6a7ddf1fe5650466cb7173d16014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The folded and unfolded structure of recurrent neural networks'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to a classical Feedforward Neural Network (FNN), a simple recurrent
    neural network (RNN) [[134](#bib.bib134)] contains three layers, i.e. input layer,
    hidden layer, output layer [[135](#bib.bib135)]. What differentiates RNN from
    FNN is the hidden layer. It passes information forward to the output layer in
    FNN while in RNN, it also transmits information back into itself forming a cycle
    [[133](#bib.bib133)]. For this reason, the hidden layer in RNN is called recurrent
    hidden layer. Such cycling trick can retain historical information, enabling RNN
    to process time series data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose there are $\mathbf{F_{I}}$, $\mathbf{F_{H}}$, $\mathbf{F_{O}}$ units
    in the input, hidden, output layer of RNN respectively. The input layer takes
    time series data $\mathbf{X}=[\mathbf{X}_{1},\cdots,\mathbf{X}_{\mathbf{P}}]\in\mathbb{R}^{\mathbf{P}\times\mathbf{F_{I}}}$
    in. For each element $\mathbf{X}_{t}\in\mathbb{R}^{\mathbf{F_{I}}}$ at time $t$,
    the hidden layer transforms it to $\mathbf{H}_{t}\in\mathbb{R}^{\mathbf{F_{H}}}$
    and the output layer maps $\mathbf{H}_{t}$ to $\mathbf{Y}_{t}\in\mathbb{R}^{\mathbf{F_{O}}}$.
    Note that the hidden layer not only takes $\mathbf{X}_{t}$ as input but also takes
    $\mathbf{H}_{t-1}$ as input. Such cycling mechanism enables RNN to memorize the
    past information (as shown in Figure [7](#S5.F7 "Figure 7 ‣ V-B1 RNN ‣ V-B RNNs
    ‣ V Deep Learning Techniques Perspective ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey")). The mathematical notations of hidden
    layer and output layer are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathbf{H}_{t}&amp;=\boldsymbol{tanh}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{h}+b_{h})\\
    \mathbf{Y}_{t}&amp;=\boldsymbol{\rho}(\mathbf{H}_{t}\boldsymbol{\cdot}W_{y}+b_{y})\end{split}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $W_{h}\in\mathbb{R}^{(\mathbf{F_{I}+\mathbf{F_{H}}})\times\mathbf{F_{H}}}$,
    $W_{y}\in\mathbb{R}^{\mathbf{F_{H}}\times\mathbf{F_{O}}}$, $b_{h}\in\mathbb{R}^{\mathbf{F_{H}}}$,
    $b_{y}\in\mathbb{R}^{\mathbf{F_{O}}}$ are trainable parameters. $t=1,\cdots,\mathbf{P}$
    and $\mathbf{P}$ is the input sequence length. $\mathbf{H}_{0}$ is initialized
    using small non-zero elements which can improve overall performance and stability
    of the network [[136](#bib.bib136)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In a word, RNN takes sequential data as input and generates another sequence
    with the same length: $[\mathbf{X}_{1},\cdots,\mathbf{X}_{\mathbf{P}}]\stackrel{{\scriptstyle
    RNN}}{{\longrightarrow}}[\mathbf{Y}_{1},\cdots,\mathbf{Y}_{\mathbf{P}}]$. Note
    that we can deepen RNN through stacking multiple recurrent hidden layers.'
  prefs: []
  type: TYPE_NORMAL
- en: V-B2 LSTM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although the hidden state enables RNN to memorize the input information over
    past time steps, it also introduces matrix multiplication over the (potentially
    very long) sequence. Small values in the matrix multiplication cause the gradients
    to decrease at each time step, resulting in final vanish phenomenon. Oppositely
    big values lead to exploding problem [[137](#bib.bib137)]. The vanishing or exploding
    gradients actually hinder the capacity of RNN to learn long-term sequential dependencies
    in data [[135](#bib.bib135)].
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome this hurdle, Long Short-Term Memory (LSTM) neural networks[[138](#bib.bib138)]
    are proposed to capture long-term dependency in sequence learning. Compared with
    the hidden layer in RNN, LSTM hidden layer has extra four parts, i.e. a memory
    cell, input gate, forget gate, and output gate. These three gates ranging in [0,1]
    can control information flow into the memory cell and preserve the extracted features
    from previous time steps. These simple changes enable the memory cell to store
    and read as much long-term information as possible. The mathematical notations
    of LSTM hidden layer are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="\begin{split}i_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{i}+b_{i})\\
    o_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{o}+b_{o})\\'
  prefs: []
  type: TYPE_NORMAL
- en: f_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{f}+b_{f})\\
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{C}_{t}&amp;=f_{t}\boldsymbol{\odot}\mathbf{C}_{t-1}+i_{t}\boldsymbol{\odot}\boldsymbol{tanh}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{c}+b_{c})\\
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{H}_{t}&amp;=o_{t}\boldsymbol{\odot}\boldsymbol{tanh}(\mathbf{C}_{t})\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msub
     ><mi mathsize="80%" 
    >i</mi><mi mathsize="80%" 
    >t</mi></msub></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow 
    ><mi mathsize="80%"  >𝝈</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mrow
     ><mrow
     ><mo
    maxsize="80%" minsize="80%"  >[</mo><msub
     ><mi
    mathsize="80%"  >𝐇</mi><mrow
     ><mi mathsize="80%"
     >t</mi><mo mathsize="80%"
     >−</mo><mn mathsize="80%"
     >1</mn></mrow></msub><mo
    mathsize="80%"  >,</mo><msub
     ><mi
    mathsize="80%"  >𝐗</mi><mi
    mathsize="80%"  >t</mi></msub><mo
    maxsize="80%" minsize="80%" rspace="0.055em"  >]</mo></mrow><mo
    class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" rspace="0.222em"
     >⋅</mo><msub
     ><mi
    mathsize="80%"  >W</mi><mi
    mathsize="80%"  >i</mi></msub></mrow><mo
    mathsize="80%"  >+</mo><msub
     ><mi
    mathsize="80%"  >b</mi><mi
    mathsize="80%"  >i</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msub
     ><mi mathsize="80%" 
    >o</mi><mi mathsize="80%" 
    >t</mi></msub></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow 
    ><mi mathsize="80%" 
    >𝝈</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >(</mo><mrow 
    ><mrow 
    ><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >[</mo><msub 
    ><mi mathsize="80%" 
    >𝐇</mi><mrow 
    ><mi mathsize="80%" 
    >t</mi><mo mathsize="80%" 
    >−</mo><mn mathsize="80%" 
    >1</mn></mrow></msub><mo mathsize="80%"
     >,</mo><msub 
    ><mi mathsize="80%" 
    >𝐗</mi><mi mathsize="80%" 
    >t</mi></msub><mo maxsize="80%" minsize="80%"
    rspace="0.055em"  >]</mo></mrow><mo
    class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" rspace="0.222em"
     >⋅</mo><msub
     ><mi
    mathsize="80%"  >W</mi><mi
    mathsize="80%"  >o</mi></msub></mrow><mo
    mathsize="80%"  >+</mo><msub
     ><mi
    mathsize="80%"  >b</mi><mi
    mathsize="80%"  >o</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msub
     ><mi mathsize="80%" 
    >f</mi><mi mathsize="80%" 
    >t</mi></msub></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow 
    ><mi mathsize="80%" 
    >𝝈</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >(</mo><mrow 
    ><mrow 
    ><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >[</mo><msub 
    ><mi mathsize="80%" 
    >𝐇</mi><mrow 
    ><mi mathsize="80%" 
    >t</mi><mo mathsize="80%" 
    >−</mo><mn mathsize="80%" 
    >1</mn></mrow></msub><mo mathsize="80%"
     >,</mo><msub 
    ><mi mathsize="80%" 
    >𝐗</mi><mi mathsize="80%" 
    >t</mi></msub><mo maxsize="80%" minsize="80%"
    rspace="0.055em"  >]</mo></mrow><mo
    class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" rspace="0.222em"
     >⋅</mo><msub
     ><mi
    mathsize="80%"  >W</mi><mi
    mathsize="80%"  >f</mi></msub></mrow><mo
    mathsize="80%"  >+</mo><msub
     ><mi
    mathsize="80%"  >b</mi><mi
    mathsize="80%"  >f</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msub
     ><mi mathsize="80%" 
    >𝐂</mi><mi mathsize="80%" 
    >t</mi></msub></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow 
    ><mrow  ><msub
     ><mi mathsize="80%"
     >f</mi><mi mathsize="80%"
     >t</mi></msub><mo
    class="ltx_mathvariant_bold" lspace="0.222em" mathsize="80%" mathvariant="bold"
    rspace="0.222em"  >⊙</mo><msub
     ><mi mathsize="80%"
     >𝐂</mi><mrow 
    ><mi mathsize="80%" 
    >t</mi><mo mathsize="80%" 
    >−</mo><mn mathsize="80%" 
    >1</mn></mrow></msub></mrow><mo mathsize="80%"
     >+</mo><mrow 
    ><mrow 
    ><msub 
    ><mi mathsize="80%" 
    >i</mi><mi mathsize="80%" 
    >t</mi></msub><mo class="ltx_mathvariant_bold"
    lspace="0.222em" mathsize="80%" mathvariant="bold" rspace="0.222em" 
    >⊙</mo><mi mathsize="80%" 
    >𝒕</mi></mrow><mo lspace="0em" rspace="0em"
     >​</mo><mi
    mathsize="80%"  >𝒂</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒏</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒉</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    maxsize="80%" minsize="80%"  >(</mo><mrow
     ><mrow
     ><mrow
     ><mo
    maxsize="80%" minsize="80%"  >[</mo><msub
     ><mi
    mathsize="80%"  >𝐇</mi><mrow
     ><mi
    mathsize="80%"  >t</mi><mo
    mathsize="80%"  >−</mo><mn
    mathsize="80%"  >1</mn></mrow></msub><mo
    mathsize="80%"  >,</mo><msub
     ><mi
    mathsize="80%"  >𝐗</mi><mi
    mathsize="80%"  >t</mi></msub><mo
    maxsize="80%" minsize="80%" rspace="0.055em"  >]</mo></mrow><mo
    class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" rspace="0.222em"
     >⋅</mo><msub
     ><mi
    mathsize="80%"  >W</mi><mi
    mathsize="80%"  >c</mi></msub></mrow><mo
    mathsize="80%"  >+</mo><msub
     ><mi
    mathsize="80%"  >b</mi><mi
    mathsize="80%"  >c</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msub
     ><mi mathsize="80%" 
    >𝐇</mi><mi mathsize="80%" 
    >t</mi></msub></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow 
    ><mrow  ><msub
     ><mi mathsize="80%"
     >o</mi><mi mathsize="80%"
     >t</mi></msub><mo
    class="ltx_mathvariant_bold" lspace="0.222em" mathsize="80%" mathvariant="bold"
    rspace="0.222em"  >⊙</mo><mi
    mathsize="80%"  >𝒕</mi></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒂</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒏</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒉</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><msub
     ><mi
    mathsize="80%"  >𝐂</mi><mi
    mathsize="80%"  >t</mi></msub><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑖</ci><ci 
    >𝑡</ci></apply><apply 
    ><ci  >𝝈</ci><apply
     ><apply 
    ><ci  >bold-⋅</ci><interval
    closure="closed"  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐗</ci><ci 
    >𝑡</ci></apply></interval><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑊</ci><ci 
    >𝑖</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑏</ci><ci 
    >𝑖</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑜</ci><ci  >𝑡</ci></apply></apply></apply><apply
     ><apply 
    ><ci  >𝝈</ci><apply
     ><apply 
    ><ci  >bold-⋅</ci><interval
    closure="closed"  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><apply
     ><ci 
    >𝑡</ci><cn type="integer" 
    >1</cn></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐗</ci><ci 
    >𝑡</ci></apply></interval><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑊</ci><ci 
    >𝑜</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑏</ci><ci 
    >𝑜</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑓</ci><ci  >𝑡</ci></apply></apply></apply><apply
     ><apply 
    ><ci  >𝝈</ci><apply
     ><apply 
    ><ci  >bold-⋅</ci><interval
    closure="closed"  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><apply
     ><ci 
    >𝑡</ci><cn type="integer" 
    >1</cn></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐗</ci><ci 
    >𝑡</ci></apply></interval><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑊</ci><ci 
    >𝑓</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑏</ci><ci 
    >𝑓</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐂</ci><ci  >𝑡</ci></apply></apply></apply><apply
     ><apply 
    ><apply  ><csymbol
    cd="latexml"  >direct-product</csymbol><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑓</ci><ci 
    >𝑡</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐂</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply></apply><apply
     ><apply 
    ><csymbol cd="latexml" 
    >direct-product</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑖</ci><ci 
    >𝑡</ci></apply><ci 
    >𝒕</ci></apply><ci 
    >𝒂</ci><ci 
    >𝒏</ci><ci 
    >𝒉</ci><apply 
    ><apply  ><ci
     >bold-⋅</ci><interval
    closure="closed"  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><apply
     ><ci
     >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐗</ci><ci
     >𝑡</ci></apply></interval><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑊</ci><ci
     >𝑐</ci></apply></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑏</ci><ci
     >𝑐</ci></apply></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐇</ci><ci 
    >𝑡</ci></apply></apply></apply></apply><apply
     ><apply 
    ><apply  ><csymbol
    cd="latexml"  >direct-product</csymbol><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑜</ci><ci 
    >𝑡</ci></apply><ci 
    >𝒕</ci></apply><ci 
    >𝒂</ci><ci  >𝒏</ci><ci
     >𝒉</ci><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐂</ci><ci
     >𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}i_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{i}+b_{i})\\
    o_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{o}+b_{o})\\
    f_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{f}+b_{f})\\
    \mathbf{C}_{t}&=f_{t}\boldsymbol{\odot}\mathbf{C}_{t-1}+i_{t}\boldsymbol{\odot}\boldsymbol{tanh}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{c}+b_{c})\\
    \mathbf{H}_{t}&=o_{t}\boldsymbol{\odot}\boldsymbol{tanh}(\mathbf{C}_{t})\end{split}</annotation></semantics></math>
    |  | (9) |
  prefs: []
  type: TYPE_NORMAL
- en: where $i_{t}$, $o_{t}$, $f_{t}$ are the input gate, output gate, forget gate
    at time $t$ respectively. $\mathbf{C}_{t}$ is the memory cell at time $t$.
  prefs: []
  type: TYPE_NORMAL
- en: V-B3 GRU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While LSTM is a viable option for avoiding vanishing or exploding gradients,
    its complex structure leads to more memory requirement and longer training time.
    Chung et al. [[139](#bib.bib139)] proposed a simple yet powerful variant of LSTM,
    i.e. Gated Recurrent Unit (GRU). The LSTM cell has three gates, but the GRU cell
    only has two gates, resulting in fewer parameters thus shorter training time.
    However, GRU is equally effective as LSTM empirically [[139](#bib.bib139)] and
    is widely used in various tasks. The mathematical notations of GRU hidden layer
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="\begin{split}r_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{r}+b_{r})\\
    u_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{u}+b_{u})\\'
  prefs: []
  type: TYPE_NORMAL
- en: \tilde{\mathbf{H}_{t}}&amp;=\boldsymbol{tanh}(r_{t}\boldsymbol{\odot}[\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{h}+b_{h})\\
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{H}_{t}&amp;=u_{t}\boldsymbol{\odot}\mathbf{H}_{t-1}+(1-u_{t})\boldsymbol{\odot}\tilde{\mathbf{H}_{t}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt"
    displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msub
     ><mi mathsize="80%" 
    >r</mi><mi mathsize="80%" 
    >t</mi></msub></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow 
    ><mi mathsize="80%"  >𝝈</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mrow 
    ><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >[</mo><msub 
    ><mi mathsize="80%"  >𝐇</mi><mrow
     ><mi mathsize="80%"
     >t</mi><mo
    mathsize="80%"  >−</mo><mn
    mathsize="80%"  >1</mn></mrow></msub><mo
    mathsize="80%"  >,</mo><msub
     ><mi
    mathsize="80%"  >𝐗</mi><mi
    mathsize="80%"  >t</mi></msub><mo
    maxsize="80%" minsize="80%" rspace="0.055em" 
    >]</mo></mrow><mo class="ltx_mathvariant_bold" mathsize="80%"
    mathvariant="bold" rspace="0.222em"  >⋅</mo><msub
     ><mi
    mathsize="80%"  >W</mi><mi
    mathsize="80%"  >r</mi></msub></mrow><mo
    mathsize="80%"  >+</mo><msub
     ><mi mathsize="80%"
     >b</mi><mi
    mathsize="80%"  >r</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msub
     ><mi mathsize="80%" 
    >u</mi><mi mathsize="80%" 
    >t</mi></msub></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow
     ><mi mathsize="80%"
     >𝝈</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mrow 
    ><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >[</mo><msub 
    ><mi mathsize="80%" 
    >𝐇</mi><mrow 
    ><mi mathsize="80%" 
    >t</mi><mo mathsize="80%" 
    >−</mo><mn mathsize="80%" 
    >1</mn></mrow></msub><mo mathsize="80%"
     >,</mo><msub 
    ><mi mathsize="80%" 
    >𝐗</mi><mi mathsize="80%" 
    >t</mi></msub><mo maxsize="80%" minsize="80%"
    rspace="0.055em"  >]</mo></mrow><mo
    class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" rspace="0.222em"
     >⋅</mo><msub
     ><mi
    mathsize="80%"  >W</mi><mi
    mathsize="80%"  >u</mi></msub></mrow><mo
    mathsize="80%"  >+</mo><msub
     ><mi mathsize="80%"
     >b</mi><mi
    mathsize="80%"  >u</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><mover
    accent="true"  ><msub
     ><mi mathsize="80%"
     >𝐇</mi><mi
    mathsize="80%"  >t</mi></msub><mo
    mathsize="80%"  >~</mo></mover></mtd><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow
     ><mi mathsize="80%"
     >𝒕</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒂</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒏</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒉</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mrow 
    ><mrow 
    ><msub 
    ><mi mathsize="80%" 
    >r</mi><mi mathsize="80%" 
    >t</mi></msub><mo class="ltx_mathvariant_bold"
    lspace="0.222em" mathsize="80%" mathvariant="bold" rspace="0.222em" 
    >⊙</mo><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >[</mo><msub 
    ><mi mathsize="80%" 
    >𝐇</mi><mrow 
    ><mi mathsize="80%" 
    >t</mi><mo mathsize="80%" 
    >−</mo><mn mathsize="80%" 
    >1</mn></mrow></msub><mo mathsize="80%"
     >,</mo><msub 
    ><mi mathsize="80%" 
    >𝐗</mi><mi mathsize="80%" 
    >t</mi></msub><mo maxsize="80%" minsize="80%"
    rspace="0.055em"  >]</mo></mrow></mrow><mo
    class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" rspace="0.222em"
     >⋅</mo><msub
     ><mi
    mathsize="80%"  >W</mi><mi
    mathsize="80%"  >h</mi></msub></mrow><mo
    mathsize="80%"  >+</mo><msub
     ><mi mathsize="80%"
     >b</mi><mi
    mathsize="80%"  >h</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msub
     ><mi mathsize="80%" 
    >𝐇</mi><mi mathsize="80%" 
    >t</mi></msub></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow
     ><mrow 
    ><msub  ><mi
    mathsize="80%"  >u</mi><mi
    mathsize="80%"  >t</mi></msub><mo
    class="ltx_mathvariant_bold" lspace="0.222em" mathsize="80%" mathvariant="bold"
    rspace="0.222em"  >⊙</mo><msub
     ><mi mathsize="80%"
     >𝐇</mi><mrow
     ><mi mathsize="80%"
     >t</mi><mo
    mathsize="80%"  >−</mo><mn
    mathsize="80%"  >1</mn></mrow></msub></mrow><mo
    mathsize="80%"  >+</mo><mrow
     ><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >(</mo><mrow 
    ><mn mathsize="80%" 
    >1</mn><mo mathsize="80%" 
    >−</mo><msub 
    ><mi mathsize="80%" 
    >u</mi><mi mathsize="80%" 
    >t</mi></msub></mrow><mo maxsize="80%"
    minsize="80%" rspace="0.055em"  >)</mo></mrow><mo
    class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" rspace="0.222em"
     >⊙</mo><mover
    accent="true"  ><msub
     ><mi
    mathsize="80%"  >𝐇</mi><mi
    mathsize="80%"  >t</mi></msub><mo
    mathsize="80%"  >~</mo></mover></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑟</ci><ci 
    >𝑡</ci></apply><apply 
    ><ci  >𝝈</ci><apply
     ><apply 
    ><ci  >bold-⋅</ci><interval
    closure="closed"  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐗</ci><ci
     >𝑡</ci></apply></interval><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑊</ci><ci
     >𝑟</ci></apply></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑏</ci><ci
     >𝑟</ci></apply></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑢</ci><ci 
    >𝑡</ci></apply></apply></apply><apply 
    ><apply  ><ci
     >𝝈</ci><apply
     ><apply 
    ><ci  >bold-⋅</ci><interval
    closure="closed"  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><apply
     ><ci 
    >𝑡</ci><cn type="integer" 
    >1</cn></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐗</ci><ci 
    >𝑡</ci></apply></interval><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑊</ci><ci 
    >𝑢</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑏</ci><ci 
    >𝑢</ci></apply></apply><apply 
    ><ci  >~</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><ci
     >𝑡</ci></apply></apply></apply></apply><apply
     ><apply 
    ><ci  >𝒕</ci><ci
     >𝒂</ci><ci 
    >𝒏</ci><ci  >𝒉</ci><apply
     ><apply 
    ><ci  >bold-⋅</ci><apply
     ><csymbol cd="latexml"
     >direct-product</csymbol><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑟</ci><ci 
    >𝑡</ci></apply><interval closure="closed" 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐇</ci><apply 
    ><ci 
    >𝑡</ci><cn type="integer" 
    >1</cn></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐗</ci><ci 
    >𝑡</ci></apply></interval></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑊</ci><ci 
    >ℎ</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑏</ci><ci 
    >ℎ</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐇</ci><ci 
    >𝑡</ci></apply></apply></apply><apply 
    ><apply  ><apply
     ><csymbol cd="latexml"
     >direct-product</csymbol><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑢</ci><ci 
    >𝑡</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐇</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply></apply><apply
     ><csymbol cd="latexml"
     >direct-product</csymbol><apply
     ><cn type="integer"
     >1</cn><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑢</ci><ci
     >𝑡</ci></apply></apply><apply
     ><ci 
    >~</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐇</ci><ci 
    >𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}r_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{r}+b_{r})\\
    u_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{u}+b_{u})\\
    \tilde{\mathbf{H}_{t}}&=\boldsymbol{tanh}(r_{t}\boldsymbol{\odot}[\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{h}+b_{h})\\
    \mathbf{H}_{t}&=u_{t}\boldsymbol{\odot}\mathbf{H}_{t-1}+(1-u_{t})\boldsymbol{\odot}\tilde{\mathbf{H}_{t}}\\
    \end{split}</annotation></semantics></math> |  | (10) |
  prefs: []
  type: TYPE_NORMAL
- en: where $r_{t}$ is the reset gate, $u_{t}$ is the update gate.
  prefs: []
  type: TYPE_NORMAL
- en: V-B4 RNNs in Traffic Domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RNNs have shown impressive capability of processing time series data. Since
    traffic data has a distinct temporal dependency, RNNs are usually leveraged to
    capture temporal correlation in traffic data. Among the works we survey, only
    Geng et al. [[89](#bib.bib89)] utilized RNN to capture temporal dependency in
    traffic data while more than a half adopted GRU and some employed LSTM. This can
    be explained that RNN survives severe gradient disappearance or gradient explosion
    while LSTM and GRU handle this successfully and GRU can reduce the training time.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there are many tricks to augment RNNs’ capacity to model the complex
    temporal dynamics in traffic domain, such as attention mechanism, gating mechanism
    and residual mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, Geng et al. [[89](#bib.bib89)] incorporated the contextual information,
    i.e. output of SGCN which contains information of related regions, into an attention
    operation to model the correlations between observations at different timestamps:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="\begin{split}z&amp;=F_{pool}(\mathbf{X}_{t},SGCN(\mathbf{X}_{t}))\\
    S&amp;=\boldsymbol{\sigma}(W_{1}\boldsymbol{ReLU}(W_{2}z))\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{H}_{t}&amp;=RNN([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\odot}S)\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt"
    displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><mi
    mathsize="80%"  >z</mi></mtd><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow 
    ><msub  ><mi
    mathsize="80%"  >F</mi><mrow
     ><mi mathsize="80%"
     >p</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >l</mi></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><msub
     ><mi mathsize="80%"
     >𝐗</mi><mi mathsize="80%"
     >t</mi></msub><mo
    mathsize="80%"  >,</mo><mrow
     ><mi mathsize="80%"
     >S</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
    mathsize="80%"  >G</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >C</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >N</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    maxsize="80%" minsize="80%"  >(</mo><msub
     ><mi
    mathsize="80%"  >𝐗</mi><mi
    mathsize="80%"  >t</mi></msub><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><mi
    mathsize="80%"  >S</mi></mtd><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow
     ><mi mathsize="80%"
     >𝝈</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><msub 
    ><mi mathsize="80%" 
    >W</mi><mn mathsize="80%" 
    >1</mn></msub><mo lspace="0em" rspace="0em"
     >​</mo><mi
    mathsize="80%"  >𝑹</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒆</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝑳</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝑼</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    maxsize="80%" minsize="80%"  >(</mo><mrow
     ><msub
     ><mi
    mathsize="80%"  >W</mi><mn
    mathsize="80%"  >2</mn></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >z</mi></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msub
     ><mi mathsize="80%" 
    >𝐇</mi><mi mathsize="80%" 
    >t</mi></msub></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow
     ><mi mathsize="80%"
     >R</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
    mathsize="80%"  >N</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >N</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >[</mo><msub 
    ><mi mathsize="80%" 
    >𝐇</mi><mrow 
    ><mi mathsize="80%" 
    >t</mi><mo mathsize="80%" 
    >−</mo><mn mathsize="80%" 
    >1</mn></mrow></msub><mo mathsize="80%"
     >,</mo><msub 
    ><mi mathsize="80%" 
    >𝐗</mi><mi mathsize="80%" 
    >t</mi></msub><mo maxsize="80%" minsize="80%"
    rspace="0.055em"  >]</mo></mrow><mo
    class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" rspace="0.222em"
     >⊙</mo><mi
    mathsize="80%"  >S</mi></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><ci
     >𝑧</ci><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐹</ci><apply 
    ><ci  >𝑝</ci><ci
     >𝑜</ci><ci
     >𝑜</ci><ci
     >𝑙</ci></apply></apply><interval
    closure="open"  ><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐗</ci><ci 
    >𝑡</ci></apply><apply 
    ><ci  >𝑆</ci><ci
     >𝐺</ci><ci
     >𝐶</ci><ci
     >𝑁</ci><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐗</ci><ci
     >𝑡</ci></apply></apply></interval><ci
     >𝑆</ci></apply></apply><apply
     ><apply 
    ><ci  >𝝈</ci><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑊</ci><cn type="integer" 
    >1</cn></apply><ci 
    >𝑹</ci><ci  >𝒆</ci><ci
     >𝑳</ci><ci 
    >𝑼</ci><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑊</ci><cn
    type="integer"  >2</cn></apply><ci
     >𝑧</ci></apply></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐇</ci><ci 
    >𝑡</ci></apply></apply></apply><apply 
    ><apply  ><ci
     >𝑅</ci><ci 
    >𝑁</ci><ci  >𝑁</ci><apply
     ><csymbol cd="latexml"
     >direct-product</csymbol><interval
    closure="closed"  ><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐇</ci><apply
     ><ci 
    >𝑡</ci><cn type="integer" 
    >1</cn></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐗</ci><ci 
    >𝑡</ci></apply></interval><ci 
    >𝑆</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}z&=F_{pool}(\mathbf{X}_{t},SGCN(\mathbf{X}_{t}))\\
    S&=\boldsymbol{\sigma}(W_{1}\boldsymbol{ReLU}(W_{2}z))\\ \mathbf{H}_{t}&=RNN([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\odot}S)\\
    \end{split}</annotation></semantics></math> |  | (11) |
  prefs: []
  type: TYPE_NORMAL
- en: where $F_{pool}(\boldsymbol{\cdot})$ is a global average pooling layer, $RNN(\boldsymbol{\cdot})$
    denotes the RNN hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chen et al. [[112](#bib.bib112)] took external factors into consideration by
    embedding external attributes into the input. In addition, they added the previous
    hidden states to the next hidden states through a residual shortcut path, which
    they believed can make GRU more sensitive and robust to sudden changes in traffic
    historical observations. The new hidden state is formulated as: $\mathbf{H}_{t}=GRU([\mathbf{H}_{t-1},\mathbf{X}_{t}],\mathbf{E}_{t})+\mathbf{H}_{t-1}W$,
    where $\mathbf{E}_{t}$ is the external features at time $t$, $W$ is a linear trainable
    parameter, $\mathbf{H}_{t-1}W$ is the residual shortcut.'
  prefs: []
  type: TYPE_NORMAL
- en: Yu et al. [[105](#bib.bib105)] inserted a dilated skip connection into GRU by
    changing hidden state from $\mathbf{H}_{t}=GRU([\mathbf{H}_{t-1},\mathbf{X}_{t}])$
    to $\mathbf{H}_{t}=GRU(\mathbf{H}_{t-s},\mathbf{X}_{t})$, where $s$ refers to
    the skip length or dilation rate of each layer, $GRU(\boldsymbol{\cdot})$ denotes
    the GRU hidden layer. Such hierarchical design of dilation brings in multiple
    temporal scales for recurrent units at different layers which achieves multi-timescale
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the tricks above, some works replace the matrix multiplication in RNNs’
    hidden layer with spectral graph convolution (SGC) or diffusion graph convolution
    (DGC), to capture spatial-temporal correlations jointly. Take GRU as example:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="\begin{split}r_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{*_{\mathcal{G}}}W_{r}+b_{r})\\
    u_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{*_{\mathcal{G}}}W_{u}+b_{u})\\'
  prefs: []
  type: TYPE_NORMAL
- en: \tilde{\mathbf{H}_{t}}&amp;=\boldsymbol{tanh}(r_{t}\boldsymbol{\odot}[\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{*_{\mathcal{G}}}W_{h}+b_{h})\\
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{H}_{t}&amp;=u_{t}\boldsymbol{\odot}\mathbf{H}_{t-1}+(1-u_{t})\boldsymbol{\odot}\tilde{\mathbf{H}_{t}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt"
    displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msub
     ><mi mathsize="80%" 
    >r</mi><mi mathsize="80%" 
    >t</mi></msub></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow 
    ><mi mathsize="80%"  >𝝈</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mrow 
    ><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >[</mo><msub 
    ><mi mathsize="80%"  >𝐇</mi><mrow
     ><mi mathsize="80%"
     >t</mi><mo
    mathsize="80%"  >−</mo><mn
    mathsize="80%"  >1</mn></mrow></msub><mo
    mathsize="80%"  >,</mo><msub
     ><mi
    mathsize="80%"  >𝐗</mi><mi
    mathsize="80%"  >t</mi></msub><mo
    maxsize="80%" minsize="80%" rspace="0.055em" 
    >]</mo></mrow><msub 
    ><mo class="ltx_mathvariant_bold" mathsize="80%"
    mathvariant="bold" rspace="0.222em"  >∗</mo><mi
    class="ltx_font_mathcaligraphic" mathsize="80%" 
    >𝓖</mi></msub><msub 
    ><mi mathsize="80%" 
    >W</mi><mi mathsize="80%" 
    >r</mi></msub></mrow><mo mathsize="80%"
     >+</mo><msub
     ><mi mathsize="80%"
     >b</mi><mi
    mathsize="80%"  >r</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msub
     ><mi mathsize="80%" 
    >u</mi><mi mathsize="80%" 
    >t</mi></msub></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow
     ><mi mathsize="80%"
     >𝝈</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mrow 
    ><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >[</mo><msub 
    ><mi mathsize="80%" 
    >𝐇</mi><mrow 
    ><mi mathsize="80%" 
    >t</mi><mo mathsize="80%" 
    >−</mo><mn mathsize="80%" 
    >1</mn></mrow></msub><mo mathsize="80%"
     >,</mo><msub 
    ><mi mathsize="80%" 
    >𝐗</mi><mi mathsize="80%" 
    >t</mi></msub><mo maxsize="80%" minsize="80%"
    rspace="0.055em"  >]</mo></mrow><msub
     ><mo
    class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" rspace="0.222em"
     >∗</mo><mi
    class="ltx_font_mathcaligraphic" mathsize="80%" 
    >𝓖</mi></msub><msub 
    ><mi mathsize="80%" 
    >W</mi><mi mathsize="80%" 
    >u</mi></msub></mrow><mo mathsize="80%"
     >+</mo><msub
     ><mi mathsize="80%"
     >b</mi><mi
    mathsize="80%"  >u</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><mover
    accent="true"  ><msub
     ><mi mathsize="80%"
     >𝐇</mi><mi
    mathsize="80%"  >t</mi></msub><mo
    mathsize="80%"  >~</mo></mover></mtd><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow
     ><mi mathsize="80%"
     >𝒕</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒂</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒏</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒉</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mrow 
    ><mrow 
    ><msub 
    ><mi mathsize="80%" 
    >r</mi><mi mathsize="80%" 
    >t</mi></msub><mo class="ltx_mathvariant_bold"
    lspace="0.222em" mathsize="80%" mathvariant="bold" rspace="0.222em" 
    >⊙</mo><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >[</mo><msub 
    ><mi mathsize="80%" 
    >𝐇</mi><mrow 
    ><mi mathsize="80%" 
    >t</mi><mo mathsize="80%" 
    >−</mo><mn mathsize="80%" 
    >1</mn></mrow></msub><mo mathsize="80%"
     >,</mo><msub 
    ><mi mathsize="80%" 
    >𝐗</mi><mi mathsize="80%" 
    >t</mi></msub><mo maxsize="80%" minsize="80%"
    rspace="0.055em"  >]</mo></mrow></mrow><msub
     ><mo
    class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" rspace="0.222em"
     >∗</mo><mi
    class="ltx_font_mathcaligraphic" mathsize="80%" 
    >𝓖</mi></msub><msub 
    ><mi mathsize="80%" 
    >W</mi><mi mathsize="80%" 
    >h</mi></msub></mrow><mo mathsize="80%"
     >+</mo><msub
     ><mi mathsize="80%"
     >b</mi><mi
    mathsize="80%"  >h</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msub
     ><mi mathsize="80%" 
    >𝐇</mi><mi mathsize="80%" 
    >t</mi></msub></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow
     ><mrow 
    ><msub  ><mi
    mathsize="80%"  >u</mi><mi
    mathsize="80%"  >t</mi></msub><mo
    class="ltx_mathvariant_bold" lspace="0.222em" mathsize="80%" mathvariant="bold"
    rspace="0.222em"  >⊙</mo><msub
     ><mi mathsize="80%"
     >𝐇</mi><mrow
     ><mi mathsize="80%"
     >t</mi><mo
    mathsize="80%"  >−</mo><mn
    mathsize="80%"  >1</mn></mrow></msub></mrow><mo
    mathsize="80%"  >+</mo><mrow
     ><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >(</mo><mrow 
    ><mn mathsize="80%" 
    >1</mn><mo mathsize="80%" 
    >−</mo><msub 
    ><mi mathsize="80%" 
    >u</mi><mi mathsize="80%" 
    >t</mi></msub></mrow><mo maxsize="80%"
    minsize="80%" rspace="0.055em"  >)</mo></mrow><mo
    class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" rspace="0.222em"
     >⊙</mo><mover
    accent="true"  ><msub
     ><mi
    mathsize="80%"  >𝐇</mi><mi
    mathsize="80%"  >t</mi></msub><mo
    mathsize="80%"  >~</mo></mover></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑟</ci><ci 
    >𝑡</ci></apply><apply 
    ><ci  >𝝈</ci><apply
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝓖</ci></apply><interval
    closure="closed"  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐗</ci><ci
     >𝑡</ci></apply></interval><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑊</ci><ci
     >𝑟</ci></apply></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑏</ci><ci
     >𝑟</ci></apply></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑢</ci><ci 
    >𝑡</ci></apply></apply></apply><apply 
    ><apply  ><ci
     >𝝈</ci><apply
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝓖</ci></apply><interval
    closure="closed"  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><apply
     ><ci 
    >𝑡</ci><cn type="integer" 
    >1</cn></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐗</ci><ci 
    >𝑡</ci></apply></interval><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑊</ci><ci 
    >𝑢</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑏</ci><ci 
    >𝑢</ci></apply></apply><apply 
    ><ci  >~</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><ci
     >𝑡</ci></apply></apply></apply></apply><apply
     ><apply 
    ><ci  >𝒕</ci><ci
     >𝒂</ci><ci 
    >𝒏</ci><ci  >𝒉</ci><apply
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝓖</ci></apply><apply
     ><csymbol cd="latexml"
     >direct-product</csymbol><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑟</ci><ci 
    >𝑡</ci></apply><interval closure="closed" 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐇</ci><apply 
    ><ci 
    >𝑡</ci><cn type="integer" 
    >1</cn></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐗</ci><ci 
    >𝑡</ci></apply></interval></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑊</ci><ci 
    >ℎ</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑏</ci><ci 
    >ℎ</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐇</ci><ci 
    >𝑡</ci></apply></apply></apply><apply 
    ><apply  ><apply
     ><csymbol cd="latexml"
     >direct-product</csymbol><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑢</ci><ci 
    >𝑡</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐇</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply></apply><apply
     ><csymbol cd="latexml"
     >direct-product</csymbol><apply
     ><cn type="integer"
     >1</cn><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑢</ci><ci
     >𝑡</ci></apply></apply><apply
     ><ci 
    >~</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐇</ci><ci 
    >𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}r_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{*_{\mathcal{G}}}W_{r}+b_{r})\\
    u_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{*_{\mathcal{G}}}W_{u}+b_{u})\\
    \tilde{\mathbf{H}_{t}}&=\boldsymbol{tanh}(r_{t}\boldsymbol{\odot}[\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{*_{\mathcal{G}}}W_{h}+b_{h})\\
    \mathbf{H}_{t}&=u_{t}\boldsymbol{\odot}\mathbf{H}_{t-1}+(1-u_{t})\boldsymbol{\odot}\tilde{\mathbf{H}_{t}}\\
    \end{split}</annotation></semantics></math> |  | (12) |
  prefs: []
  type: TYPE_NORMAL
- en: The $\boldsymbol{*_{\mathcal{G}}}$ can represent SGC, DGC or other convolution
    operations. In the literatures we survey, most replacements happen in GRU and
    only one in LSTM [[66](#bib.bib66)]. Among GRU related traffic works, [[112](#bib.bib112)],
    [[108](#bib.bib108)], [[106](#bib.bib106)], [[96](#bib.bib96)],[[118](#bib.bib118)]
    replaced matrix multiplication with DGC, [[42](#bib.bib42)], [[105](#bib.bib105)],
    [[77](#bib.bib77)] with SGC, [[104](#bib.bib104)], [[119](#bib.bib119)] with GAT.
  prefs: []
  type: TYPE_NORMAL
- en: Note that besides RNNs, other techniques (e.g. TCN in the next subsection) are
    also popular choices to extract the temporal dynamics in traffic tasks.
  prefs: []
  type: TYPE_NORMAL
- en: V-C TCN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although RNN-based models become widespread in time-series analysis, RNNs for
    traffic prediction still suffer from time-consuming iteration, complex gate mechanism,
    and slow response to dynamic changes [[92](#bib.bib92)]. On the contrary, 1D-CNN
    has the superiority of fast training, simple structure, and no constraints to
    previous steps [[140](#bib.bib140)]. However, 1D-CNN is less common than RNNs
    in practice due to its lack of memory for a long sequence [[141](#bib.bib141)].
    In 2016, a novel convolution operation integrating causal convolution and dilated
    convolution [[142](#bib.bib142)] is proposed, which outperforms RNNs in text-to-speech
    tasks. The prediction of causal convolution depends on previous elements but not
    on future elements. Dilated convolution expands the receptive field of original
    filter by dilating it with zeros [[143](#bib.bib143)]. Bai et al. [[144](#bib.bib144)]
    simplified the causal dilated convolution [[142](#bib.bib142)] for sequence modeling
    problem and renamed it as temporal convolution network (TCN). Recently, more and
    more works employ TCN to process traffic data [[92](#bib.bib92)], [[70](#bib.bib70)],
    [[102](#bib.bib102)], [[111](#bib.bib111)].
  prefs: []
  type: TYPE_NORMAL
- en: V-C1 Sequence Modeling and 1-D TCN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given an input sequence with length $\mathbf{P}$ denoted as $\mathbf{x}=[\mathbf{x}_{1},\cdots,\mathbf{x}_{\mathbf{P}}]\in\mathbb{R}^{\mathbf{P}}$,
    sequence modeling aims to generate an output sequence with the same length, denoted
    as $\mathbf{y}=[\mathbf{y}_{1},\cdots,\mathbf{y}_{\mathbf{P}}]\in\mathbb{R}^{\mathbf{P}}$.
    The key assumption is that the output at current time $\mathbf{y}_{t}$ only depends
    on historical data $[\mathbf{x}_{1},\cdots,\mathbf{x}_{t}]$ but does not depend
    on any future inputs $[\mathbf{x}_{t+1},\cdots,\mathbf{x}_{\mathbf{P}}]$, i.e.
    $\mathbf{y}_{t}=f(\mathbf{x}_{1},\cdots,\mathbf{x}_{t})$, $f$ is the mapping function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, RNN, LSTM and GRU can be solutions to sequence modeling tasks. However,
    TCN can tackle sequence modeling problem more efficiently than RNNs for that it
    can capture long sequence properly in a non-recursive manner. The dilated causal
    convolution in TCN is formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{y}_{t}=\Theta*_{\mathcal{T}^{\mathbf{d}}}\mathbf{x}_{t}=\sum_{k=0}^{\mathbf{K}-1}w_{k}\mathbf{x}_{t-\mathbf{d}k}$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: 'where $*_{\mathcal{T}^{\mathbf{d}}}$ is the dilated causal operator with dilation
    rate $\mathbf{d}$ controlling the skipping distance, $\Theta=[w_{0},\cdots,w_{\mathbf{K-1}}]\in\mathbb{R}^{\mathbf{K}}$
    is the kernel. Zero padding strategy is utilized to keep the output length the
    same as the input length (as shown in Figure [8](#S5.F8 "Figure 8 ‣ V-C1 Sequence
    Modeling and 1-D TCN ‣ V-C TCN ‣ V Deep Learning Techniques Perspective ‣ How
    to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")).
    Without padding, the output length is shortened by $(\mathbf{K}-1)\mathbf{d}$
    [[92](#bib.bib92)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bedaffa3c763518bfcbf7854cc55384f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Multiple dilated causal convolution layers in TCN: $[\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3}]$
    is the input sequence and $[\mathbf{y}_{1},\mathbf{y}_{2},\mathbf{y}_{3}]$ is
    the output sequence with the same length. The size of kernel is $2$ and the dilation
    rate sequence is $[1,2,4]$. Zero padding strategy is taken.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To enlarge the receptive field, TCN stacks multiple dilated causal convolution
    layers with $\mathbf{d}=2^{l}$ as the dilation rate of $l^{th}$ layer (as shown
    in Figure [8](#S5.F8 "Figure 8 ‣ V-C1 Sequence Modeling and 1-D TCN ‣ V-C TCN
    ‣ V Deep Learning Techniques Perspective ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey")). Therefore, the receptive field in
    the network grows exponentially without requiring many convolutional layers or
    larger filter, which can handle longer sequence with less layers and save computation
    resources [[102](#bib.bib102)].'
  prefs: []
  type: TYPE_NORMAL
- en: V-C2 TCN in Traffic Domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are many traffic works related with sequence modeling, especially traffic
    spatial-temporal forecasting tasks. Compared with RNNs, the non-recursive calculation
    manner enables TCN to alleviate the gradient explosion problem and facilitate
    the training by parallel computation. Therefore, some works adopt TCN to capture
    the temporal dependency in traffic data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most graph-based traffic data is 3-D signal denoted as $\mathcal{X}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{I}}}$,
    which requires the generalization of 1-D TCN to 3-D TCN. The dilated causal convolution
    can be adopted to produce the $j^{th}$ output feature of node $i$ at time $t$
    as follows [[70](#bib.bib70)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathcal{Y}_{t,j}^{i}&amp;=\boldsymbol{\rho}(\Theta_{j}*_{\mathcal{T}^{\mathbf{d}}}\mathcal{X}_{t}^{i})=\boldsymbol{\rho}(\sum_{m=1}^{\mathbf{F_{I}}}\sum_{k=0}^{\mathbf{K}-1}w_{j,m,k}\mathcal{X}_{t-\mathbf{d}k,m}^{i})\end{split}$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: where $1\leq j\leq\mathbf{F_{O}}$, $\mathcal{Y}_{t,j}^{i}\in\mathbb{R}$ is the
    $j^{th}$ output feature of node $i$ at time $t$. $\mathcal{X}_{t-\mathbf{d}k,m}^{i}\in\mathbb{R}$
    is the $m^{th}$ input feature of node $i$ at time $t-\mathbf{d}k$. The kernel
    $\Theta_{j}\in\mathbb{R}^{\mathbf{K}\times\mathbf{F_{I}}}$ is trainable. $\mathbf{F_{O}}$
    is the number of output features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same convolution kernel is applied to all nodes in the traffic network
    and each node produces $\mathbf{F_{O}}$ new features. The mathematical formulation
    of each layer is as follows [[70](#bib.bib70)],[[111](#bib.bib111)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{Y}=\boldsymbol{\rho}(\Theta*_{\mathcal{T}^{\mathbf{d}}}\mathcal{X})$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{X}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{I}}}$
    represents the historical observations of the whole traffic network over past
    $\mathbf{P}$ time slices, $\Theta\in\mathbb{R}^{\mathbf{K}\times\mathbf{F_{I}}\times\mathbf{F_{O}}}$
    represents the related convolution kernel, $\mathcal{Y}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{O}}}$
    is the output of TCN layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some tricks to enhance the performance of TCN in specific traffic
    tasks. For instance, Fang et al. [[111](#bib.bib111)] stacked multiple TCN layers
    to extract the short-term neighboring dependency by bottom layer and long-term
    temporal dependency by higher layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{Y}^{(l+1)}=\boldsymbol{\sigma}(\Theta^{l}*_{\mathcal{T}^{\mathbf{d}}}\mathcal{Y}^{(l)})$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{Y}^{(l)}$ is the input of $l^{th}$ layer, $\mathcal{Y}^{(l+1)}$
    is the output and $\mathcal{Y}^{(0)}=\mathcal{X}$. $\mathbf{d}=2^{l}$ is the dilation
    rate of $l^{th}$ layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce the complexity of model training, Ge et al. [[70](#bib.bib70)] constructed
    a residual block containing two TCN layers with the same dilation rate. The block
    input was added to last TCN layer to get the block output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{Y}^{(l+1)}=\mathcal{Y}^{(l)}+\boldsymbol{ReLU}(\Theta_{1}^{l}*_{\mathcal{T}^{\mathbf{d}}}(\boldsymbol{ReLU}(\Theta_{0}^{l}*_{\mathcal{T}^{\mathbf{d}}}\mathcal{Y}^{(l)})))$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: where $\Theta^{l}_{1},\Theta^{l}_{2}$ are the convolution kernels of the first
    layer and the second layer respectively. $\mathcal{Y}^{(l)}$ is the input of residual
    block and $\mathcal{Y}^{(l+1)}$ is its output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Wu et al. [[102](#bib.bib102)] integrated gating mechanism[[141](#bib.bib141)]
    with TCN to learn complex temporal dependency in traffic data:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{Y}=\boldsymbol{\rho}_{1}(\Theta_{1}*_{\mathcal{T}^{\mathbf{d}}}\mathcal{X}+b_{1})\boldsymbol{\odot}\boldsymbol{\rho}_{2}(\Theta_{2}*_{\mathcal{T}^{\mathbf{d}}}\mathcal{X}+b_{2})$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{\rho}_{2}(\boldsymbol{\cdot})\in[0,1]$ determines the ratio
    of information passed to the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Yu et al. [[92](#bib.bib92)] used the Gated TCN and set the dilation
    rate $\mathbf{d}=1$ without zero padding to shorten the output length as $\mathcal{Y}=(\Theta_{1}*_{\mathcal{T}^{1}}\mathcal{X})\boldsymbol{\odot}\boldsymbol{\sigma}(\Theta_{2}*_{\mathcal{T}^{1}}\mathcal{X})$.
    They argued that this can discover variances in time series traffic data.
  prefs: []
  type: TYPE_NORMAL
- en: V-D Seq2Seq
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-D1 Seq2Seq
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d31237e6367284e901b690e15b64ae89.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Sequence to Sequence Structure without attention mechanism'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequence to Sequence (Seq2Seq) model proposed in 2014 [[145](#bib.bib145)]
    has been widely used in sequence prediction such as machine translation [[146](#bib.bib146)].
    Seq2Seq architecture consists of two components, i.e. an encoder in charge of
    converting the input sequence $\mathbf{X}$ into a fixed latent vector $\mathbf{C}$,
    and a decoder responsible for converting $\mathbf{C}$ into an output sequence
    $\mathbf{Y}$ (as shown in Figure [9](#S5.F9 "Figure 9 ‣ V-D1 Seq2Seq ‣ V-D Seq2Seq
    ‣ V Deep Learning Techniques Perspective ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey")). Note that $\mathbf{X}$ and $\mathbf{Y}$
    can have different lengths.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{X}\!=\![\mathbf{X}_{1}\!,\cdots,\!\mathbf{X}_{i},\cdots,\mathbf{X}_{\mathbf{P}}]\stackrel{{\scriptstyle
    Seq2Seq}}{{\longrightarrow}}\mathbf{Y}=[\mathbf{Y}_{1},\cdots,\mathbf{Y}_{j},\cdots,\mathbf{Y}_{\mathbf{Q}}]$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{P}$ is the input length and $\mathbf{Q}$ is the output length.
    $\mathbf{X}_{i}$ is the input at time step $i$. $\mathbf{Y}_{j}$ is the output
    at time step $j$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The specific calculation of $\mathbf{Y}_{j}$ is denoted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="\begin{split}\mathbf{H}_{i}&amp;=Encoder(\mathbf{X}_{i},\mathbf{H}_{i-1})\\
    \mathbf{C}&amp;=\mathbf{H}_{\mathbf{P}},\mathbf{S}_{0}=\mathbf{H}_{\mathbf{P}}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{S}_{j}&amp;=Decoder(\mathbf{C},\mathbf{Y}_{j-1},\mathbf{S}_{j-1})\\
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{Y}_{j}&amp;=\mathbf{S}_{j}W\end{split}" display="block"><semantics ><mtable
    columnspacing="0pt" displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd class="ltx_align_right" columnalign="right" ><msub
    ><mi mathsize="80%"  >𝐇</mi><mi
    mathsize="80%"  >i</mi></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mo
    mathsize="80%"  >=</mo><mrow
    ><mi mathsize="80%" 
    >E</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >n</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >c</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >o</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >d</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >e</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >r</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mrow ><mo
    maxsize="80%" minsize="80%"  >(</mo><msub
    ><mi mathsize="80%" 
    >𝐗</mi><mi mathsize="80%" 
    >i</mi></msub><mo mathsize="80%" 
    >,</mo><msub ><mi
    mathsize="80%"  >𝐇</mi><mrow
     ><mi
    mathsize="80%"  >i</mi><mo
    mathsize="80%"  >−</mo><mn
    mathsize="80%"  >1</mn></mrow></msub><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_right" columnalign="right" ><mi
    mathsize="80%"  >𝐂</mi></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mrow
    ><mo mathsize="80%" 
    >=</mo><msub ><mi
    mathsize="80%"  >𝐇</mi><mi
    mathsize="80%"  >𝐏</mi></msub></mrow><mo
    mathsize="80%"  >,</mo><mrow
    ><msub ><mi
    mathsize="80%"  >𝐒</mi><mn
    mathsize="80%"  >0</mn></msub><mo
    mathsize="80%"  >=</mo><msub
    ><mi mathsize="80%" 
    >𝐇</mi><mi mathsize="80%" 
    >𝐏</mi></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_right" columnalign="right" ><msub
    ><mi mathsize="80%"  >𝐒</mi><mi
    mathsize="80%"  >j</mi></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mo
    mathsize="80%"  >=</mo><mrow
    ><mi mathsize="80%" 
    >D</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >e</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >c</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >o</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >d</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >e</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >r</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mrow ><mo
    maxsize="80%" minsize="80%"  >(</mo><mi
    mathsize="80%"  >𝐂</mi><mo
    mathsize="80%"  >,</mo><msub
    ><mi mathsize="80%" 
    >𝐘</mi><mrow 
    ><mi mathsize="80%" 
    >j</mi><mo mathsize="80%" 
    >−</mo><mn mathsize="80%" 
    >1</mn></mrow></msub><mo mathsize="80%"
     >,</mo><msub ><mi
    mathsize="80%"  >𝐒</mi><mrow
     ><mi
    mathsize="80%"  >j</mi><mo
    mathsize="80%"  >−</mo><mn
    mathsize="80%"  >1</mn></mrow></msub><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_right" columnalign="right" ><msub
    ><mi mathsize="80%"  >𝐘</mi><mi
    mathsize="80%"  >j</mi></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mo
    mathsize="80%"  >=</mo><mrow
    ><msub ><mi mathsize="80%"
     >𝐒</mi><mi mathsize="80%"
     >j</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >W</mi></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><csymbol cd="ambiguous" 
    >formulae-sequence</csymbol><apply 
    ><apply  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><ci 
    >𝑖</ci></apply><apply 
    ><ci  >𝐸</ci><ci
     >𝑛</ci><ci 
    >𝑐</ci><ci  >𝑜</ci><ci
     >𝑑</ci><ci 
    >𝑒</ci><ci  >𝑟</ci><interval
    closure="open"  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐗</ci><ci
     >𝑖</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><apply
     ><ci
     >𝑖</ci><cn
    type="integer"  >1</cn></apply></apply></interval><ci
     >𝐂</ci></apply></apply><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐇</ci><ci 
    >𝐏</ci></apply></apply></apply><apply 
    ><apply  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐒</ci><cn type="integer"
     >0</cn></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><ci 
    >𝐏</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐒</ci><ci 
    >𝑗</ci></apply></apply></apply><apply 
    ><apply 
    ><ci  >𝐷</ci><ci
     >𝑒</ci><ci 
    >𝑐</ci><ci  >𝑜</ci><ci
     >𝑑</ci><ci 
    >𝑒</ci><ci 
    >𝑟</ci><vector 
    ><ci 
    >𝐂</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐘</ci><apply 
    ><ci 
    >𝑗</ci><cn type="integer" 
    >1</cn></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐒</ci><apply 
    ><ci 
    >𝑗</ci><cn type="integer" 
    >1</cn></apply></apply></vector><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐘</ci><ci 
    >𝑗</ci></apply></apply></apply><apply 
    ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐒</ci><ci 
    >𝑗</ci></apply><ci 
    >𝑊</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\mathbf{H}_{i}&=Encoder(\mathbf{X}_{i},\mathbf{H}_{i-1})\\
    \mathbf{C}&=\mathbf{H}_{\mathbf{P}},\mathbf{S}_{0}=\mathbf{H}_{\mathbf{P}}\\ \mathbf{S}_{j}&=Decoder(\mathbf{C},\mathbf{Y}_{j-1},\mathbf{S}_{j-1})\\
    \mathbf{Y}_{j}&=\mathbf{S}_{j}W\end{split}</annotation></semantics></math> |  |
    (20) |
  prefs: []
  type: TYPE_NORMAL
- en: here, $\mathbf{H}_{i}$ is the hidden state of encoder. $\mathbf{H}_{0}$ is initialized
    using small non-zero elements. $\mathbf{S}_{j}$ is the decoder hidden state. $\mathbf{Y}_{0}$
    is the representation of beginning sign. Note that the encoder and decoder can
    be any model as long as it can accept sequence and produce sequence, such as RNN,
    LSTM, GRU or other novel models.
  prefs: []
  type: TYPE_NORMAL
- en: A major limitation of Seq2Seq is that the latent vector $\mathbf{C}$ is fixed
    for each $\mathbf{Y}_{j}$ while $\mathbf{Y}_{j}$ might have stronger correlation
    with $\mathbf{X}_{j}$ than other elements. To address this issue, attention mechanism
    is integrated into Seq2Seq, allowing the decoder to focus on task-relevant parts
    of the input sequence, helping the decoder make better prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="\begin{split}\mathbf{H}_{i}&amp;=Encoder(\mathbf{X}_{i},\mathbf{H}_{i-1})\\
    \mathbf{C}_{j}&amp;=\sum_{i=1}^{\mathbf{P}}(\theta_{ji}\mathbf{H}_{i}),\mathbf{S}_{0}=\mathbf{H}_{\mathbf{P}}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{S}_{j}&amp;=Decoder(\mathbf{C}_{j},\mathbf{Y}_{j-1},\mathbf{S}_{j-1})\\
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{Y}_{j}&amp;=\mathbf{S}_{j}W\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt"
    displaystyle="true" rowspacing="0pt" ><mtr ><mtd
    class="ltx_align_right" columnalign="right" ><msub ><mi
    mathsize="80%"  >𝐇</mi><mi
    mathsize="80%"  >i</mi></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mo
    mathsize="80%"  >=</mo><mrow
    ><mi mathsize="80%" 
    >E</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >n</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >c</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >o</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >d</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >e</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >r</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mrow ><mo
    maxsize="80%" minsize="80%"  >(</mo><msub
    ><mi mathsize="80%" 
    >𝐗</mi><mi mathsize="80%" 
    >i</mi></msub><mo mathsize="80%" 
    >,</mo><msub ><mi
    mathsize="80%"  >𝐇</mi><mrow
     ><mi
    mathsize="80%"  >i</mi><mo
    mathsize="80%"  >−</mo><mn
    mathsize="80%"  >1</mn></mrow></msub><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_right" columnalign="right" ><msub
    ><mi mathsize="80%"  >𝐂</mi><mi
    mathsize="80%"  >j</mi></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mrow
    ><mo mathsize="80%" rspace="0.111em" 
    >=</mo><mrow ><munderover
    ><mo maxsize="80%" minsize="80%" movablelimits="false"
    rspace="0em" stretchy="true"  >∑</mo><mrow
     ><mi mathsize="80%"
     >i</mi><mo
    mathsize="80%"  >=</mo><mn
    mathsize="80%"  >1</mn></mrow><mi
    mathsize="80%"  >𝐏</mi></munderover><mrow
    ><mo maxsize="80%" minsize="80%" 
    >(</mo><mrow ><msub
    ><mi mathsize="80%" 
    >θ</mi><mrow 
    ><mi mathsize="80%" 
    >j</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >i</mi></mrow></msub><mo lspace="0em"
    rspace="0em"  >​</mo><msub
    ><mi mathsize="80%" 
    >𝐇</mi><mi mathsize="80%" 
    >i</mi></msub></mrow><mo maxsize="80%"
    minsize="80%"  >)</mo></mrow></mrow></mrow><mo
    mathsize="80%"  >,</mo><mrow
    ><msub ><mi
    mathsize="80%"  >𝐒</mi><mn
    mathsize="80%"  >0</mn></msub><mo
    mathsize="80%"  >=</mo><msub
    ><mi mathsize="80%" 
    >𝐇</mi><mi mathsize="80%" 
    >𝐏</mi></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_right" columnalign="right" ><msub
    ><mi mathsize="80%"  >𝐒</mi><mi
    mathsize="80%"  >j</mi></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mo
    mathsize="80%"  >=</mo><mrow
    ><mi mathsize="80%" 
    >D</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >e</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >c</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >o</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >d</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >e</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >r</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mrow ><mo
    maxsize="80%" minsize="80%"  >(</mo><msub
    ><mi mathsize="80%" 
    >𝐂</mi><mi mathsize="80%" 
    >j</mi></msub><mo mathsize="80%" 
    >,</mo><msub ><mi
    mathsize="80%"  >𝐘</mi><mrow
     ><mi
    mathsize="80%"  >j</mi><mo
    mathsize="80%"  >−</mo><mn
    mathsize="80%"  >1</mn></mrow></msub><mo
    mathsize="80%"  >,</mo><msub
    ><mi mathsize="80%" 
    >𝐒</mi><mrow 
    ><mi mathsize="80%" 
    >j</mi><mo mathsize="80%" 
    >−</mo><mn mathsize="80%" 
    >1</mn></mrow></msub><mo maxsize="80%"
    minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_right" columnalign="right" ><msub
    ><mi mathsize="80%"  >𝐘</mi><mi
    mathsize="80%"  >j</mi></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mo
    mathsize="80%"  >=</mo><mrow
    ><msub ><mi mathsize="80%"
     >𝐒</mi><mi mathsize="80%"
     >j</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >W</mi></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><csymbol cd="ambiguous" 
    >formulae-sequence</csymbol><apply 
    ><apply  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><ci 
    >𝑖</ci></apply><apply 
    ><ci  >𝐸</ci><ci
     >𝑛</ci><ci 
    >𝑐</ci><ci  >𝑜</ci><ci
     >𝑑</ci><ci 
    >𝑒</ci><ci  >𝑟</ci><interval
    closure="open"  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐗</ci><ci
     >𝑖</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><apply
     ><ci
     >𝑖</ci><cn
    type="integer"  >1</cn></apply></apply></interval><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐂</ci><ci 
    >𝑗</ci></apply></apply></apply><apply 
    ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><apply 
    ><ci  >𝑖</ci><cn
    type="integer"  >1</cn></apply></apply><ci
     >𝐏</ci></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝜃</ci><apply
     ><ci 
    >𝑗</ci><ci 
    >𝑖</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐇</ci><ci 
    >𝑖</ci></apply></apply></apply></apply></apply><apply
     ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐒</ci><cn type="integer" 
    >0</cn></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐇</ci><ci 
    >𝐏</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐒</ci><ci 
    >𝑗</ci></apply></apply></apply><apply 
    ><apply 
    ><ci  >𝐷</ci><ci
     >𝑒</ci><ci 
    >𝑐</ci><ci  >𝑜</ci><ci
     >𝑑</ci><ci 
    >𝑒</ci><ci 
    >𝑟</ci><vector 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐂</ci><ci 
    >𝑗</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐘</ci><apply 
    ><ci 
    >𝑗</ci><cn type="integer" 
    >1</cn></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐒</ci><apply 
    ><ci 
    >𝑗</ci><cn type="integer" 
    >1</cn></apply></apply></vector><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐘</ci><ci 
    >𝑗</ci></apply></apply></apply><apply 
    ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐒</ci><ci 
    >𝑗</ci></apply><ci 
    >𝑊</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\mathbf{H}_{i}&=Encoder(\mathbf{X}_{i},\mathbf{H}_{i-1})\\
    \mathbf{C}_{j}&=\sum_{i=1}^{\mathbf{P}}(\theta_{ji}\mathbf{H}_{i}),\mathbf{S}_{0}=\mathbf{H}_{\mathbf{P}}\\
    \mathbf{S}_{j}&=Decoder(\mathbf{C}_{j},\mathbf{Y}_{j-1},\mathbf{S}_{j-1})\\ \mathbf{Y}_{j}&=\mathbf{S}_{j}W\\
    \end{split}</annotation></semantics></math> |  | (21) |
  prefs: []
  type: TYPE_NORMAL
- en: where $\theta_{ji}=\frac{\exp(f_{ji})}{\sum_{k=1}^{\mathbf{P}}\exp(f_{jk})}$
    is the normalized attention score, and $f_{ji}=f(\mathbf{H}_{j},\mathbf{S}_{i-1})$
    [[146](#bib.bib146)] is a function to measure the correlation between $i^{th}$
    input and $j^{th}$ output, for instance, Luong et al. [[147](#bib.bib147)] proposed
    three kinds of attention score calculation.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="f_{ji}=\left\{\begin{array}[]{ll}\mathbf{H}_{j}^{T}\mathbf{S}_{i-1}&amp;\text{
    dot }\\ \mathbf{H}_{j}^{T}\boldsymbol{W}_{\boldsymbol{a}}\mathbf{S}_{i-1}&amp;\text{
    general }\\'
  prefs: []
  type: TYPE_NORMAL
- en: \boldsymbol{v}_{a}^{T}\tanh\left(\boldsymbol{W}_{\boldsymbol{a}}\left[\mathbf{H}_{j},\mathbf{S}_{i-1}\right]\right)&amp;\text{
    concat }\end{array}\right." display="block"><semantics ><mrow
     ><msub  ><mi
    mathsize="80%"  >f</mi><mrow
     ><mi mathsize="80%" 
    >j</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >i</mi></mrow></msub><mo mathsize="80%" 
    >=</mo><mrow  ><mo
     >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd class="ltx_align_left" columnalign="left"
     ><mrow  ><msubsup
     ><mi mathsize="80%"
     >𝐇</mi><mi
    mathsize="80%"  >j</mi><mi
    mathsize="80%"  >T</mi></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi mathsize="80%"
     >𝐒</mi><mrow 
    ><mi mathsize="80%" 
    >i</mi><mo mathsize="80%" 
    >−</mo><mn mathsize="80%" 
    >1</mn></mrow></msub></mrow></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mtext mathsize="80%"
     > dot </mtext></mtd></mtr><mtr
     ><mtd class="ltx_align_left" columnalign="left"
     ><mrow  ><msubsup
     ><mi mathsize="80%"
     >𝐇</mi><mi
    mathsize="80%"  >j</mi><mi
    mathsize="80%"  >T</mi></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi mathsize="80%"
     >𝑾</mi><mi mathsize="80%"
     >𝒂</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi mathsize="80%"
     >𝐒</mi><mrow 
    ><mi mathsize="80%" 
    >i</mi><mo mathsize="80%" 
    >−</mo><mn mathsize="80%" 
    >1</mn></mrow></msub></mrow></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mtext mathsize="80%"
     > general </mtext></mtd></mtr><mtr
     ><mtd class="ltx_align_left" columnalign="left"
     ><mrow  ><msubsup
     ><mi mathsize="80%"
     >𝒗</mi><mi
    mathsize="80%"  >a</mi><mi
    mathsize="80%"  >T</mi></msubsup><mo
    lspace="0.167em" rspace="0em"  >​</mo><mrow
     ><mi mathsize="80%"
     >tanh</mi><mo 
    >⁡</mo><mrow 
    ><mo  >(</mo><mrow
     ><msub
     ><mi
    mathsize="80%"  >𝑾</mi><mi
    mathsize="80%"  >𝒂</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
     >[</mo><msub
     ><mi
    mathsize="80%"  >𝐇</mi><mi
    mathsize="80%"  >j</mi></msub><mo
    mathsize="80%"  >,</mo><msub
     ><mi
    mathsize="80%"  >𝐒</mi><mrow
     ><mi
    mathsize="80%"  >i</mi><mo
    mathsize="80%"  >−</mo><mn
    mathsize="80%"  >1</mn></mrow></msub><mo
     >]</mo></mrow></mrow><mo
     >)</mo></mrow></mrow></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left"  ><mtext
    mathsize="80%"  > concat </mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply
     ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci  >𝑓</ci><apply
     ><ci 
    >𝑗</ci><ci  >𝑖</ci></apply></apply><apply
     ><csymbol cd="latexml" 
    >cases</csymbol><matrix  ><matrixrow
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐇</ci><ci
     >𝑗</ci></apply><ci
     >𝑇</ci></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐒</ci><apply
     ><ci 
    >𝑖</ci><cn type="integer" 
    >1</cn></apply></apply></apply><ci 
    ><mtext mathsize="80%" 
    > dot </mtext></ci></matrixrow><matrixrow 
    ><apply  ><apply
     ><csymbol cd="ambiguous"
     >superscript</csymbol><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐇</ci><ci
     >𝑗</ci></apply><ci
     >𝑇</ci></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑾</ci><ci 
    >𝒂</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐒</ci><apply 
    ><ci  >𝑖</ci><cn
    type="integer"  >1</cn></apply></apply></apply><ci
     ><mtext mathsize="80%"
     > general </mtext></ci></matrixrow><matrixrow
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝒗</ci><ci
     >𝑎</ci></apply><ci
     >𝑇</ci></apply><apply
     ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑾</ci><ci 
    >𝒂</ci></apply><interval closure="closed"
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐇</ci><ci
     >𝑗</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐒</ci><apply
     ><ci
     >𝑖</ci><cn
    type="integer"  >1</cn></apply></apply></interval></apply></apply></apply><ci
     ><mtext mathsize="80%"
     > concat </mtext></ci></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >f_{ji}=\left\{\begin{array}[]{ll}\mathbf{H}_{j}^{T}\mathbf{S}_{i-1}&\text{
    dot }\\ \mathbf{H}_{j}^{T}\boldsymbol{W}_{\boldsymbol{a}}\mathbf{S}_{i-1}&\text{
    general }\\ \boldsymbol{v}_{a}^{T}\tanh\left(\boldsymbol{W}_{\boldsymbol{a}}\left[\mathbf{H}_{j},\mathbf{S}_{i-1}\right]\right)&\text{
    concat }\end{array}\right.</annotation></semantics></math> |  | (22) |
  prefs: []
  type: TYPE_NORMAL
- en: Another way to enhance Seq2Seq performance is the scheduled sampling technique
    [[148](#bib.bib148)]. The inputs of decoder during training and testing phases
    are different. Decoder during training phase is fed with true labels of training
    datasets while it is fed with predictions generated by itself during testing phase,
    which accumulates error at testing time and causes degraded performance. To mitigate
    this issue, scheduled sampling is integrated into the model. At $j^{th}$ iteration
    during the training process, the probability of feeding the decoder with true
    label is set as $\epsilon_{j}$ and the probability of feeding the decoder with
    prediction at the previous step is set as $1-\epsilon_{j}$. Probability $\epsilon_{j}$
    gradually decreases to 0, allowing the decoder to learn the testing distribution
    [[108](#bib.bib108)], keeping the training and testing as same as possible.
  prefs: []
  type: TYPE_NORMAL
- en: V-D2 Seq2Seq in Traffic Domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since Seq2Seq can take in an input sequence and generate an output sequence
    with different length, it is applied on multi-step prediction in many traffic
    tasks. The encoder encodes the historical traffic data into a latent space vector.
    Then, the latent vector is fed into a decoder to generate the future traffic conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanism is usually incorporated into Seq2Seq to model the different
    influence on future prediction from previous traffic observations at different
    time slots [[100](#bib.bib100)],[[98](#bib.bib98)], [[110](#bib.bib110)],[[76](#bib.bib76)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder and decoder in many traffic literatures are in charge of capturing
    spatial-temporal dependencies. For instance, Li et al. [[108](#bib.bib108)] proposed
    DCGRU to be the encoder and decoder, which can capture spatial and temporal dynamics
    jointly. The design of encoder and decoder is usually the core contribution and
    novel part of relative works. Note that the encoder and decoder are not necessarily
    the same and we have made a summarization of Seq2Seq structure in previous graph-based
    traffic works (as shown in Table [III](#S5.T3 "TABLE III ‣ V-D2 Seq2Seq in Traffic
    Domain ‣ V-D Seq2Seq ‣ V Deep Learning Techniques Perspective ‣ How to Build a
    Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: The encoders and decoders of sequence to sequence architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '| References | Encoder | Decoder |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[108](#bib.bib108)] | GRU+DGCN | Same as encoder |'
  prefs: []
  type: TYPE_TB
- en: '| [[100](#bib.bib100)] | SGCN +LSTM | LSTM+SGCN |'
  prefs: []
  type: TYPE_TB
- en: '| [[98](#bib.bib98)] | STAtt Block | Same as encoder |'
  prefs: []
  type: TYPE_TB
- en: '| [[41](#bib.bib41)] | MLPs | An MLP |'
  prefs: []
  type: TYPE_TB
- en: '| [[109](#bib.bib109)] | SGCN+Pooling+GRU | GCN+Upooling+GRU |'
  prefs: []
  type: TYPE_TB
- en: '| [[104](#bib.bib104)] | GRU with graph self-attention | Same as encoder |'
  prefs: []
  type: TYPE_TB
- en: '| [[42](#bib.bib42)] | GRU+SGCN | Same as encoder |'
  prefs: []
  type: TYPE_TB
- en: '| [[110](#bib.bib110)] | SGCN+ bidirectional GRU | Same as encoder |'
  prefs: []
  type: TYPE_TB
- en: '| [[76](#bib.bib76)] | Long-term encoder (Gated SGCN) | Short-term encoder
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[113](#bib.bib113)] | SGCN+LSTM | LSTM |'
  prefs: []
  type: TYPE_TB
- en: '| [[96](#bib.bib96)] | SGCN+GRU | Same as encoder |'
  prefs: []
  type: TYPE_TB
- en: '| [[77](#bib.bib77)] | CGRM (GRU, SGCN) | Same as encoder |'
  prefs: []
  type: TYPE_TB
- en: '| [[103](#bib.bib103)] | LSTM+RGC | RGC |'
  prefs: []
  type: TYPE_TB
- en: '| [[48](#bib.bib48)] | LSTM | Same as encoder |'
  prefs: []
  type: TYPE_TB
- en: RNNs-based decoder has a severe error accumulation problem during testing inference
    due to that each previous predicted step is the input to produce the next step
    prediction. The scheduled sampling to alleviate this problem is adopted in [[108](#bib.bib108)],[[104](#bib.bib104)].
    RNNs-based decoder is replaced with a short-term and long-term decoder to take
    in last step prediction exclusively, thus easing error accumulation [[76](#bib.bib76)].
    The utilization of Seq2Seq technique in traffic domain is flexible. For instance,
    Seq2Seq is integrated into a bigger framework, being the generator and discriminator
    of GAN [[100](#bib.bib100)].
  prefs: []
  type: TYPE_NORMAL
- en: V-E GAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-E1 GAN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/651837ea4ea84ac50ca890f443f44404.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Generative Adversarial Network: Generator $G$ is in charge of producing
    a generated sample $x_{f}=G(z)$ from a random vector $z$, which is sampled from
    a prior distribution $p_{z}$. Discriminator $D$ is in charge of discriminating
    between the fake sample $x_{f}$ generated from $G$ and the real sample $x_{r}$
    from the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative Adversarial Network (GAN) [[149](#bib.bib149)] is a powerful deep
    generative model aiming to generate artificial samples as indistinguishable as
    possible from their real counterparts. GAN, inspired by game theory, is composed
    of two players, a generative neural network called Generator $G$ and an adversarial
    network called Discriminator $D$ (as shown in Figure [10](#S5.F10 "Figure 10 ‣
    V-E1 GAN ‣ V-E GAN ‣ V Deep Learning Techniques Perspective ‣ How to Build a Graph-Based
    Deep Learning Architecture in Traffic Domain: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator $D$ tries to determine whether the input samples belong to the
    generated data or the real data while Generator $G$ tries to cheat on Discriminator
    $D$ by producing samples as true as possible. The two mutually adversarial and
    optimized processes are alternately trained, which strengthens the performance
    of both $D$ and $G$. When the fake sample produced by $G$ is very close to the
    ground truth and $D$ is unable to distinguish them any more, it is considered
    that Generator $G$ has learned the true distribution of the real data and the
    model converges. At this time, we can consider this game to reach a Nash equilibrium
    [[150](#bib.bib150)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, such process can be formulated to minimize their losses $Loss_{G}$
    and $Loss_{D}$. With the loss function being cross entropy denoted as $f$, we
    can have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="\begin{split}Loss_{G}&amp;=f(D(G(z)),1)=-\sum\log
    D(G(z))\\ \phi^{*}&amp;=\underset{\phi}{\operatorname{argmin}}(Loss_{G})=\underset{\phi}{\operatorname{argmax}}(-Loss_{G})\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;=\underset{\phi}{\operatorname{argmax}}\mathbb{E}(\log D(G(z)))\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr 
    ><mtd class="ltx_align_right" columnalign="right"
     ><mrow 
    ><mi mathsize="80%"  >L</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >s</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi mathsize="80%" 
    >s</mi><mi mathsize="80%" 
    >G</mi></msub></mrow></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow 
    ><mi mathsize="80%"  >f</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mi mathsize="80%"
     >D</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow
     ><mo
    maxsize="80%" minsize="80%"  >(</mo><mrow
     ><mi
    mathsize="80%"  >G</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    maxsize="80%" minsize="80%"  >(</mo><mi
    mathsize="80%"  >z</mi><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    mathsize="80%"  >,</mo><mn
    mathsize="80%"  >1</mn><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    mathsize="80%"  >=</mo><mrow
     ><mo mathsize="80%"
     >−</mo><mrow
     ><mo maxsize="80%"
    minsize="80%" movablelimits="false" stretchy="true" 
    >∑</mo><mrow 
    ><mrow  ><mi
    mathsize="80%"  >log</mi><mo
    lspace="0.167em"  >⁡</mo><mi
    mathsize="80%"  >D</mi></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mi
    mathsize="80%"  >G</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    maxsize="80%" minsize="80%"  >(</mo><mi
    mathsize="80%"  >z</mi><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msup
     ><mi mathsize="80%" 
    >ϕ</mi><mo mathsize="80%" 
    >∗</mo></msup></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow
     ><munder accentunder="true"
     ><mi mathsize="80%"
     >argmin</mi><mo
    class="ltx_mathvariant_italic" mathsize="80%" mathvariant="italic" 
    >ϕ</mo></munder><mo lspace="0em" rspace="0em"
     >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mi mathsize="80%"
     >L</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
    mathsize="80%"  >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >s</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi mathsize="80%"
     >s</mi><mi mathsize="80%"
     >G</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    mathsize="80%"  >=</mo><mrow
     ><munder accentunder="true"
     ><mi mathsize="80%"
     >argmax</mi><mo
    class="ltx_mathvariant_italic" mathsize="80%" mathvariant="italic" 
    >ϕ</mo></munder><mo lspace="0em" rspace="0em"
     >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mo mathsize="80%"
     >−</mo><mrow
     ><mi mathsize="80%"
     >L</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >s</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
    mathsize="80%"  >s</mi><mi
    mathsize="80%"  >G</mi></msub></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow
     ><munder accentunder="true"
     ><mi mathsize="80%"
     >argmax</mi><mo
    class="ltx_mathvariant_italic" mathsize="80%" mathvariant="italic" 
    >ϕ</mo></munder><mo lspace="0.167em" rspace="0em"
     >​</mo><mi
    mathsize="80%"  >𝔼</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mrow
     ><mi
    mathsize="80%"  >log</mi><mo
    lspace="0.167em"  >⁡</mo><mi
    mathsize="80%"  >D</mi></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    maxsize="80%" minsize="80%"  >(</mo><mrow
     ><mi
    mathsize="80%"  >G</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    maxsize="80%" minsize="80%"  >(</mo><mi
    mathsize="80%"  >z</mi><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><apply
     ><ci 
    >𝐿</ci><ci  >𝑜</ci><ci
     >𝑠</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci  >𝐺</ci></apply></apply><apply
     ><ci 
    >𝑓</ci><interval closure="open" 
    ><apply  ><ci
     >𝐷</ci><apply 
    ><ci  >𝐺</ci><ci
     >𝑧</ci></apply></apply><cn
    type="integer"  >1</cn></interval></apply></apply><apply
     ><apply 
    ><apply  ><apply
     ><apply 
    ><ci  >𝐷</ci></apply><apply
     ><ci 
    >𝐺</ci><ci 
    >𝑧</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >italic-ϕ</ci></apply></apply></apply></apply></apply><apply
     ><apply 
    ><apply  ><ci
     >italic-ϕ</ci><ci
     >argmin</ci></apply><apply
     ><ci 
    >𝐿</ci><ci  >𝑜</ci><ci
     >𝑠</ci><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑠</ci><ci 
    >𝐺</ci></apply></apply></apply></apply><apply
     ><apply 
    ><apply  ><ci
     >italic-ϕ</ci><ci
     >argmax</ci></apply><apply
     ><apply 
    ><ci  >𝐿</ci><ci
     >𝑜</ci><ci
     >𝑠</ci><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝐺</ci></apply></apply></apply></apply></apply><apply
     ><apply 
    ><apply  ><ci
     >italic-ϕ</ci><ci
     >argmax</ci></apply><ci
     >𝔼</ci><apply
     ><apply 
    ><ci  >𝐷</ci></apply><apply
     ><ci 
    >𝐺</ci><ci 
    >𝑧</ci></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}Loss_{G}&=f(D(G(z)),1)=-\sum\log
    D(G(z))\\ \phi^{*}&=\underset{\phi}{\operatorname{argmin}}(Loss_{G})=\underset{\phi}{\operatorname{argmax}}(-Loss_{G})\\
    &=\underset{\phi}{\operatorname{argmax}}\mathbb{E}(\log D(G(z)))\end{split}</annotation></semantics></math>
    |  | (23) |'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="\begin{split}Loss_{D}&amp;=f(D(x_{r}),1,D(x_{f}),0)\\
    &amp;=-\sum\log D(x_{r})-\sum\log(1-D(x_{f}))\\'
  prefs: []
  type: TYPE_NORMAL
- en: \theta^{*}&amp;=\underset{\theta}{\operatorname{argmin}}(Loss_{D})=\underset{\theta}{\operatorname{argmax}}(-Loss_{D})\\
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;=\underset{\theta}{\operatorname{argmax}}(\mathbb{E}(\log D(x_{r})+\log(1-D(x_{f}))))\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><mrow
     ><mi mathsize="80%" 
    >L</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >o</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi mathsize="80%" 
    >s</mi><mo lspace="0em" rspace="0em" 
    >​</mo><msub  ><mi
    mathsize="80%"  >s</mi><mi
    mathsize="80%"  >D</mi></msub></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow 
    ><mi mathsize="80%"  >f</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mi mathsize="80%"
     >D</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow
     ><mo
    maxsize="80%" minsize="80%"  >(</mo><msub
     ><mi
    mathsize="80%"  >x</mi><mi
    mathsize="80%"  >r</mi></msub><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    mathsize="80%"  >,</mo><mn
    mathsize="80%"  >1</mn><mo
    mathsize="80%"  >,</mo><mrow
     ><mi mathsize="80%"
     >D</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    maxsize="80%" minsize="80%"  >(</mo><msub
     ><mi
    mathsize="80%"  >x</mi><mi
    mathsize="80%"  >f</mi></msub><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    mathsize="80%"  >,</mo><mn
    mathsize="80%"  >0</mn><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow
     ><mrow 
    ><mo mathsize="80%" 
    >−</mo><mrow 
    ><mo maxsize="80%" minsize="80%" movablelimits="false"
    stretchy="true"  >∑</mo><mrow
     ><mrow
     ><mi
    mathsize="80%"  >log</mi><mo
    lspace="0.167em"  >⁡</mo><mi
    mathsize="80%"  >D</mi></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    maxsize="80%" minsize="80%"  >(</mo><msub
     ><mi
    mathsize="80%"  >x</mi><mi
    mathsize="80%"  >r</mi></msub><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mrow><mo
    mathsize="80%" rspace="0.055em"  >−</mo><mrow
     ><mo maxsize="80%"
    minsize="80%" movablelimits="false" stretchy="true" 
    >∑</mo><mrow 
    ><mi mathsize="80%" 
    >log</mi><mo 
    >⁡</mo><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >(</mo><mrow 
    ><mn mathsize="80%" 
    >1</mn><mo mathsize="80%" 
    >−</mo><mrow 
    ><mi mathsize="80%" 
    >D</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >(</mo><msub 
    ><mi mathsize="80%" 
    >x</mi><mi mathsize="80%" 
    >f</mi></msub><mo maxsize="80%" minsize="80%"
     >)</mo></mrow></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_right"
    columnalign="right"  ><msup
     ><mi mathsize="80%" 
    >θ</mi><mo mathsize="80%" 
    >∗</mo></msup></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow
     ><munder accentunder="true"
     ><mi mathsize="80%"
     >argmin</mi><mo
    mathsize="80%"  >𝜃</mo></munder><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mi
    mathsize="80%"  >L</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >s</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
    mathsize="80%"  >s</mi><mi
    mathsize="80%"  >D</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    mathsize="80%"  >=</mo><mrow
     ><munder accentunder="true"
     ><mi mathsize="80%"
     >argmax</mi><mo
    mathsize="80%"  >𝜃</mo></munder><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mo
    mathsize="80%"  >−</mo><mrow
     ><mi
    mathsize="80%"  >L</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >s</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
    mathsize="80%"  >s</mi><mi
    mathsize="80%"  >D</mi></msub></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mo mathsize="80%"
     >=</mo><mrow
     ><munder accentunder="true"
     ><mi mathsize="80%"
     >argmax</mi><mo
    mathsize="80%"  >𝜃</mo></munder><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow
     ><mi
    mathsize="80%"  >𝔼</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    maxsize="80%" minsize="80%"  >(</mo><mrow
     ><mrow
     ><mrow
     ><mi
    mathsize="80%"  >log</mi><mo
    lspace="0.167em"  >⁡</mo><mi
    mathsize="80%"  >D</mi></mrow><mo
    lspace="0em" rspace="0em" 
    >​</mo><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >(</mo><msub 
    ><mi mathsize="80%" 
    >x</mi><mi mathsize="80%" 
    >r</mi></msub><mo maxsize="80%" minsize="80%"
     >)</mo></mrow></mrow><mo
    mathsize="80%"  >+</mo><mrow
     ><mi
    mathsize="80%"  >log</mi><mo
     >⁡</mo><mrow
     ><mo
    maxsize="80%" minsize="80%"  >(</mo><mrow
     ><mn
    mathsize="80%"  >1</mn><mo
    mathsize="80%"  >−</mo><mrow
     ><mi
    mathsize="80%"  >D</mi><mo
    lspace="0em" rspace="0em" 
    >​</mo><mrow 
    ><mo maxsize="80%" minsize="80%" 
    >(</mo><msub 
    ><mi mathsize="80%" 
    >x</mi><mi mathsize="80%" 
    >f</mi></msub><mo maxsize="80%" minsize="80%"
     >)</mo></mrow></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><apply
     ><ci 
    >𝐿</ci><ci  >𝑜</ci><ci
     >𝑠</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci  >𝐷</ci></apply></apply><apply
     ><ci 
    >𝑓</ci><vector  ><apply
     ><ci 
    >𝐷</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑥</ci><ci 
    >𝑟</ci></apply></apply><cn type="integer" 
    >1</cn><apply 
    ><ci  >𝐷</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑥</ci><ci
     >𝑓</ci></apply></apply><cn
    type="integer"  >0</cn></vector></apply></apply><apply
     ><apply 
    ><apply  ><apply
     ><apply 
    ><apply  ><ci
     >𝐷</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑥</ci><ci 
    >𝑟</ci></apply></apply></apply></apply><apply
     ><apply 
    ><apply  ><apply
     ><cn type="integer"
     >1</cn><apply
     ><ci 
    >𝐷</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑥</ci><ci 
    >𝑓</ci></apply></apply></apply></apply><apply
     ><csymbol cd="ambiguous"
     >superscript</csymbol><ci
     >𝜃</ci></apply></apply></apply></apply></apply><apply
     ><apply 
    ><apply  ><ci
     >𝜃</ci><ci
     >argmin</ci></apply><apply
     ><ci 
    >𝐿</ci><ci  >𝑜</ci><ci
     >𝑠</ci><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑠</ci><ci 
    >𝐷</ci></apply></apply></apply></apply><apply
     ><apply 
    ><apply  ><ci
     >𝜃</ci><ci
     >argmax</ci></apply><apply
     ><apply 
    ><ci  >𝐿</ci><ci
     >𝑜</ci><ci
     >𝑠</ci><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝐷</ci></apply></apply></apply></apply></apply><apply
     ><apply 
    ><apply  ><ci
     >𝜃</ci><ci
     >argmax</ci></apply><apply
     ><ci 
    >𝔼</ci><apply 
    ><apply 
    ><apply 
    ><ci  >𝐷</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑥</ci><ci 
    >𝑟</ci></apply></apply><apply 
    ><apply 
    ><cn type="integer" 
    >1</cn><apply 
    ><ci  >𝐷</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑥</ci><ci
     >𝑓</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}Loss_{D}&=f(D(x_{r}),1,D(x_{f}),0)\\
    &=-\sum\log D(x_{r})-\sum\log(1-D(x_{f}))\\ \theta^{*}&=\underset{\theta}{\operatorname{argmin}}(Loss_{D})=\underset{\theta}{\operatorname{argmax}}(-Loss_{D})\\
    &=\underset{\theta}{\operatorname{argmax}}(\mathbb{E}(\log D(x_{r})+\log(1-D(x_{f}))))\end{split}</annotation></semantics></math>
    |  | (24) |'
  prefs: []
  type: TYPE_NORMAL
- en: where $1$ is the label of true sample $x_{r}$. $0$ is the label of fake sample
    $x_{f}=G(z)$. $\phi$ and $\theta$ are the trainable parameters of $G$ and $D$
    respectively. Note that when $G$ is trained, $D$ is untrainable. Interested readers
    can refer to [[151](#bib.bib151)],[[152](#bib.bib152)] for surveys of GAN.
  prefs: []
  type: TYPE_NORMAL
- en: V-E2 GAN in Traffic Domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When GAN is applied in traffic prediction tasks [[153](#bib.bib153)],[[154](#bib.bib154)],
    Generator $G$ is usually employed to generate future traffic observations based
    on the historical observations. Then the generated data and the future real data
    are fed into Discriminator $D$ to train it. After training, Generator $G$ can
    learn the distribution of the real traffic flow data through a large number of
    historical data and can be used to predict the future traffic states [[100](#bib.bib100)].
    GAN can be also utilized to solve the sparsity problem of traffic data for its
    efficacy in handling data generation [[95](#bib.bib95)].
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the generator or discriminator of GAN can be any model, such as
    RNNs, Seq2Seq, depending on the specific traffic tasks.
  prefs: []
  type: TYPE_NORMAL
- en: VI Challenges Perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traffic tasks are very challenging due to the complicated spatial dependency,
    temporal dependency in traffic data. In addition, external factors such as holiday
    or event can also affect the traffic conditions. In this section, we introduce
    four common challenges in traffic domain. We carefully examine each challenge
    and its corresponding solutions, making necessary comparison.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Spatial Dependency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a3dca06c8862d2f2e5dc10d162e1fd0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The formulation of a bidirectional road: The traffic condition of
    road $R_{1}$ is only influenced by the same side road $R_{2}$ and has weak correlation
    with the opposite side road $R_{3}$. But if this region is modeled as grids, $R_{3}$
    has similar impact on $R_{1}$ as $R_{2}$, which is against the truth. If it is
    model as a graph, $R_{1}$ is connected with $R_{2}$ and disconnected with $R_{3}$,
    which can reflect the true relationship.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in previous section, some literatures[[131](#bib.bib131)],[[132](#bib.bib132)],[[155](#bib.bib155)]
    extract spatial features through decomposing the whole traffic network into grids
    and then employing CNNs to process the grid-based data. However, the grid-based
    assumption actually violates the nature topology of traffic network. Many traffic
    networks are physically organized as a graph and the graph topology information
    is obviously valuable for traffic prediction (as shown in Figure [11](#S6.F11
    "Figure 11 ‣ VI-A Spatial Dependency ‣ VI Challenges Perspective ‣ How to Build
    a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")). According
    to our survey, graph neural networks can model spatial dependencies in graph-based
    traffic networks much better than grid-based approaches. In addition, the complicated
    spatial dependencies in traffic network can be categorized into three spatial
    attributes, i.e. spatial locality, multiple relationships and global connectivity.
    Different kinds of GNNs combining with other deep learning techniques are utilized
    to solve different kinds of spatial attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-A1 Spatial Locality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Spatial locality refers that adjacent regions are usually highly relevant to
    each other. For example, the passenger flow of a station in a subway is obviously
    affected by its connected stations. $\mathbf{K}$-localized spectral graph convolution
    network (SGCN) is widely adopted to aggregate the information of $0$ to $\mathbf{K}-1$
    hop neighbors to the central region. In addition, some works make different assumptions
    about the spatial locality and utilize some novel tricks.
  prefs: []
  type: TYPE_NORMAL
- en: The adjacency matrix representing the traffic topology is usually pre-defined
    while some works [[69](#bib.bib69)],[[42](#bib.bib42)] argued that neighboring
    locations are dynamically correlated with each other. They incorporated the attention
    mechanism into SGCN to adaptively capture the dynamic correlations among surrounding
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: SGCN requires all the regions to have the same local statistics and its convolution
    kernel is location-independent. However, Zhang et al. [[68](#bib.bib68)] clarified
    that the local statistics of traffic data changed from region to region and they
    designed location-dependent kernels for different regions automatically.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A2 Multiple Relationships
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While locality attribute focuses on spatial proximity, the target region can
    be correlated with distant regions through various non-Euclidean relationships
    such as functional similarity, transportation connectivity (as shown in Figure
    [5](#S4.F5 "Figure 5 ‣ IV-C1 Nodes and Node Features Construction ‣ IV-C Graph
    Construction from Traffic Datasets ‣ IV Problem Formulation and Graph Construction
    ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")),
    semantic neighbors. Functional similarity refers that distant region is similar
    to the target region in terms of functionality, which can be characterized by
    the surrounding POIs [[89](#bib.bib89)],[[70](#bib.bib70)]. Transportation connectivity
    suggests that those geographically distant but conveniently reachable can be correlated
    [[89](#bib.bib89)]. The reachable way can be motorway, highway, subway. Semantic
    neighbors are adopted to model the correlation between origins and destinations
    [[101](#bib.bib101)]. The correlation is measured by the passenger flow between
    them. To explicitly extract these correlation information, different types of
    correlations using multiple graphs are encoded [[89](#bib.bib89)] and multi-graph
    convolution is leveraged.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-A3 Global Connectivity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Both spatial proximity and multi-relationship focus on parts of the network
    while ignore the whole structure. Global connectivity refers that traffic conditions
    of different regions have influenced each other at a whole network scale. There
    are several strategies to exploit the global structure information of traffic
    network.
  prefs: []
  type: TYPE_NORMAL
- en: A popular way to capture global connectivity is to model the changing traffic
    conditions in the traffic network as a diffusion process that happens at a network
    scale, which is presented by a power series of transition matrices. Then, diffusion
    graph convolution network (DGCN) is adopted to extract the spatial dependency
    globally [[112](#bib.bib112)], [[108](#bib.bib108)], [[100](#bib.bib100)], [[102](#bib.bib102)],
    [[96](#bib.bib96)],[[118](#bib.bib118)].
  prefs: []
  type: TYPE_NORMAL
- en: A novel spatial graph pooling layer with path growing algorithm is designed
    to produce a coarser graph [[105](#bib.bib105)]. This pooling layer is stacked
    before SGC layer to get multi-granularity graph convolutions, which can extract
    spatial features at various scopes.
  prefs: []
  type: TYPE_NORMAL
- en: A SGC layer with a self-adaptive adjacency matrix is proposed [[102](#bib.bib102)]
    to capture the hidden global spatial dependency in the data. This self-adaptive
    adjacency matrix is learned from the data through an end-to-end supervised training.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Temporal Dependency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Temporal dependency refers that prediction of traffic conditions at a certain
    time is usually correlated with various historical observations [[92](#bib.bib92)].
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated in Section [V](#S5 "V Deep Learning Techniques Perspective ‣ How
    to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey"),
    many works extract the temporal dependency by RNNs-based approaches. However,
    RNNs-based approaches suffer from time-consuming iterations and confront gradient
    vanishing/explosion problem for capturing long sequence. Compared with RNNs-based
    approaches, TCN-based approaches have the superiority of simple structures, parallel
    computing and stable gradients. Therefore, some works [[92](#bib.bib92)],[[70](#bib.bib70)]
    adopt TCN-based approaches to capture the temporal pattern in traffic data. In
    addition, TCN is able to handle different temporal levels by stacking multiple
    layers. For instance, Fang et al. [[111](#bib.bib111)] and Wu et al. [[102](#bib.bib102)]
    stacked multiple TCN layers with the bottom layers extracting short-term neighboring
    dependencies and the higher layers learning long-term temporal patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-B1 Multi-timescale
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some works extract the temporal dependency at a multi-timescale perspective
    [[69](#bib.bib69)],[[116](#bib.bib116)]. Temporal dependency is decomposed into
    recent, daily and weekly dependencies [[69](#bib.bib69)]. The recent dependency
    refers that the future traffic conditions are influenced by the traffic conditions
    recently. For instance, the traffic congestion at 9 am inevitably influences traffic
    flow at the following hours. Daily dependency describes that the repeated daily
    pattern in traffic data due to the regular daily routine of people, such as morning
    peak and evening peak. Weekly dependency considers the influence caused by the
    same week attributes. For instance, all Mondays share similar traffic pattern
    in a short-term. Guo et al. [[69](#bib.bib69)] set three parallel components with
    the same structure to model these three temporal attributes respectively.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B2 Different Weights
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some works argue that the correlations between historical and future observations
    are varying at different previous time slices. Guo et al. [[69](#bib.bib69)] adopted
    a temporal attention mechanism to adaptively attach different importance to historical
    data.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Spatiotemporal Dependency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many works capture the spatial and temporal dependency separately in a sequential
    manner [[110](#bib.bib110)], [[100](#bib.bib100)], [[94](#bib.bib94)], [[65](#bib.bib65)],[[91](#bib.bib91)],[[120](#bib.bib120)],[[88](#bib.bib88)]
    while the spatial and temporal dependencies are closely intertwined in traffic
    data. Guo et al. [[69](#bib.bib69)] argued that the historical observations in
    different locations at different times have varying impacts on central region
    in the future. Take an obvious example, a traffic accident in a critical road
    results in serious disruptions over related roads but at different time, due to
    the gradual formation and dispersion of traffic congestion.
  prefs: []
  type: TYPE_NORMAL
- en: 'A limitation of separately modeling is that the potential interactions between
    spatial features and temporal features are ignored, which may hurt the prediction
    performance. To overcome such limitation, a popular way is to incorporate the
    graph convolution operations (e.g. SGC, DGC) to RNNs (as stated in Section [VI](#S6
    "VI Challenges Perspective ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey")) to capture spatial-temporal correlations jointly
    [[66](#bib.bib66)], [[112](#bib.bib112)], [[108](#bib.bib108)], [[106](#bib.bib106)],
    [[96](#bib.bib96)],[[118](#bib.bib118)],[[42](#bib.bib42)], [[105](#bib.bib105)],
    [[77](#bib.bib77)].'
  prefs: []
  type: TYPE_NORMAL
- en: VI-D External Factors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Factors such as holidays, time attributes (e.g. hour, day, week, month, season,
    year) [[70](#bib.bib70)],[[116](#bib.bib116)], weather (e.g. rainfall, temperature,
    air quality)[[116](#bib.bib116)], special events, POIs[[89](#bib.bib89)] and traffic
    incidents (e.g. incident time, incident type) [[91](#bib.bib91)] can influence
    the traffic prediction in some extent, which we refer as external factors or context
    factors. In addition, Zhang et al. [[110](#bib.bib110)] considered historical
    statistical speed information (e.g. average or standard deviation of traffic speed)
    as external factor.
  prefs: []
  type: TYPE_NORMAL
- en: Some factors such as day attributes, holidays and weather conditions are encoded
    as discrete values and they are usually transformed into binary vectors by one-hot
    encoding. Other factors including temperature, wind speed are encoded as continual
    values and they are usually normalized by Min-Max normalization or Z-score normalization.
  prefs: []
  type: TYPE_NORMAL
- en: There are two approaches to handle external factors in the literatures we survey.
    The first approach is to concatenate the external factors with other features
    and feed them into model [[112](#bib.bib112)], [[70](#bib.bib70)]. The second
    approach is to design an external component in charge of processing external factors
    alone. The external component usually contains two fully connected layers, of
    which the first extracting important features and the second mapping low dimension
    features to high dimension features [[70](#bib.bib70)], [[91](#bib.bib91)],[[116](#bib.bib116)],[[48](#bib.bib48)].
    Bai et al. [[113](#bib.bib113)] employed multi-LSTM layers to extract representation
    of external factors. The output of external component is fused with other components
    to generate the final result.
  prefs: []
  type: TYPE_NORMAL
- en: VII Public Datasets and Open Source Codes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE IV: Some open traffic datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Links | References |'
  prefs: []
  type: TYPE_TB
- en: '| NYC taxi | https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page
    | [[99](#bib.bib99)], [[116](#bib.bib116)],[[91](#bib.bib91)],[[103](#bib.bib103)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| NYC bike | https://www.citibikenyc.com/system-data | [[116](#bib.bib116)],
    [[48](#bib.bib48)], [[76](#bib.bib76)], [[113](#bib.bib113)] |'
  prefs: []
  type: TYPE_TB
- en: '| San Francisco taxi | https://crawdad.org/ crawdad/epfl/mobility/20090224/
    | [[91](#bib.bib91)] |'
  prefs: []
  type: TYPE_TB
- en: '| Chicago bike | https://www.divvybikes.com/system-data | [[48](#bib.bib48)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| BikeDC (Bike Washington) | https://www.capitalbikeshare.com/system-data |
    [[116](#bib.bib116)] |'
  prefs: []
  type: TYPE_TB
- en: '| California -PEMS | http://pems.dot.ca.gov/ | [[92](#bib.bib92)],[[70](#bib.bib70)],[[69](#bib.bib69)],[[99](#bib.bib99)],[[112](#bib.bib112)],[[71](#bib.bib71)],[[98](#bib.bib98)],[[102](#bib.bib102)],[[106](#bib.bib106)],[[66](#bib.bib66)],[[96](#bib.bib96)]
    |'
  prefs: []
  type: TYPE_TB
- en: VII-A Public Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We summarize some public datasets (as shown in Table [IV](#S7.T4 "TABLE IV
    ‣ VII Public Datasets and Open Source Codes ‣ How to Build a Graph-Based Deep
    Learning Architecture in Traffic Domain: A Survey")) in the literatures we survey
    to help successors participate in this domain and produce more valuable works.'
  prefs: []
  type: TYPE_NORMAL
- en: VII-B Open Source Codes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Open-source implementations are helpful for researchers to compare their approaches.
    We provide the hyperlinks of public source codes of the literatures reviewed in
    this paper (as shown in Table [V](#S7.T5 "TABLE V ‣ VII-B Open Source Codes ‣
    VII Public Datasets and Open Source Codes ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey")) to facilitate the baseline experiments
    in traffic domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Some open source codes'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Model | Year | Framework | Github |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[108](#bib.bib108)] | DCRNN | 2018 | Tensorflow | https://github.com/liyaguang/DCRNN
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[97](#bib.bib97)] | GCNN | 2018 | Keras | https://github.com/RingBDStack/GCNN-In-Traffic
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[93](#bib.bib93)] | T-GCN | 2019 | Tensorflow | https://github.com/lehaifeng/T-GCN
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[98](#bib.bib98)] | GMAN | 2019 | Tensorflow | https://github.com/zhengchuanpan/GMAN
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[102](#bib.bib102)] | Graph-WaveNet | 2019 | Torch | https://github.com/nnzhan/Graph-WaveNet
    |'
  prefs: []
  type: TYPE_TB
- en: VIII Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have investigated the latest advances in graph-based traffic literatures
    and made a summary of these literatures in Table [II](#S5.T2 "TABLE II ‣ V Deep
    Learning Techniques Perspective ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey"). Further, we suggest some directions for researchers
    to explore, which can be divided into three categories, i.e. application related,
    technique related, external factor related directions.'
  prefs: []
  type: TYPE_NORMAL
- en: VIII-1 Application Related Directions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Table [II](#S5.T2 "TABLE II ‣ V Deep Learning Techniques Perspective
    ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey"),
    there are many works utilizing graph-based deep learning architectures to tackle
    traffic state prediction and traffic demand prediction, which have achieved state-of-the-art
    performance. However, there are only a handful of works analyzing traffic data
    in a graph perspective in other research directions, such as vehicle behavior
    classification [[65](#bib.bib65)], optimal dynamic electronic toll collection
    (DETC) scheme [[57](#bib.bib57)], path availability [[66](#bib.bib66)], traffic
    signal control [[67](#bib.bib67)]. When it comes to traffic incident detection,
    vehicle detection, origin-destination travel demand prediction and transfer learning
    from City to City, works adopting graph-based deep learning techniques are rare
    up to now. Therefore, the upcoming participators can explore these directions
    in a graph perspective and learn the successful experiences from existing works.'
  prefs: []
  type: TYPE_NORMAL
- en: VIII-2 Technique Related Directions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On one hand, most existing works have employed spectral graph convolution network
    (SGCN) and diffusion graph convolution network (DGCN), two popular kinds of GNNs,
    to analyze traffic tasks. There are only a handful of works utilizing Graph attention
    networks (GATs) in traffic domain [[122](#bib.bib122)], [[98](#bib.bib98)], [[104](#bib.bib104)],
    [[107](#bib.bib107)],[[119](#bib.bib119)]. Other kinds of GNNs, such as graph
    auto-encoders (GAEs) [[156](#bib.bib156)],[[157](#bib.bib157)], recurrent graph
    neural networks (RecGNNs) [[158](#bib.bib158)] have achieved state-of-the-art
    performance in other domains, but they are seldom explored in traffic domain up
    to now. Therefore, it is worth to extend these branches of GNNs to traffic domain.
    On the other hand, recent works have combined GNNs with other deep learning techniques
    such as RNNs, TCN, Seq2Seq, GAN to solve the challenges in traffic tasks. However,
    few traffic works consider transfer learning, continue learning and reinforcement
    learning together with GNNs, which might be a promising direction for researchers.
    In addition, most of the graph-based traffic works are regression tasks, while
    classification tasks are few [[66](#bib.bib66)],[[65](#bib.bib65)]. Researchers
    can explore the classification traffic tasks in a graph perspective.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-3 External Factors Related Directions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally, many existing traffic models do not take external factors into consideration,
    for that external factors are hard to collect and have various formats. The data
    sparsity of external factors is still a challenge confronted by the research community.
    In addition, the techniques to process external factors are rather naive, e.g.
    a simple fully connected layer. There should be more approaches to process external
    factors.
  prefs: []
  type: TYPE_NORMAL
- en: IX Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we conduct a comprehensive review of various graph-based deep
    learning architectures in recent traffic works. More specifically, we summarize
    a general graph-based formulation of traffic problem and graph construction from
    various traffic datasets. Further, we decompose all the investigated architectures
    and analyze the common modules they share, including graph neural networks (GNNs),
    recurrent neural networks (RNNs), temporal convolution network (TCN), Sequence
    to Sequence (Seq2Seq) model, generative adversarial network (GAN). We provide
    a thorough description of their variants in traffic tasks, hoping to provide upcoming
    researchers insights into how to design novel techniques for their own traffic
    tasks. We also summarize the common challenges in many traffic scenarios, such
    as spatial dependency, temporal dependency, external factors. More than that,
    we present multiple deep learning based solutions for each challenge. In addition,
    we provide some hyperlinks of public datasets and codes in related works to facilitate
    the upcoming researches. Finally, we suggest some future directions for participators
    interested in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors would like to thank anonymous reviewers for their valuable comments.
  prefs: []
  type: TYPE_NORMAL
- en: This work is supported by the National Key R&D Program of China (No.2019YFB2102100),
    National Natural Science Foundation of China (No.61802387), China’s Post-doctoral
    Science Fund (No.2019M663183), National Natural Science Foundation of Shenzhen
    (No.JCYJ20190812153212464), Shenzhen Engineering Research Center for Beidou Positioning
    Service Improvement Technology (No.XMHT20190101035), Science and Technology Development
    Fund of Macao S.A.R (FDCT) under number 0015/2019/AKP, Shenzhen Discipline Construction
    Project for Urban Computing and Data Intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] G. Yu and C. Zhang, “Switching ARIMA model based forecasting for traffic
    flow,” in *ICASSP*, 2004, pp. 429–432.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] B. M. Williams and L. A. Hoel, “Modeling and forecasting vehicular traffic
    flow as a seasonal arima process: Theoretical basis and empirical results,” *Journal
    of Transportation Engineering*, vol. 129, no. 6, pp. 664–672, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] S. R. Chandra and H. Al-Deek, “Predictions of freeway traffic speeds and
    volumes using vector autoregressive models,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 13, no. 2, pp. 53–72, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Y. Xie, Y. Zhang, and Z. Ye, “Short-term traffic volume forecasting using
    kalman filter with discrete wavelet decomposition,” *Computer-Aided Civil and
    Infrastructure Engineering*, vol. 22, no. 5, pp. 326–334, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] H. Fu, H. Ma, Y. Liu, and D. Lu, “A vehicle classification system based
    on hierarchical multi-svms in crowded traffic scenes,” *Neurocomputing*, vol.
    211, pp. 182–190, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. May, D. Hecker, C. Körner, S. Scheider, and D. Schulz, “A vector-geometry
    based spatial knn-algorithm for traffic frequency predictions,” in *ICDM Workshops*,
    2008, pp. 442–447.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] J. Liu, T. Li, P. Xie, S. Du, F. Teng, and X. Yang, “Urban big data fusion
    based on deep learning: An overview,” *Information Fusion*, vol. 53, pp. 123–133,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Z. Lv, J. Xu, K. Zheng, H. Yin, P. Zhao, and X. Zhou, “LC-RNN: A deep learning
    model for traffic speed prediction,” in *IJCAI*, 2018, pp. 3470–3476.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] X. Ma, Z. Dai, Z. He, J. Ma, Y. Wang, and Y. Wang, “Learning traffic as
    images: a deep convolutional neural network for large-scale transportation network
    speed prediction,” *Sensors*, vol. 17, no. 4, p. 818, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] L. Yan, H. Shen, J. Zhao, C. Xu, F. Luo, and C. Qiu, “Catcharger: Deploying
    wireless charging lanes in a metropolitan road network through categorization
    and clustering of vehicle traffic,” in *INFOCOM*, 2017, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Y. Sun, X. Yu, R. Bie, and H. Song, “Discovering time-dependent shortest
    path on traffic graph for drivers towards green driving,” *Journal of Network
    and Computer Applications*, vol. 83, pp. 204–212, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] H. Sun, J. Wu, D. Ma, and J. Long, “Spatial distribution complexities
    of traffic congestion and bottlenecks in different network topologies,” *Applied
    Mathematical Modelling*, vol. 38, no. 2, pp. 496–505, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. Gori, G. Monfardini, and F. Scarselli, “A new model for learning in
    graph domains,” in *IJCNN*, vol. 2, 2005, pp. 729–734.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE Transactions on Neural Networks*, vol. 20,
    no. 1, pp. 61–80, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] M. Henaff, J. Bruna, and Y. LeCun, “Deep convolutional networks on graph-structured
    data,” *arXiv:1506.05163*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, “Learning deep
    generative models of graphs,” *arXiv:1803.03324*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Z.-M. Chen, X.-S. Wei, P. Wang, and Y. Guo, “Multi-label image recognition
    with graph convolutional networks,” in *CVPR*, 2019, pp. 5177–5186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Z. Guo, Y. Zhang, and W. Lu, “Attention guided graph convolutional networks
    for relation extraction,” in *ACL*, 2019, pp. 241–251.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, and e. Bombarell, “Convolutional
    networks on graphs for learning molecular fingerprints,” in *NIPS*, 2015, pp.
    2224–2232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec,
    “Graph convolutional neural networks for web-scale recommender systems,” in *KDD*,
    2018, pp. 974–983.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] M. G. Karlaftis and E. I. Vlahogianni, “Statistical methods versus neural
    networks in transportation research: Differences, similarities and some insights,”
    *Transportation Research Part C: Emerging Technologies*, vol. 19, no. 3, pp. 387–399,
    2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] E. I. Vlahogianni, M. G. Karlaftis, and J. C. Golias, “Short-term traffic
    forecasting: Where we are and where we’re going,” *Transportation Research Part
    C: Emerging Technologies*, vol. 43, pp. 3–19, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] P. Xie, T. Li, J. Liu, S. Du, X. Yang, and J. Zhang, “Urban flow prediction
    from spatiotemporal data using machine learning: A survey,” *Information Fusion*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] H. Nguyen, L.-M. Kieu, T. Wen, and C. Cai, “Deep learning methods in transportation
    domain: a review,” *IET Intelligent Transport Systems*, vol. 12, no. 9, pp. 998–1004,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Wang, D. Zhang, Y. Liu, B. Dai, and L. H. Lee, “Enhancing transportation
    systems via deep learning: A survey,” *Transportation Research Part C: Emerging
    Technologies*, vol. 99, pp. 144–163, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] M. Veres and M. Moussa, “Deep learning for intelligent transportation
    systems: A survey of emerging trends,” *IEEE Transactions on Intelligent Transportation
    Systems*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Q. Chen, W. Wang, F. Wu, S. De, and e. Wang, “A survey on an emerging
    area: Deep learning for smart city data,” *IEEE Transactions on Emerging Topics
    in Computational Intelligence*, vol. 3, no. 5, pp. 392–410, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] S. Wang, J. Cao, and P. S. Yu, “Deep learning for spatio-temporal data
    mining: A survey,” *arXiv:1906.04928*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, “Geometric
    deep learning: going beyond euclidean data,” *IEEE Signal Processing Magazine*,
    vol. 34, no. 4, pp. 18–42, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Zhou, G. Cui, Z. Zhang, and e. Yang, “Graph neural networks: A review
    of methods and applications,” *arXiv:1812.08434*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Zhang, “Graph neural networks for small graph and giant network representation
    learning: An overview,” *arXiv:1908.00187*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] P. Quan, Y. Shi, M. Lei, J. Leng, T. Zhang, and L. Niu, “A brief review
    of receptive fields in graph convolutional networks,” in *IEEE/WIC/ACM International
    Conference on Web Intelligence-Companion Volume*, 2019, pp. 106–110.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] S. Zhang, H. Tong, J. Xu, and R. Maciejewski, “Graph convolutional networks:
    a comprehensive review,” *Computational Social Networks*, vol. 6, no. 1, p. 11,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Z. Wu, S. Pan, F. Chen, G. Long, and e. Zhang, “A comprehensive survey
    on graph neural networks,” *IEEE Transactions on Neural Networks and Learning
    Systems*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Chen, Y. Lv, Z. Li, and F. Wang, “Long short-term memory model for
    traffic congestion prediction with online open data,” in *ITSC*, 2016, pp. 132–137.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] L. Yan and H. Shen, “TOP: optimizing vehicle driving speed with vehicle
    trajectories for travel time minimization and road congestion avoidance,” *ACM
    Trans. Cyber Phys. Syst.*, vol. 4, no. 2, pp. 17:1–17:25, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] L. Yan, H. Shen, and K. Chen, “Mobit: Distributed and congestion-resilient
    trajectory-based routing for vehicular delay tolerant networks,” *IEEE/ACM Trans.
    Netw.*, vol. 26, no. 3, pp. 1078–1091, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] X. Ma, H. Yu, Y. Wang, and Y. Wang, “Large-scale transportation network
    congestion evolution prediction using deep learning theory,” *PloS one*, vol. 10,
    no. 3, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] F. Sun, A. Dubey, and J. White, “Dxnat—deep neural networks for explaining
    non-recurring traffic congestion,” in *Big Data*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] L. Yan, H. Shen, and K. Chen, “Mobit: A distributed and congestion-resilient
    trajectory based routing algorithm for vehicular delay tolerant networks,” in
    *Proceedings of the Second International Conference on Internet-of-Things Design
    and Implementation, IoTDI 2017, Pittsburgh, PA, USA, April 18-21, 2017*, 2017,
    pp. 209–214.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] L. Wei, Z. Yu, Z. Jin, L. Xie, J. Huang, D. Cai, X. He, and X.-S. Hua,
    “Dual graph for traffic forecasting,” *IEEE Access*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] W. Chen, L. Chen, Y. Xie, W. Cao, Y. Gao, and X. Feng, “Multi-range attentive
    bicomponent graph convolutional network for traffic forecasting,” *AAAI*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Z. Cao, S. Jiang, J. Zhang, and H. Guo, “A unified framework for vehicle
    rerouting and traffic light control to reduce traffic congestion,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 18, no. 7, pp. 1958–1973, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] L. Qi, M. Zhou, and W. Luan, “A two-level traffic light control strategy
    for preventing incident-based urban traffic congestion,” *IEEE Transactions on
    Intelligent Transportation Systems*, vol. 19, no. 1, pp. 13–24, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J. Ye, J. Zhao, K. Ye, and C. Xu, “Multi-stgcnet: A graph convolution
    based spatial-temporal framework for subway passenger flow forecasting,” in *2020
    International Joint Conference on Neural Networks (IJCNN)*, 2020, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] F. Rodrigues, I. Markou, and F. C. Pereira, “Combining time-series and
    textual data for taxi demand prediction in event areas: A deep learning approach,”
    *Information Fusion*, vol. 49, pp. 120–129, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] D. Wang, W. Cao, J. Li, and J. Ye, “Deepsd: Supply-demand prediction for
    online car-hailing services using deep neural networks,” in *ICDE*, 2017, pp.
    243–254.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] D. Chai, L. Wang, and Q. Yang, “Bike flow prediction with multi-graph
    convolutional networks,” in *SIGSPATIAL*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] L. Lin, Z. He, and S. Peeta, “Predicting station-level hourly demand in
    a large-scale bike-sharing network: A graph convolutional neural network approach,”
    *Transportation Research Part C: Emerging Technologies*, vol. 97, pp. 258–276,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] X. Han, T. Grubenmann, R. Cheng, S. C. Wong, X. Li, and W. Sun, “Traffic
    incident detection: A trajectory-based approach,” in *ICDE*, 2020, pp. 1866–1869.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Z. Zhang, Q. He, J. Gao, and M. Ni, “A deep learning approach for detecting
    traffic accidents from social media data,” *Transportation Research Part C: Emerging
    Technologies*, vol. 86, pp. 580–596, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] M. I. Sameen and B. Pradhan, “Severity prediction of traffic accidents
    with recurrent neural networks,” *Applied Sciences*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] S. Alkheder, M. Taamneh, and S. Taamneh, “Severity prediction of traffic
    accident using an artificial neural network,” *Journal of Forecasting*, vol. 36,
    no. 1, pp. 100–108, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A. M. Kashevnik, I. Lashkov, and A. V. Gurtov, “Methodology and mobile
    application for driver behavior analysis and accident prevention,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 21, no. 6, pp. 2427–2436, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] M. Hanninen, “Bayesian networks for maritime traffic accident prevention:
    benefits and challenges,” *Accident Analysis & Prevention*, vol. 73, pp. 305–312,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] B. Jo, Y. Lee, R. M. A. Khan, J. Kim, and D. Kim, “Robust construction
    safety system (RCSS) for collision accidents prevention on construction sites,”
    *Sensors*, vol. 19, no. 4, p. 932, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] W. Qiu, H. Chen, and B. An, “Dynamic electronic toll collection via multi-agent
    deep reinforcement learning with edge-based graph convolutional networks,” in
    *IJCAI*, 2019, pp. 4568–4574.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] H. Li, P. Wang, and C. Shen, “Toward end-to-end car license plate detection
    and recognition with deep neural networks,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 20, no. 3, pp. 1126–1136, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] X. Chen, S. Xiang, C.-L. Liu, and C.-H. Pan, “Vehicle detection in satellite
    images by hybrid deep convolutional neural networks,” *IEEE Geoscience and remote
    sensing letters*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] S. Zhang, J. Yang, and B. Schiele, “Occluded pedestrian detection through
    guided attention in cnns,” in *CVPR*, 2018, pp. 6995–7003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] H. Tayara, K. G. Soo, and K. T. Chong, “Vehicle detection and counting
    in high-resolution aerial images using convolutional regression neural network,”
    *IEEE Access*, vol. 6, pp. 2220–2230, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, “Scale-aware fast
    r-cnn for pedestrian detection,” *IEEE transactions on Multimedia*, vol. 20, no. 4,
    pp. 985–996, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] M. A. S. Kamal, T. Hayakawa, and J. Imura, “Development and evaluation
    of an adaptive traffic signal control scheme under a mixed-automated traffic scenario,”
    *IEEE Transactions on Intelligent Transportation Systems*, vol. 21, no. 2, pp.
    590–602, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. Li, H. Ma, Z. Zhang, and M. Tomizuka, “Social-wagdat: Interaction-aware
    trajectory prediction via wasserstein graph double-attention network,” *arxiv:2002.06241*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. Mylavarapu, M. Sandhu, P. Vijayan, K. M. Krishna, B. Ravindran, and
    A. Namboodiri, “Towards accurate vehicle behaviour classification with multi-relational
    graph convolutional networks,” *arXiv:2002.00786*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. Li, Z. Han, H. Cheng, J. Su, P. Wang, J. Zhang, and L. Pan, “Predicting
    path failure in time-evolving graphs,” in *KDD*, 2019, pp. 1279–1289.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] T. Nishi, K. Otaki, K. Hayakawa, and T. Yoshimura, “Traffic signal control
    based on reinforcement learning with graph convolutional neural nets,” in *ITSC*,
    2018, pp. 877–883.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Q. Zhang, Q. Jin, J. Chang, S. Xiang, and C. Pan, “Kernel-weighted graph
    convolutional network: A deep learning approach for traffic forecasting,” in *ICPR*,
    2018, pp. 1018–1023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] S. Guo, Y. Lin, N. Feng, C. Song, and H. Wan, “Attention based spatial-temporal
    graph convolutional networks for traffic flow forecasting,” in *AAAI*, 2019, pp.
    922–929.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] L. Ge, H. Li, J. Liu, and A. Zhou, “Temporal graph convolutional networks
    for traffic speed prediction considering external factors,” in *MDM*, 2019, pp.
    234–242.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] B. Yu, M. Li, J. Zhang, and Z. Zhu, “3d graph convolutional networks with
    temporal graphs: A spatial information free framework for traffic forecasting,”
    *arXiv:1903.00919*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] J. Hu, C. Guo, B. Yang, and C. S. Jensen, “Stochastic weight completion
    for road networks using graph convolutional networks,” in *ICDE*, 2019, pp. 1274–1285.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] D. Wang, J. Zhang, W. Cao, J. Li, and Y. Zheng, “When will you arrive?
    estimating travel time based on deep neural networks,” in *AAAI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] L. Yan, H. Shen, Z. Li, A. Sarker, J. A. Stankovic, C. Qiu, J. Zhao, and
    C. Xu, “Employing opportunistic charging for electric taxicabs to reduce idle
    time,” *Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.*, vol. 2, no. 1,
    pp. 47:1–47:25, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] X. Geng, X. Wu, L. Zhang, Q. Yang, Y. Liu, and J. Ye, “Multi-modal graph
    interaction for multi-graph convolution network in urban spatiotemporal forecasting,”
    *arXiv:1905.11395*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] L. Bai, L. Yao, S. S. Kanhere, X. Wang, and Q. Z. Sheng, “Stg2seq: Spatial-temporal
    graph to sequence model for multi-step passenger demand forecasting,” in *IJCAI*,
    2019, pp. 1981–1987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] J. Chen, L. Liu, H. Wu, J. Zhen, G. Li, and L. Lin, “Physical-virtual
    collaboration graph network for station-level metro ridership prediction,” *arXiv:2001.04889*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Z. Li, N. D. Sergin, H. Yan, C. Zhang, and F. Tsung, “Tensor completion
    for weakly-dependent data on graph for metro passenger flow prediction,” *AAAI*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] J. Ye, J. Zhao, L. Zhang, C. Xu, J. Zhang, and K. Ye, “A data-driven method
    for dynamic OD passenger flow matrix estimation in urban metro systems,” in *BigData
    2020*, vol. 12402.   Springer, 2020, pp. 116–126.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] H. Shi, Q. Yao, Q. Guo, Y. Li, L. Zhang, J. Ye, Y. Li, and Y. Liu, “Predicting
    origin-destination flow via multi-perspective graph convolutional network,” in
    *ICDE*, 2020, pp. 1818–1821.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] J. Hu, B. Yang, C. Guo, C. S. Jensen, and H. Xiong, “Stochastic origin-destination
    matrix forecasting using dual-stage graph convolutional, recurrent neural networks,”
    in *ICDE*, 2020, pp. 1417–1428.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] K. F. Chu, A. Y. S. Lam, and V. O. K. Li, “Deep multi-scale convolutional
    LSTM network for travel demand and origin-destination predictions,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 21, no. 8, pp. 3219–3232, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Y. Sun, T. He, J. Hu, H. Huang, and B. Chen, “Socially-aware graph convolutional
    network for human trajectory prediction,” in *ITNEC*, 2019, pp. 325–333.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] A. Monti, A. Bertugli, S. Calderara, and R. Cucchiara, “Dag-net: Double
    attentive graph neural network for trajectory forecasting,” *Carxiv:2005.12661*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] A. Mohamed, K. Qian, M. Elhoseiny, and C. Claudel, “Social-stgcnn: A social
    spatio-temporal graph convolutional neural network for human trajectory prediction,”
    in *CVPR*, 2020, pp. 14 412–14 420.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. Li, F. Yang, M. Tomizuka, and C. Choi, “Evolvegraph: Heterogeneous
    multi-agent multi-modal trajectory prediction with evolving interaction graphs,”
    *arxiv:2003.13924*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] V. Kosaraju, A. Sadeghian, R. Martín-Martín, I. D. Reid, H. Rezatofighi,
    and S. Savarese, “Social-bigat: Multimodal trajectory forecasting using bicycle-gan
    and graph attention networks,” in *NIPS*, 2019, pp. 137–146.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Z. Zhao, H. Fang, Z. Jin, and Q. Qiu, “Gisnet: Graph-based information
    sharing network for vehicle trajectory prediction,” *arXiv:2003.11973*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] X. Geng, Y. Li, L. Wang, L. Zhang, Q. Yang, J. Ye, and Y. Liu, “Spatiotemporal
    multi-graph convolution network for ride-hailing demand forecasting,” in *AAAI*,
    vol. 33, 2019, pp. 3656–3663.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] N. Laptev, J. Yosinski, L. E. Li, and S. Smyl, “Time-series extreme event
    forecasting with neural networks at uber,” in *ICML*, vol. 34, 2017, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Q. Xie, T. Guo, Y. Chen, Y. Xiao, X. Wang, and B. Y. Zhao, “How do urban
    incidents affect traffic speed? A deep graph convolutional network for incident-driven
    traffic speed prediction,” *arXiv:1912.01242*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] B. Yu, H. Yin, and Z. Zhu, “Spatio-temporal graph convolutional networks:
    A deep learning framework for traffic forecasting,” in *IJCAI*, 2018, pp. 3634–3640.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] L. Zhao, Y. Song, C. Zhang, Y. Liu, P. Wang, T. Lin, M. Deng, and H. Li,
    “T-gcn: A temporal graph convolutional network for traffic prediction,” *IEEE
    Transactions on Intelligent Transportation Systems*, pp. 1–11, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Z. Cui, K. Henrickson, R. Ke, and Y. Wang, “Traffic graph convolutional
    recurrent neural network: A deep learning framework for network-scale traffic
    learning and forecasting,” *IEEE Transactions on Intelligent Transportation Systems*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] J. J. Q. Yu and J. Gu, “Real-time traffic speed estimation with graph
    convolutional generative autoencoder,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 20, no. 10, pp. 3940–3951, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Y. Huang, Y. Weng, S. Yu, and X. Chen, “Diffusion convolutional recurrent
    neural network with rank influence learning for traffic forecasting,” in *TrustCom*,
    2019, pp. 678–685.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. Li, H. Peng, L. Liu, G. Xiong, B. Du, H. Ma, L. Wang, and M. Z. A.
    Bhuiyan, “Graph cnns for urban traffic passenger flows prediction,” in *SmartWorld*,
    2018, pp. 29–36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] C. Zheng, X. Fan, C. Wang, and J. Qi, “Gman: A graph multi-attention network
    for traffic prediction,” 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Z. Diao, X. Wang, D. Zhang, Y. Liu, K. Xie, and S. He, “Dynamic spatial-temporal
    graph convolutional neural networks for traffic forecasting,” in *AAAI*, 2019,
    pp. 890–897.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Y. Zhang, S. Wang, B. Chen, and J. Cao, “GCGAN: generative adversarial
    nets with graph CNN for network-scale traffic prediction,” in *IJCNN*, 2019, pp.
    1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Y. Wang, H. Yin, H. Chen, T. Wo, J. Xu, and K. Zheng, “Origin-destination
    matrix prediction via graph convolution: a new perspective of passenger demand
    modeling,” in *KDD*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang, “Graph wavenet for deep
    spatial-temporal graph modeling,” in *IJCAI*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] J. Ke, X. Qin, H. Yang, Z. Zheng, Z. Zhu, and J. Ye, “Predicting origin-destination
    ride-sourcing demand with a spatio-temporal encoder-decoder residual multi-graph
    convolutional network,” *arXiv:1910.09103*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Z. Kang, H. Xu, J. Hu, and X. Pei, “Learning dynamic graph embedding
    for traffic flow forecasting: A graph self-attentive method,” 2019, pp. 2570–2576.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] B. Yu, H. Yin, and Z. Zhu, “St-unet: A spatio-temporal u-network for
    graph-structured time series modeling,” *arXiv:1903.05631*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] K. Guo, Y. Hu, Z. Qian, H. Liu, and e. Zhang, “Optimized graph convolution
    recurrent neural network for traffic prediction,” *IEEE Transactions on Intelligent
    Transportation Systems*, pp. 1–12, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] C. Zhang, J. J. Q. Yu, and Y. Liu, “Spatial-temporal graph attention
    networks: A deep learning approach for traffic forecasting,” *IEEE Access*, vol. 7,
    pp. 166 246–166 256, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional recurrent
    neural network: Data-driven traffic forecasting,” in *ICLR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] M. Lu, K. Zhang, H. Liu, and N. Xiong, “Graph hierarchical convolutional
    recurrent neural network (GHCRNN) for vehicle condition prediction,” *arXiv:1903.06261*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Z. Zhang, M. Li, X. Lin, Y. Wang, and F. He, “Multistep speed prediction
    on traffic networks: A deep learning approach considering spatio-temporal dependencies,”
    *Transportation Research Part C: Emerging Technologies*, vol. 105, pp. 297–322,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] S. Fang, Q. Zhang, G. Meng, S. Xiang, and C. Pan, “Gstnet: Global spatial-temporal
    network for traffic flow prediction,” in *IJCAI*, 2019, pp. 2286–2293.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] C. Chen, K. Li, S. G. Teo, X. Zou, K. Wang, J. Wang, and Z. Zeng, “Gated
    residual recurrent graph neural networks for traffic prediction,” in *AAAI*, 2019,
    pp. 485–492.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] L. Bai, L. Yao, S. S. Kanhere, X. Wang, W. Liu, and Z. Yang, “Spatio-temporal
    graph convolutional and recurrent networks for citywide passenger demand prediction,”
    in *CIKM*, 2019, pp. 2293–2296.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] S. Yan, Y. Xiong, and D. Lin, “Spatial temporal graph convolutional networks
    for skeleton-based action recognition,” in *AAAI*, 2018, pp. 7444–7452.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] M. Wang, B. Lai, Z. Jin, Y. Lin, X. Gong, J. Huang, and X. Hua, “Dynamic
    spatio-temporal graph-based cnns for traffic prediction,” *arXiv:1812.02019*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] J. Sun, J. Zhang, Q. Li, X. Yi, and Y. Zheng, “Predicting citywide crowd
    flows in irregular regions using multi-view graph convolutional networks,” *arXiv:1903.07789*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Y. Zhang, T. Cheng, and Y. Ren, “A graph deep learning method for short-term
    traffic forecasting on large road networks,” *Computer-Aided Civil and Infrastructure
    Engineering*, vol. 34, no. 10, pp. 877–896, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] X. Zhou, Y. Shen, and L. Huang, “Revisiting flow information for traffic
    prediction,” *arXiv:1906.00560*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, and D. Yeung, “Gaan: Gated
    attention networks for learning on large and spatiotemporal graphs,” in *UAI*,
    2018, pp. 339–349.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Y. Y. Shin and Y. Yoon, “Incorporating dynamicity of transportation network
    with multi-weight traffic graph convolution for traffic forecasting,” *arXiv:1909.07105*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] A. Majumdar, “Graph structured autoencoder,” *Neural Networks*, vol.
    106, pp. 271–280, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio,
    “Graph attention networks,” in *ICLR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning
    on large graphs,” in *NIPS*, 2017, pp. 1024–1034.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst,
    “The emerging field of signal processing on graphs: Extending high-dimensional
    data analysis to networks and other irregular domains,” *IEEE Signal Processing
    Magazine*, vol. 30, no. 3, pp. 83–98, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and
    locally connected networks on graphs,” in *ICLR*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural
    networks on graphs with fast localized spectral filtering,” in *NIPS*, 2016, pp.
    3837–3845.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] D. K. Hammond, P. Vandergheynst, and R. Gribonval, “Wavelets on graphs
    via spectral graph theory,” *Applied and Computational Harmonic Analysis*, vol. 30,
    no. 2, pp. 129–150, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *ICLR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] S. Teng, “Scalable algorithms for data and network analysis,” *Foundations
    and Trends in Theoretical Computer Science*, vol. 12, no. 1-2, pp. 1–274, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] S.-H. Teng, “Scalable algorithms for data and network analysis,” *Foundations
    and Trends® in Theoretical Computer Science*, vol. 12, no. 1-2, pp. 1–274, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] J. Zhang, Y. Zheng, and D. Qi, “Deep spatio-temporal residual networks
    for citywide crowd flows prediction,” in *AAAI*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] S. Du, T. Li, X. Gong, and S. Horng, “A hybrid method for traffic flow
    forecasting using multimodal deep learning,” *International Journal of Computational
    Intelligence Systems*, vol. 13, no. 1, pp. 85–97, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] R. M. Schmidt, “Recurrent neural networks (rnns): A gentle introduction
    and overview,” *arXiv:1912.05911*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Y. Bengio, P. Y. Simard, and P. Frasconi, “Learning long-term dependencies
    with gradient descent is difficult,” *IEEE Trans. Neural Networks*, vol. 5, no. 2,
    pp. 157–166, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] H. Salehinejad, S. Sankar, J. Barfett, E. Colak, and S. Valaee, “Recent
    advances in recurrent neural networks,” *arXiv:1801.01078*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton, “On the importance
    of initialization and momentum in deep learning,” in *ICML*, vol. 28, 2013, pp.
    1139–1147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] G. Chen, “A gentle tutorial of recurrent neural network with error backpropagation,”
    *arXiv:1610.02583*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
    gated recurrent neural networks on sequence modeling,” in *NIPS Workshop*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] N. Kalchbrenner, L. Espeholt, K. Simonyan, A. v. d. Oord, A. Graves,
    and K. Kavukcuoglu, “Neural machine translation in linear time,” *arXiv:1610.10099*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language modeling with
    gated convolutional networks,” in *ICML*, vol. 70, 2017, pp. 933–941.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves,
    N. Kalchbrenner, A. W. Senior, and K. Kavukcuoglu, “Wavenet: A generative model
    for raw audio,” in *ISCA Workshop*, 2016, pp. 124–125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] F. Yu and V. Koltun, “Multi-scale context aggregation by dilated convolutions,”
    in *ICLR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] S. Bai, J. Z. Kolter, and V. Koltun, “An empirical evaluation of generic
    convolutional and recurrent networks for sequence modeling,” *arXiv:1803.01271*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in *NIPS*, 2014, pp. 3104–3112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” in *ICLR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based
    neural machine translation,” in *EMNLP*, 2015, pp. 1412–1421.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, “Scheduled sampling
    for sequence prediction with recurrent neural networks,” in *NIPS*, 2015, pp.
    1171–1179.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] I. Goodfellow, J. Pouget-Abadie, M. Mirza, and e. Xu, “Generative adversarial
    nets,” in *NIPS*, 2014, pp. 2672–2680.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
    “Gans trained by a two time-scale update rule converge to a local nash equilibrium,”
    in *NIPS*, 2017, pp. 6626–6637.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta, and
    A. A. Bharath, “Generative adversarial networks: An overview,” *IEEE Signal Process.
    Mag.*, vol. 35, no. 1, pp. 53–65, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] K. Wang, C. Gou, Y. Duan, Y. Lin, X. Zheng, and F. Wang, “Generative
    adversarial networks: introduction and outlook,” *IEEE CAA J. Autom. Sinica*,
    vol. 4, no. 4, pp. 588–598, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Y. Lin, X. Dai, L. Li, and F. Wang, “Pattern sensitive prediction of
    traffic flow based on generative adversarial framework,” *IEEE Transactions on
    Intelligent Transportation Systems*, vol. 20, no. 6, pp. 2395–2400, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Y. Liang, Z. Cui, Y. Tian, H. Chen, and Y. Wang, “A deep generative adversarial
    architecture for network-wide spatial-temporal traffic-state estimation,” *Transportation
    Research Record*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] H. Yao, F. Wu, J. Ke, X. Tang, Y. Jia, S. Lu, P. Gong, J. Ye, and Z. Li,
    “Deep multi-view spatial-temporal network for taxi demand prediction,” in *AAAI*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] A. Hasanzadeh, E. Hajiramezanali, K. R. Narayanan, N. Duffield, M. Zhou,
    and X. Qian, “Semi-implicit graph variational auto-encoders,” in *NIPS*, 2019,
    pp. 10 711–10 722.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] M. Simonovsky and N. Komodakis, “Graphvae: Towards generation of small
    graphs using variational autoencoders,” in *International Conference on Artificial
    Neural Networks*, 2018, pp. 412–422.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] H. Dai, Z. Kozareva, B. Dai, A. Smola, and L. Song, “Learning steady-states
    of iterative algorithms over graphs,” in *ICML*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/1d7fb309c9e595c71b931c456a475c51.png) | Jiexia
    Ye received the Bachelor’s degree in Economics from Sun Yat-sen University in
    2012\. She is currently working toward M.S. degree in Shenzhen Institutes of Advanced
    Technology, Chinese Academy of Sciences. Her research interests include graph
    neural networks / graph embedding in traffic and finance domain. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/1e81511f7c9e9ea7831b5064d31aa045.png) | Juanjuan
    Zhao received her Ph.D degree from Shenzhen College of Advanced Technology, University
    of Chinese Academy of Sciences in 2017, and received the M.S. degree from the
    Department of Computer Science, Wuhan University of Technology in 2009\. She is
    an Assistant Professor at Shenzhen Institutes of Advanced Technology, Chinese
    Academy of Sciences. Her research topics include data-driven urban systems, mobile
    data collection, cross-domain data fusion, heterogeneous model integration. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/4cd2e27d31c079a359306593a72a77c4.png) | Kejiang
    Ye received his BSc and Ph.D degree in Computer Science from Zhejiang University
    in 2008 and 2013 respectively. He was also a joint Ph.D student at The University
    of Sydney from 2012 to 2013\. After graduation, he worked as Post-Doc Researcher
    at Carnegie Mellon University from 2014 to 2015 and Wayne State University from
    2015 to 2016\. He is currently an Associate Professor at Shenzhen Institutes of
    Advanced Technology, Chinese Academy of Science. His research interests include
    cloud computing, big data and network systems. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/42aa77c8c307ff7b9a0d293d5cee2820.png) | Chengzhong
    Xu received his Ph.D degree from the University of Hong Kong, China in 1993\.
    He is the Dean of the Faculty of State Key Lab of IOTSC, Department of Computer
    Science, University of Macau, Macao SAR, China and a Chair Professor of Computer
    Science of UM. He was a Chief Scientist of Shenzhen Institutes of Advanced Technology
    (SIAT) of Chinese Academy of Sciences and the Director of Institute of Advanced
    Computing and Digital Engineering of SIAT. He was also in the faculty of Wayne
    State University, USA for 18 years. Dr. Xu’s research interest is mainly in the
    areas of parallel and distributed systems, cloud and edge computing, and data-driven
    intelligence. He has published over 300 peer-reviewed papers on these topics with
    over 10K citations. Dr. Xu served in the editorial boards of leading journals,
    including IEEE Transactions on Computers, IEEE Transactions on Cloud Computing,
    IEEE Transactions on Parallel and Distributed Systems and Journal of Parallel
    and Distributed Computing. He is the Associate Editor-in-Chief of ZTE Communication.
    He is IEEE Fellow and the Chair of IEEE Technical Committee of Distributed Processing.
    |'
  prefs: []
  type: TYPE_TB
