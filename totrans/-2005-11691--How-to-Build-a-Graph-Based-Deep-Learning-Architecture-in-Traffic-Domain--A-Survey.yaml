- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 20:01:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 20:01:09'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2005.11691] How to Build a Graph-Based Deep Learning Architecture in Traffic
    Domain: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2005.11691] 如何在交通领域构建基于图的深度学习架构: 调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2005.11691](https://ar5iv.labs.arxiv.org/html/2005.11691)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2005.11691](https://ar5iv.labs.arxiv.org/html/2005.11691)
- en: 'How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A
    Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '如何在交通领域构建基于图的深度学习架构: 调查'
- en: 'Jiexia Ye, Juanjuan Zhao*, Kejiang Ye, IEEE Member,  Chengzhong Xu, IEEE Fellow
    *Corresponding author: Juanjuan Zhao Jiexia Ye, Juanjuan Zhao, Kejiang Ye are
    with Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,
    China (E-mail: {jx.ye, jj.zhao, kj.ye}@siat.ac.cn). Chengzhong Xu is with State
    Key Lab of IOTSC, Department of Computer Science, University of Macau, Macau SAR,
    China (E-mail: czxu@um.edu.mo).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jiexia Ye, Juanjuan Zhao*, Kejiang Ye, IEEE 会员, Chengzhong Xu, IEEE 研究员 *通讯作者:
    Juanjuan Zhao Jiexia Ye, Juanjuan Zhao, Kejiang Ye 在中国科学院深圳先进技术研究院工作（电子邮件: {jx.ye,
    jj.zhao, kj.ye}@siat.ac.cn）。Chengzhong Xu 在澳门大学计算机科学系物联网系统国家重点实验室工作（电子邮件: czxu@um.edu.mo）。'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In recent years, various deep learning architectures have been proposed to solve
    complex challenges (e.g. spatial dependency, temporal dependency) in traffic domain,
    which have achieved satisfactory performance. These architectures are composed
    of multiple deep learning techniques in order to tackle various challenges in
    traffic tasks. Traditionally, convolution neural networks (CNNs) are utilized
    to model spatial dependency by decomposing the traffic network as grids. However,
    many traffic networks are graph-structured in nature. In order to utilize such
    spatial information fully, it’s more appropriate to formulate traffic networks
    as graphs mathematically. Recently, various novel deep learning techniques have
    been developed to process graph data, called graph neural networks (GNNs). More
    and more works combine GNNs with other deep learning techniques to construct an
    architecture dealing with various challenges in a complex traffic task, where
    GNNs are responsible for extracting spatial correlations in traffic network. These
    graph-based architectures have achieved state-of-the-art performance. To provide
    a comprehensive and clear picture of such emerging trend, this survey carefully
    examines various graph-based deep learning architectures in many traffic applications.
    We first give guidelines to formulate a traffic problem based on graph and construct
    graphs from various kinds of traffic datasets. Then we decompose these graph-based
    architectures to discuss their shared deep learning techniques, clarifying the
    utilization of each technique in traffic tasks. What’s more, we summarize some
    common traffic challenges and the corresponding graph-based deep learning solutions
    to each challenge. Finally, we provide benchmark datasets, open source codes and
    future research directions in this rapidly growing field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，已经提出了各种深度学习架构以解决交通领域中的复杂挑战（例如空间依赖性、时间依赖性），这些架构已取得令人满意的性能。这些架构由多个深度学习技术组成，以应对交通任务中的各种挑战。传统上，卷积神经网络（CNNs）用于通过将交通网络分解为网格来建模空间依赖性。然而，许多交通网络本质上是图结构的。为了充分利用这些空间信息，更合适的做法是将交通网络在数学上表述为图。最近，已经开发了各种新颖的深度学习技术来处理图数据，这些技术称为图神经网络（GNNs）。越来越多的工作将GNNs与其他深度学习技术结合，以构建一种架构来处理复杂交通任务中的各种挑战，其中GNNs负责提取交通网络中的空间关联。这些基于图的架构已实现了最先进的性能。为了全面清晰地呈现这一新兴趋势，本调查仔细检查了许多交通应用中的各种基于图的深度学习架构。我们首先提供了将交通问题基于图进行表述的指南，并从各种交通数据集中构建图。然后，我们对这些基于图的架构进行分解，讨论它们共享的深度学习技术，澄清每种技术在交通任务中的应用。此外，我们总结了一些常见的交通挑战及相应的基于图的深度学习解决方案。最后，我们提供了基准数据集、开源代码和该快速发展的领域的未来研究方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '索引词:'
- en: Graph Neural Networks, GNNs, Graph Convolution Network, GCN, Graph, Deep Learning,
    Traffic Forecasting, Traffic Domain, ITS
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络，GNNs，图卷积网络，GCN，图，深度学习，交通预测，交通领域，智能交通系统
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Along with the acceleration of urbanization process, mass population is quickly
    gathering together towards cities. In many cities, especially cities in developing
    countries, the rapidly increasing number of private vehicles and growing demand
    of public transport services are putting great pressure on their current transportation
    systems. The traffic problems such as frequent traffic jams, serious traffic accidents
    and long commute have seriously decreased the operation efficiency of cities and
    degraded the travel experience of passengers. To address these challenges, many
    cities are committed to develop an Intelligent Transportation System (ITS) which
    can provide efficient traffic management, accurate traffic resources allocation
    and high-quality transportation service. Such a system can reduce traffic accidents,
    relieve traffic congestion and ensure public traffic safety.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着城市化进程的加速，大量人口迅速向城市聚集。在许多城市，尤其是发展中国家的城市，私人车辆数量的快速增长和对公共交通服务需求的增加对现有交通系统造成了巨大压力。交通问题如频繁的交通拥堵、严重的交通事故和长时间的通勤严重降低了城市的运行效率，并恶化了乘客的出行体验。为了应对这些挑战，许多城市致力于开发一个智能交通系统（ITS），以提供高效的交通管理、准确的交通资源分配和高质量的交通服务。这样的系统可以减少交通事故、缓解交通拥堵，并确保公共交通安全。
- en: To construct an Intelligent Transportation System which makes cities smart,
    there are mainly two indispensable components, i.e. intelligent infrastructures
    and advanced algorithms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个使城市智能化的智能交通系统，主要有两个不可或缺的组件，即智能基础设施和先进算法。
- en: On one hand, with the increasing investment in transportation infrastructures,
    there are more and more traffic equipments and systems, including loop detectors,
    probes, cameras on road networks, GPS in taxis or buses, smart cards on subways
    and buses, automatic fare collection system and online ride-hailing system. These
    infrastructures produce traffic data around-the-clock, which are heterogeneous
    data, including numeric data (e.g. GPS trajectories, traffic measurements), image/video
    data (e.g. vehicle images) and textual data (e.g. incident reports). These transportation
    data are enormous in volume and complicated in structure, containing complex traffic
    patterns (e.g. spatiotemporal dependency, highly nonlinearity, complex dynamics).
    There is an urgent need to utilize more intelligent and powerful approaches to
    process such traffic data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，随着对交通基础设施投资的增加，道路网络上出现了越来越多的交通设备和系统，包括环形探测器、探头、道路摄像头、出租车或公交车上的GPS、地铁和公交车上的智能卡、自动收费系统和在线打车系统。这些基础设施全天候产生交通数据，这些数据是异构的，包括数值数据（例如GPS轨迹、交通测量）、图像/视频数据（例如车辆图像）和文本数据（例如事件报告）。这些交通数据在体量上庞大，结构上复杂，包含复杂的交通模式（例如时空依赖性、高度非线性、复杂的动态性）。急需利用更智能、更强大的方法来处理这些交通数据。
- en: On the other hand, in transportation domain, researchers have witnessed the
    algorithms evolving from statistical methods, to machine learning models and recently
    to deep learning approaches. In the early stage, statistic methods including ARIMA
    and its variants [[1](#bib.bib1)],[[2](#bib.bib2)], VAR[[3](#bib.bib3)], Kalman
    filtering [[4](#bib.bib4)] were prevalent, as they have solid and widely accepted
    mathematical foundations. However, the linear and stationarity assumptions of
    these methods are violated by the highly non-linearity and dynamics in traffic
    data, resulting in poor performance in practice. Traditional machine learning
    approaches such as Support Vector Machine [[5](#bib.bib5)], K-Nearest Neighbors[[6](#bib.bib6)]
    can model non-linearity and extract more complex correlations in traffic data.
    However, the shallow architecture, manual feature selection and separated learning
    in these models are considered to be unsatisfactory in big data scenarios [[7](#bib.bib7)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在交通领域，研究人员目睹了算法从统计方法、到机器学习模型、最近到深度学习方法的演变。在早期阶段，统计方法包括ARIMA及其变体[[1](#bib.bib1)]、[[2](#bib.bib2)]，VAR[[3](#bib.bib3)]，Kalman滤波[[4](#bib.bib4)]曾经广泛使用，因为它们具有坚实且被广泛接受的数学基础。然而，这些方法的线性和稳定性假设被交通数据中的高度非线性和动态性所违背，导致实际表现不佳。传统的机器学习方法如支持向量机[[5](#bib.bib5)]、K-近邻[[6](#bib.bib6)]可以建模非线性，并提取交通数据中的更复杂的关联。然而，这些模型的浅层架构、手动特征选择和分离学习在大数据场景中被认为是不令人满意的[[7](#bib.bib7)]。
- en: The breakthrough of deep learning in many domains, including computer vision,
    natural language processing has attracted attention from transportation industry
    and research community. Deep learning techniques overcome the handcrafted feature
    engineering by providing an end-to-end learning from raw traffic data. The powerful
    capacities of deep learning techniques to approximate any complex functions in
    theory can model more complicated patterns in various traffic tasks. In recent
    years, due to the increasing computing power (e.g. GPU) and sufficient traffic
    data [[7](#bib.bib7)], deep learning based techniques have been widely employed
    and achieved state-of-the-art performance in various traffic applications. The
    Recurrent neural networks (RNNs) and Convolutional neural networks (CNNs) based
    architectures used to be popular in extracting spatiotemporal dependencies. In
    these architectures, RNN or its variants are employed to extract the temporal
    correlations in traffic data [[8](#bib.bib8)]. CNNs are used to capture the spatial
    correlations in grid-based traffic network [[9](#bib.bib9)]. However, many traffic
    networks are graph-structured in nature, e.g. road network [[10](#bib.bib10)]
    and subway network. The spatial features learned in CNN are not optimal for representing
    the graph-based traffic network. Although some previous works have analyzed traffic
    problems in a graph view [[11](#bib.bib11)],[[12](#bib.bib12)], these traditional
    approaches are not powerful enough to process big data and tackle complicated
    correlations in traffic network.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在许多领域的突破，包括计算机视觉和自然语言处理，已引起交通行业和研究界的关注。深度学习技术通过从原始交通数据中提供端到端学习，克服了手工特征工程的限制。深度学习技术理论上能够近似任何复杂函数，从而能够建模各种交通任务中的复杂模式。近年来，由于计算能力的提高（例如
    GPU）和足够的交通数据 [[7](#bib.bib7)]，基于深度学习的技术已被广泛应用，并在各种交通应用中取得了最先进的性能。基于递归神经网络（RNNs）和卷积神经网络（CNNs）的架构曾在提取时空依赖性方面非常流行。在这些架构中，RNN
    或其变体被用来提取交通数据中的时间相关性 [[8](#bib.bib8)]。CNNs 用于捕捉网格状交通网络中的空间相关性 [[9](#bib.bib9)]。然而，许多交通网络本质上是图结构的，例如道路网络
    [[10](#bib.bib10)] 和地铁网络。在 CNN 中学习到的空间特征并不适合表示基于图的交通网络。尽管一些先前的工作已经从图的角度分析了交通问题
    [[11](#bib.bib11)], [[12](#bib.bib12)]，这些传统方法在处理大数据和解决交通网络中的复杂相关性方面并不够强大。
- en: Recently, many researchers have extended deep learning approaches on graph data
    to exploit graph structure information [[13](#bib.bib13)] and proposed a new group
    of neural networks called graph neural networks (GNNs)[[14](#bib.bib14)],[[15](#bib.bib15)],[[16](#bib.bib16)],
    which aims to address graph-related applications. GNNs have become the state-of-the-art
    approaches in many domains, including computer vision [[17](#bib.bib17)], natural
    language processing [[18](#bib.bib18)], biology [[19](#bib.bib19)], recommendation
    system [[20](#bib.bib20)]. Since many traffic data are graph-structured, many
    existing works incorporate GNNs into a deep learning architecture to capture the
    spatial dependency. Recent works have shown that such GNNs-based architectures
    can achieve better performance than CNNs-based architectures, for that most traffic
    networks are graph-structured naturally and GNNs can extract the spatial dependency
    more accurately. In addition, some tasks inherently require researchers to conduct
    prediction based on a graph, e.g. prediction in traffic network with irregular
    shapes. Many related works have been produced during the last couple of years
    and more are on the road. Under this circumstance, a comprehensive literature
    review on these graph-based deep learning architectures in transportation domain
    would be very timely, which is exactly our work.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，许多研究者扩展了深度学习方法在图数据上的应用，以利用图结构信息 [[13](#bib.bib13)]，并提出了一组新的神经网络，称为图神经网络（GNNs）[[14](#bib.bib14)],
    [[15](#bib.bib15)], [[16](#bib.bib16)]，旨在解决图相关的应用。GNNs 已成为许多领域的最先进方法，包括计算机视觉 [[17](#bib.bib17)]、自然语言处理
    [[18](#bib.bib18)]、生物学 [[19](#bib.bib19)]、推荐系统 [[20](#bib.bib20)]。由于许多交通数据是图结构的，许多现有的工作将
    GNNs 融入深度学习架构中，以捕捉空间依赖性。近期的工作表明，这种基于 GNNs 的架构可以比基于 CNNs 的架构实现更好的性能，因为大多数交通网络自然是图结构的，GNNs
    能够更准确地提取空间依赖性。此外，一些任务本质上要求研究人员基于图进行预测，例如预测具有不规则形状的交通网络。近年来已经产生了许多相关工作，还有更多正在进行中。在这种情况下，对这些图基深度学习架构在交通领域的全面文献综述将非常及时，这正是我们的工作。
- en: To our best knowledge, we are the first to provide a comprehensive survey on
    graph-based deep learning works in traffic domain. Note that some works we review
    actually work on similar traffic problems with similar techniques. Our work can
    help the upcoming researchers avoid repetitive works and focus on new solutions.
    What’s more, the practical and clear guidance in this survey enables participators
    to apply these new emerging approaches in real-world traffic tasks quickly.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，我们是首个对交通领域中的基于图的深度学习工作的全面调查进行综述的研究。值得注意的是，我们评审的一些工作实际上解决了类似的交通问题，并使用了类似的技术。我们的工作可以帮助未来的研究者避免重复的工作，集中于新的解决方案。此外，本调查中的实际和清晰的指导使参与者能够迅速将这些新兴方法应用于现实世界的交通任务中。
- en: 'To sum up, the main contributions of this paper are as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本文的主要贡献如下：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We systematically outline traffic problems, related research directions, challenges
    and techniques in traffic domain, which can help related researchers to locate
    or expand their researches.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们系统地概述了交通问题、相关研究方向、挑战和交通领域的技术，这可以帮助相关研究人员定位或扩展他们的研究。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We summarize a general formulation about various traffic problems and provide
    a specific guidance to construct graphs from several typical kinds of raw traffic
    datasets. Such thorough summarization is quite practical and can accelerate the
    applications of graph-based approaches in traffic domain.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们总结了关于各种交通问题的一般性公式，并提供了从几种典型的原始交通数据集中构建图的具体指导。这种彻底的总结非常实用，可以加速基于图的方法在交通领域的应用。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comprehensive review over typical deep learning techniques widely
    used in graph-based traffic works. We elaborate their theoretical aspects, advantages,
    limitations and variants in specific traffic tasks, hoping to inspire the followers
    to develop more novel models.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了对广泛应用于基于图的交通工作中的典型深度学习技术的全面回顾。我们详细阐述了它们的理论方面、优点、局限性和在特定交通任务中的变体，希望能够激发研究者开发更多创新模型。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss some challenges shared by most graph-based traffic tasks. For each
    challenge, we conclude multiple deep learning-based solutions and make necessary
    comparison, providing useful suggestions for model selection in traffic tasks.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了大多数基于图的交通任务所面临的一些挑战。对于每个挑战，我们总结了多种基于深度学习的解决方案，并进行必要的比较，为交通任务中的模型选择提供有用的建议。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We collect benchmark datasets, open-source codes in related papers to facilitate
    baseline experiments in traffic domain. Finally, we propose some future research
    directions.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们收集了基准数据集和相关论文中的开源代码，以促进交通领域的基线实验。最后，我们提出了一些未来的研究方向。
- en: 'The rest of the paper is organized as follows. Section [II](#S2 "II Related
    Research Surveys ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic
    Domain: A Survey") presents some surveys in traffic domain and some reviews about
    graph neural networks. Section [III](#S3 "III Problems, Research Directions and
    Challenges ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic
    Domain: A Survey") briefly outlines several traffic problems and the corresponding
    research directions, challenges and solutions. Section [IV](#S4 "IV Problem Formulation
    and Graph Construction ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey") summarizes a general formulation about traffic problems
    and the graph construction from traffic datasets. Section [V](#S5 "V Deep Learning
    Techniques Perspective ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey") analyzes the functionality, advantages and defects
    of GNNs and other deep learning techniques, as well as examining the tricks to
    create novel variants of these techniques in specific traffic tasks. Section [VI](#S6
    "VI Challenges Perspective ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey") discusses common challenges in traffic domain and
    the corresponding multiple solutions. Section [VII](#S7 "VII Public Datasets and
    Open Source Codes ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic
    Domain: A Survey") provides hyperlinks of datasets and open codes in papers we
    investigate. Section [VIII](#S8 "VIII Future Directions ‣ How to Build a Graph-Based
    Deep Learning Architecture in Traffic Domain: A Survey") presents future directions.
    Section [IX](#S9 "IX Conclusion ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey") concludes the paper.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '论文的其余部分组织如下。第[II](#S2 "II Related Research Surveys ‣ How to Build a Graph-Based
    Deep Learning Architecture in Traffic Domain: A Survey")节介绍了交通领域的一些调查以及图神经网络的一些评论。第[III](#S3
    "III Problems, Research Directions and Challenges ‣ How to Build a Graph-Based
    Deep Learning Architecture in Traffic Domain: A Survey")节简要概述了几个交通问题以及相应的研究方向、挑战和解决方案。第[IV](#S4
    "IV Problem Formulation and Graph Construction ‣ How to Build a Graph-Based Deep
    Learning Architecture in Traffic Domain: A Survey")节总结了有关交通问题的一般性公式化以及从交通数据集中构建图的过程。第[V](#S5
    "V Deep Learning Techniques Perspective ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey")节分析了GNNs和其他深度学习技术的功能、优点和缺陷，并探讨了在特定交通任务中创建这些技术的新变体的技巧。第[VI](#S6
    "VI Challenges Perspective ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey")节讨论了交通领域的常见挑战以及相应的多种解决方案。第[VII](#S7 "VII Public Datasets
    and Open Source Codes ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey")节提供了我们调查论文中的数据集和开源代码的超链接。第[VIII](#S8 "VIII Future
    Directions ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic
    Domain: A Survey")节介绍了未来的研究方向。第[IX](#S9 "IX Conclusion ‣ How to Build a Graph-Based
    Deep Learning Architecture in Traffic Domain: A Survey")节总结了论文内容。'
- en: II Related Research Surveys
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关研究调查
- en: There have been some surveys summarizing the development process of algorithms
    in traffic tasks from different perspectives. Karlaftis et al. [[21](#bib.bib21)]
    discussed differences and similarities between statistical methods and neural
    networks to promote the comprehension between these two communities. Vlahogianni
    et al. [[22](#bib.bib22)] reviewed ten challenges on short-term traffic forecasting,
    which stemmed from the changing needs of ITS applications. Xie et al. [[23](#bib.bib23)]
    conducted a comprehensive overview of approaches in urban flow forecasting. Liu
    et al. [[7](#bib.bib7)] classified deep learning based urban big data fusion methods
    into three categories, i.e. DL-output-based fusion, DL-input-based fusion and
    DL-double-stage-based fusion. Deep learning approaches for popular topics including
    traffic network representation, traffic flow forecasting, traffic signal control,
    automatic vehicle detection are discussed in [[24](#bib.bib24)], [[25](#bib.bib25)].
    Veres et al. [[26](#bib.bib26)] and Chen et al.[[27](#bib.bib27)] gave a similar
    but more elaborate analysis on new emerging deep learning models in various transportation
    topics. Wang et al. [[28](#bib.bib28)] provided a spatial-temporal perspective
    to summarize deep learning techniques in traffic domain and other domains. However,
    all these surveys do not take graph neural networks (GNNs) related literatures
    into consideration, except that Wang et al. [[28](#bib.bib28)] mentioned GNNs
    but in a very short subsection.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 已有一些综述从不同的角度总结了交通任务中算法的发展过程。Karlaftis 等人 [[21](#bib.bib21)] 讨论了统计方法和神经网络之间的异同，以促进这两个社区之间的理解。Vlahogianni
    等人 [[22](#bib.bib22)] 回顾了短期交通预测的十个挑战，这些挑战源于 ITS 应用需求的变化。Xie 等人 [[23](#bib.bib23)]
    对城市流量预测的方法进行了全面的概述。Liu 等人 [[7](#bib.bib7)] 将基于深度学习的城市大数据融合方法分为三类，即 DL 输出基融合、DL
    输入基融合和 DL 双阶段融合。有关交通网络表示、交通流预测、交通信号控制、自动车辆检测等热门话题的深度学习方法在 [[24](#bib.bib24)]、[[25](#bib.bib25)]
    中进行了讨论。Veres 等人 [[26](#bib.bib26)] 和 Chen 等人 [[27](#bib.bib27)] 对各种交通运输主题中新兴的深度学习模型进行了类似但更详细的分析。Wang
    等人 [[28](#bib.bib28)] 从时空角度总结了交通领域和其他领域的深度学习技术。然而，除了 Wang 等人 [[28](#bib.bib28)]
    在非常简短的子章节中提到 GNN 外，所有这些综述均未考虑图神经网络（GNN）相关的文献。
- en: On the other hand, in recent years, there are several reviews summarizing literatures
    about GNNs in different aspects. Bronstein et al. [[29](#bib.bib29)] is the first
    to overview deep learning techniques on processing data in non-Euclidean space
    (e.g. graph data). Zhou et al. [[30](#bib.bib30)] categorized GNNs into graph
    types, propagation types and training types. In addition, they divided related
    applications into structural scenarios, non-structural scenarios, and other scenarios.
    Zhang et al.[[31](#bib.bib31)] introduced GNNs on small graphs and giant graphs
    respectively. Quan et al. [[32](#bib.bib32)] and Zhang et al. [[33](#bib.bib33)]
    focused on reviewing works in a specific branch of GNNs, i.e. graph convolutional
    network (GCN). However, they seldom introduce GNNs works in traffic scenarios.
    Wu et.al proposed [[34](#bib.bib34)] the only survey spending a paragraph to describe
    GNNs in traffic domain, which is obviously not enough for anyone desiring to explore
    this field.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，近年来有几篇综述总结了关于 GNN 的不同方面的文献。Bronstein 等人 [[29](#bib.bib29)] 是首个概述处理非欧几里得空间（如图数据）中的深度学习技术的研究。Zhou
    等人 [[30](#bib.bib30)] 将 GNN 按图类型、传播类型和训练类型进行了分类。此外，他们还将相关应用分为结构场景、非结构场景和其他场景。Zhang
    等人 [[31](#bib.bib31)] 分别介绍了小图和大图中的 GNN。Quan 等人 [[32](#bib.bib32)] 和 Zhang 等人 [[33](#bib.bib33)]
    专注于回顾 GNN 的特定分支，即图卷积网络（GCN）的研究。然而，他们很少介绍 GNN 在交通场景中的工作。Wu 等人 [[34](#bib.bib34)]
    提出了唯一一个在交通领域描述 GNN 的综述，然而仅用一段话描述显然不足以满足任何希望深入探讨这一领域的人的需求。
- en: In summary, there still lacks a systematic and elaborated survey to explore
    the rapidly developed graph-based deep learning techniques in traffic domain recently.
    Our work aims to fill this gap and promote understanding of the new emerging techniques
    in transportation community.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，目前仍缺乏一个系统且详尽的综述，以探讨最近在交通领域快速发展的基于图的深度学习技术。我们的工作旨在填补这一空白，促进交通社区对新兴技术的理解。
- en: III Problems, Research Directions and Challenges
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 问题、研究方向和挑战
- en: '![Refer to caption](img/b4882e1dfd04f5f90bba98791b446b60.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/b4882e1dfd04f5f90bba98791b446b60.png)'
- en: 'Figure 1: Typical traffic problems and the corresponding research directions'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：典型交通问题及其对应的研究方向
- en: 'In this section, we introduce background knowledge in traffic domain briefly,
    including some important traffic problems and the corresponding research directions
    (as shown in Figure [1](#S3.F1 "Figure 1 ‣ III Problems, Research Directions and
    Challenges ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic
    Domain: A Survey")), as well as common challenges and techniques under these problems.
    On one hand, we believe that such a concise but systematic introduction can help
    readers understand this domain quickly. On the other hand, our survey shows that
    existing works related with graph-based deep learning techniques only cover some
    research directions, which inspires successors to transfer similar techniques
    to remaining directions.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们简要介绍交通领域的背景知识，包括一些重要的交通问题及其相应的研究方向（如图 [1](#S3.F1 "Figure 1 ‣ III Problems,
    Research Directions and Challenges ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey") 所示），以及这些问题下的常见挑战和技术。一方面，我们相信这样简洁但系统的介绍可以帮助读者迅速理解这一领域。另一方面，我们的调查显示，现有与图基深度学习技术相关的工作仅涵盖了部分研究方向，这激励了后继者将类似技术转移到剩余方向。'
- en: III-A Traffic Problems
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 交通问题
- en: The goals the transportation community aims to achieve include relieving traffic
    congestion, satisfying travel demand, enhancing traffic management, ensuring transportation
    safety and realizing automatic driving. Each problem under the corresponding traffic
    goal can be partitioned into several research directions and each direction can
    serve more than one problem.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 交通社区的目标包括缓解交通拥堵、满足旅行需求、增强交通管理、确保交通安全和实现自动驾驶。每个与相应交通目标相关的问题可以细分为若干研究方向，每个方向可以服务于多个问题。
- en: III-A1 Traffic Congestion
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 交通拥堵
- en: Traffic congestion [[35](#bib.bib35)] is one of the most important and urgent
    problems in modern cities in terms of significant time loss, air pollution and
    energy waste. The congestion can be solved by increasing the traffic efficiency
    [[36](#bib.bib36)], [[37](#bib.bib37)], alleviating the traffic congestion on
    road network [[38](#bib.bib38)], [[39](#bib.bib39)], [[40](#bib.bib40)], controlling
    the road conditions by traffic state prediction[[41](#bib.bib41)],[[42](#bib.bib42)],
    optimizing vehicle flow by controlling traffic signals [[43](#bib.bib43)],[[44](#bib.bib44)],
    optimizing passenger flow by predicting passenger demand in public transportation
    systems [[45](#bib.bib45)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 交通拥堵[[35](#bib.bib35)]是现代城市中最重要和最紧迫的问题之一，涉及显著的时间损失、空气污染和能源浪费。可以通过提高交通效率[[36](#bib.bib36)]，[[37](#bib.bib37)]、缓解道路网络上的交通拥堵[[38](#bib.bib38)]，[[39](#bib.bib39)]，[[40](#bib.bib40)]、通过交通状态预测控制道路条件[[41](#bib.bib41)]，[[42](#bib.bib42)]、通过控制交通信号优化车辆流量[[43](#bib.bib43)]，[[44](#bib.bib44)]、通过预测公共交通系统中的乘客需求优化乘客流量[[45](#bib.bib45)]来解决拥堵问题。
- en: III-A2 Travel Demand
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 交通需求
- en: The travel demand prediction refers to the demand of traffic services, such
    as taxi, bike, metro and bus in a crowd perspective. With the emerging of online
    ride-hailing platforms (e.g. Uber, DiDi) and rapid development of public transportation
    systems (e.g. metro system and bus system), travel demand prediction has become
    more and more important for transport authorities, business sectors and individuals.
    For related authorities, it can help to better allocate resources, e.g. increase
    metro frequency at rush hours, add more buses to service hotspots. For business
    sector, it enables them to better manage taxi-hiring [[46](#bib.bib46)], carpooling
    [[47](#bib.bib47)], bike-sharing services [[48](#bib.bib48)],[[49](#bib.bib49)],
    and maximize their revenues. For individuals, it encourages users to consider
    various forms of transportation to decrease their commuting time and improve travel
    experience.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 旅行需求预测指的是从人群角度出发的交通服务需求，例如出租车、自行车、地铁和公交。随着在线打车平台（如Uber、滴滴）和公共交通系统（如地铁系统和公交系统）的快速发展，旅行需求预测对交通部门、商业部门和个人的重要性越来越高。对相关部门而言，这有助于更好地分配资源，例如在高峰时段增加地铁频次，增加热门区域的公交车数量。对商业部门而言，它使他们能够更好地管理出租车租赁[[46](#bib.bib46)]、拼车[[47](#bib.bib47)]、共享自行车服务[[48](#bib.bib48)],
    [[49](#bib.bib49)]，并最大化收入。对个人而言，它鼓励用户考虑各种交通方式，以减少通勤时间并改善旅行体验。
- en: III-A3 Transportation Safety
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 交通安全
- en: Transportation safety is an indispensable part of public safety. Traffic accidents
    can not only cause damage to victims, vehicles and road infrastructures, but also
    lead to traffic congestion and reduce efficiency of road network. Therefore, monitoring
    the traffic accidents is essential to avoid property loss and save life. Many
    researchers focus on directions such as detecting traffic incidents [[50](#bib.bib50)],
    predicting traffic accidents from social media data [[51](#bib.bib51)], predicting
    the injury severity of traffic accidents [[52](#bib.bib52)], [[53](#bib.bib53)],
    predicting prevention of accidents [[54](#bib.bib54)], [[55](#bib.bib55)], [[56](#bib.bib56)].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 交通安全是公共安全不可或缺的一部分。交通事故不仅会对受害者、车辆和道路基础设施造成损害，还会导致交通拥堵，降低道路网络的效率。因此，监控交通事故对于避免财产损失和拯救生命至关重要。许多研究者关注的方向包括检测交通事件[[50](#bib.bib50)]、从社交媒体数据预测交通事故[[51](#bib.bib51)]、预测交通事故的伤害严重性[[52](#bib.bib52)],
    [[53](#bib.bib53)]、预测事故预防[[54](#bib.bib54)], [[55](#bib.bib55)], [[56](#bib.bib56)]。
- en: III-A4 Traffic Surveillance
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A4 交通监控
- en: Nowadays, surveillance cameras have been widely deployed in city roads, generating
    numerous images and videos [[27](#bib.bib27)]. Such development has enhanced traffic
    surveillance, which includes traffic law enforcement, automatic toll collection
    [[57](#bib.bib57)] and traffic monitoring systems. The research directions of
    traffic surveillance include license plate detection[[58](#bib.bib58)], automatic
    vehicle detection [[59](#bib.bib59)], pedestrian detection [[60](#bib.bib60)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，监控摄像头已经在城市道路上广泛部署，生成了大量的图像和视频[[27](#bib.bib27)]。这种发展增强了交通监控，包括交通执法、自动收费[[57](#bib.bib57)]以及交通监控系统。交通监控的研究方向包括车牌检测[[58](#bib.bib58)]、自动车辆检测[[59](#bib.bib59)]、行人检测[[60](#bib.bib60)]。
- en: III-A5 Autonomous Driving
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A5 自动驾驶
- en: Recently, automatic driving vehicle has become a hot spot of research in transportation
    domain. Many tasks are related with visual recognition. The research directions
    of autonomous driving include lane/vehicle detection [[61](#bib.bib61)], pedestrian
    detection [[62](#bib.bib62)], traffic sign detection [[63](#bib.bib63)] and human/vehicle
    trajectory prediction [[64](#bib.bib64)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，自动驾驶车辆已成为交通领域的研究热点。许多任务涉及视觉识别。自动驾驶的研究方向包括车道/车辆检测[[61](#bib.bib61)]、行人检测[[62](#bib.bib62)]、交通标志检测[[63](#bib.bib63)]以及人/车轨迹预测[[64](#bib.bib64)]。
- en: III-B Research Directions
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 研究方向
- en: Our survey of graph-based deep learning in traffic domain shows that existing
    works focus mainly on traffic state prediction, travel demand prediction, trajectory
    prediction. A few works focus on vehicle behavior classification [[65](#bib.bib65)],
    optimal dynamic electronic toll collection (DETC) scheme [[57](#bib.bib57)], path
    availability [[66](#bib.bib66)], traffic signal control [[67](#bib.bib67)]. To
    our best knowledge, traffic incident detection and vehicle detection have not
    been explored based in a graph view yet.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对交通领域基于图的深度学习的调查显示，现有的工作主要集中在交通状态预测、旅行需求预测和轨迹预测上。少数工作关注于车辆行为分类[[65](#bib.bib65)]、优化动态电子收费（DETC）方案[[57](#bib.bib57)]、路径可用性[[66](#bib.bib66)]、交通信号控制[[67](#bib.bib67)]。据我们了解，基于图视角的交通事件检测和车辆检测尚未得到探索。
- en: III-B1 Traffic State Prediction
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 交通状态预测
- en: Traffic state in literatures refers to traffic flow, traffic speed, travel time,
    traffic density and so on. Traffic Flow Prediction (TFP) [[68](#bib.bib68)],[[69](#bib.bib69)],
    Traffic Speed Prediction (TSP) [[70](#bib.bib70)], [[71](#bib.bib71)], Travel
    Time Prediction (TTP) [[72](#bib.bib72)],[[73](#bib.bib73)], [[74](#bib.bib74)]
    are hot branches of traffic state prediction and have attracted intensive studies.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中的交通状态指的是交通流量、交通速度、旅行时间、交通密度等。交通流量预测（TFP）[[68](#bib.bib68)], [[69](#bib.bib69)]、交通速度预测（TSP）[[70](#bib.bib70)],
    [[71](#bib.bib71)]、旅行时间预测（TTP）[[72](#bib.bib72)], [[73](#bib.bib73)], [[74](#bib.bib74)]是交通状态预测的热点分支，并吸引了大量研究。
- en: III-B2 Travel Demand Prediction
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 旅行需求预测
- en: Travel demand prediction aims to estimate the future number of users who require
    traffic services. It can be categorized into two kinds, i.e. zone-level demand
    prediction and origin-destination travel demand prediction. The former one aims
    to predict the future travel demand in each region of a city, for example, to
    predict future taxi request in each area of a city [[75](#bib.bib75)],[[76](#bib.bib76)],
    or to predict the station-level passenger demand in subway system [[77](#bib.bib77)],
    [[78](#bib.bib78)], [[79](#bib.bib79)], [[45](#bib.bib45)] or to predict the bike
    hiring demand in each region of a city [[48](#bib.bib48)],[[49](#bib.bib49)].
    The latter one aims to predict the number of travel demand from one region to
    another, which can provide richer information than the zone-level demand prediction
    and is a more challenging issue worth exploration. Up to now, there are only a
    few studies [[80](#bib.bib80)], [[81](#bib.bib81)], [[82](#bib.bib82)] directed
    towards the origin-destination based travel demand prediction, which is a promising
    research direction.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 旅行需求预测旨在估计未来需要交通服务的用户数量。它可以分为两种类型，即区域级需求预测和出发地-目的地旅行需求预测。前者旨在预测城市每个区域的未来旅行需求，例如，预测城市各个区域的未来出租车请求
    [[75](#bib.bib75)],[[76](#bib.bib76)]，或预测地铁系统的车站级乘客需求 [[77](#bib.bib77)], [[78](#bib.bib78)],
    [[79](#bib.bib79)], [[45](#bib.bib45)]，或预测城市每个区域的自行车租赁需求 [[48](#bib.bib48)],[[49](#bib.bib49)]。后者旨在预测从一个区域到另一个区域的旅行需求，这比区域级需求预测提供了更丰富的信息，是一个值得深入探索的更具挑战性的问题。至今，仅有少数研究
    [[80](#bib.bib80)], [[81](#bib.bib81)], [[82](#bib.bib82)]] 关注基于出发地-目的地的旅行需求预测，这是一个有前景的研究方向。
- en: III-B3 Traffic Signal Control
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B3 交通信号控制
- en: The traffic signal control aims to properly control the traffic lights so as
    to reduce vehicle staying time at the road intersections in the long run [[25](#bib.bib25)].
    Traffic signal control [[67](#bib.bib67)] can optimize the traffic flow and reduce
    traffic congestion and vehicle emission.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 交通信号控制旨在适当控制交通灯，以减少车辆在交叉路口的停留时间，从而在长期内 [[25](#bib.bib25)]。交通信号控制 [[67](#bib.bib67)]
    可以优化交通流量，减少交通拥堵和车辆排放。
- en: III-B4 Traffic Incident Detection
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B4 交通事件检测
- en: Major incidents can cause fatal injuries to travelers and long delays on a road
    network. Therefore, understanding the main cause of incidents and the impact of
    incidents on a traffic network is crucial for a modern transportation management
    system [[50](#bib.bib50)],[[52](#bib.bib52)], [[53](#bib.bib53)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 重大事件可能导致旅客重伤和道路网络的长时间延误。因此，了解事件的主要原因以及事件对交通网络的影响对现代交通管理系统至关重要 [[50](#bib.bib50)],[[52](#bib.bib52)],
    [[53](#bib.bib53)]。
- en: III-B5 Human/Vehicle Trajectory Prediction
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B5 人员/车辆轨迹预测
- en: Trajectory Prediction [[64](#bib.bib64)], [[83](#bib.bib83)], [[84](#bib.bib84)]
    aims to forecast future positions of dynamic agents in a scene. Accurate human/vehicle
    trajectories prediction is of great importance for downstream tasks including
    autonomous driving and traffic surveillance [[85](#bib.bib85)]. For instance,
    an accurate pedestrian trajectory prediction can help controller to control the
    vehicle ahead in a dangerous environment [[86](#bib.bib86)]. It can also enable
    transportation surveillance system to identify suspicious activities [[87](#bib.bib87)].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹预测 [[64](#bib.bib64)], [[83](#bib.bib83)], [[84](#bib.bib84)] 旨在预测场景中动态代理的未来位置。准确的人员/车辆轨迹预测对包括自动驾驶和交通监控在内的下游任务非常重要
    [[85](#bib.bib85)]。例如，准确的行人轨迹预测可以帮助控制器在危险环境中控制前方车辆 [[86](#bib.bib86)]。它还可以使交通监控系统识别可疑活动
    [[87](#bib.bib87)]。
- en: III-C Challenges and Techniques Overview
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 挑战与技术概述
- en: '![Refer to caption](img/0d68339e180bc804472ff4d6cf39b0e4.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0d68339e180bc804472ff4d6cf39b0e4.png)'
- en: 'Figure 2: Traffic challenges and the corresponding deep learning techniques.
    SGCN refers spectral graph convolution network, DGCN refers diffusion graph convolution
    network, GAT refers graph attention network, TCN refers temporal convolution network,
    RNN refers recurrent neural network, GRU refers gated recurrent unit, LSTM refers
    long short term memory network, MLP refers multi-layer perceptron, Seq2Seq refers
    sequence to sequence model, GAN refers generative adversarial network.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：交通挑战及相应的深度学习技术。SGCN 指谱图卷积网络，DGCN 指扩散图卷积网络，GAT 指图注意力网络，TCN 指时间卷积网络，RNN 指递归神经网络，GRU
    指门控递归单元，LSTM 指长短期记忆网络，MLP 指多层感知器，Seq2Seq 指序列到序列模型，GAN 指生成对抗网络。
- en: Although traffic problems and the related research directions are different,
    most of them share the same challenges, e.g. spatial dependency, temporal dependency,
    external factors.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管交通问题及相关研究方向有所不同，但大多数都面临相同的挑战，例如空间依赖性、时间依赖性、外部因素。
- en: III-C1 Spatiotemporal Dependency
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 时空依赖性
- en: There are complex spatiotemporal dependency in traffic data which can affect
    the prediction in traffic tasks. For instance, to predict a traffic congestion
    in a region, its previous traffic conditions and the traffic conditions of its
    surrounding regions are important factors for prediction[[35](#bib.bib35)],[[38](#bib.bib38)],[[39](#bib.bib39)].
    In vehicle trajectory prediction, the stochastic behaviors of surrounding vehicles
    and the historical information of self-trajectory influence the prediction performance
    [[88](#bib.bib88)]. When it comes to predict the ride-hailing demand in a region,
    its previous orders as well as orders in other regions with similar functionality
    are critical for prediction[[89](#bib.bib89)]. To predict the traffic signal,
    the geometric features of multiple intersections are taken into consideration,
    as well as the previous traffic flow around [[67](#bib.bib67)].
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 交通数据中存在复杂的时空依赖性，这可能影响交通任务中的预测。例如，要预测某一地区的交通拥堵，其之前的交通状况及周围地区的交通状况是预测的重要因素[[35](#bib.bib35)],[[38](#bib.bib38)],[[39](#bib.bib39)]。在车辆轨迹预测中，周围车辆的随机行为和自身轨迹的历史信息会影响预测性能[[88](#bib.bib88)]。预测某一地区的网约车需求时，该地区的历史订单以及其他具有相似功能地区的订单对预测至关重要[[89](#bib.bib89)]。预测交通信号时，需要考虑多个交叉口的几何特征，以及周围的交通流量[[67](#bib.bib67)]。
- en: III-C2 External Factors
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 外部因素
- en: Except the spatiotemporal data, some types of data play an important role in
    traffic tasks, referred as external factors, such as holidays, weather conditions
    (e.g. rainfall, temperature, air quality), extreme events [[90](#bib.bib90)] and
    traffic incidents (e.g. incident time, incident type) [[91](#bib.bib91)]. The
    influence of external factors on traffic conditions can be observed in daily life.
    A rainstorm is likely to affect the traffic volume. A large-scale concert or football
    match results in traffic congregation, affecting traffic conditions around.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 除了时空数据外，一些类型的数据在交通任务中扮演着重要角色，称为外部因素，例如假期、天气条件（如降雨、温度、空气质量）、极端事件[[90](#bib.bib90)]和交通事件（如事件时间、事件类型）[[91](#bib.bib91)]。外部因素对交通状况的影响在日常生活中是可以观察到的。暴风雨可能会影响交通量。大规模音乐会或足球比赛会导致交通拥堵，影响周围的交通状况。
- en: To tackle challenges above, various deep learning techniques have been proposed.
    In this paper, we focus on graph-based deep learning architectures in traffic
    domain. Among these graph-based deep learning frameworks, graph neural networks
    (GNNs) are usually employed to model the spatial dependency in traffic network.
    Recurrent neural networks (RNNs) and temporal convolution network (TCN) are generally
    adopted to model the temporal dependency in traffic data. RNNs and Multi-layer
    Perceptrons (MLPs) are typically employed to process external factors. Sequence
    to Sequence (Seq2Seq) model is usually utilized to make multi-step traffic prediction.
    These techniques along with other tricks (e.g. gated mechanism, attention mechanism)
    are combined organically to improve the prediction accuracy.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为应对上述挑战，提出了各种深度学习技术。本文聚焦于交通领域的图基深度学习架构。在这些图基深度学习框架中，图神经网络（GNNs）通常用于建模交通网络中的空间依赖性。递归神经网络（RNNs）和时间卷积网络（TCN）通常用于建模交通数据中的时间依赖性。RNNs和多层感知器（MLPs）通常用于处理外部因素。序列到序列（Seq2Seq）模型通常用于进行多步骤交通预测。这些技术与其他技巧（如门控机制、注意力机制）有机结合，以提高预测准确性。
- en: 'In this paper, we aim to provide readers guidance about how to build a graph-based
    deep learning architecture and we have investigated enormous existing traffic
    works adopting graph-based deep learning solutions. In the following sections,
    we first introduce a common way to formulate the traffic problem and give detailed
    guidelines to build traffic graphs from various kinds of traffic data. Then we
    clarify the correlations between challenges and techniques (as shown in Figure
    [2](#S3.F2 "Figure 2 ‣ III-C Challenges and Techniques Overview ‣ III Problems,
    Research Directions and Challenges ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey")) in two perspectives, i.e. the techniques
    perspective and the challenges perspective. In the perspective of techniques,
    we introduce several common techniques and interpret the way they tackle challenges
    in traffic tasks. In the perspective of challenges, we elaborate each challenge
    and summarize the techniques which can tackle this challenge. In a word, we hope
    to provide insights into solving traffic challenges with various deep learning
    techniques based on a graph view.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们旨在为读者提供如何构建基于图的深度学习架构的指导，并且我们调查了大量现有的采用图基深度学习解决方案的交通相关工作。在接下来的部分中，我们首先介绍一种常见的交通问题制定方式，并提供详细的指导以从各种交通数据中构建交通图。然后，我们从两个角度澄清挑战与技术之间的关联（如图
    [2](#S3.F2 "Figure 2 ‣ III-C Challenges and Techniques Overview ‣ III Problems,
    Research Directions and Challenges ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey") 所示），即技术角度和挑战角度。在技术角度中，我们介绍几种常见技术，并解释它们如何应对交通任务中的挑战。在挑战角度中，我们详细阐述每个挑战，并总结可以解决这些挑战的技术。总之，我们希望提供基于图视角的各种深度学习技术解决交通挑战的见解。'
- en: IV Problem Formulation and Graph Construction
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 问题制定与图构建
- en: Among the graph-based deep learning traffic literatures we investigate, the
    majority of tasks (more than 80%) belong to spatiotemporal forecasting problems,
    especially traffic state prediction and travel demand prediction. In this section,
    we first list commonly used notations. Then we summarize a general formulation
    of graph-based spatiotemporal prediction in traffic domain. We provide details
    to construct graphs from various traffic datasets. We also discuss multiple definitions
    of adjacency matrix, which represents the topology of graph-based traffic network
    and is the key element of graph-based solution.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们调查的基于图的深度学习交通文献中，大多数任务（超过 80%）属于时空预测问题，特别是交通状态预测和出行需求预测。在本节中，我们首先列出常用的符号。然后我们总结了交通领域中基于图的时空预测的一般制定方式。我们提供了从各种交通数据集中构建图的详细信息。我们还讨论了邻接矩阵的多种定义，它代表了基于图的交通网络的拓扑结构，是图基解决方案的关键元素。
- en: 'TABLE I: Notations In This Paper'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：本文中的符号
- en: '| Graph related elements |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 图相关元素 |'
- en: '| --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| $\mathbf{G}$ | Graph |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{G}$ | 图 |'
- en: '| $\mathbf{E}$ | Edges of graph $\mathbf{G}$ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{E}$ | 图 $\mathbf{G}$ 的边 |'
- en: '| $\mathbf{V}$ | Vertices of graph $\mathbf{G}$ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{V}$ | 图 $\mathbf{G}$ 的顶点 |'
- en: '| $\mathbf{A}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | Adjacency matrix
    of graph $\mathbf{G}$ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{A}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | 图 $\mathbf{G}$ 的邻接矩阵
    |'
- en: '| $\mathbf{A}^{T}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | The transpose
    matrix of $\mathbf{A}$ |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{A}^{T}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | $\mathbf{A}$
    的转置矩阵 |'
- en: '| $\mathbf{\tilde{A}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | Equal to
    $\mathbf{A}+\mathbf{I_{N}}$, a self-looped $\mathbf{A}$ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{\tilde{A}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | 等于 $\mathbf{A}+\mathbf{I_{N}}$
    的自环矩阵 $\mathbf{A}$ |'
- en: '| $\mathbf{D}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | The degree matrix
    of adjacency matrix $\mathbf{A}$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{D}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | 邻接矩阵 $\mathbf{A}$
    的度矩阵 |'
- en: '| $\mathbf{D_{I}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | The in-degree
    matrix of adjacency matrix $\mathbf{A}$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{D_{I}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | 邻接矩阵 $\mathbf{A}$
    的入度矩阵 |'
- en: '| $\mathbf{D_{O}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | The out-degree
    matrix of adjacency matrix $\mathbf{A}$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{D_{O}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | 邻接矩阵 $\mathbf{A}$
    的出度矩阵 |'
- en: '| $\mathbf{L}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | Laplacian matrix
    of graph $\mathbf{G}$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{L}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | 图 $\mathbf{G}$ 的拉普拉斯矩阵
    |'
- en: '| $\mathbf{U}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | The eigenvectors
    matrix of $\mathbf{L}$ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{U}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | $\mathbf{L}$ 的特征向量矩阵
    |'
- en: '| $\mathbf{\Lambda}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | The diagonal
    eigenvalues matrix of $\mathbf{L}$ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{\Lambda}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | $\mathbf{L}$
    的对角特征值矩阵 |'
- en: '| $\boldsymbol{\lambda}_{max}$ | The max eigenvalue of $\mathbf{L}$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| $\boldsymbol{\lambda}_{max}$ | $\mathbf{L}$ 的最大特征值 |'
- en: '| $\mathbf{I_{N}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | An identity
    matrix |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{I_{N}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$ | 单位矩阵 |'
- en: '| Hyper parameters |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 |'
- en: '| $\mathbf{N}$ | The number of nodes in graph $\mathbf{G}$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{N}$ | 图 $\mathbf{G}$ 中的节点数量 |'
- en: '| $\mathbf{F_{I}}$ | The number of input features |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{F_{I}}$ | 输入特征的数量 |'
- en: '| $\mathbf{F_{H}}$ | The number of hidden features |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{F_{H}}$ | 隐藏特征的数量 |'
- en: '| $\mathbf{F_{O}}$ | The number of output features |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{F_{O}}$ | 输出特征的数量 |'
- en: '| $\mathbf{P}$ | The number of past time slices |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{P}$ | 过去时间切片的数量 |'
- en: '| $\mathbf{Q}$ | The number of future time slices |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{Q}$ | 未来时间切片的数量 |'
- en: '| $\mathbf{d}$ | The dilation rate |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{d}$ | 扩张率 |'
- en: '| Trainable parameters |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 可训练的参数 |'
- en: '| $W,b,\theta,\phi$ | The trainable parameters |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| $W,b,\theta,\phi$ | 可训练的参数 |'
- en: '| $\Theta$ | The kernel |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| $\Theta$ | 核 |'
- en: '| Activation functions |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 |'
- en: '| $\boldsymbol{\rho}(\boldsymbol{\cdot})$ | The activation function, e.g. tanh,
    sigmoid, ReLU |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| $\boldsymbol{\rho}(\boldsymbol{\cdot})$ | 激活函数，例如 tanh, sigmoid, ReLU |'
- en: '| $\boldsymbol{\sigma}(\boldsymbol{\cdot})\in[0,1]$ | The sigmoid function
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| $\boldsymbol{\sigma}(\boldsymbol{\cdot})\in[0,1]$ | Sigmoid 函数 |'
- en: '| $\boldsymbol{tanh}(\boldsymbol{\cdot})\in[-1,1]$ | The hyperbolic tangent
    function |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| $\boldsymbol{tanh}(\boldsymbol{\cdot})\in[-1,1]$ | 双曲正切函数 |'
- en: '| $\boldsymbol{ReLU}(\boldsymbol{\cdot})\in[0,x]$ | The ReLU function |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| $\boldsymbol{ReLU}(\boldsymbol{\cdot})\in[0,x]$ | ReLU 函数 |'
- en: '| Operations |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 操作 |'
- en: '| $\boldsymbol{*_{\mathcal{G}}}$ | The convolution operator on graph |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| $\boldsymbol{*_{\mathcal{G}}}$ | 图上的卷积运算符 |'
- en: '| $\boldsymbol{\odot}$ | Element-wise multiplication |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| $\boldsymbol{\odot}$ | 元素级乘法 |'
- en: '| $\boldsymbol{\cdot}$ | Matrix multiplication |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| $\boldsymbol{\cdot}$ | 矩阵乘法 |'
- en: '| Spatial variables |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 空间变量 |'
- en: '| $X\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{I}}}$ | An input graph composed
    of $\mathbf{N}$ nodes with $\mathbf{F_{I}}$ features |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| $X\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{I}}}$ | 由 $\mathbf{N}$ 个节点和 $\mathbf{F_{I}}$
    特征组成的输入图 |'
- en: '| $X_{j}\in\mathbb{R}^{\mathbf{N}}$ | The $j^{th}$ feature of an input graph
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| $X_{j}\in\mathbb{R}^{\mathbf{N}}$ | 输入图的第 $j^{th}$ 特征 |'
- en: '| $X^{i}\in\mathbb{R}^{\mathbf{F_{I}}}$ | Node $i$ in an input graph |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| $X^{i}\in\mathbb{R}^{\mathbf{F_{I}}}$ | 输入图中的节点 $i$ |'
- en: '| $x\in\mathbb{R}^{\mathbf{N}}$ | A simply input graph |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| $x\in\mathbb{R}^{\mathbf{N}}$ | 一个简单的输入图 |'
- en: '| $Y\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{O}}}$ | An output graph composed
    of $\mathbf{N}$ nodes with $\mathbf{\mathbf{F_{O}}}$ features |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| $Y\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{O}}}$ | 由 $\mathbf{N}$ 个节点和 $\mathbf{\mathbf{F_{O}}}$
    特征组成的输出图 |'
- en: '| $Y_{j}\in\mathbb{R}^{\mathbf{N}}$ | The $j^{th}$ feature of an output graph
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| $Y_{j}\in\mathbb{R}^{\mathbf{N}}$ | 输出图的第 $j^{th}$ 特征 |'
- en: '| $Y^{i}\in\mathbb{R}^{\mathbf{F_{O}}}$ | Node $i$ in an output graph |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| $Y^{i}\in\mathbb{R}^{\mathbf{F_{O}}}$ | 输出图中的节点 $i$ |'
- en: '| $y\in\mathbb{R}^{\mathbf{N}}$ | A simply output graph |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| $y\in\mathbb{R}^{\mathbf{N}}$ | 一个简单的输出图 |'
- en: '| Temporal variables |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 时间变量 |'
- en: '| $\mathbf{X}\in\mathbb{R}^{\mathbf{P}\times\mathbf{F_{I}}}$ | A sequential
    input with $\mathbf{F_{I}}$ features over $\mathbf{P}$ time slices |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{X}\in\mathbb{R}^{\mathbf{P}\times\mathbf{F_{I}}}$ | 在 $\mathbf{P}$
    时间切片上的 $\mathbf{F_{I}}$ 特征的序列输入 |'
- en: '| $\mathbf{X}_{t}\in\mathbb{R}^{\mathbf{F_{I}}}$ | The element of sequential
    input at time $t$ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{X}_{t}\in\mathbb{R}^{\mathbf{F_{I}}}$ | 时间 $t$ 的序列输入的元素 |'
- en: '| $\mathbf{x}\in\mathbb{R}^{\mathbf{P}}$ | A simply sequential input over $\mathbf{P}$
    time slices |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{x}\in\mathbb{R}^{\mathbf{P}}$ | 在 $\mathbf{P}$ 时间切片上的简单序列输入 |'
- en: '| $\mathbf{x}_{t}\in\mathbb{R}$ | The element of simply sequential input at
    time $t$ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{x}_{t}\in\mathbb{R}$ | 时间 $t$ 的简单序列输入的元素 |'
- en: '| $\mathbf{H}_{t}\in\mathbb{R}^{\mathbf{F_{H}}}$ | A hidden state with $\mathbf{\mathbf{F}_{H}}$
    features at time $t$ |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{H}_{t}\in\mathbb{R}^{\mathbf{F_{H}}}$ | 时间 $t$ 的隐藏状态，具有 $\mathbf{\mathbf{F}_{H}}$
    特征 |'
- en: '| $\mathbf{Y}\in\mathbb{R}^{\mathbf{P}\times\mathbf{F_{O}}}$ | A sequential
    output with $\mathbf{F_{O}}$ features over $\mathbf{P}$ time slices |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{Y}\in\mathbb{R}^{\mathbf{P}\times\mathbf{F_{O}}}$ | 在 $\mathbf{P}$
    时间切片上的 $\mathbf{F_{O}}$ 特征的序列输出 |'
- en: '| $\mathbf{Y}_{t}\in\mathbb{R}^{\mathbf{F_{O}}}$ | The element of sequential
    output at time $t$ |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{Y}_{t}\in\mathbb{R}^{\mathbf{F_{O}}}$ | 时间 $t$ 的序列输出的元素 |'
- en: '| $\mathbf{y}\in\mathbb{R}^{\mathbf{P}}$ | A simply sequential output over
    $\mathbf{P}$ time slices |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{y}\in\mathbb{R}^{\mathbf{P}}$ | 在 $\mathbf{P}$ 时间切片上简单的序列输出 |'
- en: '| $\mathbf{y}_{t}\in\mathbb{R}$ | The element of simply sequential output at
    time $t$ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{y}_{t}\in\mathbb{R}$ | 时间$t$时简单顺序输出的元素 |'
- en: '| Spatiotemporal variables |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 空间-时间变量 |'
- en: '| $\mathcal{X}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{I}}}$
    | A series of input graphs composed of $\mathbf{N}$ nodes with $\mathbf{F_{I}}$
    features over $\mathbf{P}$ time slices |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{X}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{I}}}$
    | 一系列输入图，由$\mathbf{N}$个节点和$\mathbf{F_{I}}$特征组成，覆盖$\mathbf{P}$个时间片段 |'
- en: '| $\mathcal{X}_{t}\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{I}}}$ | An input
    graph at time $t$ |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{X}_{t}\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{I}}}$ | 时间$t$时的输入图
    |'
- en: '| $\mathcal{X}^{i}_{t}\in\mathbb{R}^{\mathbf{F_{I}}}$ | node $i$ in an input
    graph at time $t$ |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{X}^{i}_{t}\in\mathbb{R}^{\mathbf{F_{I}}}$ | 时间$t$时输入图中的节点$i$ |'
- en: '| $\mathcal{X}_{t,j}\in\mathbb{R}^{\mathbf{N}}$ | the $j^{th}$ feature of an
    input graph at time $t$ |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{X}_{t,j}\in\mathbb{R}^{\mathbf{N}}$ | 时间$t$时输入图的第$j^{th}$特征 |'
- en: '| $\mathcal{X}^{i}_{t,j}\in\mathbb{R}$ | the $j^{th}$ feature of node $i$ in
    an input graph at time $t$ |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{X}^{i}_{t,j}\in\mathbb{R}$ | 时间$t$时输入图中节点$i$的第$j^{th}$特征 |'
- en: '| $\mathcal{Y}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{O}}}$
    | A series of output graphs composed of $\mathbf{N}$ nodes with $\mathbf{F_{O}}$
    features over $\mathbf{P}$ time slices |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{Y}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{O}}}$
    | 一系列输出图，由$\mathbf{N}$个节点和$\mathbf{F_{O}}$特征组成，覆盖$\mathbf{P}$个时间片段 |'
- en: '| $\mathcal{Y}_{t}\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{O}}}$ | An output
    graph at time $t$ |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{Y}_{t}\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{O}}}$ | 时间$t$时的输出图
    |'
- en: '| $\mathcal{Y}^{i}_{t}\in\mathbb{R}^{\mathbf{F_{O}}}$ | node $i$ in an output
    graph at time $t$ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{Y}^{i}_{t}\in\mathbb{R}^{\mathbf{F_{O}}}$ | 时间$t$时输出图中的节点$i$ |'
- en: '| $\mathcal{Y}_{t,j}\in\mathbb{R}^{\mathbf{N}}$ | the $j^{th}$ feature of an
    output graph at time $t$ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{Y}_{t,j}\in\mathbb{R}^{\mathbf{N}}$ | 时间$t$时输出图的第$j^{th}$特征 |'
- en: '| $\mathcal{Y}^{i}_{t,j}\in\mathbb{R}$ | the $j^{th}$ feature of node $i$ in
    an output graph at time $t$ |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{Y}^{i}_{t,j}\in\mathbb{R}$ | 时间$t$时输出图中节点$i$的第$j^{th}$特征 |'
- en: IV-A Notations
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 符号
- en: In this section, we have denoted some commonly used notations, including graph
    related elements, variables, parameters (hyper or trainable), activation functions,
    and operations. The variables are comprised of input variables {$x$, $X$, $\mathbf{x}$,
    $\mathbf{X}$, $\mathcal{X}$} and output variables {$y$, $Y$, $\mathbf{y}$, $\mathbf{Y}$,
    $\mathcal{Y}$}. These variables can divided into spatial variables, temporal variables,
    spatiotemporal variables. The spatial variables are only related with spatial
    attributes and the temporal variables are only related with temporal attributes.
    The spatiotemporal variables are related with both spatial and temporal attributes.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已标记一些常用符号，包括图相关元素、变量、参数（超参数或可训练的）、激活函数和操作。变量包括输入变量{$x$, $X$, $\mathbf{x}$,
    $\mathbf{X}$, $\mathcal{X}$}和输出变量{$y$, $Y$, $\mathbf{y}$, $\mathbf{Y}$, $\mathcal{Y}$}。这些变量可以分为空间变量、时间变量和空间-时间变量。空间变量仅与空间属性相关，时间变量仅与时间属性相关，而空间-时间变量与空间和时间属性都相关。
- en: IV-B Graph-based Spatio-Temporal Forecasting
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 基于图的空间-时间预测
- en: To our best knowledge, most existing graph-based deep learning traffic works
    can be categorized into spatial-temporal forecasting due to that most traffic
    datasets have both spatial attributes and temporal attributes. They formalize
    their prediction problems in a very similar way despite different mathematical
    notations and representations. We summarize their works to provide a general formulation
    for graph-based spatial-temporal problems in traffic domain.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们了解，大多数现有的基于图的深度学习交通工作可以归类为空间-时间预测，因为大多数交通数据集同时具有空间属性和时间属性。尽管数学符号和表示方式不同，它们以非常相似的方式形式化其预测问题。我们总结了它们的工作，以提供一种通用的图形空间-时间问题的公式化方法。
- en: The traffic network is represented as a graph $\mathbf{G}=(\mathbf{V},\mathbf{E},\mathbf{A})$,
    which can be weighted [[92](#bib.bib92)],[[72](#bib.bib72)],[[68](#bib.bib68)]
    or unweighted [[66](#bib.bib66)],[[93](#bib.bib93)],[[94](#bib.bib94)], directed
    [[66](#bib.bib66)],[[95](#bib.bib95)],[[96](#bib.bib96)] or undirected [[69](#bib.bib69)],[[97](#bib.bib97)],
    depending on specific tasks. $\mathbf{V}$ is a set of nodes and $|\mathbf{V}|=\mathbf{N}$
    refers $\mathbf{N}$ nodes in the graph. Each node represents a traffic object,
    which can be a sensor [[70](#bib.bib70)],[[69](#bib.bib69)],[[98](#bib.bib98)],
    a road segment [[92](#bib.bib92)],[[99](#bib.bib99)],[[100](#bib.bib100)], a road
    intersection [[72](#bib.bib72)],[[95](#bib.bib95)], [[68](#bib.bib68)]. $\mathbf{E}$
    is a set of edges referring the connectivity between nodes.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 交通网络表示为一个图 $\mathbf{G}=(\mathbf{V},\mathbf{E},\mathbf{A})$，可以是加权的 [[92](#bib.bib92)],
    [[72](#bib.bib72)], [[68](#bib.bib68)] 或非加权的 [[66](#bib.bib66)], [[93](#bib.bib93)],
    [[94](#bib.bib94)]，有向的 [[66](#bib.bib66)], [[95](#bib.bib95)], [[96](#bib.bib96)]
    或无向的 [[69](#bib.bib69)], [[97](#bib.bib97)]，这取决于具体任务。 $\mathbf{V}$ 是节点的集合，$|\mathbf{V}|=\mathbf{N}$
    表示图中的$\mathbf{N}$个节点。每个节点代表一个交通对象，可以是传感器 [[70](#bib.bib70)], [[69](#bib.bib69)],
    [[98](#bib.bib98)]，一个道路段 [[92](#bib.bib92)], [[99](#bib.bib99)], [[100](#bib.bib100)]，一个道路交叉口
    [[72](#bib.bib72)], [[95](#bib.bib95)], [[68](#bib.bib68)]。$\mathbf{E}$ 是表示节点之间连通性的边的集合。
- en: $\mathbf{A}=(\mathbf{a}_{ij})_{\mathbf{N}\times\mathbf{N}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$
    is the adjacency matrix containing the topology information of the traffic network,
    which is valuable for traffic prediction. The entry $\mathbf{a}_{ij}$ in matrix
    $\mathbf{A}$ represents the node proximity and is different in various applications.
    It can be a binary value $0$ or $1$ [[69](#bib.bib69)],[[93](#bib.bib93)],[[94](#bib.bib94)].
    Specifically, $0$ indicates no edge between node $i$ and node $j$ while $1$ indicates
    an edge between these two nodes. It can also be a float value representing some
    kind of relationship between nodes [[92](#bib.bib92)],[[101](#bib.bib101)], e.g.
    the road distance between two sensors [[70](#bib.bib70)],[[102](#bib.bib102)],[[96](#bib.bib96)].
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathbf{A}=(\mathbf{a}_{ij})_{\mathbf{N}\times\mathbf{N}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$
    是包含交通网络拓扑信息的邻接矩阵，这对交通预测非常有价值。矩阵$\mathbf{A}$中的条目$\mathbf{a}_{ij}$表示节点的接近度，在不同的应用中有所不同。它可以是二进制值$0$或$1$
    [[69](#bib.bib69)], [[93](#bib.bib93)], [[94](#bib.bib94)]。具体而言，$0$表示节点$i$和节点$j$之间没有边，而$1$表示这两个节点之间有边。它也可以是浮点值，表示节点之间某种关系
    [[92](#bib.bib92)], [[101](#bib.bib101)]，例如两个传感器之间的道路距离 [[70](#bib.bib70)], [[102](#bib.bib102)],
    [[96](#bib.bib96)]。
- en: $\mathcal{X}_{t}=[\mathcal{X}_{t}^{1},\cdots,\mathcal{X}_{t}^{i},\cdots,\mathcal{X}_{t}^{\mathbf{N}}]\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{I}}}$
    is a feature matrix of the whole graph at time $t$. $\mathcal{X}^{i}_{t}\in\mathbb{R}^{\mathbf{F_{I}}}$
    represents node $i$ with $\mathbf{F_{I}}$ features at time $t$. The features are
    usually traffic indicators, such as traffic flow [[97](#bib.bib97)],[[96](#bib.bib96)],
    traffic speed [[70](#bib.bib70)],[[99](#bib.bib99)],[[95](#bib.bib95)], or rail-hail
    orders [[89](#bib.bib89)],[[101](#bib.bib101)], passenger flow [[77](#bib.bib77)],[[78](#bib.bib78)].
    Usually, continuous indicators are normalized during data preprocessing phase.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{X}_{t}=[\mathcal{X}_{t}^{1},\cdots,\mathcal{X}_{t}^{i},\cdots,\mathcal{X}_{t}^{\mathbf{N}}]\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{I}}}$
    是时间$t$时刻的整个图的特征矩阵。 $\mathcal{X}^{i}_{t}\in\mathbb{R}^{\mathbf{F_{I}}}$ 代表时间$t$时刻具有$\mathbf{F_{I}}$个特征的节点$i$。这些特征通常是交通指标，如交通流量
    [[97](#bib.bib97)], [[96](#bib.bib96)], 交通速度 [[70](#bib.bib70)], [[99](#bib.bib99)],
    [[95](#bib.bib95)], 或铁路运单 [[89](#bib.bib89)], [[101](#bib.bib101)], 乘客流量 [[77](#bib.bib77)],
    [[78](#bib.bib78)]。通常，连续指标在数据预处理阶段会进行归一化处理。
- en: 'Given historical indicators of the whole traffic network over past $\mathbf{P}$
    time slices, denoted as $\mathcal{X}=[\mathcal{X}_{1},\cdots,\mathcal{X}_{t},\cdots,\mathcal{X}_{\mathbf{P}}]\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{I}}}$,
    the spatial-temporal forecasting problem in traffic domain aims to predict the
    future traffic indicators over the next $\mathbf{Q}$ time slices, denoted as $\mathcal{Y}=[\mathcal{Y}_{1},\cdots,\mathcal{Y}_{t},\cdots,\mathcal{Y}_{\mathbf{Q}}]\in\mathbb{R}^{\mathbf{Q}\times\mathbf{N}\times\mathbf{F_{O}}}$,
    where $\mathcal{Y}_{t}\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{O}}}$ represents
    output graph with $\mathbf{F_{O}}$ features at time $t$. The problem (as shown
    in Figure [3](#S4.F3 "Figure 3 ‣ IV-B Graph-based Spatio-Temporal Forecasting
    ‣ IV Problem Formulation and Graph Construction ‣ How to Build a Graph-Based Deep
    Learning Architecture in Traffic Domain: A Survey")) can be formulated as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '给定过去 $\mathbf{P}$ 个时间切片的整个交通网络历史指标，记作 $\mathcal{X}=[\mathcal{X}_{1},\cdots,\mathcal{X}_{t},\cdots,\mathcal{X}_{\mathbf{P}}]\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{I}}}$，交通领域中的时空预测问题旨在预测未来
    $\mathbf{Q}$ 个时间切片的交通指标，记作 $\mathcal{Y}=[\mathcal{Y}_{1},\cdots,\mathcal{Y}_{t},\cdots,\mathcal{Y}_{\mathbf{Q}}]\in\mathbb{R}^{\mathbf{Q}\times\mathbf{N}\times\mathbf{F_{O}}}$，其中
    $\mathcal{Y}_{t}\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{O}}}$ 表示时间 $t$ 处具有 $\mathbf{F_{O}}$
    特征的输出图。该问题（如图 [3](#S4.F3 "Figure 3 ‣ IV-B Graph-based Spatio-Temporal Forecasting
    ‣ IV Problem Formulation and Graph Construction ‣ How to Build a Graph-Based Deep
    Learning Architecture in Traffic Domain: A Survey") 所示）可以表述如下：'
- en: '|  | $\mathcal{Y}=f(\mathcal{X};\mathbf{G})$ |  | (1) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{Y}=f(\mathcal{X};\mathbf{G})$ |  | (1) |'
- en: '![Refer to caption](img/0af64006d3a74e004443f8d9429b10ed.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0af64006d3a74e004443f8d9429b10ed.png)'
- en: 'Figure 3: The graph-based spatial-temporal problem formulation in traffic domain'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：交通领域中的图基时空问题表述
- en: Some works predict multiple traffic indicators in the future (i.e. $\mathbf{F_{O}}>1$)
    while other works predict one traffic indicator (i.e. $\mathbf{F_{O}}=1$), such
    as traffic speed [[99](#bib.bib99)], [[95](#bib.bib95)], rail-hide orders [[89](#bib.bib89)],[[101](#bib.bib101)].
    Some works only consider one-step prediction [[103](#bib.bib103)],[[75](#bib.bib75)],[[57](#bib.bib57)],
    i.e. forecasting traffic conditions in the next time step and $\mathbf{Q}=1$.
    But models designed for one-step prediction can’t be directly applied to predict
    multiple steps, because they are optimized by reducing error during the training
    stage for the next-step instead of the subsequent time steps [[76](#bib.bib76)].
    Many works focus on multi-step forecasting (i.e. $\mathbf{Q}>1$) [[104](#bib.bib104)],[[42](#bib.bib42)],[[105](#bib.bib105)].
    According to our survey, there are mainly three kinds of techniques to generate
    a multi-step output, i.e. FC layer, Seq2Seq, dilation technique. Fully connected
    (FC) layer is the simplest technique as being the output layer to obtain a desired
    output shape [[70](#bib.bib70)], [[69](#bib.bib69)], [[106](#bib.bib106)], [[93](#bib.bib93)],
    [[91](#bib.bib91)], [[107](#bib.bib107)]. Some works adopt the Sequence to Sequence
    (Seq2Seq) architecture with a RNNs-based decoder to generate output recursively
    through multiple steps [[108](#bib.bib108)],[[98](#bib.bib98)],[[109](#bib.bib109)],[[104](#bib.bib104)],[[110](#bib.bib110)],[[96](#bib.bib96)].
    Dilation technique is adopted to get a desired output length [[102](#bib.bib102)],
    [[105](#bib.bib105)].
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究预测未来的多个交通指标（即 $\mathbf{F_{O}}>1$），而其他研究预测单一交通指标（即 $\mathbf{F_{O}}=1$），例如交通速度
    [[99](#bib.bib99)], [[95](#bib.bib95)], 铁路隐藏订单 [[89](#bib.bib89)], [[101](#bib.bib101)]。一些研究仅考虑一步预测
    [[103](#bib.bib103)], [[75](#bib.bib75)], [[57](#bib.bib57)]，即预测下一时间步的交通状况，并且
    $\mathbf{Q}=1$。但是，为一步预测设计的模型不能直接应用于多步预测，因为它们是在训练阶段通过减少下一步的误差来优化的，而不是随后的时间步 [[76](#bib.bib76)]。许多研究集中于多步预测（即
    $\mathbf{Q}>1$） [[104](#bib.bib104)], [[42](#bib.bib42)], [[105](#bib.bib105)]。根据我们的调查，生成多步输出的主要有三种技术，即全连接（FC）层、Seq2Seq
    和扩张技术。全连接（FC）层是最简单的技术，它作为输出层来获得所需的输出形状 [[70](#bib.bib70)], [[69](#bib.bib69)],
    [[106](#bib.bib106)], [[93](#bib.bib93)], [[91](#bib.bib91)], [[107](#bib.bib107)]。一些研究采用了基于
    RNN 的 Seq2Seq 架构，通过多个步骤递归生成输出 [[108](#bib.bib108)], [[98](#bib.bib98)], [[109](#bib.bib109)],
    [[104](#bib.bib104)], [[110](#bib.bib110)], [[96](#bib.bib96)]。扩张技术则被用来获得所需的输出长度
    [[102](#bib.bib102)], [[105](#bib.bib105)]。
- en: '![Refer to caption](img/b2b985f7fa0050c683c29a5f25dacf96.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b2b985f7fa0050c683c29a5f25dacf96.png)'
- en: 'Figure 4: Graph construction from various traffic datasets: a) In a sensor
    graph, sensor represents node and there is an edge between adjacent sensors on
    the same side of a road. b) In a road segment graph, road segment represents node
    and two connected segments have an edge. c) In a road intersection graph, road
    intersection represents node and there is an edge between two road intersections
    connected by a road segment. Most works consider the edge direction being the
    traffic flow direction[[70](#bib.bib70)],[[98](#bib.bib98)],[[66](#bib.bib66)],[[96](#bib.bib96)],[[68](#bib.bib68)],[[111](#bib.bib111)],
    while some works ignore the direction and construct an undirected graph [[69](#bib.bib69)],[[102](#bib.bib102)],[[94](#bib.bib94)][[100](#bib.bib100)],[[95](#bib.bib95)].'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：来自各种交通数据集的图构建：a) 在传感器图中，传感器表示节点，且在道路同侧的相邻传感器之间有一条边。b) 在道路段图中，道路段表示节点，相连的两个段之间有一条边。c)
    在道路交叉口图中，道路交叉口表示节点，且在由道路段连接的两个道路交叉口之间有一条边。大多数研究认为边的方向是交通流向[[70](#bib.bib70)],[[98](#bib.bib98)],[[66](#bib.bib66)],[[96](#bib.bib96)],[[68](#bib.bib68)],[[111](#bib.bib111)]，而有些研究忽略方向，构建无向图[[69](#bib.bib69)],[[102](#bib.bib102)],[[94](#bib.bib94)][[100](#bib.bib100)],[[95](#bib.bib95)]。
- en: 'In addition, some works not only consider traffic indicators, but also take
    external factors (e.g. time attributes, weather) [[70](#bib.bib70)],[[112](#bib.bib112)],[[91](#bib.bib91)],[[113](#bib.bib113)]
    into consideration. Therefore, the problem formulation becomes:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些研究不仅考虑交通指标，还考虑外部因素（例如时间属性、天气）[[70](#bib.bib70)],[[112](#bib.bib112)],[[91](#bib.bib91)],[[113](#bib.bib113)]。因此，问题的表述变为：
- en: '|  | $\mathcal{Y}=f(\mathcal{X},\mathcal{E};\mathbf{G})$ |  | (2) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{Y}=f(\mathcal{X},\mathcal{E};\mathbf{G})$ |  | (2) |'
- en: where $\mathcal{E}$ is the external factors.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{E}$是外部因素。
- en: IV-C Graph Construction from Traffic Datasets
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 来自交通数据集的图构建
- en: To model a traffic network as a graph is vital for any works that intend to
    utilize graph-based deep learning architectures to solve traffic problems. A traffic
    graph $\mathbf{G}$ for prediction is generally composed of four parts, i.e. nodes
    $\mathbf{V}$, node features (feature matrix $\mathcal{X}_{t}$), edges $\mathbf{E}$,
    edge weight $\mathbf{a}_{ij}$. Note that edges and edge weight can be represented
    by adjacency matrix $\mathbf{A}=(\mathbf{a}_{ij})_{\mathbf{N}\times\mathbf{N}}$.
    Nodes and node features can be constructed from traffic datasets. The construction
    of adjacency matrix not only depends on traffic datasets but also depends on the
    assumption of node relationship, which can be static or dynamic. We first introduce
    how to construct node and node features from various kinds of traffic datasets
    and then we give a systematic introduction to the popular adjacency matrices.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 将交通网络建模为图对于任何打算利用图基深度学习架构解决交通问题的研究来说都是至关重要的。用于预测的交通图$\mathbf{G}$通常由四部分组成，即节点$\mathbf{V}$、节点特征（特征矩阵$\mathcal{X}_{t}$）、边$\mathbf{E}$、边权重$\mathbf{a}_{ij}$。请注意，边和边权重可以通过邻接矩阵$\mathbf{A}=(\mathbf{a}_{ij})_{\mathbf{N}\times\mathbf{N}}$表示。节点和节点特征可以从交通数据集中构建。邻接矩阵的构建不仅依赖于交通数据集，还依赖于节点关系的假设，这可以是静态的或动态的。我们首先介绍如何从各种交通数据集中构建节点和节点特征，然后系统地介绍流行的邻接矩阵。
- en: IV-C1 Nodes and Node Features Construction
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 节点和节点特征构建
- en: 'Many works are different in graph construction due to the different traffic
    datasets they collect. We divide these datasets into four categories according
    to the traffic infrastructures: data collected by the sensors deployed on road
    network [[70](#bib.bib70)],[[69](#bib.bib69)],[[71](#bib.bib71)], vehicle GPS
    trajectories [[68](#bib.bib68)],[[111](#bib.bib111)],[[95](#bib.bib95)], orders
    of rail-hailing system [[101](#bib.bib101)],[[76](#bib.bib76)],[[113](#bib.bib113)],
    transaction records of subway system [[77](#bib.bib77)],[[78](#bib.bib78)] or
    bus system [[111](#bib.bib111)]. For each category, we describe the datasets and
    explain the construction of nodes $\mathbf{V}$, feature matrix $\mathcal{X}_{t}$.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所收集的交通数据集不同，许多研究在图构建上有所不同。我们根据交通基础设施将这些数据集分为四类：由道路网络上部署的传感器收集的数据[[70](#bib.bib70)],[[69](#bib.bib69)],[[71](#bib.bib71)]，车辆GPS轨迹[[68](#bib.bib68)],[[111](#bib.bib111)],[[95](#bib.bib95)]，铁路叫车系统的订单[[101](#bib.bib101)],[[76](#bib.bib76)],[[113](#bib.bib113)]，地铁系统的交易记录[[77](#bib.bib77)],[[78](#bib.bib78)]或公交系统[[111](#bib.bib111)]。对于每一类，我们描述数据集并解释节点$\mathbf{V}$、特征矩阵$\mathcal{X}_{t}$的构建。
- en: Sensors Datasets Traffic measurements (e.g. traffic speed) are generally collected
    during a short time interval by the sensors (e.g. loop detectors, probes) on a
    road network in metropolises like Beijing [[92](#bib.bib92)], California [[71](#bib.bib71)],
    Los Angeles [[70](#bib.bib70)], New York [[99](#bib.bib99)], Philadelphia [[106](#bib.bib106)],
    Seattle [[94](#bib.bib94)], Xiamen [[98](#bib.bib98)], and Washington [[106](#bib.bib106)].
    Sensor datasets are the most prevalent datasets in existing works, especially
    PEMS dataset from California. Generally, a road network contains traffic objects
    such as sensors, road segments.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器数据集 交通测量（例如交通速度）通常在大城市（如北京 [[92](#bib.bib92)]、加州 [[71](#bib.bib71)]、洛杉矶 [[70](#bib.bib70)]、纽约
    [[99](#bib.bib99)]、费城 [[106](#bib.bib106)]、西雅图 [[94](#bib.bib94)]、厦门 [[98](#bib.bib98)]
    和华盛顿 [[106](#bib.bib106)]）的道路网络上由传感器（例如环形探测器、探针）在短时间间隔内收集。传感器数据集是现有研究中最为普遍的数据集，特别是来自加州的
    PEMS 数据集。通常，道路网络包含交通对象，如传感器、路段。
- en: 'A sensor graph (as shown in Figure [4](#S4.F4 "Figure 4 ‣ IV-B Graph-based
    Spatio-Temporal Forecasting ‣ IV Problem Formulation and Graph Construction ‣
    How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey"))
    is constructed in [[70](#bib.bib70)],[[69](#bib.bib69)],[[96](#bib.bib96)] where
    a sensor represents a node and features of this node are traffic measurements
    collected by its corresponding sensor.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '传感器图（如图 [4](#S4.F4 "Figure 4 ‣ IV-B Graph-based Spatio-Temporal Forecasting
    ‣ IV Problem Formulation and Graph Construction ‣ How to Build a Graph-Based Deep
    Learning Architecture in Traffic Domain: A Survey") 所示）在 [[70](#bib.bib70)]，[[69](#bib.bib69)]，[[96](#bib.bib96)]
    中被构建，其中传感器表示一个节点，该节点的特征是其对应传感器收集的交通测量值。'
- en: 'A road segment graph (as shown in Figure [4](#S4.F4 "Figure 4 ‣ IV-B Graph-based
    Spatio-Temporal Forecasting ‣ IV Problem Formulation and Graph Construction ‣
    How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey"))
    is constructed in [[92](#bib.bib92)],[[99](#bib.bib99)],[[106](#bib.bib106)] where
    a road segment represents a node and features of this node are average traffic
    measurements (e.g. traffic speed) recorded by all the sensors on its corresponding
    road segment.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '路段图（如图 [4](#S4.F4 "Figure 4 ‣ IV-B Graph-based Spatio-Temporal Forecasting
    ‣ IV Problem Formulation and Graph Construction ‣ How to Build a Graph-Based Deep
    Learning Architecture in Traffic Domain: A Survey") 所示）在 [[92](#bib.bib92)]，[[99](#bib.bib99)]，[[106](#bib.bib106)]
    中被构建，其中路段表示一个节点，该节点的特征是其对应路段上所有传感器记录的平均交通测量值（例如交通速度）。'
- en: GPS Datasets GPS trajectories datasets are usually generated by numbers of taxis
    over some period of time in a city, e.g. Beijing [[68](#bib.bib68)], Chengdu [[68](#bib.bib68)],
    Shenzhen [[93](#bib.bib93)], Cologne [[95](#bib.bib95)], and Chicago [[100](#bib.bib100)].
    Each taxi produces substantial GPS records with time, location, speed information
    every day. Every GPS record is fitted to its nearest road on the city road map.
    All roads are divided into multiple road segments through road intersections.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: GPS 数据集 GPS 轨迹数据集通常由城市中大量出租车在一段时间内生成，例如北京 [[68](#bib.bib68)]、成都 [[68](#bib.bib68)]、深圳
    [[93](#bib.bib93)]、科隆 [[95](#bib.bib95)] 和芝加哥 [[100](#bib.bib100)]。每辆出租车每天生成大量的
    GPS 记录，包括时间、位置、速度信息。每条 GPS 记录都被拟合到城市道路地图上最近的道路上。所有道路通过道路交叉口被划分为多个路段。
- en: 'A road segment graph (as shown in Figure [4](#S4.F4 "Figure 4 ‣ IV-B Graph-based
    Spatio-Temporal Forecasting ‣ IV Problem Formulation and Graph Construction ‣
    How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey"))
    is constructed in [[100](#bib.bib100)], [[93](#bib.bib93)] where a road segment
    represents a node and features of this node are average traffic measurements recorded
    by all the GPS points on its corresponding road segment.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '路段图（如图 [4](#S4.F4 "Figure 4 ‣ IV-B Graph-based Spatio-Temporal Forecasting
    ‣ IV Problem Formulation and Graph Construction ‣ How to Build a Graph-Based Deep
    Learning Architecture in Traffic Domain: A Survey") 所示）在 [[100](#bib.bib100)]，[[93](#bib.bib93)]
    中被构建，其中路段表示一个节点，该节点的特征是其对应路段上所有 GPS 点记录的平均交通测量值。'
- en: 'A road intersection graph (as shown in Figure [4](#S4.F4 "Figure 4 ‣ IV-B Graph-based
    Spatio-Temporal Forecasting ‣ IV Problem Formulation and Graph Construction ‣
    How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey"))
    is constructed in [[72](#bib.bib72)],[[68](#bib.bib68)],[[95](#bib.bib95)] where
    a road intersection represents a node and features of this node are sum-up of
    the traffic measurements through it.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '道路交叉口图（如图 [4](#S4.F4 "Figure 4 ‣ IV-B Graph-based Spatio-Temporal Forecasting
    ‣ IV Problem Formulation and Graph Construction ‣ How to Build a Graph-Based Deep
    Learning Architecture in Traffic Domain: A Survey") 所示）在 [[72](#bib.bib72)]，[[68](#bib.bib68)]，[[95](#bib.bib95)]
    中被构建，其中道路交叉口表示一个节点，该节点的特征是通过它的交通测量值的总和。'
- en: 'Rail-hailing Datasets These datasets record car/taxi/bicycle demand orders
    over a period of time in cities like Beijing [[89](#bib.bib89)],[[101](#bib.bib101)],
    Chengdu [[101](#bib.bib101)], and Shanghai [[89](#bib.bib89)], Manhattan, New
    York [[99](#bib.bib99)]. The target city with an OpenStreetMap is divided into
    equal-size grid-based regions (as shown in Figure [5](#S4.F5 "Figure 5 ‣ IV-C1
    Nodes and Node Features Construction ‣ IV-C Graph Construction from Traffic Datasets
    ‣ IV Problem Formulation and Graph Construction ‣ How to Build a Graph-Based Deep
    Learning Architecture in Traffic Domain: A Survey")). Each region is defined as
    a node in a graph. The feature of each node is the number of orders in the corresponding
    region during a given interval.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 铁路召车数据集 这些数据集记录了在北京[[89](#bib.bib89)],[[101](#bib.bib101)], 成都[[101](#bib.bib101)]和上海[[89](#bib.bib89)],
    曼哈顿, 纽约[[99](#bib.bib99)]等城市的一段时间内的汽车/出租车/自行车需求订单。目标城市使用OpenStreetMap被划分为相等大小的网格区域（如图[5](#S4.F5
    "图 5 ‣ IV-C1 节点及节点特征构建 ‣ IV-C 图构建来自交通数据集 ‣ IV 问题表述和图构建 ‣ 如何在交通领域构建基于图的深度学习架构：综述")所示）。每个区域定义为图中的一个节点。每个节点的特征是在给定时间间隔内相应区域的订单数量。
- en: '![Refer to caption](img/0e06f62f993496e47efadcc8c998b317.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0e06f62f993496e47efadcc8c998b317.png)'
- en: 'Figure 5: Multi-relationships: a) A spatial locality graph: This graph is based
    on spatial proximity and it constructs edges between a region and its 8 adjacent
    regions in a 3 x 3 grid. b) A functional similarity graph: This graph assumes
    that regions sharing similar functionality might have similar demand patterns.
    Edges are constructed between regions with similar surrounding POIs (Point of
    Interests). c) A transportation connectivity graph: This graph assumes that regions
    which are geographically distant from the target region but conveniently reachable
    by transportation (e.g. motorway, highway or subway) have strong correlations
    with the target region. There should be edges between them.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：多重关系：a) 空间局部性图：该图基于空间接近性，并在一个3 x 3网格中构建区域与其8个相邻区域之间的边。b) 功能相似性图：该图假设共享相似功能的区域可能具有类似的需求模式。边在具有类似周边兴趣点（POI）的区域之间构建。c)
    交通连接性图：该图假设地理上与目标区域距离较远但通过交通（如高速公路、高速公路或地铁）便捷到达的区域与目标区域具有较强的相关性。它们之间应存在边。
- en: Transactions Datasets These datasets are collected by automatic fare collection
    (AFC) system deployed in public transit network, such as subway network and bus
    network. A subway graph is constructed in [[77](#bib.bib77)],[[78](#bib.bib78)],[[111](#bib.bib111)].
    Each station in the subway system is treated as a node. The features of a station
    usually contain the number of passengers departing at the station and the number
    of passengers arriving at the station during a given time interval based on transaction
    records collected by subwayAFC systems, which log when each passenger enters and
    leaves a metro system.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 交易数据集 这些数据集由自动票务收集（AFC）系统在公共交通网络中收集，例如地铁网络和公交网络。在[[77](#bib.bib77)],[[78](#bib.bib78)],[[111](#bib.bib111)]中构建了地铁图。地铁系统中的每个车站被视为一个节点。车站的特征通常包括在给定时间间隔内从车站离开的乘客数量和到达车站的乘客数量，基于地铁AFC系统收集的交易记录，这些系统记录了每个乘客何时进入和离开地铁系统。
- en: A bus graph is constructed in [[111](#bib.bib111)]. Each bus stop is treated
    as a node. The features of a bus stop usually contain the number of departing
    passengers at the station during a given time interval, but not the number of
    arriving passengers, since most bus AFC systems only log the boarding record of
    each passenger.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[111](#bib.bib111)]中构建了公交图。每个公交站被视为一个节点。公交站的特征通常包括在给定时间间隔内离开车站的乘客数量，但不包括到达乘客的数量，因为大多数公交AFC系统仅记录每个乘客的登车记录。
- en: IV-C2 Adjacency Matrix Construction
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 邻接矩阵构建
- en: The adjacency matrix $\mathbf{A}=(\mathbf{a}_{ij})_{\mathbf{N}\times\mathbf{N}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$
    is the key to capture spatial dependency which is valuable for prediction. Element
    $\mathbf{a}_{ij}$ (unweighted or weighted) represents heterogeneous pairwise relationship
    between nodes. However, there are different assumptions of node relationships
    in different traffic scenarios, based on which the adjacency matrix can be designed
    differently, e.g. fixed matrix, dynamic matrix, evolving matrix.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Fixed Matrix Many works assume that the correlations between nodes are fixed
    and do not change over time. Therefore, a fixed matrix is designed and unchanged
    during the whole experiment. Researchers have designed various fixed adjacency
    matrices to capture various kinds of pre-defined correlations between nodes in
    a traffic graph, like function similarity and transportation connectivity [[89](#bib.bib89)],
    semantic connection [[101](#bib.bib101)], temporal similarity [[71](#bib.bib71)].
    Here, we introduce several popular adjacency matrices.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Connection matrix measures the connectivity between nodes. The entry value in
    the matrix is defined as $1$ (connection) or $0$ (disconnection) [[69](#bib.bib69)],[[106](#bib.bib106)],[[93](#bib.bib93)],[[94](#bib.bib94)].
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Distance matrix measures the closeness between nodes in terms of geometrical
    distance. The entry value is defined as a function of distance between nodes [[85](#bib.bib85)].
    For example, some works [[72](#bib.bib72)],[[68](#bib.bib68)],[[100](#bib.bib100)],[[97](#bib.bib97)],[[76](#bib.bib76)],[[95](#bib.bib95)]
    used threshold Gaussian Kernel to define $\mathbf{a}_{ij}$ as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{a}_{ij}=\left\{\begin{array}[]{l}\exp\left(-\frac{\mathbf{d}_{ij}^{2}}{\sigma^{2}}\right),i\neq
    j\text{ and }\mathbf{d}_{ij}\geq\epsilon\\ 0\quad,i=j\text{ or }\mathbf{d}_{ij}<\epsilon\end{array}\right.$
    |  | (3) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{d}_{ij}$ is the distance between node $i$ and node $j$. Hyper
    parameters $\sigma^{2}$ and $\epsilon$ are thresholds to control the distribution
    and sparsity of matrix $\mathbf{A}$.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Functional similarity matrix measures whether two nodes are similar in terms
    of functionality (e.g. both of them are business zones). The corresponding functional
    similarity graph is shown in Figure [5](#S4.F5 "Figure 5 ‣ IV-C1 Nodes and Node
    Features Construction ‣ IV-C Graph Construction from Traffic Datasets ‣ IV Problem
    Formulation and Graph Construction ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey"). It assumes that regions sharing similar
    functionality might have similar demand patterns [[89](#bib.bib89)]. Edges are
    constructed between regions with similar surrounding POIs (Point of Interests).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Transportation connectivity matrix measures the correlation between regions
    that are geographically distant but conveniently reachable by motorway, highway
    or subway. The corresponding transportation connectivity graph is shown in Figure
    [5](#S4.F5 "Figure 5 ‣ IV-C1 Nodes and Node Features Construction ‣ IV-C Graph
    Construction from Traffic Datasets ‣ IV Problem Formulation and Graph Construction
    ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey").
    There should be edges between them [[89](#bib.bib89)].'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Matrix Some works argue that the pre-defined matrix does not necessarily
    reflect the true dependency among nodes due to the defective prior knowledge or
    incomplete data [[72](#bib.bib72)]. A novel adaptive matrix is proposed and learned
    through data. Experiments in [[102](#bib.bib102)],[[72](#bib.bib72)],[[99](#bib.bib99)]
    have proven that adaptive matrix can precisely capture the hidden spatial dependency
    more precisely in some traffic tasks.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Evolving Matrix In some scenarios, the graph structure can evolve over time
    as some edges may become unavailable, like road congestion or closure, and become
    available again after alleviating congestion. An evolving topological structure
    [[66](#bib.bib66)], [[114](#bib.bib114)] is incorporated into the model to capture
    such dynamic spatial change.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: V Deep Learning Techniques Perspective
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE II: The decomposition of graph-based deep learning architectures investigated
    in this paper'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Year | Directions | Models | Modules |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| [[83](#bib.bib83)] | 2019 | Human/Vehicle Trajectory Prediction | SAGCN |
    SGCN, TCN, Attention |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| [[87](#bib.bib87)] | 2019 | Human/Vehicle Trajectory Prediction | Social-BiGAT
    | GAT, LSTM, GAN |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| [[85](#bib.bib85)] | 2020 | Human/Vehicle Trajectory Prediction | Social-STGCNN
    | SGCN, TCN |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| [[64](#bib.bib64)] | 2020 | Human/Vehicle Trajectory Prediction | Social-WaGDAT
    | GAT, Seq2Seq, MLP |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| [[88](#bib.bib88)] | 2020 | Human/Vehicle Trajectory Prediction |  | SGCN,
    LSTM |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| [[57](#bib.bib57)] | 2019 | Optimal DETC Scheme |  | SGCN |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| [[65](#bib.bib65)] | 2020 | Vehicle Behaviour Classification | MR-GCN | SGCN,
    LSTM |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| [[67](#bib.bib67)] | 2018 | Traffic Signal Control |  | SGCN, Reinforcement
    Learning |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| [[66](#bib.bib66)] | 2019 | Path Availability | LRGCN-SAPE | SGCN, LSTM |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| [[72](#bib.bib72)] | 2019 | Travel Time Prediction |  | SGCN |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| [[68](#bib.bib68)] | 2018 | Traffic Flow Prediction | KW-GCN | SGCN, LCN
    |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| [[97](#bib.bib97)] | 2018 | Traffic Flow Prediction | Graph-CNN | CNN, Graph
    Matrix |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| [[115](#bib.bib115)] | 2018 | Traffic Flow Prediction | DST-GCNN | SGCN |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| [[69](#bib.bib69)] | 2019 | Traffic Flow Prediction |  | SGCN, CNN, Attention
    Mechanism |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| [[111](#bib.bib111)] | 2019 | Traffic Flow Prediction |  | SGCN, TCN, Residual
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| [[109](#bib.bib109)] | 2019 | Traffic Flow Prediction | GHCRNN | SGCN, GRU,
    Seq2Seq |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| [[104](#bib.bib104)] | 2019 | Traffic Flow Prediction | STGSA | GAT, GRU,
    Seq2Seq |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| [[96](#bib.bib96)] | 2019 | Traffic Flow Prediction | DCRNN-RIL | DGCN, GRU,
    Seq2Seq |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| [[116](#bib.bib116)] | 2019 | Traffic Flow Prediction | MVGCN | SGCN, FNN,
    Gate Mechanism, Residual |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| [[117](#bib.bib117)] | 2019 | Traffic Flow Prediction | STGI- ResNet | SGCN,
    Residual |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| [[118](#bib.bib118)] | 2020 | Traffic Flow Prediction | FlowConvGRU | DGCN,
    GRU |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| [[45](#bib.bib45)] | 2020 | Traffic Flow Prediction | Multi-STGCnet | SGCN,
    LSTM |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| [[119](#bib.bib119)] | 2018 | Traffic Speed Prediction |  | GAT, GRU, Gate
    Mechanism |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| [[70](#bib.bib70)] | 2019 | Traffic Speed Prediction | GTCN | SGCN, TCN,
    Residual |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| [[71](#bib.bib71)] | 2019 | Traffic Speed Prediction | 3D-TGCN | SGCN, Gate
    Mechanism |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| [[91](#bib.bib91)] | 2019 | Traffic Speed Prediction | DIGC-Net | SGCN, LSTM
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| [[120](#bib.bib120)] | 2019 | Traffic Speed Prediction | MW-TGC | SGCN, LSTM
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| [[110](#bib.bib110)] | 2019 | Traffic Speed Prediction | AGC-Seq2Seq | SGCN,
    GRU, Seq2Seq, Attention Mechanism |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| [[95](#bib.bib95)] | 2019 | Traffic Speed Prediction | GCGA | SGCN, GAN |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| [[107](#bib.bib107)] | 2019 | Traffic Speed Prediction | ST-GAT | GAT, LSTM
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| [[92](#bib.bib92)] | 2018 | Traffic State Prediction | STGCN | SGCN, TCN,
    Gate Mechanism |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| [[108](#bib.bib108)] | 2018 | Traffic State Prediction | DCRNN | DGCN, GRU,
    Seq2Seq |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| [[99](#bib.bib99)] | 2019 | Traffic State Prediction |  | SGCN, CNN, Gate
    Mechanism |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| [[112](#bib.bib112)] | 2019 | Traffic State Prediction | MRes-RGNN | DGCN,
    GRU, Residual, Gate Mechanism |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| [[100](#bib.bib100)] | 2019 | Traffic State Prediction | GCGAN | DGCN, LSTM,
    GAN, Seq2Seq, Attention Mechanism |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| [[102](#bib.bib102)] | 2019 | Traffic State Prediction | Graph WaveNet |
    DGCN, TCN, Residual, Gate Mechanism |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| [[93](#bib.bib93)] | 2019 | Traffic State Prediction | T-GCN | SGCN, GRU
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| [[94](#bib.bib94)] | 2019 | Traffic State Prediction | TGC-LSTM | SGCN, LSTM
    |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| [[41](#bib.bib41)] | 2019 | Traffic State Prediction | DualGraph | Seq2Seq,
    MLP, Graph Matirx |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| [[105](#bib.bib105)] | 2019 | Traffic State Prediction | ST-UNet | SGCN,
    GRU |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| [[98](#bib.bib98)] | 2020 | Traffic State Prediction | GMAN | GAT, Gate Mechanism,
    Seq2Seq, Attention Mechanism |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| [[106](#bib.bib106)] | 2020 | Traffic State Prediction | OGCRNN | SGCN, GRU,
    Attention Mechanism |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| [[42](#bib.bib42)] | 2020 | Traffic State Prediction | MRA-BGCN | SGCN, GRU,
    Seq2Seq, Attention Mechanism |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| [[48](#bib.bib48)] | 2018 | Travel Demand-Bike |  | SGCN, LSTM, Seq2Seq |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| [[49](#bib.bib49)] | 2018 | Travel Demand-Bike | GCNN-DDGF | SGCN, LSTM |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| [[77](#bib.bib77)] | 2020 | Travel Demand-Subway | PVCGN | SGCN, GRU, Seq2Seq,
    Attention Mechanism |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| [[78](#bib.bib78)] | 2019 | Travel Demand-Subway | WDGTC | Tensor Completion,
    Graph Matrix |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| [[89](#bib.bib89)] | 2019 | Travel Demand-Taxi | CGRNN | SGCN, RNN, Attention
    Mechanism, Gate Mechanism |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| [[101](#bib.bib101)] | 2019 | Travel Demand-Taxi | GEML | SGCN, LSTM |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| [[101](#bib.bib101)] | 2019 | 旅行需求-出租车 | GEML | SGCN, LSTM |'
- en: '| [[75](#bib.bib75)] | 2019 | Travel Demand-Taxi | MGCN | SGCN |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| [[75](#bib.bib75)] | 2019 | 旅行需求-出租车 | MGCN | SGCN |'
- en: '| [[76](#bib.bib76)] | 2019 | Travel Demand-Taxi | STG2Seq | SGCN, Seq2Seq,
    Attention Mechanism, Gate Mechanism, Residual |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| [[76](#bib.bib76)] | 2019 | 旅行需求-出租车 | STG2Seq | SGCN, Seq2Seq, 注意力机制, 门控机制,
    残差 |'
- en: '| [[113](#bib.bib113)] | 2019 | Travel Demand-Taxi |  | SGCN, LSTM, Seq2Seq
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| [[113](#bib.bib113)] | 2019 | 旅行需求-出租车 |  | SGCN, LSTM, Seq2Seq |'
- en: '| [[103](#bib.bib103)] | 2019 | Travel Demand-Taxi | ST-ED-RMGC | SGCN, LSTM,
    Seq2Seq, Residual |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| [[103](#bib.bib103)] | 2019 | 旅行需求-出租车 | ST-ED-RMGC | SGCN, LSTM, Seq2Seq,
    残差 |'
- en: 'We summarize the graph-based deep learning architectures in existing traffic
    literatures and find that most of them are composed of graph neural networks (GNNs)
    and other modules, such as recurrent neural networks (RNNs), temporal convolution
    network (TCN), Sequence to Sequence (Seq2Seq) model, generative adversarial network
    (GAN) (as shown in Table [II](#S5.T2 "TABLE II ‣ V Deep Learning Techniques Perspective
    ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")).
    It is the cooperation of GNNs and other deep learning techniques that achieves
    state-of-the-art performance in many traffic scenarios. This section aims to introduce
    the functionality, advantages, defects and variants of these techniques in traffic
    tasks, which can help participators understand how to utilize these deep learning
    techniques in traffic domain.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '我们总结了现有交通文献中的基于图的深度学习架构，发现其中大多数由图神经网络（GNNs）和其他模块组成，如递归神经网络（RNNs）、时间卷积网络（TCN）、序列到序列（Seq2Seq）模型、生成对抗网络（GAN）（如表格
    [II](#S5.T2 "TABLE II ‣ V Deep Learning Techniques Perspective ‣ How to Build
    a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey") 所示）。GNNs
    与其他深度学习技术的合作在许多交通场景中实现了最先进的性能。本节旨在介绍这些技术在交通任务中的功能、优点、缺陷和变体，这可以帮助参与者了解如何在交通领域利用这些深度学习技术。'
- en: V-A GNNs
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 图神经网络
- en: '![Refer to caption](img/ad1d77a3d24e51a148d2049a2244ed57.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad1d77a3d24e51a148d2049a2244ed57.png)'
- en: 'Figure 6: The structure of Graph Neural Network is generally composed of two
    kind of layers: 1) Aggregation layer: In each feature dimension, the features
    of adjacent nodes are aggregated to the central node. Mathematically, the output
    of aggregation layer is the product of adjacency matrix and features matrix. 2)
    Non-linear transformation layer: All the aggregated features of each node are
    fed into the non-linear transformation layer to create higher level feature representation.
    All nodes share the same transformation kernel. $\{1,2,3,4\}$ are node indexes.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：图神经网络的结构通常由两种类型的层组成：1) 聚合层：在每个特征维度上，相邻节点的特征被聚合到中央节点上。从数学上讲，聚合层的输出是邻接矩阵和特征矩阵的乘积。2)
    非线性变换层：每个节点的所有聚合特征都输入到非线性变换层，以创建更高层次的特征表示。所有节点共享相同的变换核。$\{1,2,3,4\}$ 是节点索引。
- en: In the last couple of years, motivated by the huge success of deep learning
    approaches (e.g. CNNs, RNNs), there is an increasing interest in generalizing
    neural networks to arbitrarily structured graphs and such networks are classified
    as graph neural networks (GNNs). In the early stage, the studies about GNNs can
    be categorized into recurrent graph neural networks (RecGNNs) which are inspired
    by RNNs [[34](#bib.bib34)]. Subsequently, inspired by the huge success of CNNs,
    many works focus on extending the convolution of CNN on graph data and these works
    can be categorized into convolutional graph neural networks (ConvGNNs) [[34](#bib.bib34)].
    There are also other branches of GNNs developed in recent years, e.g. graph auto-encoders
    (GAEs) [[121](#bib.bib121)] and graph attention networks (GATs) [[122](#bib.bib122)].
    According to our investigation, most traffic works focus on ConvGNNs and there
    are only a few studies [[119](#bib.bib119)] employing other branches of GNNs up
    to now. Further, ConvGNNs can be divided into two main streams, i.e. the spectral-based
    approaches which develop graph convolutions based on the spectral theory and the
    spatial-based approaches which define graph convolutions based on spatial relations
    between nodes[[123](#bib.bib123)]. Recently, many novel spatial-based convolutions
    have emerged, among which diffusion convolution is a popular spatial-based graph
    convolution which regards graph convolution as a diffusion process.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'According to our survey, most existing traffic works utilize either spectral
    graph convolution or diffusion graph convolution. There are also other novel convolutions
    [[68](#bib.bib68)] but their applications in traffic domain are relatively few.
    Therefore, in this section, we focus on introducing spectral graph convolution
    (SGC) and diffusion graph convolution (DGC) in traffic domain. In this paper,
    we refer the graph neural network with spectral graph convolution as SGCN and
    that with diffusion graph convolution as DGCN. Note that SGC is for undirected
    graph while DGC can be applied in both directed graph and undirected graph. In
    addition, both SGC and DGC aim to generate new feature representations for each
    node in a graph through feature aggregation and non-linear transformation (as
    shown in Figure [6](#S5.F6 "Figure 6 ‣ V-A GNNs ‣ V Deep Learning Techniques Perspective
    ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")).'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: V-A1 Spectral Graph Convolution
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the spectral theory, a graph is represented by its corresponding normalized
    Laplacian matrix $\mathbf{L}=\mathbf{I_{N}}-\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$.
    The real symmetric matrix $\mathbf{L}$ can be diagonalized via eigendecomposition
    as $\mathbf{L}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{T}$ where $\mathbf{U}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$
    is the eigenvectors matrix and $\mathbf{\Lambda}\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$
    is the diagonal eigenvalues matrix. Since $\mathbf{U}$ is also an orthogonal matrix,
    Shuman et al. [[124](#bib.bib124)] adopted it as a graph Fourier basis, defining
    graph Fourier transform of a graph signal $x\in\mathbb{R}^{\mathbf{N}}$ as $\hat{x}=\mathbf{U}^{T}x$,
    and its inverse as $x=\mathbf{U}\hat{x}$.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Bruna et al. [[125](#bib.bib125)] tried to build an analogue of CNN convolution
    in spectral domain and defined the spectral convolution as $y=\Theta\boldsymbol{*_{\mathcal{G}}}x=\mathbf{U}\Theta\mathbf{U}^{T}x$,
    i.e. transforming $x$ into spectral domain, adjusting its amplitude by a diagonal
    kernel $\Theta=\operatorname{diag}(\theta_{0},\ldots,\theta_{\mathbf{N}-1})\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$,
    and doing inverse Fourier transform to get the final result $y$ in spatial domain.
    Although such convolution is theoretically guaranteed, it is computationally expensive
    as multiplication with $\mathbf{U}$ is $\mathcal{O}(\mathbf{N}^{2})$ and the eigendecomposition
    of $\mathbf{L}$ is intolerable for large scale graphs. In addition, it considers
    all nodes by the kernel $\Theta$ with $\mathbf{N}$ parameters and can’t extract
    spatial localization.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: To avoid such limitations, Defferrard et al. [[126](#bib.bib126)] localized
    the convolution and reduced its parameters by restricting the kernel $\Theta$
    to be a polynomial of eigenvalues matrix $\mathbf{\Lambda}$ as $\Theta=\sum_{k=0}^{\mathbf{K}-1}\theta_{k}\mathbf{\Lambda}^{k}$
    and $\mathbf{K}$ determines the maximum radius of the convolution from a central
    node. Thus, the convolution can be rewritten as $\Theta\boldsymbol{*_{\mathcal{G}}}x=\sum_{k=0}^{\mathbf{K}-1}\theta_{k}\mathbf{U}\mathbf{\Lambda}^{k}\mathbf{U}^{T}x=\sum_{k=0}^{\mathbf{K}-1}\theta_{k}\mathbf{L}^{k}x$.
    Further more, Defferrard et al. [[126](#bib.bib126)] adopted the Chebyshev polynomials
    $T_{k}(x)$ to approximate $\mathbf{L}^{k}$, resulting in $\Theta\boldsymbol{*_{\mathcal{G}}}x\approx\sum_{k=0}^{\mathbf{K}-1}\theta_{k}T_{k}(\tilde{\mathbf{L}})x$
    with a rescaled $\tilde{\mathbf{L}}=\frac{2}{\boldsymbol{\lambda}_{\max}}\mathbf{L}-\mathbf{I_{N}}$
    where $\boldsymbol{\lambda}_{\max}$ is the largest eigenvalue of $\mathbf{L}$
    and $T_{k}(x)=2xT_{k-1}(x)-T_{k-2}(x)$, $T_{0}(x)=1$, $T_{1}(x)=x$ [[127](#bib.bib127)].
    By recursively computing $T_{k}(x)$, the complexity of this $\mathbf{K}$-localized
    convolution can be reduced to $\mathcal{O}(\mathbf{K}|\mathbf{E}|)$ with $|\mathbf{E}|$
    being the number of edges.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on [[126](#bib.bib126)], Kipf et al. [[128](#bib.bib128)] simplified
    the spectral graph convolution by limiting $\mathbf{K}=2$ and with $T_{0}(\tilde{\mathbf{L}})=1$,
    $T_{1}(\tilde{\mathbf{L}})=\tilde{\mathbf{L}}$. They got $\Theta\boldsymbol{*_{\mathcal{G}}}x\approx\theta_{0}T_{0}(\tilde{\mathbf{L}})x+\theta_{1}T_{1}(\tilde{\mathbf{L}})x=\theta_{0}x+\theta_{1}\tilde{\mathbf{L}}x$.
    Noticing that $\tilde{\mathbf{L}}=\frac{2}{\lambda_{\max}}\mathbf{L}-\mathbf{I_{N}}$,
    they set $\boldsymbol{\lambda}_{\max}=2$, resulting in $\Theta\boldsymbol{*_{\mathcal{G}}}x\approx\theta_{0}x+\theta_{1}(\mathbf{L}-\mathbf{I_{N}})x$.
    For that $\mathbf{L}=\mathbf{I_{N}}-\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}$
    and $\mathbf{L}-\mathbf{I_{N}}=-\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}$,
    they got $\Theta\boldsymbol{*_{\mathcal{G}}}x\approx\theta_{0}x-\theta_{1}(\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}})x$.
    Further, they reduced the number of parameters by setting $\theta=\theta_{0}=-\theta_{1}$
    to address overfitting and got $\Theta\boldsymbol{*_{\mathcal{G}}}x\approx\theta(\mathbf{I_{N}}+\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}})x$.
    They defined $\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I_{N}}$ and adopted a renormalization
    trick to get $y=\Theta\boldsymbol{\boldsymbol{*_{\mathcal{G}}}}x\approx\theta\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}x$,
    where $\tilde{\mathbf{D}}$ is the degree matrix of $\tilde{\mathbf{A}}$. Finally,
    Kipf et al.[[128](#bib.bib128)] proposed a spectral graph convolution layer as
    follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}Y_{j}&amp;=\boldsymbol{\rho}(\Theta_{j}\boldsymbol{*_{\mathcal{G}}}X)=\boldsymbol{\rho}(\sum_{i=1}^{\mathbf{F_{I}}}\theta_{i,j}\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}X_{i}),1\leq
    j\leq\mathbf{F_{O}}\\ Y&amp;=\boldsymbol{\rho}(\tilde{\mathbf{D}}^{-\frac{1}{2}}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}XW)\end{split}$
    |  | (4) |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: here, $X\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{I}}}$ is the layer input with
    $\mathbf{F_{I}}$ features, $X_{i}\in\mathbb{R}^{\mathbf{N}}$ is its $i^{th}$ feature.
    $Y\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{O}}}$ is the layer output with $\mathbf{F_{O}}$
    features, $Y_{j}\in\mathbb{R}^{\mathbf{N}}$ is its $j^{th}$ feature. $W\in\mathbb{R}^{\mathbf{F_{I}}\times\mathbf{F_{O}}}$
    is a trainable parameter. $\boldsymbol{\rho}(\boldsymbol{\cdot})$ is the activation
    function. Such layer can aggregate information of 1-hop neighbors. The receptive
    field of neighborhood can be expanded by stacking multiple graph convolution layers
    [[42](#bib.bib42)].
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: V-A2 Diffusion Graph Convolution
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Spectral graph convolution requires a symmetric Laplacian matrix to implement
    eigendecomposition. It becomes invalid for a directed graph with an asymmetric
    Laplacian matrix. Diffusion convolution origins from graph diffusion and has no
    constraint on graph. Graph diffusion [[129](#bib.bib129)], [[130](#bib.bib130)]
    can be represented as a transition matrix power series giving the probability
    of jumping from one node to another node at each step. After many steps, such
    Markov process converges to a stationary distribution $\mathcal{P}=\sum_{k=0}^{\infty}\alpha(1-\alpha)^{k}(\mathbf{D_{O}}^{-1}\mathbf{A})^{k}$,
    where $\mathbf{D_{O}}^{-1}\mathbf{A}$ is the transition matrix, $\alpha\in[0,1]$
    is the restart probability and $k$ is the diffusion step. In practice, a finite
    $\mathbf{K}$-step truncation of the diffusion process is adopted and each step
    is assigned a trainable weight $\theta$. Based on the $\mathbf{K}$-step diffusion
    process, Li et al. [[108](#bib.bib108)] defined diffusion graph convolution as:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y=\Theta\boldsymbol{*_{\mathcal{G}}}x=\sum_{k=0}^{\mathbf{K}-1}(\theta_{k,1}(\mathbf{D_{O}}^{-1}\mathbf{A})^{k}+\theta_{k,2}(\mathbf{D_{I}}^{-1}\mathbf{A}^{T})^{k})x$
    |  | (5) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: 'here, $\mathbf{D_{O}}^{-1}\mathbf{A}$ represents the transition matrix and
    $\mathbf{D_{I}}^{-1}\mathbf{A}^{T}$ is its transpose. Such bidirectional diffusion
    enables the operation to capture the spatial correlation on a directed graph[[108](#bib.bib108)].
    Similar to spectral graph convolution layer, a diffusion graph convolutional layer
    is built as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}Y_{j}&amp;=\boldsymbol{\rho}(\sum_{k=0}^{\mathbf{K}-1}\sum_{i=1}^{\mathbf{F_{I}}}(\theta_{k,1,i,j}(\mathbf{D_{O}}^{-1}\mathbf{A})^{k}+\theta_{k,2,i,j}(\mathbf{D_{I}}^{-1}\mathbf{A}^{T})^{k})X_{i})\\
    Y&amp;=\boldsymbol{\rho}(\sum_{k=0}^{\mathbf{K}-1}(\mathbf{D_{O}}^{-1}\mathbf{A})^{k}XW_{k1}+(\mathbf{D_{I}}^{-1}\mathbf{A}^{T})^{k}XW_{k2})\end{split}$
    |  | (6) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: where $1\leq j\leq\mathbf{F_{O}}$, parameters $W_{k1},W_{k2}\in\mathbb{R}^{\mathbf{F_{I}}\times\mathbf{F_{O}}}$
    are trainable.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: V-A3 GNNs in Traffic Domain
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many traffic works, such as subway network and road network, are graph structure
    naturally (See Section [IV](#S4 "IV Problem Formulation and Graph Construction
    ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")).
    Compared with previous works modeling traffic network as grids [[131](#bib.bib131)],[[132](#bib.bib132)],
    the works modeling traffic network as graph can fully utilize spatial information.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: By now, many works employ convolution operation directly on traffic graph to
    capture the complex spatial dependency of traffic data. Most of them adopt spectral
    graph convolution (SGC) while some employ diffusion graph convolution (DGC) [[112](#bib.bib112)],
    [[108](#bib.bib108)], [[100](#bib.bib100)], [[102](#bib.bib102)], [[96](#bib.bib96)],[[118](#bib.bib118)].
    There are also some other graph based deep learning techniques such as graph attention
    network (GAT) [[119](#bib.bib119)], [[107](#bib.bib107)], [[98](#bib.bib98)],[[104](#bib.bib104)],
    tensor decomposition and completion on graph [[78](#bib.bib78)], but their related
    works are few, which might be a future research direction.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: The key difference between SGC and DGC lies in their matrices which represent
    different assumptions on the spatial correlations in traffic network. The adjacency
    matrix in SGC infers that a central node in a graph has stronger correlation with
    its adjacent nodes than other distant ones [[89](#bib.bib89)],[[70](#bib.bib70)].
    The state transition matrix in DGC indicates that the spatial dependency is stochastic
    depending on the restart probability and dynamic instead of being fixed. The traffic
    flow is related to a diffusion process on a traffic graph to model its dynamic
    spatial correlations. In addition, the bidirectional diffusion in DGC offers the
    model more flexibility to capture the influence from both upstream and downstream
    traffic [[108](#bib.bib108)]. In a word, DGC is more complicated than SGC. DGC
    can be adopted in both symmetric or asymmetric traffic network graph while SGC
    can be only utilized to process symmetric traffic graph.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Existing graph convolution theories are mainly applied on 2-D signal $X\in\mathbb{R}^{\mathbf{N}\times\mathbf{F_{I}}}$.
    However, the traffic data with both spatial and temporal attributes are usually
    3-D signal $\mathcal{X}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{I}}}$.
    The convolution operations need to be further generalized to 3-D signal. Equal
    convolution operation (e.g. SGC, DGC) with the same kernel is imposed on each
    time step of 3-D signal $\mathcal{X}$ in parallel [[92](#bib.bib92)], [[70](#bib.bib70)],
    [[111](#bib.bib111)],[[115](#bib.bib115)].
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: In order to enhance the performance of graph convolution in traffic tasks, many
    works develop various variants of SGC.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'Guo et al. [[69](#bib.bib69)] redefined SGC with attention mechanism to adaptively
    capture the dynamic correlations in traffic network: $\Theta\boldsymbol{*_{\mathcal{G}}}x\approx\sum_{k=0}^{\mathbf{K}-1}\theta_{k}(T_{k}(\tilde{\mathbf{L}})\boldsymbol{\odot}\mathbf{S})x$
    , where $\mathbf{S}=W_{1}\boldsymbol{\odot}\boldsymbol{\rho}((XW_{2})W_{3}(W_{4}X)^{T}+b)\in\mathbb{R}^{\mathbf{N}\times\mathbf{N}}$
    is the spatial attention.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Yu et al. [[71](#bib.bib71)] generalized SGC on both spatial and temporal dimensions
    by scanning $\mathbf{K}$ order neighbors on graph and $\mathbf{K}_{t}$ neighbors
    on time-axis without padding. The equation is as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{Y}_{t,j}=\boldsymbol{\rho}(\sum_{t^{\prime}=0}^{\mathbf{K}_{t}-1}\sum_{k=0}^{\mathbf{K}-1}\sum_{i=1}^{\mathbf{F_{I}}}\theta_{j,t^{\prime},k,i}\tilde{\mathbf{L}}^{k}\mathcal{X}_{t-t^{\prime},i})$
    |  | (7) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{X}_{t-t^{\prime},i}\in\mathbb{R}^{\mathbf{N}}$ is the $i^{th}$
    feature of input $\mathcal{X}$ at time $t-t^{\prime}$ , $\mathcal{Y}_{t,j}\in\mathbb{R}^{\mathbf{N}}$
    is the $j^{th}$ feature of output $\mathcal{Y}$ at time $t$.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Zhao et al. [[94](#bib.bib94)] changed SGC as $\Theta\boldsymbol{*_{\mathcal{G}}}x=(W\boldsymbol{\odot}\tilde{\mathbf{A}}^{\mathbf{K}}\boldsymbol{\odot}\mathcal{FFR})x$
    , where $\tilde{\mathbf{A}}^{\mathbf{K}}$ is the $\mathbf{K}$-hop neighborhood
    matrix and $\mathcal{FFR}$ is a matrix representing physical properties of road
    network. Some researchers [[120](#bib.bib120)],[[110](#bib.bib110)] followed this
    work and redefined $\Theta\boldsymbol{*_{\mathcal{G}}}x=(W\boldsymbol{\odot}Bi(\mathbf{A}^{\mathbf{K}}+\mathbf{I_{N}}))x$,
    where $Bi(.)$ is a function clipping each nonzero element in matrix to 1.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Sun et al. [[116](#bib.bib116)] modified adjacency matrix $\mathbf{A}$ in SGC
    as $\mathbf{S}=\mathbf{A}\boldsymbol{\odot}\omega$ to integrate the geospatial
    positions information into the model and $\omega$ is a matrix calculated via a
    thresholded Gaussian kernel weighting function. The layer is built as $Y=\boldsymbol{\rho}(\tilde{\mathbf{Q}}^{-\frac{1}{2}}\tilde{\mathbf{S}}\tilde{\mathbf{Q}}^{-\frac{1}{2}}XW)$,
    where $\tilde{\mathbf{Q}}$ is the degree matrix of $\tilde{\mathbf{S}}=\mathbf{S}+\mathbf{I_{N}}$.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Qiu et al. [[57](#bib.bib57)] designed a novel edge-based SGC on road network
    to extract the spatiotemporal correlations of the edge features. Both the feature
    matrix $X$ and adjacency matrix $\mathbf{A}$ are defined on edges instead of nodes.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: V-B RNNs
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNNs) are a type of neural network architecture which
    is mainly used to detect patterns in sequential data [[133](#bib.bib133)]. The
    traffic data collected in many traffic tasks are time series data, thus RNNs are
    commonly utilized in these traffic tasks to capture the temporal dependency in
    traffic data. In this subsection, we introduce three classical models of RNNs
    (i.e. RNN, LSTM, GRU) and the correlations among them, providing theoretical evidence
    for participators to choose appropriate models for specific traffic problems.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: V-B1 RNN
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b80c6a7ddf1fe5650466cb7173d16014.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The folded and unfolded structure of recurrent neural networks'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Similar to a classical Feedforward Neural Network (FNN), a simple recurrent
    neural network (RNN) [[134](#bib.bib134)] contains three layers, i.e. input layer,
    hidden layer, output layer [[135](#bib.bib135)]. What differentiates RNN from
    FNN is the hidden layer. It passes information forward to the output layer in
    FNN while in RNN, it also transmits information back into itself forming a cycle
    [[133](#bib.bib133)]. For this reason, the hidden layer in RNN is called recurrent
    hidden layer. Such cycling trick can retain historical information, enabling RNN
    to process time series data.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose there are $\mathbf{F_{I}}$, $\mathbf{F_{H}}$, $\mathbf{F_{O}}$ units
    in the input, hidden, output layer of RNN respectively. The input layer takes
    time series data $\mathbf{X}=[\mathbf{X}_{1},\cdots,\mathbf{X}_{\mathbf{P}}]\in\mathbb{R}^{\mathbf{P}\times\mathbf{F_{I}}}$
    in. For each element $\mathbf{X}_{t}\in\mathbb{R}^{\mathbf{F_{I}}}$ at time $t$,
    the hidden layer transforms it to $\mathbf{H}_{t}\in\mathbb{R}^{\mathbf{F_{H}}}$
    and the output layer maps $\mathbf{H}_{t}$ to $\mathbf{Y}_{t}\in\mathbb{R}^{\mathbf{F_{O}}}$.
    Note that the hidden layer not only takes $\mathbf{X}_{t}$ as input but also takes
    $\mathbf{H}_{t-1}$ as input. Such cycling mechanism enables RNN to memorize the
    past information (as shown in Figure [7](#S5.F7 "Figure 7 ‣ V-B1 RNN ‣ V-B RNNs
    ‣ V Deep Learning Techniques Perspective ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey")). The mathematical notations of hidden
    layer and output layer are as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathbf{H}_{t}&amp;=\boldsymbol{tanh}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{h}+b_{h})\\
    \mathbf{Y}_{t}&amp;=\boldsymbol{\rho}(\mathbf{H}_{t}\boldsymbol{\cdot}W_{y}+b_{y})\end{split}$
    |  | (8) |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: where $W_{h}\in\mathbb{R}^{(\mathbf{F_{I}+\mathbf{F_{H}}})\times\mathbf{F_{H}}}$,
    $W_{y}\in\mathbb{R}^{\mathbf{F_{H}}\times\mathbf{F_{O}}}$, $b_{h}\in\mathbb{R}^{\mathbf{F_{H}}}$,
    $b_{y}\in\mathbb{R}^{\mathbf{F_{O}}}$ are trainable parameters. $t=1,\cdots,\mathbf{P}$
    and $\mathbf{P}$ is the input sequence length. $\mathbf{H}_{0}$ is initialized
    using small non-zero elements which can improve overall performance and stability
    of the network [[136](#bib.bib136)].
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'In a word, RNN takes sequential data as input and generates another sequence
    with the same length: $[\mathbf{X}_{1},\cdots,\mathbf{X}_{\mathbf{P}}]\stackrel{{\scriptstyle
    RNN}}{{\longrightarrow}}[\mathbf{Y}_{1},\cdots,\mathbf{Y}_{\mathbf{P}}]$. Note
    that we can deepen RNN through stacking multiple recurrent hidden layers.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: V-B2 LSTM
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although the hidden state enables RNN to memorize the input information over
    past time steps, it also introduces matrix multiplication over the (potentially
    very long) sequence. Small values in the matrix multiplication cause the gradients
    to decrease at each time step, resulting in final vanish phenomenon. Oppositely
    big values lead to exploding problem [[137](#bib.bib137)]. The vanishing or exploding
    gradients actually hinder the capacity of RNN to learn long-term sequential dependencies
    in data [[135](#bib.bib135)].
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome this hurdle, Long Short-Term Memory (LSTM) neural networks[[138](#bib.bib138)]
    are proposed to capture long-term dependency in sequence learning. Compared with
    the hidden layer in RNN, LSTM hidden layer has extra four parts, i.e. a memory
    cell, input gate, forget gate, and output gate. These three gates ranging in [0,1]
    can control information flow into the memory cell and preserve the extracted features
    from previous time steps. These simple changes enable the memory cell to store
    and read as much long-term information as possible. The mathematical notations
    of LSTM hidden layer are as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}i_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{i}+b_{i})\\
    o_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{o}+b_{o})\\'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: f_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{f}+b_{f})\\
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{C}_{t}&amp;=f_{t}\boldsymbol{\odot}\mathbf{C}_{t-1}+i_{t}\boldsymbol{\odot}\boldsymbol{tanh}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{c}+b_{c})\\
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{H}_{t}&amp;=o_{t}\boldsymbol{\odot}\boldsymbol{tanh}(\mathbf{C}_{t})\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="right"  ><msub ><mi mathsize="80%" >i</mi><mi mathsize="80%"
    >t</mi></msub></mtd><mtd columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow
    ><mi mathsize="80%"  >𝝈</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow ><mrow ><mrow ><mo maxsize="80%" minsize="80%"  >[</mo><msub
    ><mi mathsize="80%"  >𝐇</mi><mrow ><mi mathsize="80%" >t</mi><mo mathsize="80%"
    >−</mo><mn mathsize="80%" >1</mn></mrow></msub><mo mathsize="80%"  >,</mo><msub
    ><mi mathsize="80%"  >𝐗</mi><mi mathsize="80%"  >t</mi></msub><mo maxsize="80%"
    minsize="80%" rspace="0.055em"  >]</mo></mrow><mo mathsize="80%" mathvariant="bold"
    rspace="0.222em" >⋅</mo><msub ><mi mathsize="80%"  >W</mi><mi mathsize="80%"  >i</mi></msub></mrow><mo
    mathsize="80%"  >+</mo><msub ><mi mathsize="80%"  >b</mi><mi mathsize="80%"  >i</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd
    columnalign="right"  ><msub ><mi mathsize="80%" >o</mi><mi mathsize="80%" >t</mi></msub></mtd><mtd
    columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow ><mi mathsize="80%"
    >𝝈</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo maxsize="80%" minsize="80%"
    >(</mo><mrow ><mrow ><mrow ><mo maxsize="80%" minsize="80%" >[</mo><msub ><mi
    mathsize="80%" >𝐇</mi><mrow ><mi mathsize="80%" >t</mi><mo mathsize="80%" >−</mo><mn
    mathsize="80%" >1</mn></mrow></msub><mo mathsize="80%" >,</mo><msub ><mi mathsize="80%"
    >𝐗</mi><mi mathsize="80%" >t</mi></msub><mo maxsize="80%" minsize="80%" rspace="0.055em"  >]</mo></mrow><mo
    mathsize="80%" mathvariant="bold" rspace="0.222em" >⋅</mo><msub ><mi mathsize="80%"  >W</mi><mi
    mathsize="80%"  >o</mi></msub></mrow><mo mathsize="80%"  >+</mo><msub ><mi mathsize="80%"  >b</mi><mi
    mathsize="80%"  >o</mi></msub></mrow><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><msub ><mi mathsize="80%" >f</mi><mi mathsize="80%"
    >t</mi></msub></mtd><mtd columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow
    ><mi mathsize="80%" >𝝈</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo maxsize="80%"
    minsize="80%" >(</mo><mrow ><mrow ><mrow ><mo maxsize="80%" minsize="80%" >[</mo><msub
    ><mi mathsize="80%" >𝐇</mi><mrow ><mi mathsize="80%" >t</mi><mo mathsize="80%"
    >−</mo><mn mathsize="80%" >1</mn></mrow></msub><mo mathsize="80%" >,</mo><msub
    ><mi mathsize="80%" >𝐗</mi><mi mathsize="80%" >t</mi></msub><mo maxsize="80%"
    minsize="80%" rspace="0.055em"  >]</mo></mrow><mo mathsize="80%" mathvariant="bold"
    rspace="0.222em" >⋅</mo><msub ><mi mathsize="80%"  >W</mi><mi mathsize="80%"  >f</mi></msub></mrow><mo
    mathsize="80%"  >+</mo><msub ><mi mathsize="80%"  >b</mi><mi mathsize="80%"  >f</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd
    columnalign="right"  ><msub ><mi mathsize="80%" >𝐂</mi><mi mathsize="80%" >t</mi></msub></mtd><mtd
    columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow ><mrow  ><msub ><mi
    mathsize="80%" >f</mi><mi mathsize="80%" >t</mi></msub><mo lspace="0.222em" mathsize="80%"
    mathvariant="bold" rspace="0.222em"  >⊙</mo><msub ><mi mathsize="80%" >𝐂</mi><mrow
    ><mi mathsize="80%" >t</mi><mo mathsize="80%" >−</mo><mn mathsize="80%" >1</mn></mrow></msub></mrow><mo
    mathsize="80%" >+</mo><mrow ><mrow ><msub ><mi mathsize="80%" >i</mi><mi mathsize="80%"
    >t</mi></msub><mo lspace="0.222em" mathsize="80%" mathvariant="bold" rspace="0.222em"
    >⊙</mo><mi mathsize="80%" >𝒕</mi></mrow><mo lspace="0em" rspace="0em" >​</mo><mi
    mathsize="80%"  >𝒂</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >𝒏</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >𝒉</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow ><mrow
    ><mrow ><mo maxsize="80%" minsize="80%"  >[</mo><msub ><mi mathsize="80%"  >𝐇</mi><mrow
    ><mi mathsize="80%"  >t</mi><mo mathsize="80%"  >−</mo><mn mathsize="80%"  >1</mn></mrow></msub><mo
    mathsize="80%"  >,</mo><msub ><mi mathsize="80%"  >𝐗</mi><mi mathsize="80%"  >t</mi></msub><mo
    maxsize="80%" minsize="80%" rspace="0.055em"  >]</mo></mrow><mo mathsize="80%"
    mathvariant="bold" rspace="0.222em" >⋅</mo><msub ><mi mathsize="80%"  >W</mi><mi
    mathsize="80%"  >c</mi></msub></mrow><mo mathsize="80%"  >+</mo><msub ><mi mathsize="80%"  >b</mi><mi
    mathsize="80%"  >c</mi></msub></mrow><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><msub ><mi mathsize="80%" >𝐇</mi><mi mathsize="80%"
    >t</mi></msub></mtd><mtd columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow
    ><mrow  ><msub ><mi mathsize="80%" >o</mi><mi mathsize="80%" >t</mi></msub><mo
    lspace="0.222em" mathsize="80%" mathvariant="bold" rspace="0.222em"  >⊙</mo><mi
    mathsize="80%"  >𝒕</mi></mrow><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >𝒂</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >𝒏</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="80%"  >𝒉</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo maxsize="80%" minsize="80%"  >(</mo><msub ><mi mathsize="80%"  >𝐂</mi><mi
    mathsize="80%"  >t</mi></msub><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑖</ci><ci >𝑡</ci></apply><apply ><ci  >𝝈</ci><apply ><apply ><ci  >bold-⋅</ci><interval
    closure="closed"  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐇</ci><apply
    ><ci  >𝑡</ci><cn type="integer"  >1</cn></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐗</ci><ci >𝑡</ci></apply></interval><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑊</ci><ci >𝑖</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑏</ci><ci >𝑖</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑜</ci><ci  >𝑡</ci></apply></apply></apply><apply ><apply ><ci  >𝝈</ci><apply
    ><apply ><ci  >bold-⋅</ci><interval closure="closed"  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐇</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐗</ci><ci >𝑡</ci></apply></interval><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑊</ci><ci >𝑜</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑏</ci><ci >𝑜</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑓</ci><ci  >𝑡</ci></apply></apply></apply><apply
    ><apply ><ci  >𝝈</ci><apply ><apply ><ci  >bold-⋅</ci><interval closure="closed"  ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐇</ci><apply ><ci >𝑡</ci><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐗</ci><ci >𝑡</ci></apply></interval><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑊</ci><ci >𝑓</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑏</ci><ci >𝑓</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐂</ci><ci  >𝑡</ci></apply></apply></apply><apply ><apply ><apply  ><csymbol cd="latexml"  >direct-product</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑓</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐂</ci><apply ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply></apply><apply ><apply ><csymbol cd="latexml"
    >direct-product</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑖</ci><ci >𝑡</ci></apply><ci >𝒕</ci></apply><ci >𝒂</ci><ci >𝒏</ci><ci >𝒉</ci><apply
    ><apply  ><ci >bold-⋅</ci><interval closure="closed"  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐇</ci><apply ><ci >𝑡</ci><cn type="integer"  >1</cn></apply></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝐗</ci><ci >𝑡</ci></apply></interval><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑊</ci><ci >𝑐</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑏</ci><ci >𝑐</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐇</ci><ci >𝑡</ci></apply></apply></apply></apply><apply
    ><apply ><apply  ><csymbol cd="latexml"  >direct-product</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑜</ci><ci >𝑡</ci></apply><ci >𝒕</ci></apply><ci
    >𝒂</ci><ci  >𝒏</ci><ci >𝒉</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐂</ci><ci >𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}i_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{i}+b_{i})\\
    o_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{o}+b_{o})\\
    f_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{f}+b_{f})\\
    \mathbf{C}_{t}&=f_{t}\boldsymbol{\odot}\mathbf{C}_{t-1}+i_{t}\boldsymbol{\odot}\boldsymbol{tanh}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{c}+b_{c})\\
    \mathbf{H}_{t}&=o_{t}\boldsymbol{\odot}\boldsymbol{tanh}(\mathbf{C}_{t})\end{split}</annotation></semantics></math>
    |  | (9) |
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: where $i_{t}$, $o_{t}$, $f_{t}$ are the input gate, output gate, forget gate
    at time $t$ respectively. $\mathbf{C}_{t}$ is the memory cell at time $t$.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: V-B3 GRU
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While LSTM is a viable option for avoiding vanishing or exploding gradients,
    its complex structure leads to more memory requirement and longer training time.
    Chung et al. [[139](#bib.bib139)] proposed a simple yet powerful variant of LSTM,
    i.e. Gated Recurrent Unit (GRU). The LSTM cell has three gates, but the GRU cell
    only has two gates, resulting in fewer parameters thus shorter training time.
    However, GRU is equally effective as LSTM empirically [[139](#bib.bib139)] and
    is widely used in various tasks. The mathematical notations of GRU hidden layer
    are as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}r_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{r}+b_{r})\\
    u_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{u}+b_{u})\\'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: \tilde{\mathbf{H}_{t}}&amp;=\boldsymbol{tanh}(r_{t}\boldsymbol{\odot}[\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{h}+b_{h})\\
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{H}_{t}&amp;=u_{t}\boldsymbol{\odot}\mathbf{H}_{t-1}+(1-u_{t})\boldsymbol{\odot}\tilde{\mathbf{H}_{t}}\\
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><msub ><mi mathsize="80%"
    >r</mi><mi mathsize="80%" >t</mi></msub></mtd><mtd columnalign="left"  ><mrow
    ><mo mathsize="80%" >=</mo><mrow ><mi mathsize="80%"  >𝝈</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow ><mrow
    ><mrow ><mo maxsize="80%" minsize="80%" >[</mo><msub ><mi mathsize="80%"  >𝐇</mi><mrow
    ><mi mathsize="80%" >t</mi><mo mathsize="80%"  >−</mo><mn mathsize="80%"  >1</mn></mrow></msub><mo
    mathsize="80%"  >,</mo><msub ><mi mathsize="80%"  >𝐗</mi><mi mathsize="80%"  >t</mi></msub><mo
    maxsize="80%" minsize="80%" rspace="0.055em" >]</mo></mrow><mo  mathsize="80%"
    mathvariant="bold" rspace="0.222em"  >⋅</mo><msub ><mi mathsize="80%"  >W</mi><mi
    mathsize="80%"  >r</mi></msub></mrow><mo mathsize="80%"  >+</mo><msub ><mi mathsize="80%"
    >b</mi><mi mathsize="80%"  >r</mi></msub></mrow><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><msub ><mi mathsize="80%" >u</mi><mi mathsize="80%"
    >t</mi></msub></mtd><mtd columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow
    ><mi mathsize="80%" >𝝈</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow ><mrow ><mrow ><mo maxsize="80%" minsize="80%" >[</mo><msub
    ><mi mathsize="80%" >𝐇</mi><mrow ><mi mathsize="80%" >t</mi><mo mathsize="80%"
    >−</mo><mn mathsize="80%" >1</mn></mrow></msub><mo mathsize="80%" >,</mo><msub
    ><mi mathsize="80%" >𝐗</mi><mi mathsize="80%" >t</mi></msub><mo maxsize="80%"
    minsize="80%" rspace="0.055em"  >]</mo></mrow><mo mathsize="80%" mathvariant="bold"
    rspace="0.222em" >⋅</mo><msub ><mi mathsize="80%"  >W</mi><mi mathsize="80%"  >u</mi></msub></mrow><mo
    mathsize="80%"  >+</mo><msub ><mi mathsize="80%" >b</mi><mi mathsize="80%"  >u</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd
    columnalign="right"  ><mover accent="true"  ><msub ><mi mathsize="80%" >𝐇</mi><mi
    mathsize="80%"  >t</mi></msub><mo mathsize="80%"  >~</mo></mover></mtd><mtd columnalign="left"  ><mrow
    ><mo mathsize="80%" >=</mo><mrow ><mi mathsize="80%" >𝒕</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒂</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >𝒏</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >𝒉</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow ><mrow
    ><mrow ><msub ><mi mathsize="80%" >r</mi><mi mathsize="80%" >t</mi></msub><mo
    lspace="0.222em" mathsize="80%" mathvariant="bold" rspace="0.222em" >⊙</mo><mrow
    ><mo maxsize="80%" minsize="80%" >[</mo><msub ><mi mathsize="80%" >𝐇</mi><mrow
    ><mi mathsize="80%" >t</mi><mo mathsize="80%" >−</mo><mn mathsize="80%" >1</mn></mrow></msub><mo
    mathsize="80%" >,</mo><msub ><mi mathsize="80%" >𝐗</mi><mi mathsize="80%" >t</mi></msub><mo
    maxsize="80%" minsize="80%" rspace="0.055em"  >]</mo></mrow></mrow><mo mathsize="80%"
    mathvariant="bold" rspace="0.222em" >⋅</mo><msub ><mi mathsize="80%"  >W</mi><mi
    mathsize="80%"  >h</mi></msub></mrow><mo mathsize="80%"  >+</mo><msub ><mi mathsize="80%"
    >b</mi><mi mathsize="80%"  >h</mi></msub></mrow><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><msub ><mi mathsize="80%" >𝐇</mi><mi mathsize="80%"
    >t</mi></msub></mtd><mtd columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow
    ><mrow ><msub  ><mi mathsize="80%"  >u</mi><mi mathsize="80%"  >t</mi></msub><mo
    lspace="0.222em" mathsize="80%" mathvariant="bold" rspace="0.222em"  >⊙</mo><msub
    ><mi mathsize="80%" >𝐇</mi><mrow ><mi mathsize="80%" >t</mi><mo mathsize="80%"  >−</mo><mn
    mathsize="80%"  >1</mn></mrow></msub></mrow><mo mathsize="80%"  >+</mo><mrow ><mrow
    ><mo maxsize="80%" minsize="80%" >(</mo><mrow ><mn mathsize="80%" >1</mn><mo mathsize="80%"
    >−</mo><msub ><mi mathsize="80%" >u</mi><mi mathsize="80%" >t</mi></msub></mrow><mo
    maxsize="80%" minsize="80%" rspace="0.055em"  >)</mo></mrow><mo mathsize="80%"
    mathvariant="bold" rspace="0.222em" >⊙</mo><mover accent="true"  ><msub ><mi mathsize="80%"  >𝐇</mi><mi
    mathsize="80%"  >t</mi></msub><mo mathsize="80%"  >~</mo></mover></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑟</ci><ci >𝑡</ci></apply><apply ><ci  >𝝈</ci><apply ><apply ><ci  >bold-⋅</ci><interval
    closure="closed"  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐇</ci><apply
    ><ci  >𝑡</ci><cn type="integer"  >1</cn></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐗</ci><ci >𝑡</ci></apply></interval><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑊</ci><ci >𝑟</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑏</ci><ci >𝑟</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑢</ci><ci >𝑡</ci></apply></apply></apply><apply ><apply  ><ci >𝝈</ci><apply ><apply
    ><ci  >bold-⋅</ci><interval closure="closed"  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐇</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐗</ci><ci >𝑡</ci></apply></interval><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑊</ci><ci >𝑢</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑏</ci><ci >𝑢</ci></apply></apply><apply
    ><ci  >~</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐇</ci><ci
    >𝑡</ci></apply></apply></apply></apply><apply ><apply ><ci  >𝒕</ci><ci >𝒂</ci><ci
    >𝒏</ci><ci  >𝒉</ci><apply ><apply ><ci  >bold-⋅</ci><apply ><csymbol cd="latexml"
    >direct-product</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑟</ci><ci >𝑡</ci></apply><interval closure="closed" ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐇</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐗</ci><ci >𝑡</ci></apply></interval></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑊</ci><ci >ℎ</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑏</ci><ci >ℎ</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐇</ci><ci >𝑡</ci></apply></apply></apply><apply
    ><apply  ><apply ><csymbol cd="latexml" >direct-product</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑢</ci><ci >𝑡</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐇</ci><apply ><ci  >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><csymbol cd="latexml" >direct-product</csymbol><apply ><cn type="integer" >1</cn><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑢</ci><ci >𝑡</ci></apply></apply><apply
    ><ci >~</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐇</ci><ci
    >𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}r_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{r}+b_{r})\\
    u_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{u}+b_{u})\\
    \tilde{\mathbf{H}_{t}}&=\boldsymbol{tanh}(r_{t}\boldsymbol{\odot}[\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\cdot}W_{h}+b_{h})\\
    \mathbf{H}_{t}&=u_{t}\boldsymbol{\odot}\mathbf{H}_{t-1}+(1-u_{t})\boldsymbol{\odot}\tilde{\mathbf{H}_{t}}\\
    \end{split}</annotation></semantics></math> |  | (10) |
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: where $r_{t}$ is the reset gate, $u_{t}$ is the update gate.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: V-B4 RNNs in Traffic Domain
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RNNs have shown impressive capability of processing time series data. Since
    traffic data has a distinct temporal dependency, RNNs are usually leveraged to
    capture temporal correlation in traffic data. Among the works we survey, only
    Geng et al. [[89](#bib.bib89)] utilized RNN to capture temporal dependency in
    traffic data while more than a half adopted GRU and some employed LSTM. This can
    be explained that RNN survives severe gradient disappearance or gradient explosion
    while LSTM and GRU handle this successfully and GRU can reduce the training time.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there are many tricks to augment RNNs’ capacity to model the complex
    temporal dynamics in traffic domain, such as attention mechanism, gating mechanism
    and residual mechanism.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, Geng et al. [[89](#bib.bib89)] incorporated the contextual information,
    i.e. output of SGCN which contains information of related regions, into an attention
    operation to model the correlations between observations at different timestamps:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}z&amp;=F_{pool}(\mathbf{X}_{t},SGCN(\mathbf{X}_{t}))\\
    S&amp;=\boldsymbol{\sigma}(W_{1}\boldsymbol{ReLU}(W_{2}z))\\'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{H}_{t}&amp;=RNN([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\odot}S)\\
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><mi mathsize="80%"  >z</mi></mtd><mtd
    columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow ><msub  ><mi mathsize="80%"  >F</mi><mrow
    ><mi mathsize="80%" >p</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >o</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="80%"  >l</mi></mrow></msub><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><msub ><mi
    mathsize="80%" >𝐗</mi><mi mathsize="80%" >t</mi></msub><mo mathsize="80%"  >,</mo><mrow
    ><mi mathsize="80%" >S</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >G</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >C</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="80%"  >N</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo maxsize="80%" minsize="80%"  >(</mo><msub ><mi mathsize="80%"  >𝐗</mi><mi
    mathsize="80%"  >t</mi></msub><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd
    columnalign="right"  ><mi mathsize="80%"  >S</mi></mtd><mtd columnalign="left"  ><mrow
    ><mo mathsize="80%" >=</mo><mrow ><mi mathsize="80%" >𝝈</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo maxsize="80%" minsize="80%"  >(</mo><mrow ><msub ><mi mathsize="80%" >W</mi><mn
    mathsize="80%" >1</mn></msub><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%"  >𝑹</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >𝒆</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="80%"  >𝑳</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝑼</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow ><msub ><mi mathsize="80%"  >W</mi><mn mathsize="80%"  >2</mn></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >z</mi></mrow><mo maxsize="80%"
    minsize="80%"  >)</mo></mrow></mrow><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><msub ><mi mathsize="80%" >𝐇</mi><mi mathsize="80%"
    >t</mi></msub></mtd><mtd columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow
    ><mi mathsize="80%" >R</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >N</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >N</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow ><mrow
    ><mo maxsize="80%" minsize="80%" >[</mo><msub ><mi mathsize="80%" >𝐇</mi><mrow
    ><mi mathsize="80%" >t</mi><mo mathsize="80%" >−</mo><mn mathsize="80%" >1</mn></mrow></msub><mo
    mathsize="80%" >,</mo><msub ><mi mathsize="80%" >𝐗</mi><mi mathsize="80%" >t</mi></msub><mo
    maxsize="80%" minsize="80%" rspace="0.055em"  >]</mo></mrow><mo mathsize="80%"
    mathvariant="bold" rspace="0.222em" >⊙</mo><mi mathsize="80%"  >S</mi></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><ci >𝑧</ci><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝐹</ci><apply ><ci  >𝑝</ci><ci >𝑜</ci><ci
    >𝑜</ci><ci >𝑙</ci></apply></apply><interval closure="open"  ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐗</ci><ci >𝑡</ci></apply><apply ><ci  >𝑆</ci><ci
    >𝐺</ci><ci >𝐶</ci><ci >𝑁</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐗</ci><ci >𝑡</ci></apply></apply></interval><ci >𝑆</ci></apply></apply><apply
    ><apply ><ci  >𝝈</ci><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑊</ci><cn type="integer" >1</cn></apply><ci >𝑹</ci><ci  >𝒆</ci><ci >𝑳</ci><ci
    >𝑼</ci><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑊</ci><cn
    type="integer"  >2</cn></apply><ci >𝑧</ci></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐇</ci><ci >𝑡</ci></apply></apply></apply><apply ><apply  ><ci
    >𝑅</ci><ci >𝑁</ci><ci  >𝑁</ci><apply ><csymbol cd="latexml" >direct-product</csymbol><interval
    closure="closed"  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐇</ci><apply
    ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐗</ci><ci >𝑡</ci></apply></interval><ci >𝑆</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}z&=F_{pool}(\mathbf{X}_{t},SGCN(\mathbf{X}_{t}))\\
    S&=\boldsymbol{\sigma}(W_{1}\boldsymbol{ReLU}(W_{2}z))\\ \mathbf{H}_{t}&=RNN([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{\odot}S)\\
    \end{split}</annotation></semantics></math> |  | (11) |
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: where $F_{pool}(\boldsymbol{\cdot})$ is a global average pooling layer, $RNN(\boldsymbol{\cdot})$
    denotes the RNN hidden layer.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Chen et al. [[112](#bib.bib112)] took external factors into consideration by
    embedding external attributes into the input. In addition, they added the previous
    hidden states to the next hidden states through a residual shortcut path, which
    they believed can make GRU more sensitive and robust to sudden changes in traffic
    historical observations. The new hidden state is formulated as: $\mathbf{H}_{t}=GRU([\mathbf{H}_{t-1},\mathbf{X}_{t}],\mathbf{E}_{t})+\mathbf{H}_{t-1}W$,
    where $\mathbf{E}_{t}$ is the external features at time $t$, $W$ is a linear trainable
    parameter, $\mathbf{H}_{t-1}W$ is the residual shortcut.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Yu et al. [[105](#bib.bib105)] inserted a dilated skip connection into GRU by
    changing hidden state from $\mathbf{H}_{t}=GRU([\mathbf{H}_{t-1},\mathbf{X}_{t}])$
    to $\mathbf{H}_{t}=GRU(\mathbf{H}_{t-s},\mathbf{X}_{t})$, where $s$ refers to
    the skip length or dilation rate of each layer, $GRU(\boldsymbol{\cdot})$ denotes
    the GRU hidden layer. Such hierarchical design of dilation brings in multiple
    temporal scales for recurrent units at different layers which achieves multi-timescale
    modeling.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the tricks above, some works replace the matrix multiplication in RNNs’
    hidden layer with spectral graph convolution (SGC) or diffusion graph convolution
    (DGC), to capture spatial-temporal correlations jointly. Take GRU as example:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}r_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{*_{\mathcal{G}}}W_{r}+b_{r})\\
    u_{t}&amp;=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{*_{\mathcal{G}}}W_{u}+b_{u})\\'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: \tilde{\mathbf{H}_{t}}&amp;=\boldsymbol{tanh}(r_{t}\boldsymbol{\odot}[\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{*_{\mathcal{G}}}W_{h}+b_{h})\\
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{H}_{t}&amp;=u_{t}\boldsymbol{\odot}\mathbf{H}_{t-1}+(1-u_{t})\boldsymbol{\odot}\tilde{\mathbf{H}_{t}}\\
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><msub ><mi mathsize="80%"
    >r</mi><mi mathsize="80%" >t</mi></msub></mtd><mtd columnalign="left"  ><mrow
    ><mo mathsize="80%" >=</mo><mrow ><mi mathsize="80%"  >𝝈</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow ><mrow
    ><mrow ><mo maxsize="80%" minsize="80%" >[</mo><msub ><mi mathsize="80%"  >𝐇</mi><mrow
    ><mi mathsize="80%" >t</mi><mo mathsize="80%"  >−</mo><mn mathsize="80%"  >1</mn></mrow></msub><mo
    mathsize="80%"  >,</mo><msub ><mi mathsize="80%"  >𝐗</mi><mi mathsize="80%"  >t</mi></msub><mo
    maxsize="80%" minsize="80%" rspace="0.055em" >]</mo></mrow><msub ><mo  mathsize="80%"
    mathvariant="bold" rspace="0.222em"  >∗</mo><mi mathsize="80%" >𝓖</mi></msub><msub
    ><mi mathsize="80%" >W</mi><mi mathsize="80%" >r</mi></msub></mrow><mo mathsize="80%"
    >+</mo><msub ><mi mathsize="80%" >b</mi><mi mathsize="80%"  >r</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd
    columnalign="right"  ><msub ><mi mathsize="80%" >u</mi><mi mathsize="80%" >t</mi></msub></mtd><mtd
    columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow ><mi mathsize="80%"
    >𝝈</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow
    ><mrow ><mrow ><mo maxsize="80%" minsize="80%" >[</mo><msub ><mi mathsize="80%"
    >𝐇</mi><mrow ><mi mathsize="80%" >t</mi><mo mathsize="80%" >−</mo><mn mathsize="80%"
    >1</mn></mrow></msub><mo mathsize="80%" >,</mo><msub ><mi mathsize="80%" >𝐗</mi><mi
    mathsize="80%" >t</mi></msub><mo maxsize="80%" minsize="80%" rspace="0.055em"  >]</mo></mrow><msub
    ><mo mathsize="80%" mathvariant="bold" rspace="0.222em" >∗</mo><mi mathsize="80%"
    >𝓖</mi></msub><msub ><mi mathsize="80%" >W</mi><mi mathsize="80%" >u</mi></msub></mrow><mo
    mathsize="80%" >+</mo><msub ><mi mathsize="80%" >b</mi><mi mathsize="80%"  >u</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd
    columnalign="right"  ><mover accent="true"  ><msub ><mi mathsize="80%" >𝐇</mi><mi
    mathsize="80%"  >t</mi></msub><mo mathsize="80%"  >~</mo></mover></mtd><mtd columnalign="left"  ><mrow
    ><mo mathsize="80%" >=</mo><mrow ><mi mathsize="80%" >𝒕</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >𝒂</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >𝒏</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >𝒉</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow ><mrow
    ><mrow ><msub ><mi mathsize="80%" >r</mi><mi mathsize="80%" >t</mi></msub><mo
    lspace="0.222em" mathsize="80%" mathvariant="bold" rspace="0.222em" >⊙</mo><mrow
    ><mo maxsize="80%" minsize="80%" >[</mo><msub ><mi mathsize="80%" >𝐇</mi><mrow
    ><mi mathsize="80%" >t</mi><mo mathsize="80%" >−</mo><mn mathsize="80%" >1</mn></mrow></msub><mo
    mathsize="80%" >,</mo><msub ><mi mathsize="80%" >𝐗</mi><mi mathsize="80%" >t</mi></msub><mo
    maxsize="80%" minsize="80%" rspace="0.055em"  >]</mo></mrow></mrow><msub ><mo
    mathsize="80%" mathvariant="bold" rspace="0.222em" >∗</mo><mi mathsize="80%" >𝓖</mi></msub><msub
    ><mi mathsize="80%" >W</mi><mi mathsize="80%" >h</mi></msub></mrow><mo mathsize="80%"
    >+</mo><msub ><mi mathsize="80%" >b</mi><mi mathsize="80%"  >h</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd
    columnalign="right"  ><msub ><mi mathsize="80%" >𝐇</mi><mi mathsize="80%" >t</mi></msub></mtd><mtd
    columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow ><mrow ><msub  ><mi
    mathsize="80%"  >u</mi><mi mathsize="80%"  >t</mi></msub><mo lspace="0.222em"
    mathsize="80%" mathvariant="bold" rspace="0.222em"  >⊙</mo><msub ><mi mathsize="80%"
    >𝐇</mi><mrow ><mi mathsize="80%" >t</mi><mo mathsize="80%"  >−</mo><mn mathsize="80%"  >1</mn></mrow></msub></mrow><mo
    mathsize="80%"  >+</mo><mrow ><mrow ><mo maxsize="80%" minsize="80%" >(</mo><mrow
    ><mn mathsize="80%" >1</mn><mo mathsize="80%" >−</mo><msub ><mi mathsize="80%"
    >u</mi><mi mathsize="80%" >t</mi></msub></mrow><mo maxsize="80%" minsize="80%"
    rspace="0.055em"  >)</mo></mrow><mo mathsize="80%" mathvariant="bold" rspace="0.222em"
    >⊙</mo><mover accent="true"  ><msub ><mi mathsize="80%"  >𝐇</mi><mi mathsize="80%"  >t</mi></msub><mo
    mathsize="80%"  >~</mo></mover></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑟</ci><ci >𝑡</ci></apply><apply ><ci  >𝝈</ci><apply ><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝓖</ci></apply><interval closure="closed"  ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐇</ci><apply ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐗</ci><ci >𝑡</ci></apply></interval><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑊</ci><ci >𝑟</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑏</ci><ci >𝑟</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑢</ci><ci >𝑡</ci></apply></apply></apply><apply ><apply  ><ci >𝝈</ci><apply ><apply
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝓖</ci></apply><interval
    closure="closed"  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐇</ci><apply
    ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐗</ci><ci >𝑡</ci></apply></interval><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑊</ci><ci >𝑢</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑏</ci><ci >𝑢</ci></apply></apply><apply
    ><ci  >~</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐇</ci><ci
    >𝑡</ci></apply></apply></apply></apply><apply ><apply ><ci  >𝒕</ci><ci >𝒂</ci><ci
    >𝒏</ci><ci  >𝒉</ci><apply ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝓖</ci></apply><apply ><csymbol cd="latexml" >direct-product</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑟</ci><ci >𝑡</ci></apply><interval closure="closed"
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐇</ci><apply ><ci >𝑡</ci><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐗</ci><ci >𝑡</ci></apply></interval></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑊</ci><ci >ℎ</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑏</ci><ci >ℎ</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐇</ci><ci >𝑡</ci></apply></apply></apply><apply ><apply  ><apply ><csymbol cd="latexml"
    >direct-product</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑢</ci><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐇</ci><apply ><ci  >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><csymbol cd="latexml" >direct-product</csymbol><apply ><cn type="integer" >1</cn><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑢</ci><ci >𝑡</ci></apply></apply><apply
    ><ci >~</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐇</ci><ci
    >𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}r_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{*_{\mathcal{G}}}W_{r}+b_{r})\\
    u_{t}&=\boldsymbol{\sigma}([\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{*_{\mathcal{G}}}W_{u}+b_{u})\\
    \tilde{\mathbf{H}_{t}}&=\boldsymbol{tanh}(r_{t}\boldsymbol{\odot}[\mathbf{H}_{t-1},\mathbf{X}_{t}]\boldsymbol{*_{\mathcal{G}}}W_{h}+b_{h})\\
    \mathbf{H}_{t}&=u_{t}\boldsymbol{\odot}\mathbf{H}_{t-1}+(1-u_{t})\boldsymbol{\odot}\tilde{\mathbf{H}_{t}}\\
    \end{split}</annotation></semantics></math> |  | (12) |
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: The $\boldsymbol{*_{\mathcal{G}}}$ can represent SGC, DGC or other convolution
    operations. In the literatures we survey, most replacements happen in GRU and
    only one in LSTM [[66](#bib.bib66)]. Among GRU related traffic works, [[112](#bib.bib112)],
    [[108](#bib.bib108)], [[106](#bib.bib106)], [[96](#bib.bib96)],[[118](#bib.bib118)]
    replaced matrix multiplication with DGC, [[42](#bib.bib42)], [[105](#bib.bib105)],
    [[77](#bib.bib77)] with SGC, [[104](#bib.bib104)], [[119](#bib.bib119)] with GAT.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Note that besides RNNs, other techniques (e.g. TCN in the next subsection) are
    also popular choices to extract the temporal dynamics in traffic tasks.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: V-C TCN
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although RNN-based models become widespread in time-series analysis, RNNs for
    traffic prediction still suffer from time-consuming iteration, complex gate mechanism,
    and slow response to dynamic changes [[92](#bib.bib92)]. On the contrary, 1D-CNN
    has the superiority of fast training, simple structure, and no constraints to
    previous steps [[140](#bib.bib140)]. However, 1D-CNN is less common than RNNs
    in practice due to its lack of memory for a long sequence [[141](#bib.bib141)].
    In 2016, a novel convolution operation integrating causal convolution and dilated
    convolution [[142](#bib.bib142)] is proposed, which outperforms RNNs in text-to-speech
    tasks. The prediction of causal convolution depends on previous elements but not
    on future elements. Dilated convolution expands the receptive field of original
    filter by dilating it with zeros [[143](#bib.bib143)]. Bai et al. [[144](#bib.bib144)]
    simplified the causal dilated convolution [[142](#bib.bib142)] for sequence modeling
    problem and renamed it as temporal convolution network (TCN). Recently, more and
    more works employ TCN to process traffic data [[92](#bib.bib92)], [[70](#bib.bib70)],
    [[102](#bib.bib102)], [[111](#bib.bib111)].
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: V-C1 Sequence Modeling and 1-D TCN
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given an input sequence with length $\mathbf{P}$ denoted as $\mathbf{x}=[\mathbf{x}_{1},\cdots,\mathbf{x}_{\mathbf{P}}]\in\mathbb{R}^{\mathbf{P}}$,
    sequence modeling aims to generate an output sequence with the same length, denoted
    as $\mathbf{y}=[\mathbf{y}_{1},\cdots,\mathbf{y}_{\mathbf{P}}]\in\mathbb{R}^{\mathbf{P}}$.
    The key assumption is that the output at current time $\mathbf{y}_{t}$ only depends
    on historical data $[\mathbf{x}_{1},\cdots,\mathbf{x}_{t}]$ but does not depend
    on any future inputs $[\mathbf{x}_{t+1},\cdots,\mathbf{x}_{\mathbf{P}}]$, i.e.
    $\mathbf{y}_{t}=f(\mathbf{x}_{1},\cdots,\mathbf{x}_{t})$, $f$ is the mapping function.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, RNN, LSTM and GRU can be solutions to sequence modeling tasks. However,
    TCN can tackle sequence modeling problem more efficiently than RNNs for that it
    can capture long sequence properly in a non-recursive manner. The dilated causal
    convolution in TCN is formulated as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{y}_{t}=\Theta*_{\mathcal{T}^{\mathbf{d}}}\mathbf{x}_{t}=\sum_{k=0}^{\mathbf{K}-1}w_{k}\mathbf{x}_{t-\mathbf{d}k}$
    |  | (13) |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: 'where $*_{\mathcal{T}^{\mathbf{d}}}$ is the dilated causal operator with dilation
    rate $\mathbf{d}$ controlling the skipping distance, $\Theta=[w_{0},\cdots,w_{\mathbf{K-1}}]\in\mathbb{R}^{\mathbf{K}}$
    is the kernel. Zero padding strategy is utilized to keep the output length the
    same as the input length (as shown in Figure [8](#S5.F8 "Figure 8 ‣ V-C1 Sequence
    Modeling and 1-D TCN ‣ V-C TCN ‣ V Deep Learning Techniques Perspective ‣ How
    to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")).
    Without padding, the output length is shortened by $(\mathbf{K}-1)\mathbf{d}$
    [[92](#bib.bib92)].'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bedaffa3c763518bfcbf7854cc55384f.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Multiple dilated causal convolution layers in TCN: $[\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3}]$
    is the input sequence and $[\mathbf{y}_{1},\mathbf{y}_{2},\mathbf{y}_{3}]$ is
    the output sequence with the same length. The size of kernel is $2$ and the dilation
    rate sequence is $[1,2,4]$. Zero padding strategy is taken.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'To enlarge the receptive field, TCN stacks multiple dilated causal convolution
    layers with $\mathbf{d}=2^{l}$ as the dilation rate of $l^{th}$ layer (as shown
    in Figure [8](#S5.F8 "Figure 8 ‣ V-C1 Sequence Modeling and 1-D TCN ‣ V-C TCN
    ‣ V Deep Learning Techniques Perspective ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey")). Therefore, the receptive field in
    the network grows exponentially without requiring many convolutional layers or
    larger filter, which can handle longer sequence with less layers and save computation
    resources [[102](#bib.bib102)].'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: V-C2 TCN in Traffic Domain
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are many traffic works related with sequence modeling, especially traffic
    spatial-temporal forecasting tasks. Compared with RNNs, the non-recursive calculation
    manner enables TCN to alleviate the gradient explosion problem and facilitate
    the training by parallel computation. Therefore, some works adopt TCN to capture
    the temporal dependency in traffic data.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'Most graph-based traffic data is 3-D signal denoted as $\mathcal{X}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{I}}}$,
    which requires the generalization of 1-D TCN to 3-D TCN. The dilated causal convolution
    can be adopted to produce the $j^{th}$ output feature of node $i$ at time $t$
    as follows [[70](#bib.bib70)]:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathcal{Y}_{t,j}^{i}&amp;=\boldsymbol{\rho}(\Theta_{j}*_{\mathcal{T}^{\mathbf{d}}}\mathcal{X}_{t}^{i})=\boldsymbol{\rho}(\sum_{m=1}^{\mathbf{F_{I}}}\sum_{k=0}^{\mathbf{K}-1}w_{j,m,k}\mathcal{X}_{t-\mathbf{d}k,m}^{i})\end{split}$
    |  | (14) |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: where $1\leq j\leq\mathbf{F_{O}}$, $\mathcal{Y}_{t,j}^{i}\in\mathbb{R}$ is the
    $j^{th}$ output feature of node $i$ at time $t$. $\mathcal{X}_{t-\mathbf{d}k,m}^{i}\in\mathbb{R}$
    is the $m^{th}$ input feature of node $i$ at time $t-\mathbf{d}k$. The kernel
    $\Theta_{j}\in\mathbb{R}^{\mathbf{K}\times\mathbf{F_{I}}}$ is trainable. $\mathbf{F_{O}}$
    is the number of output features.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'The same convolution kernel is applied to all nodes in the traffic network
    and each node produces $\mathbf{F_{O}}$ new features. The mathematical formulation
    of each layer is as follows [[70](#bib.bib70)],[[111](#bib.bib111)]:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{Y}=\boldsymbol{\rho}(\Theta*_{\mathcal{T}^{\mathbf{d}}}\mathcal{X})$
    |  | (15) |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{X}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{I}}}$
    represents the historical observations of the whole traffic network over past
    $\mathbf{P}$ time slices, $\Theta\in\mathbb{R}^{\mathbf{K}\times\mathbf{F_{I}}\times\mathbf{F_{O}}}$
    represents the related convolution kernel, $\mathcal{Y}\in\mathbb{R}^{\mathbf{P}\times\mathbf{N}\times\mathbf{F_{O}}}$
    is the output of TCN layer.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some tricks to enhance the performance of TCN in specific traffic
    tasks. For instance, Fang et al. [[111](#bib.bib111)] stacked multiple TCN layers
    to extract the short-term neighboring dependency by bottom layer and long-term
    temporal dependency by higher layer:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{Y}^{(l+1)}=\boldsymbol{\sigma}(\Theta^{l}*_{\mathcal{T}^{\mathbf{d}}}\mathcal{Y}^{(l)})$
    |  | (16) |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{Y}^{(l)}$ is the input of $l^{th}$ layer, $\mathcal{Y}^{(l+1)}$
    is the output and $\mathcal{Y}^{(0)}=\mathcal{X}$. $\mathbf{d}=2^{l}$ is the dilation
    rate of $l^{th}$ layer.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce the complexity of model training, Ge et al. [[70](#bib.bib70)] constructed
    a residual block containing two TCN layers with the same dilation rate. The block
    input was added to last TCN layer to get the block output:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{Y}^{(l+1)}=\mathcal{Y}^{(l)}+\boldsymbol{ReLU}(\Theta_{1}^{l}*_{\mathcal{T}^{\mathbf{d}}}(\boldsymbol{ReLU}(\Theta_{0}^{l}*_{\mathcal{T}^{\mathbf{d}}}\mathcal{Y}^{(l)})))$
    |  | (17) |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: where $\Theta^{l}_{1},\Theta^{l}_{2}$ are the convolution kernels of the first
    layer and the second layer respectively. $\mathcal{Y}^{(l)}$ is the input of residual
    block and $\mathcal{Y}^{(l+1)}$ is its output.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'Wu et al. [[102](#bib.bib102)] integrated gating mechanism[[141](#bib.bib141)]
    with TCN to learn complex temporal dependency in traffic data:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{Y}=\boldsymbol{\rho}_{1}(\Theta_{1}*_{\mathcal{T}^{\mathbf{d}}}\mathcal{X}+b_{1})\boldsymbol{\odot}\boldsymbol{\rho}_{2}(\Theta_{2}*_{\mathcal{T}^{\mathbf{d}}}\mathcal{X}+b_{2})$
    |  | (18) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{\rho}_{2}(\boldsymbol{\cdot})\in[0,1]$ determines the ratio
    of information passed to the next layer.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Yu et al. [[92](#bib.bib92)] used the Gated TCN and set the dilation
    rate $\mathbf{d}=1$ without zero padding to shorten the output length as $\mathcal{Y}=(\Theta_{1}*_{\mathcal{T}^{1}}\mathcal{X})\boldsymbol{\odot}\boldsymbol{\sigma}(\Theta_{2}*_{\mathcal{T}^{1}}\mathcal{X})$.
    They argued that this can discover variances in time series traffic data.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: V-D Seq2Seq
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-D1 Seq2Seq
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d31237e6367284e901b690e15b64ae89.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Sequence to Sequence Structure without attention mechanism'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequence to Sequence (Seq2Seq) model proposed in 2014 [[145](#bib.bib145)]
    has been widely used in sequence prediction such as machine translation [[146](#bib.bib146)].
    Seq2Seq architecture consists of two components, i.e. an encoder in charge of
    converting the input sequence $\mathbf{X}$ into a fixed latent vector $\mathbf{C}$,
    and a decoder responsible for converting $\mathbf{C}$ into an output sequence
    $\mathbf{Y}$ (as shown in Figure [9](#S5.F9 "Figure 9 ‣ V-D1 Seq2Seq ‣ V-D Seq2Seq
    ‣ V Deep Learning Techniques Perspective ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey")). Note that $\mathbf{X}$ and $\mathbf{Y}$
    can have different lengths.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{X}\!=\![\mathbf{X}_{1}\!,\cdots,\!\mathbf{X}_{i},\cdots,\mathbf{X}_{\mathbf{P}}]\stackrel{{\scriptstyle
    Seq2Seq}}{{\longrightarrow}}\mathbf{Y}=[\mathbf{Y}_{1},\cdots,\mathbf{Y}_{j},\cdots,\mathbf{Y}_{\mathbf{Q}}]$
    |  | (19) |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{P}$ is the input length and $\mathbf{Q}$ is the output length.
    $\mathbf{X}_{i}$ is the input at time step $i$. $\mathbf{Y}_{j}$ is the output
    at time step $j$.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'The specific calculation of $\mathbf{Y}_{j}$ is denoted as follows:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}\mathbf{H}_{i}&amp;=Encoder(\mathbf{X}_{i},\mathbf{H}_{i-1})\\
    \mathbf{C}&amp;=\mathbf{H}_{\mathbf{P}},\mathbf{S}_{0}=\mathbf{H}_{\mathbf{P}}\\'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{S}_{j}&amp;=Decoder(\mathbf{C},\mathbf{Y}_{j-1},\mathbf{S}_{j-1})\\
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{Y}_{j}&amp;=\mathbf{S}_{j}W\end{split}" display="block"><semantics ><mtable
    columnspacing="0pt" displaystyle="true" rowspacing="0pt" ><mtr ><mtd  columnalign="right"
    ><msub ><mi mathsize="80%"  >𝐇</mi><mi mathsize="80%"  >i</mi></msub></mtd><mtd
    columnalign="left" ><mrow ><mo mathsize="80%"  >=</mo><mrow ><mi mathsize="80%"
    >E</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%" >n</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi mathsize="80%" >c</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    mathsize="80%" >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%"
    >d</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%" >e</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi mathsize="80%" >r</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo maxsize="80%" minsize="80%"  >(</mo><msub ><mi mathsize="80%" >𝐗</mi><mi
    mathsize="80%" >i</mi></msub><mo mathsize="80%" >,</mo><msub ><mi mathsize="80%"  >𝐇</mi><mrow
    ><mi mathsize="80%"  >i</mi><mo mathsize="80%"  >−</mo><mn mathsize="80%"  >1</mn></mrow></msub><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="right"
    ><mi mathsize="80%"  >𝐂</mi></mtd><mtd columnalign="left" ><mrow ><mrow ><mo mathsize="80%"
    >=</mo><msub ><mi mathsize="80%"  >𝐇</mi><mi mathsize="80%"  >𝐏</mi></msub></mrow><mo
    mathsize="80%"  >,</mo><mrow ><msub ><mi mathsize="80%"  >𝐒</mi><mn mathsize="80%"  >0</mn></msub><mo
    mathsize="80%"  >=</mo><msub ><mi mathsize="80%" >𝐇</mi><mi mathsize="80%" >𝐏</mi></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><msub ><mi mathsize="80%"  >𝐒</mi><mi mathsize="80%"  >j</mi></msub></mtd><mtd
    columnalign="left" ><mrow ><mo mathsize="80%"  >=</mo><mrow ><mi mathsize="80%"
    >D</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%" >e</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi mathsize="80%" >c</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    mathsize="80%" >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%"
    >d</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%" >e</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi mathsize="80%" >r</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo maxsize="80%" minsize="80%"  >(</mo><mi mathsize="80%"  >𝐂</mi><mo mathsize="80%"  >,</mo><msub
    ><mi mathsize="80%" >𝐘</mi><mrow ><mi mathsize="80%" >j</mi><mo mathsize="80%"
    >−</mo><mn mathsize="80%" >1</mn></mrow></msub><mo mathsize="80%" >,</mo><msub
    ><mi mathsize="80%"  >𝐒</mi><mrow ><mi mathsize="80%"  >j</mi><mo mathsize="80%"  >−</mo><mn
    mathsize="80%"  >1</mn></mrow></msub><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><msub ><mi mathsize="80%"  >𝐘</mi><mi mathsize="80%"  >j</mi></msub></mtd><mtd
    columnalign="left" ><mrow ><mo mathsize="80%"  >=</mo><mrow ><msub ><mi mathsize="80%"
    >𝐒</mi><mi mathsize="80%" >j</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >W</mi></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply  ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐇</ci><ci >𝑖</ci></apply><apply
    ><ci  >𝐸</ci><ci >𝑛</ci><ci >𝑐</ci><ci  >𝑜</ci><ci >𝑑</ci><ci >𝑒</ci><ci  >𝑟</ci><interval
    closure="open"  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐗</ci><ci
    >𝑖</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐇</ci><apply
    ><ci >𝑖</ci><cn type="integer"  >1</cn></apply></apply></interval><ci >𝐂</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐇</ci><ci >𝐏</ci></apply></apply></apply><apply
    ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐒</ci><cn
    type="integer" >0</cn></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐇</ci><ci >𝐏</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐒</ci><ci >𝑗</ci></apply></apply></apply><apply ><apply ><ci  >𝐷</ci><ci >𝑒</ci><ci
    >𝑐</ci><ci  >𝑜</ci><ci >𝑑</ci><ci >𝑒</ci><ci >𝑟</ci><vector ><ci >𝐂</ci><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐘</ci><apply ><ci >𝑗</ci><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐒</ci><apply ><ci >𝑗</ci><cn type="integer" >1</cn></apply></apply></vector><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐘</ci><ci >𝑗</ci></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐒</ci><ci >𝑗</ci></apply><ci
    >𝑊</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{split}\mathbf{H}_{i}&=Encoder(\mathbf{X}_{i},\mathbf{H}_{i-1})\\ \mathbf{C}&=\mathbf{H}_{\mathbf{P}},\mathbf{S}_{0}=\mathbf{H}_{\mathbf{P}}\\
    \mathbf{S}_{j}&=Decoder(\mathbf{C},\mathbf{Y}_{j-1},\mathbf{S}_{j-1})\\ \mathbf{Y}_{j}&=\mathbf{S}_{j}W\end{split}</annotation></semantics></math>
    |  | (20) |
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: here, $\mathbf{H}_{i}$ is the hidden state of encoder. $\mathbf{H}_{0}$ is initialized
    using small non-zero elements. $\mathbf{S}_{j}$ is the decoder hidden state. $\mathbf{Y}_{0}$
    is the representation of beginning sign. Note that the encoder and decoder can
    be any model as long as it can accept sequence and produce sequence, such as RNN,
    LSTM, GRU or other novel models.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: A major limitation of Seq2Seq is that the latent vector $\mathbf{C}$ is fixed
    for each $\mathbf{Y}_{j}$ while $\mathbf{Y}_{j}$ might have stronger correlation
    with $\mathbf{X}_{j}$ than other elements. To address this issue, attention mechanism
    is integrated into Seq2Seq, allowing the decoder to focus on task-relevant parts
    of the input sequence, helping the decoder make better prediction.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}\mathbf{H}_{i}&amp;=Encoder(\mathbf{X}_{i},\mathbf{H}_{i-1})\\
    \mathbf{C}_{j}&amp;=\sum_{i=1}^{\mathbf{P}}(\theta_{ji}\mathbf{H}_{i}),\mathbf{S}_{0}=\mathbf{H}_{\mathbf{P}}\\'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{S}_{j}&amp;=Decoder(\mathbf{C}_{j},\mathbf{Y}_{j-1},\mathbf{S}_{j-1})\\
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{Y}_{j}&amp;=\mathbf{S}_{j}W\\
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd columnalign="right" ><msub ><mi mathsize="80%"  >𝐇</mi><mi
    mathsize="80%"  >i</mi></msub></mtd><mtd columnalign="left" ><mrow ><mo mathsize="80%"  >=</mo><mrow
    ><mi mathsize="80%" >E</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%"
    >n</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%" >c</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi mathsize="80%" >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    mathsize="80%" >d</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%"
    >e</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%" >r</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><msub ><mi
    mathsize="80%" >𝐗</mi><mi mathsize="80%" >i</mi></msub><mo mathsize="80%" >,</mo><msub
    ><mi mathsize="80%"  >𝐇</mi><mrow ><mi mathsize="80%"  >i</mi><mo mathsize="80%"  >−</mo><mn
    mathsize="80%"  >1</mn></mrow></msub><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><msub ><mi mathsize="80%"  >𝐂</mi><mi mathsize="80%"  >j</mi></msub></mtd><mtd
    columnalign="left" ><mrow ><mrow ><mo mathsize="80%" rspace="0.111em" >=</mo><mrow
    ><munderover ><mo maxsize="80%" minsize="80%" movablelimits="false" rspace="0em"
    stretchy="true"  >∑</mo><mrow ><mi mathsize="80%" >i</mi><mo mathsize="80%"  >=</mo><mn
    mathsize="80%"  >1</mn></mrow><mi mathsize="80%"  >𝐏</mi></munderover><mrow ><mo
    maxsize="80%" minsize="80%" >(</mo><mrow ><msub ><mi mathsize="80%" >θ</mi><mrow
    ><mi mathsize="80%" >j</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%"
    >i</mi></mrow></msub><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi mathsize="80%"
    >𝐇</mi><mi mathsize="80%" >i</mi></msub></mrow><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow><mo
    mathsize="80%"  >,</mo><mrow ><msub ><mi mathsize="80%"  >𝐒</mi><mn mathsize="80%"  >0</mn></msub><mo
    mathsize="80%"  >=</mo><msub ><mi mathsize="80%" >𝐇</mi><mi mathsize="80%" >𝐏</mi></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><msub ><mi mathsize="80%"  >𝐒</mi><mi mathsize="80%"  >j</mi></msub></mtd><mtd
    columnalign="left" ><mrow ><mo mathsize="80%"  >=</mo><mrow ><mi mathsize="80%"
    >D</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%" >e</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi mathsize="80%" >c</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    mathsize="80%" >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%"
    >d</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%" >e</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi mathsize="80%" >r</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo maxsize="80%" minsize="80%"  >(</mo><msub ><mi mathsize="80%" >𝐂</mi><mi
    mathsize="80%" >j</mi></msub><mo mathsize="80%" >,</mo><msub ><mi mathsize="80%"  >𝐘</mi><mrow
    ><mi mathsize="80%"  >j</mi><mo mathsize="80%"  >−</mo><mn mathsize="80%"  >1</mn></mrow></msub><mo
    mathsize="80%"  >,</mo><msub ><mi mathsize="80%" >𝐒</mi><mrow ><mi mathsize="80%"
    >j</mi><mo mathsize="80%" >−</mo><mn mathsize="80%" >1</mn></mrow></msub><mo maxsize="80%"
    minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="right"
    ><msub ><mi mathsize="80%"  >𝐘</mi><mi mathsize="80%"  >j</mi></msub></mtd><mtd
    columnalign="left" ><mrow ><mo mathsize="80%"  >=</mo><mrow ><msub ><mi mathsize="80%"
    >𝐒</mi><mi mathsize="80%" >j</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >W</mi></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply  ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐇</ci><ci >𝑖</ci></apply><apply
    ><ci  >𝐸</ci><ci >𝑛</ci><ci >𝑐</ci><ci  >𝑜</ci><ci >𝑑</ci><ci >𝑒</ci><ci  >𝑟</ci><interval
    closure="open"  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐗</ci><ci
    >𝑖</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐇</ci><apply
    ><ci >𝑖</ci><cn type="integer"  >1</cn></apply></apply></interval><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝐂</ci><ci >𝑗</ci></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><apply ><ci  >𝑖</ci><cn type="integer"  >1</cn></apply></apply><ci
    >𝐏</ci></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜃</ci><apply ><ci >𝑗</ci><ci >𝑖</ci></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐇</ci><ci >𝑖</ci></apply></apply></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐒</ci><cn type="integer"
    >0</cn></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐇</ci><ci >𝐏</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐒</ci><ci >𝑗</ci></apply></apply></apply><apply ><apply ><ci  >𝐷</ci><ci >𝑒</ci><ci
    >𝑐</ci><ci  >𝑜</ci><ci >𝑑</ci><ci >𝑒</ci><ci >𝑟</ci><vector ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐂</ci><ci >𝑗</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐘</ci><apply ><ci >𝑗</ci><cn type="integer"
    >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐒</ci><apply ><ci >𝑗</ci><cn type="integer" >1</cn></apply></apply></vector><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐘</ci><ci >𝑗</ci></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐒</ci><ci >𝑗</ci></apply><ci
    >𝑊</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{split}\mathbf{H}_{i}&=Encoder(\mathbf{X}_{i},\mathbf{H}_{i-1})\\ \mathbf{C}_{j}&=\sum_{i=1}^{\mathbf{P}}(\theta_{ji}\mathbf{H}_{i}),\mathbf{S}_{0}=\mathbf{H}_{\mathbf{P}}\\
    \mathbf{S}_{j}&=Decoder(\mathbf{C}_{j},\mathbf{Y}_{j-1},\mathbf{S}_{j-1})\\ \mathbf{Y}_{j}&=\mathbf{S}_{j}W\\
    \end{split}</annotation></semantics></math> |  | (21) |
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: where $\theta_{ji}=\frac{\exp(f_{ji})}{\sum_{k=1}^{\mathbf{P}}\exp(f_{jk})}$
    is the normalized attention score, and $f_{ji}=f(\mathbf{H}_{j},\mathbf{S}_{i-1})$
    [[146](#bib.bib146)] is a function to measure the correlation between $i^{th}$
    input and $j^{th}$ output, for instance, Luong et al. [[147](#bib.bib147)] proposed
    three kinds of attention score calculation.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="f_{ji}=\left\{\begin{array}[]{ll}\mathbf{H}_{j}^{T}\mathbf{S}_{i-1}&amp;\text{
    dot }\\ \mathbf{H}_{j}^{T}\boldsymbol{W}_{\boldsymbol{a}}\mathbf{S}_{i-1}&amp;\text{
    general }\\'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: \boldsymbol{v}_{a}^{T}\tanh\left(\boldsymbol{W}_{\boldsymbol{a}}\left[\mathbf{H}_{j},\mathbf{S}_{i-1}\right]\right)&amp;\text{
    concat }\end{array}\right." display="block"><semantics ><mrow ><msub  ><mi mathsize="80%"  >f</mi><mrow
    ><mi mathsize="80%" >j</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathsize="80%"
    >i</mi></mrow></msub><mo mathsize="80%" >=</mo><mrow  ><mo >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  columnalign="left" ><mrow  ><msubsup
    ><mi mathsize="80%" >𝐇</mi><mi mathsize="80%"  >j</mi><mi mathsize="80%"  >T</mi></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi mathsize="80%" >𝐒</mi><mrow ><mi
    mathsize="80%" >i</mi><mo mathsize="80%" >−</mo><mn mathsize="80%" >1</mn></mrow></msub></mrow></mtd><mtd
    columnalign="left"  ><mtext mathsize="80%" > dot </mtext></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow  ><msubsup ><mi mathsize="80%" >𝐇</mi><mi mathsize="80%"  >j</mi><mi mathsize="80%"  >T</mi></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi mathsize="80%" >𝑾</mi><mi mathsize="80%"
    >𝒂</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi mathsize="80%"
    >𝐒</mi><mrow ><mi mathsize="80%" >i</mi><mo mathsize="80%" >−</mo><mn mathsize="80%"
    >1</mn></mrow></msub></mrow></mtd><mtd columnalign="left"  ><mtext mathsize="80%"
    > general </mtext></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow  ><msubsup
    ><mi mathsize="80%" >𝒗</mi><mi mathsize="80%"  >a</mi><mi mathsize="80%"  >T</mi></msubsup><mo
    lspace="0.167em" rspace="0em"  >​</mo><mrow ><mi mathsize="80%" >tanh</mi><mo
    >⁡</mo><mrow ><mo  >(</mo><mrow ><msub ><mi mathsize="80%"  >𝑾</mi><mi mathsize="80%"  >𝒂</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo >[</mo><msub ><mi mathsize="80%"  >𝐇</mi><mi
    mathsize="80%"  >j</mi></msub><mo mathsize="80%"  >,</mo><msub ><mi mathsize="80%"  >𝐒</mi><mrow
    ><mi mathsize="80%"  >i</mi><mo mathsize="80%"  >−</mo><mn mathsize="80%"  >1</mn></mrow></msub><mo
    >]</mo></mrow></mrow><mo >)</mo></mrow></mrow></mrow></mtd><mtd columnalign="left"  ><mtext
    mathsize="80%"  > concat </mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >𝑓</ci><apply
    ><ci >𝑗</ci><ci  >𝑖</ci></apply></apply><apply ><csymbol cd="latexml" >cases</csymbol><matrix  ><matrixrow
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐇</ci><ci >𝑗</ci></apply><ci >𝑇</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐒</ci><apply ><ci >𝑖</ci><cn
    type="integer" >1</cn></apply></apply></apply><ci ><mtext mathsize="80%" > dot </mtext></ci></matrixrow><matrixrow
    ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐇</ci><ci >𝑗</ci></apply><ci >𝑇</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑾</ci><ci >𝒂</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐒</ci><apply ><ci  >𝑖</ci><cn
    type="integer"  >1</cn></apply></apply></apply><ci ><mtext mathsize="80%" > general </mtext></ci></matrixrow><matrixrow
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝒗</ci><ci >𝑎</ci></apply><ci >𝑇</ci></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑾</ci><ci >𝒂</ci></apply><interval
    closure="closed" ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐇</ci><ci
    >𝑗</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐒</ci><apply
    ><ci >𝑖</ci><cn type="integer"  >1</cn></apply></apply></interval></apply></apply></apply><ci
    ><mtext mathsize="80%" > concat </mtext></ci></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >f_{ji}=\left\{\begin{array}[]{ll}\mathbf{H}_{j}^{T}\mathbf{S}_{i-1}&\text{
    dot }\\ \mathbf{H}_{j}^{T}\boldsymbol{W}_{\boldsymbol{a}}\mathbf{S}_{i-1}&\text{
    general }\\ \boldsymbol{v}_{a}^{T}\tanh\left(\boldsymbol{W}_{\boldsymbol{a}}\left[\mathbf{H}_{j},\mathbf{S}_{i-1}\right]\right)&\text{
    concat }\end{array}\right.</annotation></semantics></math> |  | (22) |
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Another way to enhance Seq2Seq performance is the scheduled sampling technique
    [[148](#bib.bib148)]. The inputs of decoder during training and testing phases
    are different. Decoder during training phase is fed with true labels of training
    datasets while it is fed with predictions generated by itself during testing phase,
    which accumulates error at testing time and causes degraded performance. To mitigate
    this issue, scheduled sampling is integrated into the model. At $j^{th}$ iteration
    during the training process, the probability of feeding the decoder with true
    label is set as $\epsilon_{j}$ and the probability of feeding the decoder with
    prediction at the previous step is set as $1-\epsilon_{j}$. Probability $\epsilon_{j}$
    gradually decreases to 0, allowing the decoder to learn the testing distribution
    [[108](#bib.bib108)], keeping the training and testing as same as possible.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: V-D2 Seq2Seq in Traffic Domain
  id: totrans-366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since Seq2Seq can take in an input sequence and generate an output sequence
    with different length, it is applied on multi-step prediction in many traffic
    tasks. The encoder encodes the historical traffic data into a latent space vector.
    Then, the latent vector is fed into a decoder to generate the future traffic conditions.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanism is usually incorporated into Seq2Seq to model the different
    influence on future prediction from previous traffic observations at different
    time slots [[100](#bib.bib100)],[[98](#bib.bib98)], [[110](#bib.bib110)],[[76](#bib.bib76)].
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder and decoder in many traffic literatures are in charge of capturing
    spatial-temporal dependencies. For instance, Li et al. [[108](#bib.bib108)] proposed
    DCGRU to be the encoder and decoder, which can capture spatial and temporal dynamics
    jointly. The design of encoder and decoder is usually the core contribution and
    novel part of relative works. Note that the encoder and decoder are not necessarily
    the same and we have made a summarization of Seq2Seq structure in previous graph-based
    traffic works (as shown in Table [III](#S5.T3 "TABLE III ‣ V-D2 Seq2Seq in Traffic
    Domain ‣ V-D Seq2Seq ‣ V Deep Learning Techniques Perspective ‣ How to Build a
    Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")).'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: The encoders and decoders of sequence to sequence architecture'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '| References | Encoder | Decoder |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| [[108](#bib.bib108)] | GRU+DGCN | Same as encoder |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| [[100](#bib.bib100)] | SGCN +LSTM | LSTM+SGCN |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| [[98](#bib.bib98)] | STAtt Block | Same as encoder |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| [[41](#bib.bib41)] | MLPs | An MLP |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| [[109](#bib.bib109)] | SGCN+Pooling+GRU | GCN+Upooling+GRU |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| [[104](#bib.bib104)] | GRU with graph self-attention | Same as encoder |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| [[42](#bib.bib42)] | GRU+SGCN | Same as encoder |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| [[110](#bib.bib110)] | SGCN+ bidirectional GRU | Same as encoder |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| [[76](#bib.bib76)] | Long-term encoder (Gated SGCN) | Short-term encoder
    |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| [[113](#bib.bib113)] | SGCN+LSTM | LSTM |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| [[96](#bib.bib96)] | SGCN+GRU | Same as encoder |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| [[77](#bib.bib77)] | CGRM (GRU, SGCN) | Same as encoder |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| [[103](#bib.bib103)] | LSTM+RGC | RGC |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| [[48](#bib.bib48)] | LSTM | Same as encoder |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: RNNs-based decoder has a severe error accumulation problem during testing inference
    due to that each previous predicted step is the input to produce the next step
    prediction. The scheduled sampling to alleviate this problem is adopted in [[108](#bib.bib108)],[[104](#bib.bib104)].
    RNNs-based decoder is replaced with a short-term and long-term decoder to take
    in last step prediction exclusively, thus easing error accumulation [[76](#bib.bib76)].
    The utilization of Seq2Seq technique in traffic domain is flexible. For instance,
    Seq2Seq is integrated into a bigger framework, being the generator and discriminator
    of GAN [[100](#bib.bib100)].
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: V-E GAN
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-E1 GAN
  id: totrans-389
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/651837ea4ea84ac50ca890f443f44404.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Generative Adversarial Network: Generator $G$ is in charge of producing
    a generated sample $x_{f}=G(z)$ from a random vector $z$, which is sampled from
    a prior distribution $p_{z}$. Discriminator $D$ is in charge of discriminating
    between the fake sample $x_{f}$ generated from $G$ and the real sample $x_{r}$
    from the training data.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative Adversarial Network (GAN) [[149](#bib.bib149)] is a powerful deep
    generative model aiming to generate artificial samples as indistinguishable as
    possible from their real counterparts. GAN, inspired by game theory, is composed
    of two players, a generative neural network called Generator $G$ and an adversarial
    network called Discriminator $D$ (as shown in Figure [10](#S5.F10 "Figure 10 ‣
    V-E1 GAN ‣ V-E GAN ‣ V Deep Learning Techniques Perspective ‣ How to Build a Graph-Based
    Deep Learning Architecture in Traffic Domain: A Survey")).'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator $D$ tries to determine whether the input samples belong to the
    generated data or the real data while Generator $G$ tries to cheat on Discriminator
    $D$ by producing samples as true as possible. The two mutually adversarial and
    optimized processes are alternately trained, which strengthens the performance
    of both $D$ and $G$. When the fake sample produced by $G$ is very close to the
    ground truth and $D$ is unable to distinguish them any more, it is considered
    that Generator $G$ has learned the true distribution of the real data and the
    model converges. At this time, we can consider this game to reach a Nash equilibrium
    [[150](#bib.bib150)].
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, such process can be formulated to minimize their losses $Loss_{G}$
    and $Loss_{D}$. With the loss function being cross entropy denoted as $f$, we
    can have:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}Loss_{G}&amp;=f(D(G(z)),1)=-\sum\log D(G(z))\\
    \phi^{*}&amp;=\underset{\phi}{\operatorname{argmin}}(Loss_{G})=\underset{\phi}{\operatorname{argmax}}(-Loss_{G})\\'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;=\underset{\phi}{\operatorname{argmax}}\mathbb{E}(\log D(G(z)))\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  columnalign="right" ><mrow ><mi mathsize="80%"  >L</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="80%"  >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >s</mi><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi mathsize="80%"
    >s</mi><mi mathsize="80%" >G</mi></msub></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mo mathsize="80%" >=</mo><mrow ><mi mathsize="80%"  >f</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow ><mi
    mathsize="80%" >D</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow ><mi mathsize="80%"  >G</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo maxsize="80%" minsize="80%"  >(</mo><mi mathsize="80%"  >z</mi><mo maxsize="80%"
    minsize="80%"  >)</mo></mrow></mrow><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    mathsize="80%"  >,</mo><mn mathsize="80%"  >1</mn><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    mathsize="80%"  >=</mo><mrow ><mo mathsize="80%" >−</mo><mrow ><mo maxsize="80%"
    minsize="80%" movablelimits="false" stretchy="true" >∑</mo><mrow ><mrow  ><mi
    mathsize="80%"  >log</mi><mo lspace="0.167em"  >⁡</mo><mi mathsize="80%"  >D</mi></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow
    ><mi mathsize="80%"  >G</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="80%"
    minsize="80%"  >(</mo><mi mathsize="80%"  >z</mi><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><msup ><mi mathsize="80%" >ϕ</mi><mo mathsize="80%"
    >∗</mo></msup></mtd><mtd columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow
    ><munder accentunder="true" ><mi mathsize="80%" >argmin</mi><mo mathsize="80%"
    mathvariant="italic" >ϕ</mo></munder><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo maxsize="80%" minsize="80%"  >(</mo><mrow ><mi mathsize="80%" >L</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >o</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="80%"  >s</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi mathsize="80%" >s</mi><mi mathsize="80%" >G</mi></msub></mrow><mo maxsize="80%"
    minsize="80%"  >)</mo></mrow></mrow><mo mathsize="80%"  >=</mo><mrow ><munder
    accentunder="true" ><mi mathsize="80%" >argmax</mi><mo mathsize="80%" mathvariant="italic"
    >ϕ</mo></munder><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow ><mo mathsize="80%" >−</mo><mrow ><mi mathsize="80%"
    >L</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >o</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="80%"  >s</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi mathsize="80%"  >s</mi><mi mathsize="80%"  >G</mi></msub></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd
    columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow ><munder accentunder="true"
    ><mi mathsize="80%" >argmax</mi><mo mathsize="80%" mathvariant="italic" >ϕ</mo></munder><mo
    lspace="0.167em" rspace="0em" >​</mo><mi mathsize="80%"  >𝔼</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow ><mrow
    ><mi mathsize="80%"  >log</mi><mo lspace="0.167em"  >⁡</mo><mi mathsize="80%"  >D</mi></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow
    ><mi mathsize="80%"  >G</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="80%"
    minsize="80%"  >(</mo><mi mathsize="80%"  >z</mi><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >𝐿</ci><ci  >𝑜</ci><ci
    >𝑠</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci  >𝐺</ci></apply></apply><apply
    ><ci >𝑓</ci><interval closure="open" ><apply  ><ci >𝐷</ci><apply ><ci  >𝐺</ci><ci
    >𝑧</ci></apply></apply><cn type="integer"  >1</cn></interval></apply></apply><apply
    ><apply ><apply  ><apply ><apply ><ci  >𝐷</ci></apply><apply ><ci >𝐺</ci><ci >𝑧</ci></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >italic-ϕ</ci></apply></apply></apply></apply></apply><apply
    ><apply ><apply  ><ci >italic-ϕ</ci><ci >argmin</ci></apply><apply ><ci >𝐿</ci><ci  >𝑜</ci><ci
    >𝑠</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝐺</ci></apply></apply></apply></apply><apply
    ><apply ><apply  ><ci >italic-ϕ</ci><ci >argmax</ci></apply><apply ><apply ><ci  >𝐿</ci><ci
    >𝑜</ci><ci >𝑠</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci
    >𝐺</ci></apply></apply></apply></apply></apply><apply ><apply ><apply  ><ci >italic-ϕ</ci><ci
    >argmax</ci></apply><ci >𝔼</ci><apply ><apply ><ci  >𝐷</ci></apply><apply ><ci
    >𝐺</ci><ci >𝑧</ci></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}Loss_{G}&=f(D(G(z)),1)=-\sum\log D(G(z))\\
    \phi^{*}&=\underset{\phi}{\operatorname{argmin}}(Loss_{G})=\underset{\phi}{\operatorname{argmax}}(-Loss_{G})\\
    &=\underset{\phi}{\operatorname{argmax}}\mathbb{E}(\log D(G(z)))\end{split}</annotation></semantics></math>
    |  | (23) |'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}Loss_{D}&amp;=f(D(x_{r}),1,D(x_{f}),0)\\
    &amp;=-\sum\log D(x_{r})-\sum\log(1-D(x_{f}))\\'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: \theta^{*}&amp;=\underset{\theta}{\operatorname{argmin}}(Loss_{D})=\underset{\theta}{\operatorname{argmax}}(-Loss_{D})\\
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;=\underset{\theta}{\operatorname{argmax}}(\mathbb{E}(\log D(x_{r})+\log(1-D(x_{f}))))\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="right"  ><mrow ><mi mathsize="80%" >L</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi mathsize="80%" >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    mathsize="80%" >s</mi><mo lspace="0em" rspace="0em" >​</mo><msub  ><mi mathsize="80%"  >s</mi><mi
    mathsize="80%"  >D</mi></msub></mrow></mtd><mtd columnalign="left"  ><mrow ><mo
    mathsize="80%" >=</mo><mrow ><mi mathsize="80%"  >f</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo maxsize="80%" minsize="80%"  >(</mo><mrow ><mi mathsize="80%" >D</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><msub
    ><mi mathsize="80%"  >x</mi><mi mathsize="80%"  >r</mi></msub><mo maxsize="80%"
    minsize="80%"  >)</mo></mrow></mrow><mo mathsize="80%"  >,</mo><mn mathsize="80%"  >1</mn><mo
    mathsize="80%"  >,</mo><mrow ><mi mathsize="80%" >D</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo maxsize="80%" minsize="80%"  >(</mo><msub ><mi mathsize="80%"  >x</mi><mi
    mathsize="80%"  >f</mi></msub><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo
    mathsize="80%"  >,</mo><mn mathsize="80%"  >0</mn><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow ><mrow ><mo
    mathsize="80%" >−</mo><mrow ><mo maxsize="80%" minsize="80%" movablelimits="false"
    stretchy="true"  >∑</mo><mrow ><mrow ><mi mathsize="80%"  >log</mi><mo lspace="0.167em"  >⁡</mo><mi
    mathsize="80%"  >D</mi></mrow><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    maxsize="80%" minsize="80%"  >(</mo><msub ><mi mathsize="80%"  >x</mi><mi mathsize="80%"  >r</mi></msub><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mrow><mo mathsize="80%"
    rspace="0.055em"  >−</mo><mrow ><mo maxsize="80%" minsize="80%" movablelimits="false"
    stretchy="true" >∑</mo><mrow ><mi mathsize="80%" >log</mi><mo >⁡</mo><mrow ><mo
    maxsize="80%" minsize="80%" >(</mo><mrow ><mn mathsize="80%" >1</mn><mo mathsize="80%"
    >−</mo><mrow ><mi mathsize="80%" >D</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo maxsize="80%" minsize="80%" >(</mo><msub ><mi mathsize="80%" >x</mi><mi mathsize="80%"
    >f</mi></msub><mo maxsize="80%" minsize="80%" >)</mo></mrow></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><msup ><mi mathsize="80%" >θ</mi><mo mathsize="80%"
    >∗</mo></msup></mtd><mtd columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow
    ><munder accentunder="true" ><mi mathsize="80%" >argmin</mi><mo mathsize="80%"  >𝜃</mo></munder><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow
    ><mi mathsize="80%"  >L</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="80%"  >s</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msub ><mi mathsize="80%"  >s</mi><mi mathsize="80%"  >D</mi></msub></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo mathsize="80%"  >=</mo><mrow
    ><munder accentunder="true" ><mi mathsize="80%" >argmax</mi><mo mathsize="80%"  >𝜃</mo></munder><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow
    ><mo mathsize="80%"  >−</mo><mrow ><mi mathsize="80%"  >L</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="80%"  >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="80%"  >s</mi><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi mathsize="80%"  >s</mi><mi
    mathsize="80%"  >D</mi></msub></mrow></mrow><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo mathsize="80%" >=</mo><mrow ><munder accentunder="true"
    ><mi mathsize="80%" >argmax</mi><mo mathsize="80%"  >𝜃</mo></munder><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow ><mi
    mathsize="80%"  >𝔼</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="80%"
    minsize="80%"  >(</mo><mrow ><mrow ><mrow ><mi mathsize="80%"  >log</mi><mo lspace="0.167em"  >⁡</mo><mi
    mathsize="80%"  >D</mi></mrow><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    maxsize="80%" minsize="80%" >(</mo><msub ><mi mathsize="80%" >x</mi><mi mathsize="80%"
    >r</mi></msub><mo maxsize="80%" minsize="80%" >)</mo></mrow></mrow><mo mathsize="80%"  >+</mo><mrow
    ><mi mathsize="80%"  >log</mi><mo >⁡</mo><mrow ><mo maxsize="80%" minsize="80%"  >(</mo><mrow
    ><mn mathsize="80%"  >1</mn><mo mathsize="80%"  >−</mo><mrow ><mi mathsize="80%"  >D</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo maxsize="80%" minsize="80%" >(</mo><msub
    ><mi mathsize="80%" >x</mi><mi mathsize="80%" >f</mi></msub><mo maxsize="80%"
    minsize="80%" >)</mo></mrow></mrow></mrow><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow><mo
    maxsize="80%" minsize="80%"  >)</mo></mrow></mrow><mo maxsize="80%" minsize="80%"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >𝐿</ci><ci  >𝑜</ci><ci
    >𝑠</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci  >𝐷</ci></apply></apply><apply
    ><ci >𝑓</ci><vector  ><apply ><ci >𝐷</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><ci >𝑟</ci></apply></apply><cn type="integer" >1</cn><apply ><ci  >𝐷</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><ci >𝑓</ci></apply></apply><cn
    type="integer"  >0</cn></vector></apply></apply><apply ><apply ><apply  ><apply
    ><apply ><apply  ><ci >𝐷</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><ci >𝑟</ci></apply></apply></apply></apply><apply ><apply ><apply  ><apply
    ><cn type="integer" >1</cn><apply ><ci >𝐷</ci><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑥</ci><ci >𝑓</ci></apply></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝜃</ci></apply></apply></apply></apply></apply><apply
    ><apply ><apply  ><ci >𝜃</ci><ci >argmin</ci></apply><apply ><ci >𝐿</ci><ci  >𝑜</ci><ci
    >𝑠</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝐷</ci></apply></apply></apply></apply><apply
    ><apply ><apply  ><ci >𝜃</ci><ci >argmax</ci></apply><apply ><apply ><ci  >𝐿</ci><ci
    >𝑜</ci><ci >𝑠</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci
    >𝐷</ci></apply></apply></apply></apply></apply><apply ><apply ><apply  ><ci >𝜃</ci><ci
    >argmax</ci></apply><apply ><ci >𝔼</ci><apply ><apply ><apply ><ci  >𝐷</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><ci >𝑟</ci></apply></apply><apply
    ><apply ><cn type="integer" >1</cn><apply ><ci  >𝐷</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><ci >𝑓</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}Loss_{D}&=f(D(x_{r}),1,D(x_{f}),0)\\
    &=-\sum\log D(x_{r})-\sum\log(1-D(x_{f}))\\ \theta^{*}&=\underset{\theta}{\operatorname{argmin}}(Loss_{D})=\underset{\theta}{\operatorname{argmax}}(-Loss_{D})\\
    &=\underset{\theta}{\operatorname{argmax}}(\mathbb{E}(\log D(x_{r})+\log(1-D(x_{f}))))\end{split}</annotation></semantics></math>
    |  | (24) |'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: where $1$ is the label of true sample $x_{r}$. $0$ is the label of fake sample
    $x_{f}=G(z)$. $\phi$ and $\theta$ are the trainable parameters of $G$ and $D$
    respectively. Note that when $G$ is trained, $D$ is untrainable. Interested readers
    can refer to [[151](#bib.bib151)],[[152](#bib.bib152)] for surveys of GAN.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: V-E2 GAN in Traffic Domain
  id: totrans-401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When GAN is applied in traffic prediction tasks [[153](#bib.bib153)],[[154](#bib.bib154)],
    Generator $G$ is usually employed to generate future traffic observations based
    on the historical observations. Then the generated data and the future real data
    are fed into Discriminator $D$ to train it. After training, Generator $G$ can
    learn the distribution of the real traffic flow data through a large number of
    historical data and can be used to predict the future traffic states [[100](#bib.bib100)].
    GAN can be also utilized to solve the sparsity problem of traffic data for its
    efficacy in handling data generation [[95](#bib.bib95)].
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the generator or discriminator of GAN can be any model, such as
    RNNs, Seq2Seq, depending on the specific traffic tasks.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: VI Challenges Perspective
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traffic tasks are very challenging due to the complicated spatial dependency,
    temporal dependency in traffic data. In addition, external factors such as holiday
    or event can also affect the traffic conditions. In this section, we introduce
    four common challenges in traffic domain. We carefully examine each challenge
    and its corresponding solutions, making necessary comparison.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Spatial Dependency
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a3dca06c8862d2f2e5dc10d162e1fd0d.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The formulation of a bidirectional road: The traffic condition of
    road $R_{1}$ is only influenced by the same side road $R_{2}$ and has weak correlation
    with the opposite side road $R_{3}$. But if this region is modeled as grids, $R_{3}$
    has similar impact on $R_{1}$ as $R_{2}$, which is against the truth. If it is
    model as a graph, $R_{1}$ is connected with $R_{2}$ and disconnected with $R_{3}$,
    which can reflect the true relationship.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in previous section, some literatures[[131](#bib.bib131)],[[132](#bib.bib132)],[[155](#bib.bib155)]
    extract spatial features through decomposing the whole traffic network into grids
    and then employing CNNs to process the grid-based data. However, the grid-based
    assumption actually violates the nature topology of traffic network. Many traffic
    networks are physically organized as a graph and the graph topology information
    is obviously valuable for traffic prediction (as shown in Figure [11](#S6.F11
    "Figure 11 ‣ VI-A Spatial Dependency ‣ VI Challenges Perspective ‣ How to Build
    a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")). According
    to our survey, graph neural networks can model spatial dependencies in graph-based
    traffic networks much better than grid-based approaches. In addition, the complicated
    spatial dependencies in traffic network can be categorized into three spatial
    attributes, i.e. spatial locality, multiple relationships and global connectivity.
    Different kinds of GNNs combining with other deep learning techniques are utilized
    to solve different kinds of spatial attributes.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: VI-A1 Spatial Locality
  id: totrans-410
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Spatial locality refers that adjacent regions are usually highly relevant to
    each other. For example, the passenger flow of a station in a subway is obviously
    affected by its connected stations. $\mathbf{K}$-localized spectral graph convolution
    network (SGCN) is widely adopted to aggregate the information of $0$ to $\mathbf{K}-1$
    hop neighbors to the central region. In addition, some works make different assumptions
    about the spatial locality and utilize some novel tricks.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: The adjacency matrix representing the traffic topology is usually pre-defined
    while some works [[69](#bib.bib69)],[[42](#bib.bib42)] argued that neighboring
    locations are dynamically correlated with each other. They incorporated the attention
    mechanism into SGCN to adaptively capture the dynamic correlations among surrounding
    regions.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: SGCN requires all the regions to have the same local statistics and its convolution
    kernel is location-independent. However, Zhang et al. [[68](#bib.bib68)] clarified
    that the local statistics of traffic data changed from region to region and they
    designed location-dependent kernels for different regions automatically.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: VI-A2 Multiple Relationships
  id: totrans-414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While locality attribute focuses on spatial proximity, the target region can
    be correlated with distant regions through various non-Euclidean relationships
    such as functional similarity, transportation connectivity (as shown in Figure
    [5](#S4.F5 "Figure 5 ‣ IV-C1 Nodes and Node Features Construction ‣ IV-C Graph
    Construction from Traffic Datasets ‣ IV Problem Formulation and Graph Construction
    ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey")),
    semantic neighbors. Functional similarity refers that distant region is similar
    to the target region in terms of functionality, which can be characterized by
    the surrounding POIs [[89](#bib.bib89)],[[70](#bib.bib70)]. Transportation connectivity
    suggests that those geographically distant but conveniently reachable can be correlated
    [[89](#bib.bib89)]. The reachable way can be motorway, highway, subway. Semantic
    neighbors are adopted to model the correlation between origins and destinations
    [[101](#bib.bib101)]. The correlation is measured by the passenger flow between
    them. To explicitly extract these correlation information, different types of
    correlations using multiple graphs are encoded [[89](#bib.bib89)] and multi-graph
    convolution is leveraged.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: VI-A3 Global Connectivity
  id: totrans-416
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Both spatial proximity and multi-relationship focus on parts of the network
    while ignore the whole structure. Global connectivity refers that traffic conditions
    of different regions have influenced each other at a whole network scale. There
    are several strategies to exploit the global structure information of traffic
    network.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: A popular way to capture global connectivity is to model the changing traffic
    conditions in the traffic network as a diffusion process that happens at a network
    scale, which is presented by a power series of transition matrices. Then, diffusion
    graph convolution network (DGCN) is adopted to extract the spatial dependency
    globally [[112](#bib.bib112)], [[108](#bib.bib108)], [[100](#bib.bib100)], [[102](#bib.bib102)],
    [[96](#bib.bib96)],[[118](#bib.bib118)].
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: A novel spatial graph pooling layer with path growing algorithm is designed
    to produce a coarser graph [[105](#bib.bib105)]. This pooling layer is stacked
    before SGC layer to get multi-granularity graph convolutions, which can extract
    spatial features at various scopes.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: A SGC layer with a self-adaptive adjacency matrix is proposed [[102](#bib.bib102)]
    to capture the hidden global spatial dependency in the data. This self-adaptive
    adjacency matrix is learned from the data through an end-to-end supervised training.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Temporal Dependency
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Temporal dependency refers that prediction of traffic conditions at a certain
    time is usually correlated with various historical observations [[92](#bib.bib92)].
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated in Section [V](#S5 "V Deep Learning Techniques Perspective ‣ How
    to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey"),
    many works extract the temporal dependency by RNNs-based approaches. However,
    RNNs-based approaches suffer from time-consuming iterations and confront gradient
    vanishing/explosion problem for capturing long sequence. Compared with RNNs-based
    approaches, TCN-based approaches have the superiority of simple structures, parallel
    computing and stable gradients. Therefore, some works [[92](#bib.bib92)],[[70](#bib.bib70)]
    adopt TCN-based approaches to capture the temporal pattern in traffic data. In
    addition, TCN is able to handle different temporal levels by stacking multiple
    layers. For instance, Fang et al. [[111](#bib.bib111)] and Wu et al. [[102](#bib.bib102)]
    stacked multiple TCN layers with the bottom layers extracting short-term neighboring
    dependencies and the higher layers learning long-term temporal patterns.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: VI-B1 Multi-timescale
  id: totrans-424
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some works extract the temporal dependency at a multi-timescale perspective
    [[69](#bib.bib69)],[[116](#bib.bib116)]. Temporal dependency is decomposed into
    recent, daily and weekly dependencies [[69](#bib.bib69)]. The recent dependency
    refers that the future traffic conditions are influenced by the traffic conditions
    recently. For instance, the traffic congestion at 9 am inevitably influences traffic
    flow at the following hours. Daily dependency describes that the repeated daily
    pattern in traffic data due to the regular daily routine of people, such as morning
    peak and evening peak. Weekly dependency considers the influence caused by the
    same week attributes. For instance, all Mondays share similar traffic pattern
    in a short-term. Guo et al. [[69](#bib.bib69)] set three parallel components with
    the same structure to model these three temporal attributes respectively.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: VI-B2 Different Weights
  id: totrans-426
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some works argue that the correlations between historical and future observations
    are varying at different previous time slices. Guo et al. [[69](#bib.bib69)] adopted
    a temporal attention mechanism to adaptively attach different importance to historical
    data.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Spatiotemporal Dependency
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many works capture the spatial and temporal dependency separately in a sequential
    manner [[110](#bib.bib110)], [[100](#bib.bib100)], [[94](#bib.bib94)], [[65](#bib.bib65)],[[91](#bib.bib91)],[[120](#bib.bib120)],[[88](#bib.bib88)]
    while the spatial and temporal dependencies are closely intertwined in traffic
    data. Guo et al. [[69](#bib.bib69)] argued that the historical observations in
    different locations at different times have varying impacts on central region
    in the future. Take an obvious example, a traffic accident in a critical road
    results in serious disruptions over related roads but at different time, due to
    the gradual formation and dispersion of traffic congestion.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: 'A limitation of separately modeling is that the potential interactions between
    spatial features and temporal features are ignored, which may hurt the prediction
    performance. To overcome such limitation, a popular way is to incorporate the
    graph convolution operations (e.g. SGC, DGC) to RNNs (as stated in Section [VI](#S6
    "VI Challenges Perspective ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey")) to capture spatial-temporal correlations jointly
    [[66](#bib.bib66)], [[112](#bib.bib112)], [[108](#bib.bib108)], [[106](#bib.bib106)],
    [[96](#bib.bib96)],[[118](#bib.bib118)],[[42](#bib.bib42)], [[105](#bib.bib105)],
    [[77](#bib.bib77)].'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: VI-D External Factors
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Factors such as holidays, time attributes (e.g. hour, day, week, month, season,
    year) [[70](#bib.bib70)],[[116](#bib.bib116)], weather (e.g. rainfall, temperature,
    air quality)[[116](#bib.bib116)], special events, POIs[[89](#bib.bib89)] and traffic
    incidents (e.g. incident time, incident type) [[91](#bib.bib91)] can influence
    the traffic prediction in some extent, which we refer as external factors or context
    factors. In addition, Zhang et al. [[110](#bib.bib110)] considered historical
    statistical speed information (e.g. average or standard deviation of traffic speed)
    as external factor.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: Some factors such as day attributes, holidays and weather conditions are encoded
    as discrete values and they are usually transformed into binary vectors by one-hot
    encoding. Other factors including temperature, wind speed are encoded as continual
    values and they are usually normalized by Min-Max normalization or Z-score normalization.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: There are two approaches to handle external factors in the literatures we survey.
    The first approach is to concatenate the external factors with other features
    and feed them into model [[112](#bib.bib112)], [[70](#bib.bib70)]. The second
    approach is to design an external component in charge of processing external factors
    alone. The external component usually contains two fully connected layers, of
    which the first extracting important features and the second mapping low dimension
    features to high dimension features [[70](#bib.bib70)], [[91](#bib.bib91)],[[116](#bib.bib116)],[[48](#bib.bib48)].
    Bai et al. [[113](#bib.bib113)] employed multi-LSTM layers to extract representation
    of external factors. The output of external component is fused with other components
    to generate the final result.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: VII Public Datasets and Open Source Codes
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE IV: Some open traffic datasets'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Links | References |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| NYC taxi | https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page
    | [[99](#bib.bib99)], [[116](#bib.bib116)],[[91](#bib.bib91)],[[103](#bib.bib103)]
    |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| NYC bike | https://www.citibikenyc.com/system-data | [[116](#bib.bib116)],
    [[48](#bib.bib48)], [[76](#bib.bib76)], [[113](#bib.bib113)] |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| San Francisco taxi | https://crawdad.org/ crawdad/epfl/mobility/20090224/
    | [[91](#bib.bib91)] |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| Chicago bike | https://www.divvybikes.com/system-data | [[48](#bib.bib48)]
    |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '| BikeDC (Bike Washington) | https://www.capitalbikeshare.com/system-data |
    [[116](#bib.bib116)] |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: '| California -PEMS | http://pems.dot.ca.gov/ | [[92](#bib.bib92)],[[70](#bib.bib70)],[[69](#bib.bib69)],[[99](#bib.bib99)],[[112](#bib.bib112)],[[71](#bib.bib71)],[[98](#bib.bib98)],[[102](#bib.bib102)],[[106](#bib.bib106)],[[66](#bib.bib66)],[[96](#bib.bib96)]
    |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
- en: VII-A Public Datasets
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We summarize some public datasets (as shown in Table [IV](#S7.T4 "TABLE IV
    ‣ VII Public Datasets and Open Source Codes ‣ How to Build a Graph-Based Deep
    Learning Architecture in Traffic Domain: A Survey")) in the literatures we survey
    to help successors participate in this domain and produce more valuable works.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: VII-B Open Source Codes
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Open-source implementations are helpful for researchers to compare their approaches.
    We provide the hyperlinks of public source codes of the literatures reviewed in
    this paper (as shown in Table [V](#S7.T5 "TABLE V ‣ VII-B Open Source Codes ‣
    VII Public Datasets and Open Source Codes ‣ How to Build a Graph-Based Deep Learning
    Architecture in Traffic Domain: A Survey")) to facilitate the baseline experiments
    in traffic domain.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Some open source codes'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Model | Year | Framework | Github |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
- en: '| [[108](#bib.bib108)] | DCRNN | 2018 | Tensorflow | https://github.com/liyaguang/DCRNN
    |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
- en: '| [[97](#bib.bib97)] | GCNN | 2018 | Keras | https://github.com/RingBDStack/GCNN-In-Traffic
    |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
- en: '| [[93](#bib.bib93)] | T-GCN | 2019 | Tensorflow | https://github.com/lehaifeng/T-GCN
    |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
- en: '| [[98](#bib.bib98)] | GMAN | 2019 | Tensorflow | https://github.com/zhengchuanpan/GMAN
    |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
- en: '| [[102](#bib.bib102)] | Graph-WaveNet | 2019 | Torch | https://github.com/nnzhan/Graph-WaveNet
    |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
- en: VIII Future Directions
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have investigated the latest advances in graph-based traffic literatures
    and made a summary of these literatures in Table [II](#S5.T2 "TABLE II ‣ V Deep
    Learning Techniques Perspective ‣ How to Build a Graph-Based Deep Learning Architecture
    in Traffic Domain: A Survey"). Further, we suggest some directions for researchers
    to explore, which can be divided into three categories, i.e. application related,
    technique related, external factor related directions.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: VIII-1 Application Related Directions
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Table [II](#S5.T2 "TABLE II ‣ V Deep Learning Techniques Perspective
    ‣ How to Build a Graph-Based Deep Learning Architecture in Traffic Domain: A Survey"),
    there are many works utilizing graph-based deep learning architectures to tackle
    traffic state prediction and traffic demand prediction, which have achieved state-of-the-art
    performance. However, there are only a handful of works analyzing traffic data
    in a graph perspective in other research directions, such as vehicle behavior
    classification [[65](#bib.bib65)], optimal dynamic electronic toll collection
    (DETC) scheme [[57](#bib.bib57)], path availability [[66](#bib.bib66)], traffic
    signal control [[67](#bib.bib67)]. When it comes to traffic incident detection,
    vehicle detection, origin-destination travel demand prediction and transfer learning
    from City to City, works adopting graph-based deep learning techniques are rare
    up to now. Therefore, the upcoming participators can explore these directions
    in a graph perspective and learn the successful experiences from existing works.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: VIII-2 Technique Related Directions
  id: totrans-460
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On one hand, most existing works have employed spectral graph convolution network
    (SGCN) and diffusion graph convolution network (DGCN), two popular kinds of GNNs,
    to analyze traffic tasks. There are only a handful of works utilizing Graph attention
    networks (GATs) in traffic domain [[122](#bib.bib122)], [[98](#bib.bib98)], [[104](#bib.bib104)],
    [[107](#bib.bib107)],[[119](#bib.bib119)]. Other kinds of GNNs, such as graph
    auto-encoders (GAEs) [[156](#bib.bib156)],[[157](#bib.bib157)], recurrent graph
    neural networks (RecGNNs) [[158](#bib.bib158)] have achieved state-of-the-art
    performance in other domains, but they are seldom explored in traffic domain up
    to now. Therefore, it is worth to extend these branches of GNNs to traffic domain.
    On the other hand, recent works have combined GNNs with other deep learning techniques
    such as RNNs, TCN, Seq2Seq, GAN to solve the challenges in traffic tasks. However,
    few traffic works consider transfer learning, continue learning and reinforcement
    learning together with GNNs, which might be a promising direction for researchers.
    In addition, most of the graph-based traffic works are regression tasks, while
    classification tasks are few [[66](#bib.bib66)],[[65](#bib.bib65)]. Researchers
    can explore the classification traffic tasks in a graph perspective.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: VIII-3 External Factors Related Directions
  id: totrans-462
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally, many existing traffic models do not take external factors into consideration,
    for that external factors are hard to collect and have various formats. The data
    sparsity of external factors is still a challenge confronted by the research community.
    In addition, the techniques to process external factors are rather naive, e.g.
    a simple fully connected layer. There should be more approaches to process external
    factors.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: IX Conclusion
  id: totrans-464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we conduct a comprehensive review of various graph-based deep
    learning architectures in recent traffic works. More specifically, we summarize
    a general graph-based formulation of traffic problem and graph construction from
    various traffic datasets. Further, we decompose all the investigated architectures
    and analyze the common modules they share, including graph neural networks (GNNs),
    recurrent neural networks (RNNs), temporal convolution network (TCN), Sequence
    to Sequence (Seq2Seq) model, generative adversarial network (GAN). We provide
    a thorough description of their variants in traffic tasks, hoping to provide upcoming
    researchers insights into how to design novel techniques for their own traffic
    tasks. We also summarize the common challenges in many traffic scenarios, such
    as spatial dependency, temporal dependency, external factors. More than that,
    we present multiple deep learning based solutions for each challenge. In addition,
    we provide some hyperlinks of public datasets and codes in related works to facilitate
    the upcoming researches. Finally, we suggest some future directions for participators
    interested in this domain.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors would like to thank anonymous reviewers for their valuable comments.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: This work is supported by the National Key R&D Program of China (No.2019YFB2102100),
    National Natural Science Foundation of China (No.61802387), China’s Post-doctoral
    Science Fund (No.2019M663183), National Natural Science Foundation of Shenzhen
    (No.JCYJ20190812153212464), Shenzhen Engineering Research Center for Beidou Positioning
    Service Improvement Technology (No.XMHT20190101035), Science and Technology Development
    Fund of Macao S.A.R (FDCT) under number 0015/2019/AKP, Shenzhen Discipline Construction
    Project for Urban Computing and Data Intelligence.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-469
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] G. Yu and C. Zhang, “Switching ARIMA model based forecasting for traffic
    flow,” in *ICASSP*, 2004, pp. 429–432.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] B. M. Williams and L. A. Hoel, “Modeling and forecasting vehicular traffic
    flow as a seasonal arima process: Theoretical basis and empirical results,” *Journal
    of Transportation Engineering*, vol. 129, no. 6, pp. 664–672, 2003.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] S. R. Chandra and H. Al-Deek, “Predictions of freeway traffic speeds and
    volumes using vector autoregressive models,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 13, no. 2, pp. 53–72, 2009.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Y. Xie, Y. Zhang, and Z. Ye, “Short-term traffic volume forecasting using
    kalman filter with discrete wavelet decomposition,” *Computer-Aided Civil and
    Infrastructure Engineering*, vol. 22, no. 5, pp. 326–334, 2007.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] H. Fu, H. Ma, Y. Liu, and D. Lu, “A vehicle classification system based
    on hierarchical multi-svms in crowded traffic scenes,” *Neurocomputing*, vol.
    211, pp. 182–190, 2016.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. May, D. Hecker, C. Körner, S. Scheider, and D. Schulz, “A vector-geometry
    based spatial knn-algorithm for traffic frequency predictions,” in *ICDM Workshops*,
    2008, pp. 442–447.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] J. Liu, T. Li, P. Xie, S. Du, F. Teng, and X. Yang, “Urban big data fusion
    based on deep learning: An overview,” *Information Fusion*, vol. 53, pp. 123–133,
    2020.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Z. Lv, J. Xu, K. Zheng, H. Yin, P. Zhao, and X. Zhou, “LC-RNN: A deep learning
    model for traffic speed prediction,” in *IJCAI*, 2018, pp. 3470–3476.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] X. Ma, Z. Dai, Z. He, J. Ma, Y. Wang, and Y. Wang, “Learning traffic as
    images: a deep convolutional neural network for large-scale transportation network
    speed prediction,” *Sensors*, vol. 17, no. 4, p. 818, 2017.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] L. Yan, H. Shen, J. Zhao, C. Xu, F. Luo, and C. Qiu, “Catcharger: Deploying
    wireless charging lanes in a metropolitan road network through categorization
    and clustering of vehicle traffic,” in *INFOCOM*, 2017, pp. 1–9.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Y. Sun, X. Yu, R. Bie, and H. Song, “Discovering time-dependent shortest
    path on traffic graph for drivers towards green driving,” *Journal of Network
    and Computer Applications*, vol. 83, pp. 204–212, 2017.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] H. Sun, J. Wu, D. Ma, and J. Long, “Spatial distribution complexities
    of traffic congestion and bottlenecks in different network topologies,” *Applied
    Mathematical Modelling*, vol. 38, no. 2, pp. 496–505, 2014.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. Gori, G. Monfardini, and F. Scarselli, “A new model for learning in
    graph domains,” in *IJCNN*, vol. 2, 2005, pp. 729–734.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE Transactions on Neural Networks*, vol. 20,
    no. 1, pp. 61–80, 2008.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] M. Henaff, J. Bruna, and Y. LeCun, “Deep convolutional networks on graph-structured
    data,” *arXiv:1506.05163*, 2015.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, “Learning deep
    generative models of graphs,” *arXiv:1803.03324*, 2018.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Z.-M. Chen, X.-S. Wei, P. Wang, and Y. Guo, “Multi-label image recognition
    with graph convolutional networks,” in *CVPR*, 2019, pp. 5177–5186.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Z. Guo, Y. Zhang, and W. Lu, “Attention guided graph convolutional networks
    for relation extraction,” in *ACL*, 2019, pp. 241–251.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, and e. Bombarell, “Convolutional
    networks on graphs for learning molecular fingerprints,” in *NIPS*, 2015, pp.
    2224–2232.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec,
    “Graph convolutional neural networks for web-scale recommender systems,” in *KDD*,
    2018, pp. 974–983.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] M. G. Karlaftis and E. I. Vlahogianni, “Statistical methods versus neural
    networks in transportation research: Differences, similarities and some insights,”
    *Transportation Research Part C: Emerging Technologies*, vol. 19, no. 3, pp. 387–399,
    2011.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] E. I. Vlahogianni, M. G. Karlaftis, and J. C. Golias, “Short-term traffic
    forecasting: Where we are and where we’re going,” *Transportation Research Part
    C: Emerging Technologies*, vol. 43, pp. 3–19, 2014.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] P. Xie, T. Li, J. Liu, S. Du, X. Yang, and J. Zhang, “Urban flow prediction
    from spatiotemporal data using machine learning: A survey,” *Information Fusion*,
    2020.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] H. Nguyen, L.-M. Kieu, T. Wen, and C. Cai, “Deep learning methods in transportation
    domain: a review,” *IET Intelligent Transport Systems*, vol. 12, no. 9, pp. 998–1004,
    2018.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Wang, D. Zhang, Y. Liu, B. Dai, and L. H. Lee, “Enhancing transportation
    systems via deep learning: A survey,” *Transportation Research Part C: Emerging
    Technologies*, vol. 99, pp. 144–163, 2019.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] M. Veres and M. Moussa, “Deep learning for intelligent transportation
    systems: A survey of emerging trends,” *IEEE Transactions on Intelligent Transportation
    Systems*, 2019.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Q. Chen, W. Wang, F. Wu, S. De, and e. Wang, “A survey on an emerging
    area: Deep learning for smart city data,” *IEEE Transactions on Emerging Topics
    in Computational Intelligence*, vol. 3, no. 5, pp. 392–410, 2019.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] S. Wang, J. Cao, and P. S. Yu, “Deep learning for spatio-temporal data
    mining: A survey,” *arXiv:1906.04928*, 2019.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, “Geometric
    deep learning: going beyond euclidean data,” *IEEE Signal Processing Magazine*,
    vol. 34, no. 4, pp. 18–42, 2017.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Zhou, G. Cui, Z. Zhang, and e. Yang, “Graph neural networks: A review
    of methods and applications,” *arXiv:1812.08434*, 2018.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Zhang, “Graph neural networks for small graph and giant network representation
    learning: An overview,” *arXiv:1908.00187*, 2019.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] P. Quan, Y. Shi, M. Lei, J. Leng, T. Zhang, and L. Niu, “A brief review
    of receptive fields in graph convolutional networks,” in *IEEE/WIC/ACM International
    Conference on Web Intelligence-Companion Volume*, 2019, pp. 106–110.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] S. Zhang, H. Tong, J. Xu, and R. Maciejewski, “Graph convolutional networks:
    a comprehensive review,” *Computational Social Networks*, vol. 6, no. 1, p. 11,
    2019.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Z. Wu, S. Pan, F. Chen, G. Long, and e. Zhang, “A comprehensive survey
    on graph neural networks,” *IEEE Transactions on Neural Networks and Learning
    Systems*, 2020.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Chen, Y. Lv, Z. Li, and F. Wang, “Long short-term memory model for
    traffic congestion prediction with online open data,” in *ITSC*, 2016, pp. 132–137.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] L. Yan and H. Shen, “TOP: optimizing vehicle driving speed with vehicle
    trajectories for travel time minimization and road congestion avoidance,” *ACM
    Trans. Cyber Phys. Syst.*, vol. 4, no. 2, pp. 17:1–17:25, 2020.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] L. Yan, H. Shen, and K. Chen, “Mobit: Distributed and congestion-resilient
    trajectory-based routing for vehicular delay tolerant networks,” *IEEE/ACM Trans.
    Netw.*, vol. 26, no. 3, pp. 1078–1091, 2018.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] X. Ma, H. Yu, Y. Wang, and Y. Wang, “Large-scale transportation network
    congestion evolution prediction using deep learning theory,” *PloS one*, vol. 10,
    no. 3, 2015.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] F. Sun, A. Dubey, and J. White, “Dxnat—deep neural networks for explaining
    non-recurring traffic congestion,” in *Big Data*, 2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] L. Yan, H. Shen, and K. Chen, “Mobit: A distributed and congestion-resilient
    trajectory based routing algorithm for vehicular delay tolerant networks,” in
    *Proceedings of the Second International Conference on Internet-of-Things Design
    and Implementation, IoTDI 2017, Pittsburgh, PA, USA, April 18-21, 2017*, 2017,
    pp. 209–214.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] L. Wei, Z. Yu, Z. Jin, L. Xie, J. Huang, D. Cai, X. He, and X.-S. Hua,
    “Dual graph for traffic forecasting,” *IEEE Access*, 2019.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] W. Chen, L. Chen, Y. Xie, W. Cao, Y. Gao, and X. Feng, “Multi-range attentive
    bicomponent graph convolutional network for traffic forecasting,” *AAAI*, 2020.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Z. Cao, S. Jiang, J. Zhang, and H. Guo, “A unified framework for vehicle
    rerouting and traffic light control to reduce traffic congestion,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 18, no. 7, pp. 1958–1973, 2017.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] L. Qi, M. Zhou, and W. Luan, “A two-level traffic light control strategy
    for preventing incident-based urban traffic congestion,” *IEEE Transactions on
    Intelligent Transportation Systems*, vol. 19, no. 1, pp. 13–24, 2018.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J. Ye, J. Zhao, K. Ye, and C. Xu, “Multi-stgcnet: A graph convolution
    based spatial-temporal framework for subway passenger flow forecasting,” in *2020
    International Joint Conference on Neural Networks (IJCNN)*, 2020, pp. 1–8.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] F. Rodrigues, I. Markou, and F. C. Pereira, “Combining time-series and
    textual data for taxi demand prediction in event areas: A deep learning approach,”
    *Information Fusion*, vol. 49, pp. 120–129, 2019.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] D. Wang, W. Cao, J. Li, and J. Ye, “Deepsd: Supply-demand prediction for
    online car-hailing services using deep neural networks,” in *ICDE*, 2017, pp.
    243–254.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] D. Chai, L. Wang, and Q. Yang, “Bike flow prediction with multi-graph
    convolutional networks,” in *SIGSPATIAL*, 2018.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] L. Lin, Z. He, and S. Peeta, “Predicting station-level hourly demand in
    a large-scale bike-sharing network: A graph convolutional neural network approach,”
    *Transportation Research Part C: Emerging Technologies*, vol. 97, pp. 258–276,
    2018.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] X. Han, T. Grubenmann, R. Cheng, S. C. Wong, X. Li, and W. Sun, “Traffic
    incident detection: A trajectory-based approach,” in *ICDE*, 2020, pp. 1866–1869.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Z. Zhang, Q. He, J. Gao, and M. Ni, “A deep learning approach for detecting
    traffic accidents from social media data,” *Transportation Research Part C: Emerging
    Technologies*, vol. 86, pp. 580–596, 2018.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] M. I. Sameen and B. Pradhan, “Severity prediction of traffic accidents
    with recurrent neural networks,” *Applied Sciences*, 2017.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] S. Alkheder, M. Taamneh, and S. Taamneh, “Severity prediction of traffic
    accident using an artificial neural network,” *Journal of Forecasting*, vol. 36,
    no. 1, pp. 100–108, 2017.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A. M. Kashevnik, I. Lashkov, and A. V. Gurtov, “Methodology and mobile
    application for driver behavior analysis and accident prevention,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 21, no. 6, pp. 2427–2436, 2020.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] M. Hanninen, “Bayesian networks for maritime traffic accident prevention:
    benefits and challenges,” *Accident Analysis & Prevention*, vol. 73, pp. 305–312,
    2014.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] B. Jo, Y. Lee, R. M. A. Khan, J. Kim, and D. Kim, “Robust construction
    safety system (RCSS) for collision accidents prevention on construction sites,”
    *Sensors*, vol. 19, no. 4, p. 932, 2019.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] W. Qiu, H. Chen, and B. An, “Dynamic electronic toll collection via multi-agent
    deep reinforcement learning with edge-based graph convolutional networks,” in
    *IJCAI*, 2019, pp. 4568–4574.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] H. Li, P. Wang, and C. Shen, “Toward end-to-end car license plate detection
    and recognition with deep neural networks,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 20, no. 3, pp. 1126–1136, 2019.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] X. Chen, S. Xiang, C.-L. Liu, and C.-H. Pan, “Vehicle detection in satellite
    images by hybrid deep convolutional neural networks,” *IEEE Geoscience and remote
    sensing letters*, 2014.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] S. Zhang, J. Yang, and B. Schiele, “Occluded pedestrian detection through
    guided attention in cnns,” in *CVPR*, 2018, pp. 6995–7003.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] H. Tayara, K. G. Soo, and K. T. Chong, “Vehicle detection and counting
    in high-resolution aerial images using convolutional regression neural network,”
    *IEEE Access*, vol. 6, pp. 2220–2230, 2017.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, “Scale-aware fast
    r-cnn for pedestrian detection,” *IEEE transactions on Multimedia*, vol. 20, no. 4,
    pp. 985–996, 2017.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] M. A. S. Kamal, T. Hayakawa, and J. Imura, “Development and evaluation
    of an adaptive traffic signal control scheme under a mixed-automated traffic scenario,”
    *IEEE Transactions on Intelligent Transportation Systems*, vol. 21, no. 2, pp.
    590–602, 2020.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. Li, H. Ma, Z. Zhang, and M. Tomizuka, “Social-wagdat: Interaction-aware
    trajectory prediction via wasserstein graph double-attention network,” *arxiv:2002.06241*,
    2020.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. Mylavarapu, M. Sandhu, P. Vijayan, K. M. Krishna, B. Ravindran, and
    A. Namboodiri, “Towards accurate vehicle behaviour classification with multi-relational
    graph convolutional networks,” *arXiv:2002.00786*, 2020.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. Li, Z. Han, H. Cheng, J. Su, P. Wang, J. Zhang, and L. Pan, “Predicting
    path failure in time-evolving graphs,” in *KDD*, 2019, pp. 1279–1289.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] T. Nishi, K. Otaki, K. Hayakawa, and T. Yoshimura, “Traffic signal control
    based on reinforcement learning with graph convolutional neural nets,” in *ITSC*,
    2018, pp. 877–883.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Q. Zhang, Q. Jin, J. Chang, S. Xiang, and C. Pan, “Kernel-weighted graph
    convolutional network: A deep learning approach for traffic forecasting,” in *ICPR*,
    2018, pp. 1018–1023.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] S. Guo, Y. Lin, N. Feng, C. Song, and H. Wan, “Attention based spatial-temporal
    graph convolutional networks for traffic flow forecasting,” in *AAAI*, 2019, pp.
    922–929.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] L. Ge, H. Li, J. Liu, and A. Zhou, “Temporal graph convolutional networks
    for traffic speed prediction considering external factors,” in *MDM*, 2019, pp.
    234–242.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] B. Yu, M. Li, J. Zhang, and Z. Zhu, “3d graph convolutional networks with
    temporal graphs: A spatial information free framework for traffic forecasting,”
    *arXiv:1903.00919*, 2019.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] J. Hu, C. Guo, B. Yang, and C. S. Jensen, “Stochastic weight completion
    for road networks using graph convolutional networks,” in *ICDE*, 2019, pp. 1274–1285.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] D. Wang, J. Zhang, W. Cao, J. Li, and Y. Zheng, “When will you arrive?
    estimating travel time based on deep neural networks,” in *AAAI*, 2018.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] L. Yan, H. Shen, Z. Li, A. Sarker, J. A. Stankovic, C. Qiu, J. Zhao, and
    C. Xu, “Employing opportunistic charging for electric taxicabs to reduce idle
    time,” *Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.*, vol. 2, no. 1,
    pp. 47:1–47:25, 2018.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] X. Geng, X. Wu, L. Zhang, Q. Yang, Y. Liu, and J. Ye, “Multi-modal graph
    interaction for multi-graph convolution network in urban spatiotemporal forecasting,”
    *arXiv:1905.11395*, 2019.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] L. Bai, L. Yao, S. S. Kanhere, X. Wang, and Q. Z. Sheng, “Stg2seq: Spatial-temporal
    graph to sequence model for multi-step passenger demand forecasting,” in *IJCAI*,
    2019, pp. 1981–1987.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] J. Chen, L. Liu, H. Wu, J. Zhen, G. Li, and L. Lin, “Physical-virtual
    collaboration graph network for station-level metro ridership prediction,” *arXiv:2001.04889*,
    2020.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Z. Li, N. D. Sergin, H. Yan, C. Zhang, and F. Tsung, “Tensor completion
    for weakly-dependent data on graph for metro passenger flow prediction,” *AAAI*,
    2020.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] J. Ye, J. Zhao, L. Zhang, C. Xu, J. Zhang, and K. Ye, “A data-driven method
    for dynamic OD passenger flow matrix estimation in urban metro systems,” in *BigData
    2020*, vol. 12402.   Springer, 2020, pp. 116–126.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] H. Shi, Q. Yao, Q. Guo, Y. Li, L. Zhang, J. Ye, Y. Li, and Y. Liu, “Predicting
    origin-destination flow via multi-perspective graph convolutional network,” in
    *ICDE*, 2020, pp. 1818–1821.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] J. Hu, B. Yang, C. Guo, C. S. Jensen, and H. Xiong, “Stochastic origin-destination
    matrix forecasting using dual-stage graph convolutional, recurrent neural networks,”
    in *ICDE*, 2020, pp. 1417–1428.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] K. F. Chu, A. Y. S. Lam, and V. O. K. Li, “Deep multi-scale convolutional
    LSTM network for travel demand and origin-destination predictions,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 21, no. 8, pp. 3219–3232, 2020.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Y. Sun, T. He, J. Hu, H. Huang, and B. Chen, “Socially-aware graph convolutional
    network for human trajectory prediction,” in *ITNEC*, 2019, pp. 325–333.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] A. Monti, A. Bertugli, S. Calderara, and R. Cucchiara, “Dag-net: Double
    attentive graph neural network for trajectory forecasting,” *Carxiv:2005.12661*,
    2020.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] A. Mohamed, K. Qian, M. Elhoseiny, and C. Claudel, “Social-stgcnn: A social
    spatio-temporal graph convolutional neural network for human trajectory prediction,”
    in *CVPR*, 2020, pp. 14 412–14 420.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. Li, F. Yang, M. Tomizuka, and C. Choi, “Evolvegraph: Heterogeneous
    multi-agent multi-modal trajectory prediction with evolving interaction graphs,”
    *arxiv:2003.13924*, 2020.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] V. Kosaraju, A. Sadeghian, R. Martín-Martín, I. D. Reid, H. Rezatofighi,
    and S. Savarese, “Social-bigat: Multimodal trajectory forecasting using bicycle-gan
    and graph attention networks,” in *NIPS*, 2019, pp. 137–146.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Z. Zhao, H. Fang, Z. Jin, and Q. Qiu, “Gisnet: Graph-based information
    sharing network for vehicle trajectory prediction,” *arXiv:2003.11973*, 2020.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] X. Geng, Y. Li, L. Wang, L. Zhang, Q. Yang, J. Ye, and Y. Liu, “Spatiotemporal
    multi-graph convolution network for ride-hailing demand forecasting,” in *AAAI*,
    vol. 33, 2019, pp. 3656–3663.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] N. Laptev, J. Yosinski, L. E. Li, and S. Smyl, “Time-series extreme event
    forecasting with neural networks at uber,” in *ICML*, vol. 34, 2017, pp. 1–5.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Q. Xie, T. Guo, Y. Chen, Y. Xiao, X. Wang, and B. Y. Zhao, “How do urban
    incidents affect traffic speed? A deep graph convolutional network for incident-driven
    traffic speed prediction,” *arXiv:1912.01242*, 2019.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] B. Yu, H. Yin, and Z. Zhu, “Spatio-temporal graph convolutional networks:
    A deep learning framework for traffic forecasting,” in *IJCAI*, 2018, pp. 3634–3640.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] L. Zhao, Y. Song, C. Zhang, Y. Liu, P. Wang, T. Lin, M. Deng, and H. Li,
    “T-gcn: A temporal graph convolutional network for traffic prediction,” *IEEE
    Transactions on Intelligent Transportation Systems*, pp. 1–11, 2019.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Z. Cui, K. Henrickson, R. Ke, and Y. Wang, “Traffic graph convolutional
    recurrent neural network: A deep learning framework for network-scale traffic
    learning and forecasting,” *IEEE Transactions on Intelligent Transportation Systems*,
    2019.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] J. J. Q. Yu and J. Gu, “Real-time traffic speed estimation with graph
    convolutional generative autoencoder,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 20, no. 10, pp. 3940–3951, 2019.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Y. Huang, Y. Weng, S. Yu, and X. Chen, “Diffusion convolutional recurrent
    neural network with rank influence learning for traffic forecasting,” in *TrustCom*,
    2019, pp. 678–685.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. Li, H. Peng, L. Liu, G. Xiong, B. Du, H. Ma, L. Wang, and M. Z. A.
    Bhuiyan, “Graph cnns for urban traffic passenger flows prediction,” in *SmartWorld*,
    2018, pp. 29–36.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] C. Zheng, X. Fan, C. Wang, and J. Qi, “Gman: A graph multi-attention network
    for traffic prediction,” 2020.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Z. Diao, X. Wang, D. Zhang, Y. Liu, K. Xie, and S. He, “Dynamic spatial-temporal
    graph convolutional neural networks for traffic forecasting,” in *AAAI*, 2019,
    pp. 890–897.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Y. Zhang, S. Wang, B. Chen, and J. Cao, “GCGAN: generative adversarial
    nets with graph CNN for network-scale traffic prediction,” in *IJCNN*, 2019, pp.
    1–8.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Y. Wang, H. Yin, H. Chen, T. Wo, J. Xu, and K. Zheng, “Origin-destination
    matrix prediction via graph convolution: a new perspective of passenger demand
    modeling,” in *KDD*, 2019.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang, “Graph wavenet for deep
    spatial-temporal graph modeling,” in *IJCAI*, 2019.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] J. Ke, X. Qin, H. Yang, Z. Zheng, Z. Zhu, and J. Ye, “Predicting origin-destination
    ride-sourcing demand with a spatio-temporal encoder-decoder residual multi-graph
    convolutional network,” *arXiv:1910.09103*, 2019.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Z. Kang, H. Xu, J. Hu, and X. Pei, “Learning dynamic graph embedding
    for traffic flow forecasting: A graph self-attentive method,” 2019, pp. 2570–2576.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] B. Yu, H. Yin, and Z. Zhu, “St-unet: A spatio-temporal u-network for
    graph-structured time series modeling,” *arXiv:1903.05631*, 2019.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] K. Guo, Y. Hu, Z. Qian, H. Liu, and e. Zhang, “Optimized graph convolution
    recurrent neural network for traffic prediction,” *IEEE Transactions on Intelligent
    Transportation Systems*, pp. 1–12, 2020.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] C. Zhang, J. J. Q. Yu, and Y. Liu, “Spatial-temporal graph attention
    networks: A deep learning approach for traffic forecasting,” *IEEE Access*, vol. 7,
    pp. 166 246–166 256, 2019.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional recurrent
    neural network: Data-driven traffic forecasting,” in *ICLR*, 2018.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] M. Lu, K. Zhang, H. Liu, and N. Xiong, “Graph hierarchical convolutional
    recurrent neural network (GHCRNN) for vehicle condition prediction,” *arXiv:1903.06261*,
    2019.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Z. Zhang, M. Li, X. Lin, Y. Wang, and F. He, “Multistep speed prediction
    on traffic networks: A deep learning approach considering spatio-temporal dependencies,”
    *Transportation Research Part C: Emerging Technologies*, vol. 105, pp. 297–322,
    2019.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] S. Fang, Q. Zhang, G. Meng, S. Xiang, and C. Pan, “Gstnet: Global spatial-temporal
    network for traffic flow prediction,” in *IJCAI*, 2019, pp. 2286–2293.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] C. Chen, K. Li, S. G. Teo, X. Zou, K. Wang, J. Wang, and Z. Zeng, “Gated
    residual recurrent graph neural networks for traffic prediction,” in *AAAI*, 2019,
    pp. 485–492.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] L. Bai, L. Yao, S. S. Kanhere, X. Wang, W. Liu, and Z. Yang, “Spatio-temporal
    graph convolutional and recurrent networks for citywide passenger demand prediction,”
    in *CIKM*, 2019, pp. 2293–2296.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] S. Yan, Y. Xiong, and D. Lin, “Spatial temporal graph convolutional networks
    for skeleton-based action recognition,” in *AAAI*, 2018, pp. 7444–7452.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] M. Wang, B. Lai, Z. Jin, Y. Lin, X. Gong, J. Huang, and X. Hua, “Dynamic
    spatio-temporal graph-based cnns for traffic prediction,” *arXiv:1812.02019*,
    2018.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] J. Sun, J. Zhang, Q. Li, X. Yi, and Y. Zheng, “Predicting citywide crowd
    flows in irregular regions using multi-view graph convolutional networks,” *arXiv:1903.07789*,
    2019.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Y. Zhang, T. Cheng, and Y. Ren, “A graph deep learning method for short-term
    traffic forecasting on large road networks,” *Computer-Aided Civil and Infrastructure
    Engineering*, vol. 34, no. 10, pp. 877–896, 2019.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] X. Zhou, Y. Shen, and L. Huang, “Revisiting flow information for traffic
    prediction,” *arXiv:1906.00560*, 2019.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, and D. Yeung, “Gaan: Gated
    attention networks for learning on large and spatiotemporal graphs,” in *UAI*,
    2018, pp. 339–349.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Y. Y. Shin and Y. Yoon, “Incorporating dynamicity of transportation network
    with multi-weight traffic graph convolution for traffic forecasting,” *arXiv:1909.07105*,
    2019.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] A. Majumdar, “Graph structured autoencoder,” *Neural Networks*, vol.
    106, pp. 271–280, 2018.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio,
    “Graph attention networks,” in *ICLR*, 2018.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning
    on large graphs,” in *NIPS*, 2017, pp. 1024–1034.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst,
    “The emerging field of signal processing on graphs: Extending high-dimensional
    data analysis to networks and other irregular domains,” *IEEE Signal Processing
    Magazine*, vol. 30, no. 3, pp. 83–98, 2013.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and
    locally connected networks on graphs,” in *ICLR*, 2014.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural
    networks on graphs with fast localized spectral filtering,” in *NIPS*, 2016, pp.
    3837–3845.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] D. K. Hammond, P. Vandergheynst, and R. Gribonval, “Wavelets on graphs
    via spectral graph theory,” *Applied and Computational Harmonic Analysis*, vol. 30,
    no. 2, pp. 129–150, 2011.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *ICLR*, 2017.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] S. Teng, “Scalable algorithms for data and network analysis,” *Foundations
    and Trends in Theoretical Computer Science*, vol. 12, no. 1-2, pp. 1–274, 2016.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] S.-H. Teng, “Scalable algorithms for data and network analysis,” *Foundations
    and Trends® in Theoretical Computer Science*, vol. 12, no. 1-2, pp. 1–274, 2016.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] J. Zhang, Y. Zheng, and D. Qi, “Deep spatio-temporal residual networks
    for citywide crowd flows prediction,” in *AAAI*, 2017.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] S. Du, T. Li, X. Gong, and S. Horng, “A hybrid method for traffic flow
    forecasting using multimodal deep learning,” *International Journal of Computational
    Intelligence Systems*, vol. 13, no. 1, pp. 85–97, 2020.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] R. M. Schmidt, “Recurrent neural networks (rnns): A gentle introduction
    and overview,” *arXiv:1912.05911*, 2019.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Y. Bengio, P. Y. Simard, and P. Frasconi, “Learning long-term dependencies
    with gradient descent is difficult,” *IEEE Trans. Neural Networks*, vol. 5, no. 2,
    pp. 157–166, 1994.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] H. Salehinejad, S. Sankar, J. Barfett, E. Colak, and S. Valaee, “Recent
    advances in recurrent neural networks,” *arXiv:1801.01078*, 2017.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton, “On the importance
    of initialization and momentum in deep learning,” in *ICML*, vol. 28, 2013, pp.
    1139–1147.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] G. Chen, “A gentle tutorial of recurrent neural network with error backpropagation,”
    *arXiv:1610.02583*, 2016.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
    gated recurrent neural networks on sequence modeling,” in *NIPS Workshop*, 2014.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] N. Kalchbrenner, L. Espeholt, K. Simonyan, A. v. d. Oord, A. Graves,
    and K. Kavukcuoglu, “Neural machine translation in linear time,” *arXiv:1610.10099*,
    2016.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language modeling with
    gated convolutional networks,” in *ICML*, vol. 70, 2017, pp. 933–941.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves,
    N. Kalchbrenner, A. W. Senior, and K. Kavukcuoglu, “Wavenet: A generative model
    for raw audio,” in *ISCA Workshop*, 2016, pp. 124–125.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] F. Yu and V. Koltun, “Multi-scale context aggregation by dilated convolutions,”
    in *ICLR*, 2016.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] S. Bai, J. Z. Kolter, and V. Koltun, “An empirical evaluation of generic
    convolutional and recurrent networks for sequence modeling,” *arXiv:1803.01271*,
    2018.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in *NIPS*, 2014, pp. 3104–3112.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” in *ICLR*, 2015.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based
    neural machine translation,” in *EMNLP*, 2015, pp. 1412–1421.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, “Scheduled sampling
    for sequence prediction with recurrent neural networks,” in *NIPS*, 2015, pp.
    1171–1179.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] I. Goodfellow, J. Pouget-Abadie, M. Mirza, and e. Xu, “Generative adversarial
    nets,” in *NIPS*, 2014, pp. 2672–2680.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
    “Gans trained by a two time-scale update rule converge to a local nash equilibrium,”
    in *NIPS*, 2017, pp. 6626–6637.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta, and
    A. A. Bharath, “Generative adversarial networks: An overview,” *IEEE Signal Process.
    Mag.*, vol. 35, no. 1, pp. 53–65, 2018.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] K. Wang, C. Gou, Y. Duan, Y. Lin, X. Zheng, and F. Wang, “Generative
    adversarial networks: introduction and outlook,” *IEEE CAA J. Autom. Sinica*,
    vol. 4, no. 4, pp. 588–598, 2017.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Y. Lin, X. Dai, L. Li, and F. Wang, “Pattern sensitive prediction of
    traffic flow based on generative adversarial framework,” *IEEE Transactions on
    Intelligent Transportation Systems*, vol. 20, no. 6, pp. 2395–2400, 2019.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Y. Liang, Z. Cui, Y. Tian, H. Chen, and Y. Wang, “A deep generative adversarial
    architecture for network-wide spatial-temporal traffic-state estimation,” *Transportation
    Research Record*, 2018.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] H. Yao, F. Wu, J. Ke, X. Tang, Y. Jia, S. Lu, P. Gong, J. Ye, and Z. Li,
    “Deep multi-view spatial-temporal network for taxi demand prediction,” in *AAAI*,
    2018.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] A. Hasanzadeh, E. Hajiramezanali, K. R. Narayanan, N. Duffield, M. Zhou,
    and X. Qian, “Semi-implicit graph variational auto-encoders,” in *NIPS*, 2019,
    pp. 10 711–10 722.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] M. Simonovsky and N. Komodakis, “Graphvae: Towards generation of small
    graphs using variational autoencoders,” in *International Conference on Artificial
    Neural Networks*, 2018, pp. 412–422.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] H. Dai, Z. Kozareva, B. Dai, A. Smola, and L. Song, “Learning steady-states
    of iterative algorithms over graphs,” in *ICML*, 2018.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/1d7fb309c9e595c71b931c456a475c51.png) | Jiexia
    Ye received the Bachelor’s degree in Economics from Sun Yat-sen University in
    2012\. She is currently working toward M.S. degree in Shenzhen Institutes of Advanced
    Technology, Chinese Academy of Sciences. Her research interests include graph
    neural networks / graph embedding in traffic and finance domain. |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/1e81511f7c9e9ea7831b5064d31aa045.png) | Juanjuan
    Zhao received her Ph.D degree from Shenzhen College of Advanced Technology, University
    of Chinese Academy of Sciences in 2017, and received the M.S. degree from the
    Department of Computer Science, Wuhan University of Technology in 2009\. She is
    an Assistant Professor at Shenzhen Institutes of Advanced Technology, Chinese
    Academy of Sciences. Her research topics include data-driven urban systems, mobile
    data collection, cross-domain data fusion, heterogeneous model integration. |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/4cd2e27d31c079a359306593a72a77c4.png) | Kejiang
    Ye received his BSc and Ph.D degree in Computer Science from Zhejiang University
    in 2008 and 2013 respectively. He was also a joint Ph.D student at The University
    of Sydney from 2012 to 2013\. After graduation, he worked as Post-Doc Researcher
    at Carnegie Mellon University from 2014 to 2015 and Wayne State University from
    2015 to 2016\. He is currently an Associate Professor at Shenzhen Institutes of
    Advanced Technology, Chinese Academy of Science. His research interests include
    cloud computing, big data and network systems. |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/42aa77c8c307ff7b9a0d293d5cee2820.png) | Chengzhong
    Xu received his Ph.D degree from the University of Hong Kong, China in 1993\.
    He is the Dean of the Faculty of State Key Lab of IOTSC, Department of Computer
    Science, University of Macau, Macao SAR, China and a Chair Professor of Computer
    Science of UM. He was a Chief Scientist of Shenzhen Institutes of Advanced Technology
    (SIAT) of Chinese Academy of Sciences and the Director of Institute of Advanced
    Computing and Digital Engineering of SIAT. He was also in the faculty of Wayne
    State University, USA for 18 years. Dr. Xu’s research interest is mainly in the
    areas of parallel and distributed systems, cloud and edge computing, and data-driven
    intelligence. He has published over 300 peer-reviewed papers on these topics with
    over 10K citations. Dr. Xu served in the editorial boards of leading journals,
    including IEEE Transactions on Computers, IEEE Transactions on Cloud Computing,
    IEEE Transactions on Parallel and Distributed Systems and Journal of Parallel
    and Distributed Computing. He is the Associate Editor-in-Chief of ZTE Communication.
    He is IEEE Fellow and the Chair of IEEE Technical Committee of Distributed Processing.
    |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
