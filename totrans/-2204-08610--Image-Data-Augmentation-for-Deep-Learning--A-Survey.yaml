- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:46:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:46:55
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2204.08610] Image Data Augmentation for Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2204.08610] 图像数据增强在深度学习中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2204.08610](https://ar5iv.labs.arxiv.org/html/2204.08610)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2204.08610](https://ar5iv.labs.arxiv.org/html/2204.08610)
- en: 'Image Data Augmentation for Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像数据增强在深度学习中的应用：综述
- en: Suorong Yang^(1,2)    Weikang Xiao^(1,3)    Mengchen Zhang³    Suhan Guo^(1,3)
       Jian Zhao⁴    Furao Shen^(1,2)¹¹1Contact Author ¹ State Key Laboratory for
    Novel Software Technology, Nanjing University, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 苏荣杨^(1,2)    魏康肖^(1,3)    孟晨张³    苏涵郭^(1,3)    贾恩赵⁴    傅饶申^(1,2)¹¹1联系人 ¹ 南京大学软件技术国家重点实验室，中国
- en: ² Department of Computer Science and Technology, Nanjing University, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ² 南京大学计算机科学与技术系，中国
- en: ³School of Artificial Intelligence, Nanjing University, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³南京大学人工智能学院，中国
- en: ⁴School of Electronic Science and Engineering, Nanjing University, China
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴南京大学电子科学与工程学院，中国
- en: sryang@smail.nju.edu.cn, mg20370042@smail.nju.edu.cn, zhangmengchen@pjlab.org.cn,
    dg20370004@smail.nju.edu.cn, jianzhao@nju.edu.cn, frshen@nju.edu.cn
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: sryang@smail.nju.edu.cn, mg20370042@smail.nju.edu.cn, zhangmengchen@pjlab.org.cn,
    dg20370004@smail.nju.edu.cn, jianzhao@nju.edu.cn, frshen@nju.edu.cn
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning has achieved remarkable results in many computer vision tasks.
    Deep neural networks typically rely on large amounts of training data to avoid
    overfitting. However, labeled data for real-world applications may be limited.
    By improving the quantity and diversity of training data, data augmentation has
    become an inevitable part of deep learning model training with image data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在许多计算机视觉任务中取得了显著的成果。深度神经网络通常依赖大量的训练数据以避免过拟合。然而，实际应用中的标记数据可能有限。通过提高训练数据的数量和多样性，数据增强已成为深度学习模型训练中不可避免的一部分。
- en: As an effective way to improve the sufficiency and diversity of training data,
    data augmentation has become a necessary part of successful application of deep
    learning models on image data. In this paper, we systematically review different
    image data augmentation methods. We propose a taxonomy of reviewed methods and
    present the strengths and limitations of these methods. We also conduct extensive
    experiments with various data augmentation methods on three typical computer vision
    tasks, including semantic segmentation, image classification and object detection.
    Finally, we discuss current challenges faced by data augmentation and future research
    directions to put forward some useful research guidance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提高训练数据充分性和多样性的有效方式，数据增强已成为深度学习模型在图像数据上成功应用的必要部分。本文系统回顾了不同的图像数据增强方法。我们提出了这些方法的分类，并展示了它们的优点和局限性。我们还在三个典型的计算机视觉任务上进行了广泛的实验，包括语义分割、图像分类和目标检测。最后，我们讨论了数据增强面临的当前挑战以及未来研究方向，提出了一些有用的研究指导。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Deep learning has made incredible progress in many fields, including computer
    vision(CV) Hassaballah and Awad ([2020](#bib.bib17)), recommender system (RS) Liu
    et al. ([2021a](#bib.bib35)), natural language processing (NLP) Torfi et al. ([2020](#bib.bib50))
    and so on. The development of these research field is mainly affected by the following
    three aspects: the progress of deep network architectures, great computing power,
    and access to big data.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在许多领域取得了令人难以置信的进展，包括计算机视觉（CV）Hassaballah 和 Awad ([2020](#bib.bib17))、推荐系统（RS）Liu
    等 ([2021a](#bib.bib35))、自然语言处理（NLP）Torfi 等 ([2020](#bib.bib50)) 等。这些研究领域的发展主要受到以下三个方面的影响：深度网络架构的进展、强大的计算能力以及获取大数据的能力。
- en: Firstly, the scale of the network architectures is often proportional to its
    generalization ability, such as 152-layers ResNet He et al. ([2016](#bib.bib18)),
    which, compared with shallow network, can gain significant accuracy form the increased
    depth. Secondly, the development of computing power has a significant impact on
    deep learning. With stronger computing power, it is possible to design models
    with deeper architecture. Finally, sufficient open datasets like Imagenet Russakovsky
    et al. ([2015](#bib.bib43)), MS-COCO Lin et al. ([2014](#bib.bib32)) and PASCAL
    VOC Everingham et al. ([2015](#bib.bib12)) are crucial to the development of deep
    learning models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，网络架构的规模通常与其泛化能力成正比，例如152层的ResNet He等人（[2016](#bib.bib18)），与浅层网络相比，其通过增加深度可以获得显著的准确性提升。其次，计算能力的发展对深度学习有显著影响。随着计算能力的增强，可以设计出更深层次的模型。最后，充足的开放数据集，如Imagenet Russakovsky等人（[2015](#bib.bib43)）、MS-COCO Lin等人（[2014](#bib.bib32)）和PASCAL
    VOC Everingham等人（[2015](#bib.bib12)），对深度学习模型的发展至关重要。
- en: However, we observe some imbalance among the developments of these three perspectives.
    While various network architectures for different CV tasks have been proposed
    and the computation power of graphics processing unit (GPU) have rapidly been
    increasing, fewer attention has been paid to using data augmentation methods to
    generate qualified training data. The core idea of data augmentation is to improve
    the sufficiency and diversity of training data by generating synthetic dataset.
    The augmented data can be regarded as being extracted from a distribution that
    is close to the real one. Then, the augmented dataset can represent more comprehensive
    characteristics. But some research challenges remain in image data augmentation
    methods. First, image data augmentation techniques can be applied into various
    CV tasks, such as, object detection Liu et al. ([2020](#bib.bib34)), semantic
    segmentation Minaee et al. ([2021](#bib.bib37)) and image classification Algan
    and Ulusoy ([2021](#bib.bib1)). But the challenge is that data augmentation methods
    are tasks-independent. Because the operations are performed on the image data
    and labels at the same time, and the label types are different under different
    tasks, the data augmentation methods for object detection task can not be directly
    applied to semantic segmentation task. This results in inefficiency and low scalability.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们观察到这三方面的发展存在一些不平衡。虽然为不同的计算机视觉任务提出了各种网络架构，且图形处理单元（GPU）的计算能力迅速提升，但对于使用数据增强方法生成合格训练数据的关注却较少。数据增强的核心思想是通过生成合成数据集来提高训练数据的充分性和多样性。增强的数据可以被视为从接近真实分布中提取的。然后，增强的数据集可以代表更全面的特征。但在图像数据增强方法中仍然存在一些研究挑战。首先，图像数据增强技术可以应用于各种计算机视觉任务，如目标检测 Liu等人（[2020](#bib.bib34)）、语义分割 Minaee等人（[2021](#bib.bib37)）和图像分类 Algan和Ulusoy（[2021](#bib.bib1)）。但挑战在于数据增强方法是任务独立的。因为操作同时在图像数据和标签上进行，而不同任务下标签类型不同，针对目标检测任务的数据增强方法不能直接应用于语义分割任务。这导致了效率低下和可扩展性差。
- en: Second, there is no theoretical research on data augmentation. For example,
    there is no quantitative standard on the size of sufficient training datasets.
    The size of generated training data is usually designed according to personal
    experience and extensive experiments. In addition, paradox may exist when the
    size of the original dataset is so small. We will face the challenge of how to
    generate qualified data based on very little data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，数据增强的理论研究尚未开展。例如，对于足够的训练数据集的大小没有定量标准。生成的训练数据的大小通常根据个人经验和大量实验进行设计。此外，当原始数据集的大小非常小时，可能会存在悖论。我们将面临如何基于极少的数据生成合格数据的挑战。
- en: To the best of our knowledge, works related to image data augmentation research
    did not review image data augmentation methods in terms of CV tasks. One work Wang
    et al. ([2017](#bib.bib52)) explores and compares multiple solutions to the problem
    of data augmentation in image classification, but it only relates to image classification
    task and experiments with only traditional transformations and GANs. Wang et al.
    ([2020](#bib.bib53)) reviews existing face data augmentation works from perspectives
    of the transformation types and deep learning. However, the survey is aimed at
    the face recognition tasks only. One work mainly focuses on different data augmentation
    techniques based on data warping and oversampling Khosla and Saini ([2020](#bib.bib25)).
    However, it does not provide a systematic review of different approaches. Another
    work closely related to ours is Shorten and Khoshgoftaar ([2019](#bib.bib45))
    which present some existing methods and promising developments of data augmentation.
    However, it does not provide an evaluation on the effectiveness of data augmentation
    for various actual tasks and lacks some newly proposed methods, such as, CutMix Yun
    et al. ([2019](#bib.bib54)), AugMix Hendrycks et al. ([2019](#bib.bib19)), GridMask Chen
    et al. ([2020](#bib.bib4)), etc.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，相关图像数据增强研究的工作并未从计算机视觉任务的角度回顾图像数据增强方法。Wang 等人 ([2017](#bib.bib52)) 探索并比较了图像分类中数据增强的多种解决方案，但仅涉及图像分类任务和传统变换及生成对抗网络的实验。Wang
    等人 ([2020](#bib.bib53)) 从变换类型和深度学习的角度回顾了现有的人脸数据增强工作，但调查仅针对人脸识别任务。另一项工作主要关注基于数据扭曲和过采样的不同数据增强技术，Khosla
    和 Saini ([2020](#bib.bib25))。然而，它没有提供不同方法的系统评审。与我们密切相关的另一项工作是 Shorten 和 Khoshgoftaar
    ([2019](#bib.bib45))，它展示了一些现有方法和数据增强的有前景的发展，但没有对各种实际任务的数据增强效果进行评估，并且缺少一些新提出的方法，如
    CutMix Yun 等人 ([2019](#bib.bib54))、AugMix Hendrycks 等人 ([2019](#bib.bib19))、GridMask
    Chen 等人 ([2020](#bib.bib4)) 等。
- en: 'In this paper, we aim to fill the aforementioned gaps by summarizing existing
    novel image data augmentation methods. To this end, we propose a taxonomy of image
    data augmentation methods, as illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Image Data Augmentation for Deep Learning: A Survey"). Based on this taxonomy,
    we systematically review the data augmentation techniques from the perspectives
    of common CV tasks, including object detection, semantic segmentation and image
    classification. Furthermore, we also conduct experiments from the perspectives
    of these three CV tasks. Based on experiment results, we compare the performance
    of different kinds of data augmentation methods and their combinations on various
    deep learning models with open image datasets. We will also discuss future directions
    for image data augmentation research.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们旨在通过总结现有的新颖图像数据增强方法来填补上述空白。为此，我们提出了一种图像数据增强方法的分类，如图[1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Image Data Augmentation for Deep Learning: A Survey")所示。基于这一分类，我们从常见计算机视觉任务的角度系统回顾数据增强技术，包括目标检测、语义分割和图像分类。此外，我们还从这三种计算机视觉任务的角度进行实验。根据实验结果，我们比较了不同类型的数据增强方法及其组合在各种深度学习模型上的表现，使用了开放的图像数据集。我们还将讨论图像数据增强研究的未来方向。'
- en: '![Refer to caption](img/3a78dd25c68fae8134cfc089b33b7b22.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/3a78dd25c68fae8134cfc089b33b7b22.png)'
- en: 'Figure 1: A taxonomy of image data augmentation methods.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：图像数据增强方法的分类。
- en: The reminder of this paper is organized as follows. We present the basic data
    augmentation methods first, such as traditional image manipulation, image erasing
    based methods and image mix based methods. Then we discuss some advanced techniques,
    including auto augment based methods, feature augmentation techniques, and deep
    generative models. To evaluate the effect of various kinds of data augmentation
    methods, we conduct experiments in three typical CV tasks with various common
    public image datasets. Finally, we highlight some promising directions for future
    research.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。我们首先介绍基本的数据增强方法，如传统图像处理、基于图像擦除的方法和基于图像混合的方法。然后，我们讨论一些高级技术，包括自动增强方法、特征增强技术和深度生成模型。为了评估各种数据增强方法的效果，我们在三个典型的计算机视觉任务中使用各种常见的公共图像数据集进行实验。最后，我们强调一些未来研究的有前景方向。
- en: 2 Basic Data Augmentation Methods
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 基本数据增强方法
- en: 2.1 Image Manipulation
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 图像处理
- en: 'Basic image manipulations are focusing on image transformations, such as rotation,
    flipping, and cropping, etc. Most of these techniques manipulate the images directly
    and are easy to implement. The methods considered are shown with a concise description
    in Table. [1](#S2.T1 "Table 1 ‣ 2.1 Image Manipulation ‣ 2 Basic Data Augmentation
    Methods ‣ Image Data Augmentation for Deep Learning: A Survey").'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基本图像处理侧重于图像变换，如旋转、翻转和裁剪等。大多数这些技术直接操作图像，且易于实现。考虑的方法在表格中以简洁的描述展示。[1](#S2.T1 "表1
    ‣ 2.1 图像处理 ‣ 2 基本数据增强方法 ‣ 深度学习的图像数据增强：调查")。
- en: 'Table 1: Basic image manipulations and concise description.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：基本图像处理及其简洁描述。
- en: '|       Methods |       Description |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|       方法 |       描述 |'
- en: '| --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|       Flipping |       Flip the image horizontally, vertically, or both.
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|       翻转 |       水平、垂直或两者同时翻转图像。 |'
- en: '|       Rotation |       Rotate the image at an angle. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|       旋转 |       以角度旋转图像。 |'
- en: '|       Scaling Ratio |       Increase or reduce the image size. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|       缩放比例 |       增加或减少图像大小。 |'
- en: '|       Noise injection |       Add noise into the image. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|       噪声注入 |       在图像中添加噪声。 |'
- en: '|       Color space |       Change the image color channels. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|       颜色空间 |       更改图像颜色通道。 |'
- en: '|       Contrast |       Change the image contrast. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|       对比度 |       更改图像对比度。 |'
- en: '|       Sharpening |       Modify the image sharpness. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|       锐化 |       修改图像的锐度。 |'
- en: '|       Translation |       Move the image horizontally, vertically, or both.
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|       平移 |       水平、垂直或两者同时移动图像。 |'
- en: '|       Cropping |       Crop a sub-region of the image. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|       裁剪 |       裁剪图像的子区域。 |'
- en: However, the drawbacks exist. First of all, it is meaningful to apply basic
    image manipulations only under the assumption that the existing data obeys the
    distribution close to the actual data distribution. Secondly, some basic image
    manipulation methods, such as translation and rotation, suffer from the padding
    effect. That is, after the operation, some areas of the images will be moved out
    of the boundary and lost. Therefore, some interpolation methods will be applied
    to fill in the blank part. Generally, the region outside the image boundary is
    assumed to be constant 0, which will be black after manipulation. Moreover, regardless
    of the CV task, the object of interest should not to be moved off the frame.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，存在一些缺点。首先，仅在现有数据遵循接近实际数据分布的假设下，应用基本图像处理才有意义。其次，一些基本图像处理方法，如平移和旋转，存在填充效应。也就是说，操作后，图像的一些区域会被移出边界并丢失。因此，将应用一些插值方法来填补空白部分。通常，图像边界之外的区域假定为常数0，处理后会变成黑色。此外，无论是哪个CV任务，感兴趣的对象都不应移出画框。
- en: 2.2 Image Erasing
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 图像擦除
- en: Image augmentation approaches based on image erasing typically delete one or
    more sub-regions in the image. The main idea is to replace the pixel values of
    these sub-regions with constant values or random values.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图像擦除的图像增强方法通常会删除图像中的一个或多个子区域。主要思想是用常数值或随机值替换这些子区域的像素值。
- en: In DeVries and Taylor ([2017b](#bib.bib10)), authors considered a simple regularization
    technique of randomly masking out square regions of input during training convolutional
    neural networks (CNNs), which is known as cutout. This method is capable of improving
    the robustness and overall performance of CNNs. Singh et al. ([2018](#bib.bib46))
    proposed Hide-and-Seek (HaS) to hide patches in a training image randomly, which
    can force the network to seek other relevant content while the most discriminative
    content is hidden.  Zhong et al. ([2020](#bib.bib58)) proposed random erasing,
    which selects a rectangle region in an image randomly and replaces its pixels
    with random values. This method is simple, but makes significant improvements.
    Recently, in Chen et al. ([2020](#bib.bib4)), authors analyzed the requirement
    of information dropping and then proposed a structured method, GridMask, which
    is also based on the deletion of regions in the input images. Unlike Cutout and
    HaS, GridMask neither removes a continuous region nor randomly selects squares,
    the deleted regions are a set of spatially uniformly distributed squares, which
    can be controlled in terms of density and size. Furthermore, to balance the object
    occlusion and information retention, FenceMask Li et al. ([2020](#bib.bib29))
    was proposed, which is based on the simulation of object occlusion strategy.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DeVries 和 Taylor ([2017b](#bib.bib10)) 的研究中，作者考虑了一种简单的正则化技术，即在训练卷积神经网络（CNNs）时随机遮盖输入图像的方形区域，这被称为
    cutout。这种方法能够提高 CNNs 的鲁棒性和整体性能。Singh 等人 ([2018](#bib.bib46)) 提出了 Hide-and-Seek
    (HaS) 方法，用于随机隐藏训练图像中的局部区域，这可以迫使网络在隐藏最具区分性的内容时寻求其他相关内容。Zhong 等人 ([2020](#bib.bib58))
    提出了随机擦除方法，该方法随机选择图像中的矩形区域，并用随机值替换其像素。这种方法虽然简单，但效果显著。最近，在 Chen 等人 ([2020](#bib.bib4))
    的研究中，作者分析了信息丢失的需求，并提出了一种结构化方法 GridMask，该方法也是基于删除输入图像中的区域。与 Cutout 和 HaS 不同，GridMask
    不删除连续区域，也不随机选择方形区域，而是删除一组空间均匀分布的方形区域，其密度和大小可以控制。此外，为了平衡物体遮挡和信息保留，Li 等人 ([2020](#bib.bib29))
    提出了 FenceMask，该方法基于物体遮挡策略的模拟。
- en: 2.3 Image Mix
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 图像混合
- en: Image mix data augmentation has received increasing attention in recent years.
    These methods are mainly completed by mixing two or more images or sub-regions
    of images into one.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图像混合数据增强近年来受到越来越多的关注。这些方法主要是通过将两张或更多的图像或图像的子区域混合成一张图像来完成的。
- en: In Inoue ([2018](#bib.bib23)), authors enlarge the dataset by synthesizing every
    new image with two images randomly selected in the training set, known as pairing
    samples. The synthesis method used is to average the intensity of two images on
    each pixel.  Zhang et al. ([2017](#bib.bib56)) discusses a more general synthesis
    method, Mixup. Mixup, which is not just average the intensity of two images, conducts
    convex combinations sample pairs and their labels. Therefore, Mixup establishes
    a linear relationship between data augmentation and the supervision signal and
    can regularize the neural network to favor simple linear behavior in-between training
    samples. Similar with pairing samples and Mixup,  Yun et al. ([2019](#bib.bib54))
    proposes the CutMix. Instead of simply removing pixels or mixing images from training
    set. CutMix replaces the removed regions with a patch from another image and can
    generate more natural images compared to Mixup.  Harris et al. ([2020](#bib.bib16))
    proposes Fmix that uses random binary masks obtained by applying a threshold to
    low-frequency images sampled from Fourier space. Fmix can take on a wide range
    of shapes of random masks and can improve performance over Mixup and CutMix. Instead
    of mixing multiple samples, AugMix Hendrycks et al. ([2019](#bib.bib19)) first
    mixes multiple augmentation operations into three augmentation chains and then
    mixes together the results of several augmentation chains in convex combinations.
    Therefore, the whole process is typically mixing the results generated by the
    same image in different augmentation pipelines. In ManifoldMix Verma et al. ([2019](#bib.bib51)),
    authors improve the hidden representations and decision boundaries of neural networks
    at multiple layers by mixing hidden representations rather than input samples.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Inoue ([2018](#bib.bib23)) 中，作者通过将每个新图像与训练集中随机选择的两幅图像合成来扩大数据集，这被称为配对样本。使用的合成方法是对每个像素的两个图像的强度进行平均。
    Zhang 等人 ([2017](#bib.bib56)) 讨论了一种更通用的合成方法，Mixup。Mixup 不仅仅是对两个图像的强度进行平均，而是进行凸组合样本对及其标签。因此，Mixup
    在数据增强和监督信号之间建立了线性关系，并可以规范化神经网络，使其在训练样本之间偏好简单的线性行为。与配对样本和 Mixup 类似，Yun 等人 ([2019](#bib.bib54))
    提出了 CutMix。CutMix 不仅仅是简单地去除像素或混合训练集中的图像。CutMix 将去除的区域替换为另一张图像的补丁，相比 Mixup 可以生成更自然的图像。Harris
    等人 ([2020](#bib.bib16)) 提出了 Fmix，该方法使用通过对来自傅里叶空间的低频图像应用阈值获得的随机二进制掩码。Fmix 可以采用各种形状的随机掩码，并且能提高相对于
    Mixup 和 CutMix 的性能。与多个样本混合不同，AugMix Hendrycks 等人 ([2019](#bib.bib19)) 首先将多个增强操作混合成三条增强链，然后将几条增强链的结果以凸组合的方式混合。因此，整个过程通常是混合由相同图像在不同增强管道中生成的结果。在
    ManifoldMix 中，Verma 等人 ([2019](#bib.bib51)) 通过混合隐藏表示而不是输入样本来改进神经网络在多个层次上的隐藏表示和决策边界。
- en: 3 Advanced Approaches
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 高级方法
- en: 3.1 Auto Augment
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 自动增强
- en: 'Instead of manually designing data augmentation methods, researchers try to
    automatically search augmentation approaches to obtain improved performance. Auto
    augment has been the frontier of deep learning research and been extensively studied.
    Auto augment is based on the fact that different data have different characteristics,
    so different data augmentation methods have different benefits. Automatic searching
    for augmentation methods can bring more benefits than manual design. Cubuk et
    al. ([2019](#bib.bib7)) describes a simple procedure called AutoAugment to automatically
    search for improved data augmentation policies. Specifically, AutoAugment consists
    of two parts: search algorithm and search space. The search algorithm is designed
    to find the best policy regarding highest validation accuracy. The search space
    contains many policies which details various augmentation operations and magnitudes
    with which the operations are applied. However, a key challenge of auto augmentation
    methods is to choose an effective augmentation policy from a large search space
    of candidate operations. The search algorithm usually uses Reinforcement Learning Sutton
    and Barto ([2018](#bib.bib48)), which brings high time cost. Therefore, to reduce
    the time cost of AutoAugment,  Lim et al. ([2019](#bib.bib31)) proposes Fast AutoAugment
    that finds effective augmentation policies via a more efficient search strategy
    based on density matching. In comparison to AutoAugment, this method can speed
    up the search time. Meanwhile,  Ho et al. ([2019](#bib.bib20)) proposes Population
    Based Augmentation (PBA) to reduce the time cost of AutoAugment which generates
    nonstationary augmentation policy schedules instead of a fixed augmentation policy.
    PBA can match the performance of AutoAugment on multiple datasets with less computation
    time. Recently,  Cubuk et al. ([2020](#bib.bib8)) proposed RandAugment surpassing
    all previous automated augmentation techniques, including AutoAugment and PBA.
    RandAugment dramatically reduces the search space for data augmentation by removing
    a separate search, which is computationally expensive. In addition, RandAugment
    further improves the performance of AutoAugment and PBA.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员试图自动搜索增强方法以获得更好的性能，而不是手动设计数据增强方法。Auto augment 已成为深度学习研究的前沿，并被广泛研究。Auto augment
    基于不同数据具有不同特性的事实，因此不同的数据增强方法具有不同的好处。自动搜索增强方法比手动设计能带来更多的好处。Cubuk 等人 ([2019](#bib.bib7))
    描述了一种简单的程序，称为 AutoAugment，用于自动搜索改进的数据增强策略。具体而言，AutoAugment 包含两个部分：搜索算法和搜索空间。搜索算法旨在找到最高验证准确率的最佳策略。搜索空间包含许多策略，详细说明了各种增强操作及其应用的幅度。然而，自动增强方法的一个关键挑战是从大量候选操作的搜索空间中选择有效的增强策略。搜索算法通常使用
    Reinforcement Learning Sutton 和 Barto ([2018](#bib.bib48))，这带来了高时间成本。因此，为了减少 AutoAugment
    的时间成本，Lim 等人 ([2019](#bib.bib31)) 提出了 Fast AutoAugment，通过基于密度匹配的更高效的搜索策略来寻找有效的增强策略。与
    AutoAugment 相比，这种方法可以加快搜索时间。同时，Ho 等人 ([2019](#bib.bib20)) 提出了 Population Based
    Augmentation (PBA)，以减少 AutoAugment 的时间成本，该方法生成非固定的增强策略计划而不是固定的增强策略。PBA 能在较少的计算时间内匹配
    AutoAugment 在多个数据集上的表现。最近，Cubuk 等人 ([2020](#bib.bib8)) 提出了 RandAugment，超越了所有以前的自动增强技术，包括
    AutoAugment 和 PBA。RandAugment 通过去除单独的搜索，大大减少了数据增强的搜索空间，这在计算上是昂贵的。此外，RandAugment
    进一步提高了 AutoAugment 和 PBA 的性能。
- en: However, data augmentation might introduce noisy augmented examples and bring
    negative influence on inference. Therefore,  Gong et al. ([2021](#bib.bib14))
    proposed KeepAugment to use the saliency map to detect important regions on the
    original images and then preserve these informative regions during augmentation.
    KeepAugment automatically improve the data augmentation schemes, such as AutoAugment.
    In Tian et al. ([2020](#bib.bib49)), authors observed that the augmentation operations
    in the later training period are more influential and proposed Augmentation-wise
    Weight Sharing strategy. Compared with AutoAugment, this work improves efficiency
    significantly and make it affordable to directly search on large scale datasets.
    Unlike auto-augmentation methods searching strategies in an offline manner,  Lin
    et al. ([2019](#bib.bib33)) formulates the augmentation policy as a parameterized
    probability distribution and the parameters can be optimized jointly with network
    parameters, known as OHL-Auto-Aug.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据增强可能会引入噪声增强示例，并对推断产生负面影响。因此，Gong 等人 ([2021](#bib.bib14)) 提出了 KeepAugment，利用显著性图来检测原始图像上的重要区域，然后在增强过程中保留这些信息丰富的区域。KeepAugment
    自动改进数据增强方案，如 AutoAugment。在 Tian 等人 ([2020](#bib.bib49)) 的研究中，作者观察到后期训练中的增强操作更具影响力，并提出了
    Augmentation-wise Weight Sharing 策略。与 AutoAugment 相比，该方法显著提高了效率，并使得在大规模数据集上直接搜索变得可行。与离线方式搜索的自动增强方法不同，Lin
    等人 ([2019](#bib.bib33)) 将增强策略形式化为一个参数化的概率分布，参数可以与网络参数一起优化，称为 OHL-Auto-Aug。
- en: 3.2 Feature Augmentation
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 特征增强
- en: Rather than conduct augmentation only in the input space, feature augmentation
    performs the transformation in a learned feature space. In DeVries and Taylor
    ([2017a](#bib.bib9)), authors claimed that when traversing along the manifold
    it is more likely to encounter realistic samples in feature space than compared
    to input space. Therefore, various augmentation methods by manipulating the vector
    representation of data within a learned feature space are investigated, which
    includes adding noise, nearest neighbor interpolation and extrapolation. Recently,
     Kuo et al. ([2020](#bib.bib28)) proposed FeatMatch which is a novel learned feature-based
    refinement and augmentation method to produce a varied set of complex transformations.
    Moreover, FeatMatch can utilize information from both within-class and across-class
    prototypical representations. More recently, authors in Li et al. ([2021](#bib.bib30))
    proposed an implicit data augmentation method, known as Moment Exchange, by encouraging
    the models to utilize the moment information of latent features. Specifically,
    the moments of the learned features of one training image are replaced by those
    of another.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅在输入空间中进行数据增强不同，特征增强是在学习到的特征空间中进行变换。在 DeVries 和 Taylor ([2017a](#bib.bib9))
    的研究中，作者声称在流形上遍历时，更可能在特征空间中遇到真实的样本，而不是在输入空间中。因此，研究了通过操作数据在学习到的特征空间中的向量表示来进行的各种增强方法，包括添加噪声、最近邻插值和外推。最近，Kuo
    等人 ([2020](#bib.bib28)) 提出了 FeatMatch，这是一种新颖的基于学习特征的细化和增强方法，用于产生一组复杂的变换。此外，FeatMatch
    可以利用来自类内和类间原型表示的信息。更近期的研究中，Li 等人 ([2021](#bib.bib30)) 提出了一个隐式数据增强方法，称为 Moment
    Exchange，通过鼓励模型利用潜在特征的矩信息来实现。具体来说，将一个训练图像的学习特征的矩替换为另一个图像的矩。
- en: 3.3 Deep Generative Models
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 深度生成模型
- en: The ultimate goal of data augmentation is to draw samples from the distribution,
    which represent the generating mechanism of dataset. Hence, the data distribution
    we generate data from should not be different with the original one. This is the
    core idea of deep generative models. Among all the deep generative models methods,
    generative adversarial networks (GANs) Goodfellow et al. ([2014](#bib.bib15))
    are very representative methods. On the one hand, the generator can help generate
    new images. On the other hand, the discriminator ensures that the gap between
    the newly generated images and the original images is not too large. Although
    GAN has indeed been a powerful technique to perform unsupervised generation to
    augment data Perez and Wang ([2017](#bib.bib41)), how to generate high-quality
    data and evaluate them still remains a challenging problems. In this subsection,
    we would like to introduce some image data augmentation techniques based on GAN.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强的**最终目标**是从分布中抽取样本，这些样本代表了数据集的生成机制。因此，我们生成数据的数据分布不应与原始数据分布不同。这是深度生成模型的核心思想。在所有深度生成模型方法中，生成对抗网络（GANs）**Goodfellow等人**（[2014](#bib.bib15)）是非常具代表性的方法。一方面，生成器可以帮助生成新的图像。另一方面，判别器确保新生成的图像与原始图像之间的差距不会太大。尽管GAN确实是一种强大的无监督生成技术来增强数据**Perez和Wang**（[2017](#bib.bib41)），但如何生成高质量数据并评估它们仍然是一个具有挑战性的问题。在本小节中，我们将介绍一些基于GAN的图像数据增强技术。
- en: In Isola et al. ([2017](#bib.bib24)), based on conditional adversarial networks Mirza
    and Osindero ([2014](#bib.bib38)), authors proposed Pix2Pix to learn the mapping
    from the input images to output images. However, to train Pix2Pix, a large amount
    of paired data is needed. It is challenging to collect the paired data. Therefore,
    in  Zhu et al. ([2017](#bib.bib59)), unlike Pix2Pix, a CycleGAN model is proposed
    to learn the translation on an image from the source domain $X$ to a target domain
    $Y$ in the absence of paired samples. As the number of source and target domains
    increases, CycleGAN has to train models for each paired domain separately. For
    instance, if the task is to do transformation among $n$ domains, we need to train
    $n\times(n-1)$ models between every two domains. To deal with this issue,  Choi
    et al. ([2018](#bib.bib5)) proposed StarGAN to improve the scalability and robustness
    in handling more than two domains. Generally, StarGAN builds only one model to
    perform image-to-image translation among multiply domains. In the generation phase,
    we just need to provide the generator with the source image and an attribute label
    which indicates the target domain. However, StarGAN takes the domain label as
    an additional input and learns a deterministic mapping per each domain, which
    may result in the same output per each domain given an input image. To address
    this problem,  Choi et al. ([2020](#bib.bib6)) proposed StarGAN $v2$, which is
    a scalable approach that can generate diverse images across multiple domains.
    In this work, researchers define the domain and style of images as visually distinct
    category groups and the specific appearance of each image, respectively. For example,
    the dog can be used as a domain, but there are many kinds of dogs, such as Labrador
    and Husky. Therefore, the specific dog breed can be viewed as the style of an
    image. In this way, StarGAN $v2$ can translate an image of one domain to diverse
    images of a target domain, and support multiple domains.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在**Isola等人**（[2017](#bib.bib24)）中，基于条件对抗网络**Mirza和Osindero**（[2014](#bib.bib38)），作者提出了Pix2Pix来学习从输入图像到输出图像的映射。然而，训练Pix2Pix需要大量配对数据。收集这些配对数据具有挑战性。因此，在**Zhu等人**（[2017](#bib.bib59)）中，与Pix2Pix不同，提出了CycleGAN模型来学习从源域$X$到目标域$Y$的图像转换，且无需配对样本。随着源域和目标域数量的增加，CycleGAN必须为每个配对域单独训练模型。例如，如果任务是进行$n$个域之间的转换，我们需要在每两个域之间训练$n\times(n-1)$个模型。为了解决这个问题，**Choi等人**（[2018](#bib.bib5)）提出了StarGAN，以提高处理多个域时的可扩展性和鲁棒性。一般来说，StarGAN只建立一个模型来在多个域之间进行图像到图像的转换。在生成阶段，我们只需向生成器提供源图像和指示目标域的属性标签。然而，StarGAN将域标签作为额外输入，并为每个域学习一个确定性映射，这可能导致每个域在给定输入图像时产生相同的输出。为了解决这个问题，**Choi等人**（[2020](#bib.bib6)）提出了StarGAN
    $v2$，这是一种可扩展的方法，可以在多个域之间生成多样化的图像。在这项工作中，研究人员将图像的域和风格定义为视觉上不同的类别组和每个图像的具体外观。例如，狗可以用作域，但狗的种类有很多，比如拉布拉多犬和哈士奇。因此，具体的狗品种可以视为图像的风格。通过这种方式，StarGAN
    $v2$可以将一个域的图像转换为目标域的多样化图像，并支持多个域。
- en: 4 Evaluation
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估
- en: In this section, based on our taxonomy, we conduct extensive evaluations in
    three typical CV tasks, semantic segmentation, image classification, and object
    detection, to show the effectiveness of data augmentation in improving performance.
    To show fairness, we use the most and commonly used public datasets for this task.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，根据我们的分类法，我们在三种典型的计算机视觉任务——语义分割、图像分类和目标检测——中进行了广泛的评估，以展示数据增强在提升性能方面的有效性。为了公平起见，我们使用了最常用的公共数据集进行此任务。
- en: 4.1 Semantic Segmentation
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 语义分割
- en: 'In this subsection, we conduct experiments of semantic segmentation on PASCAL
    VOC dataset. In table [2](#S4.T2 "Table 2 ‣ 4.1 Semantic Segmentation ‣ 4 Evaluation
    ‣ Image Data Augmentation for Deep Learning: A Survey"), we report the performance
    improvement on Intersection over Union(IoU) metric with several semantic segmentation
    models: deeplabv3+ Liu et al. ([2021b](#bib.bib36)), PSPNet Zhao et al. ([2017](#bib.bib57)),
    GCNet Cao et al. ([2019](#bib.bib2)), and ISANet Huang et al. ([2019](#bib.bib22)).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '在本小节中，我们对PASCAL VOC数据集进行了语义分割实验。在表 [2](#S4.T2 "Table 2 ‣ 4.1 Semantic Segmentation
    ‣ 4 Evaluation ‣ Image Data Augmentation for Deep Learning: A Survey")中，我们报告了多个语义分割模型在Intersection
    over Union（IoU）指标上的性能提升：deeplabv3+ Liu et al. ([2021b](#bib.bib36))、PSPNet Zhao
    et al. ([2017](#bib.bib57))、GCNet Cao et al. ([2019](#bib.bib2)) 和 ISANet Huang
    et al. ([2019](#bib.bib22))。'
- en: 'In our experiment, we apply different data augmentation methods based on our
    taxonomy in Fig [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Image Data Augmentation
    for Deep Learning: A Survey"). Specifically, the applied image manipulation methods
    include flipping, scaling ratio, rotation, noise injection, cropping, translation
    and sharpening. The applied image erasing methods include random erasing, GridMask,
    FenceMask, Cutout, and HaS. The applied image mix methods include mosaic, Mixup,
    CutMix, and Fmix. Table [2](#S4.T2 "Table 2 ‣ 4.1 Semantic Segmentation ‣ 4 Evaluation
    ‣ Image Data Augmentation for Deep Learning: A Survey") presents the mean IoU
    with and without data augmentation on semantic segmentation models. We observe
    that data augmentation methods bring IoU improvements for all models.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的实验中，我们根据图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Image Data Augmentation
    for Deep Learning: A Survey")中的分类法应用了不同的数据增强方法。具体而言，应用的图像操作方法包括翻转、缩放比例、旋转、噪声注入、裁剪、平移和锐化。应用的图像擦除方法包括随机擦除、GridMask、FenceMask、Cutout和HaS。应用的图像混合方法包括马赛克、Mixup、CutMix和Fmix。表 [2](#S4.T2
    "Table 2 ‣ 4.1 Semantic Segmentation ‣ 4 Evaluation ‣ Image Data Augmentation
    for Deep Learning: A Survey")展示了在语义分割模型上有无数据增强的平均IoU。我们观察到数据增强方法对所有模型都带来了IoU的提升。'
- en: '|       Model |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|       模型 |'
- en: '&#124;       w/o aug &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       无增强 &#124;'
- en: '&#124;       (%) &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       (%) &#124;'
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;       w/ aug &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       有增强 &#124;'
- en: '&#124;       (%) &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       (%) &#124;'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;       Improvement &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       提升 &#124;'
- en: '&#124;       (%) &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       (%) &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|       DeepLabV3+ |       75.32% |       75.75% |       0.53% |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|       DeepLabV3+ |       75.32% |       75.75% |       0.53% |'
- en: '|       PSPNet |       73.38% |       74.33% |       0.95% |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|       PSPNet |       73.38% |       74.33% |       0.95% |'
- en: '|       GCNet |       71.86% |       72.93% |       1.07% |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|       GCNet |       71.86% |       72.93% |       1.07% |'
- en: '|       ISANet |       71.65% |       74.26% |       2.61% |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|       ISANet |       71.65% |       74.26% |       2.61% |'
- en: 'Table 2: Semantic segmentation improvement from data augmentation based on
    IoU and accuracy.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：基于IoU和准确度的数据增强对语义分割的提升。
- en: 4.2 Image Classification
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 图像分类
- en: 'In this experiment, we compare the classification accuracy with and without
    augmentation on several widely used image classification techniques, including
    Wide-ResNet Zagoruyko and Komodakis ([2016](#bib.bib55)), DenseNet Huang et al.
    ([2017](#bib.bib21)), and Shake ResNet Gastaldi ([2017](#bib.bib13)). These models
    are evaluated on several public image classification datasets, including CIFAR-10 Krizhevsky
    and Hinton ([2010](#bib.bib26)), CIFAR-100 [Krizhevsky et al.](#bib.bib27) and
    SVHN Netzer et al. ([2011](#bib.bib39)). Moreover, the data augmentation methods
    applied are the same with those in [4.1](#S4.SS1 "4.1 Semantic Segmentation ‣
    4 Evaluation ‣ Image Data Augmentation for Deep Learning: A Survey"), which includes
    several image manipulation methods, image erasing methods and image mix methods.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本实验中，我们比较了几种广泛使用的图像分类技术在数据增强前后的分类准确率，包括 Wide-ResNet Zagoruyko 和 Komodakis ([2016](#bib.bib55))、DenseNet Huang
    等 ([2017](#bib.bib21)) 和 Shake ResNet Gastaldi ([2017](#bib.bib13))。这些模型在几个公共图像分类数据集上进行了评估，包括
    CIFAR-10 Krizhevsky 和 Hinton ([2010](#bib.bib26))、CIFAR-100 [Krizhevsky et al.](#bib.bib27)
    和 SVHN Netzer 等 ([2011](#bib.bib39))。此外，应用的数据增强方法与 [4.1](#S4.SS1 "4.1 语义分割 ‣ 4
    评估 ‣ 深度学习中的图像数据增强：调查") 中的方法相同，包括若干图像处理方法、图像擦除方法和图像混合方法。
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.2 Image Classification ‣ 4 Evaluation ‣ Image
    Data Augmentation for Deep Learning: A Survey") summarizes the image classification
    results with and without data augmentation. It can be observed that data augmentation
    leads to average accuracy improvement(AAI).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [3](#S4.T3 "表 3 ‣ 4.2 图像分类 ‣ 4 评估 ‣ 深度学习中的图像数据增强：调查") 总结了数据增强前后的图像分类结果。可以观察到，数据增强带来了平均准确率提升（AAI）。
- en: '|       Dataset |       Model |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|       数据集 |       模型 |'
- en: '&#124;       w/o aug &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       w/o aug &#124;'
- en: '&#124;       (%) &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       (%) &#124;'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;       w/ aug &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       w/ aug &#124;'
- en: '&#124;       (%) &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       (%) &#124;'
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;       AAI &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       AAI &#124;'
- en: '&#124;       (%) &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       (%) &#124;'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  |       DenseNet |       94.15 |       94.59 |       0.44 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  |       DenseNet |       94.15 |       94.59 |       0.44 |'
- en: '|       CIFAR-10 |       Wide-ResNet |       93.34 |       94.67 |       1.33
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|       CIFAR-10 |       Wide-ResNet |       93.34 |       94.67 |       1.33
    |'
- en: '|  |       Shake-ResNet |       93.7 |       94.84 |       1.11 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |       Shake-ResNet |       93.7 |       94.84 |       1.11 |'
- en: '|  |       DenseNet |       74.98 |       75.93 |       0.95 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  |       DenseNet |       74.98 |       75.93 |       0.95 |'
- en: '|       CIFAR-100 |       Wide-ResNet |       74.46 |       76.52 |       2.06
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|       CIFAR-100 |       Wide-ResNet |       74.46 |       76.52 |       2.06
    |'
- en: '|  |       Shake-ResNet |       73.96 |       76.76 |       2.80 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  |       Shake-ResNet |       73.96 |       76.76 |       2.80 |'
- en: '|  |       DenseNet |       97.91 |       97.98 |       0.07 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  |       DenseNet |       97.91 |       97.98 |       0.07 |'
- en: '|       SVHN |       Wide-ResNet |       98.23 |       98.31 |       0.80 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|       SVHN |       Wide-ResNet |       98.23 |       98.31 |       0.80 |'
- en: '|  |       Shake-ResNet |       98.37 |       98.40 |       0.30 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  |       Shake-ResNet |       98.37 |       98.40 |       0.30 |'
- en: 'Table 3: Image classification accuracy improvement from data augmentation on
    CIFAR-10, CIFAR-100, and SVHN.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 数据增强对 CIFAR-10、CIFAR-100 和 SVHN 的图像分类准确率提升。'
- en: 4.3 Object Detection
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 目标检测
- en: 'In this subsection, we compare the effectiveness of various image data augmentation
    methods on widely used COCO2017 dataset, which is usually used for object detection
    task. We demonstrate the experiment results with and without data augmentation
    in two popular object detection deep models, FasterRCNN Ren et al. ([2016](#bib.bib42))
    and CenterNet Duan et al. ([2019](#bib.bib11)). We consider the data augmentation
    methods used are the same with those in [4.1](#S4.SS1 "4.1 Semantic Segmentation
    ‣ 4 Evaluation ‣ Image Data Augmentation for Deep Learning: A Survey"), which
    includes several image manipulation methods, image erasing methods and image mix
    methods. In table [4](#S4.T4 "Table 4 ‣ 4.3 Object Detection ‣ 4 Evaluation ‣
    Image Data Augmentation for Deep Learning: A Survey"), we report the performance
    improvement on mean average precision (mAP), AP50 and AP75 and we summarize these
    metrics for all methods in average sense. We observe that the data augmentation
    methods bring promising performance improvements.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们比较了各种图像数据增强方法在广泛使用的 COCO2017 数据集上的效果，该数据集通常用于目标检测任务。我们展示了在两个流行的目标检测深度模型
    FasterRCNN Ren et al. ([2016](#bib.bib42)) 和 CenterNet Duan et al. ([2019](#bib.bib11))
    中，有无数据增强的实验结果。我们考虑的数据增强方法与 [4.1](#S4.SS1 "4.1 语义分割 ‣ 4 评估 ‣ 深度学习的图像数据增强：综述") 中使用的方法相同，包括几种图像处理方法、图像擦除方法和图像混合方法。在表 [4](#S4.T4
    "表 4 ‣ 4.3 目标检测 ‣ 4 评估 ‣ 深度学习的图像数据增强：综述") 中，我们报告了在均值平均精度（mAP）、AP50 和 AP75 上的性能提升，并总结了所有方法的平均指标。我们观察到数据增强方法带来了有希望的性能提升。
- en: '|       Metric |  |       Faster R-CNN |       CenterNet |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|       指标 |  |       Faster R-CNN |       CenterNet |'
- en: '| --- | --- | --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |       w/o aug |       36.40 |       41.42 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |       无增强 |       36.40 |       41.42 |'
- en: '|'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;       mAP &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       mAP &#124;'
- en: '&#124;       (%) &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       (%) &#124;'
- en: '|       w/ aug |       36.80 |       41.15 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|       有增强 |       36.80 |       41.15 |'
- en: '|  |       API |       2.40 |       -0.27 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  |       API |       2.40 |       -0.27 |'
- en: '|  |       w/o aug |       57.20 |       58.29 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  |       无增强 |       57.20 |       58.29 |'
- en: '|'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;       AP50 &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       AP50 &#124;'
- en: '&#124;       (%) &#124;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       (%) &#124;'
- en: '|       w/ aug |       58.0 |       58.01 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|       有增强 |       58.0 |       58.01 |'
- en: '|  |       API |       0.80 |       -0.28 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  |       API |       0.80 |       -0.28 |'
- en: '|  |       w/o aug |       39.50 |       45.53 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  |       无增强 |       39.50 |       45.53 |'
- en: '|'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;       AP75 &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       AP75 &#124;'
- en: '&#124;       (%) &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       (%) &#124;'
- en: '|       w/ aug |       40.0 |       45.30 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|       有增强 |       40.0 |       45.30 |'
- en: '|  |       API |       0.50 |       -0.23 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  |       API |       0.50 |       -0.23 |'
- en: 'Table 4: Results of object detection on COCO2017 dataset with and without data
    augmentation methods applied.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：COCO2017 数据集上应用数据增强方法与未应用数据增强方法的目标检测结果。
- en: 5 Discussion for Future Directions
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 未来方向的讨论
- en: Despite extensive efforts on image data augmentation research to bring performance
    improvement on deep learning models, several open problems remain yet to completely
    to solve, which are summarized as follows.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在图像数据增强研究中做出了大量努力以提高深度学习模型的性能，但仍存在一些未完全解决的开放问题，具体总结如下。
- en: Theoretical Research on Data Augmentation. There is a lack of theoretical research
    on data augmentation. Data augmentation is more regarded as an auxiliary tool
    to improve the performance. Specifically, some methods can improve the accuracy,
    but we do not fully understand the reasons behind, such as pairing samples and
    mixup. To human eyes, the augmented data with pairing samples and mixup are visually
    meaningless. Furthermore, there is no theory on the size of sufficient training
    datasets. The size of the dataset suitable for tasks and models is usually designed
    based on personal experience and through extensive experiments. For example, researchers
    determine the size of datasets according to the specific models, training objectives,
    and the difficulty of data collection. Rigorous and thorough interpretability
    can not only explain why some augmentation techniques are useful but also can
    help guide the process of choosing or designing the most applicable and effective
    methods to enlarge our datasets. Thus, a critical future perspective is to develop
    theoretical support for data augmentation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据增强的理论研究。目前对数据增强的理论研究仍然不足。数据增强更多被视为一种辅助工具，用于提升性能。具体来说，一些方法可以提高准确性，但我们并没有完全理解其背后的原因，例如配对样本和混合样本。对于人眼来说，配对样本和混合样本的增强数据在视觉上并没有意义。此外，对于足够训练数据集的规模也没有理论支持。适合任务和模型的数据集规模通常是基于个人经验和大量实验设计的。例如，研究人员根据具体模型、训练目标和数据收集的难度来确定数据集的规模。严格和彻底的可解释性不仅可以解释为什么一些增强技术是有效的，还可以帮助指导选择或设计最适用和有效的方法来扩展我们的数据集。因此，一个关键的未来视角是为数据增强开发理论支持。
- en: The Evaluation of Data Augmentation Methods. The quantity and diversity of training
    data are of great importance to model’s generalization ability. However, since
    there is no unified metrics, how to evaluate the synthesized image quality is
    an open problem Salimans et al. ([2016](#bib.bib44)). At this stage, researchers
    evaluate the quality of the synthetic data in several following ways. First, the
    synthetic data are usually evaluated by human eyes, which is time-consuming, labor-intensive,
    and subjective. Amazon Mechanical Turk (AMT) is often used to evaluate the realism
    of outputs. AMT evaluates the quality and authenticity of the generated images
    by asking participants to vote for various images synthesized with different methods.
    Second, some studies combine the evaluation with specific tasks, which is to evaluate
    data augmentation methods according to their effect on the tasks metrics with
    and without data augmentation, such as classification tasks with classification
    accuracy and semantic segmentation with IOU of masks. However, there is no evaluation
    index only for the synthetic data itself. Generally, the evaluation metrics are
    based on diversity of individual data and consistency of overall data distribution
    regardless of what task is. Data quality analysis can help design evaluation metrics
    .
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强方法的评估。训练数据的数量和多样性对模型的泛化能力非常重要。然而，由于没有统一的指标，如何评估合成图像的质量仍然是一个未解决的问题 Salimans
    等人 ([2016](#bib.bib44))。在这一阶段，研究人员通过以下几种方式评估合成数据的质量。首先，合成数据通常通过人工评估，这既耗时又劳动密集且主观。亚马逊机械土耳其人（AMT）常用于评估输出的真实性。AMT通过要求参与者对使用不同方法合成的各种图像进行投票，来评估生成图像的质量和真实性。其次，一些研究将评估与特定任务结合起来，即根据任务指标在有无数据增强的情况下评估数据增强方法，例如通过分类精度评估分类任务，通过掩膜的IOU评估语义分割。然而，没有专门针对合成数据本身的评估指标。通常，评估指标基于个体数据的多样性和整体数据分布的一致性，而不考虑任务是什么。数据质量分析可以帮助设计评估指标。
- en: Class Imbalance. class imbalance or very few data can severely skew the data
    distribution Sun et al. ([2009](#bib.bib47)). This situation occurs since the
    learning process is often biased toward the majority class examples, so that minority
    ones are not well modeled. Synthetic minority of oversampling technique (SMOTE) Chawla
    et al. ([2002](#bib.bib3)) is to oversample the minority class. However, the oversampling
    is repeating drawing from the current data set. This may saturate the minority
    class and cause overfitting. Ultimately, we expect generated data can simulate
    distribution similar with training data while diversity never losses.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡。类别不平衡或数据非常少会严重扭曲数据分布 Sun 等人 ([2009](#bib.bib47))。这种情况发生是因为学习过程通常对多数类样本存在偏向，从而少数类样本未得到良好建模。合成少数类过采样技术
    (SMOTE) Chawla 等人 ([2002](#bib.bib3)) 是对少数类进行过采样。然而，过采样是重复从当前数据集中抽取。这可能会饱和少数类并导致过拟合。*最终*，我们期望生成的数据可以模拟与训练数据相似的分布，同时保持多样性不丧失。
- en: The Number of Generated Data. An interesting point for data augmentation is
    that the increase in the amount of training data is not exactly proportional to
    the increase in the performance. When a certain amount of data is reached, continue
    to increase the data without improving the effect. This may partly because, despite
    the increase in the number of data, the diversity of data remains unchanged. Thus,
    how much data should be generated is good enough to improve the model performance
    remains to be further explored.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 生成数据的数量。数据增强的一个有趣点是，训练数据量的增加并不完全与性能提升成正比。当达到一定量的数据时，继续增加数据并不会改善效果。这可能部分是因为尽管数据量增加了，但数据的多样性保持不变。因此，生成多少数据才足以提高模型性能还有待进一步探索。
- en: The Selection and Combination of Data Augmentation. Since various data augmentation
    can be combined together to generate new image data, the selection and combination
    of data augmentation techniques are critical. Image recognition experiment shows
    that results from Pawara et al. ([2017](#bib.bib40)) combined methods are often
    better than single method. Therefore, how to choose and combine methods is a key
    point when performing data augmentation. However, from our evaluation, the methods
    applicable for different datasets and tasks are not the same. Therefore, the set
    of augmentation methods must be carefully designed, implemented, and tested for
    every new task and dataset.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强的选择和组合。由于各种数据增强可以组合生成新的图像数据，因此数据增强技术的选择和组合至关重要。图像识别实验显示，Pawara 等人 ([2017](#bib.bib40))
    的组合方法通常比单一方法效果更好。因此，选择和组合方法是进行数据增强时的关键点。然而，从我们的评估来看，适用于不同数据集和任务的方法并不相同。因此，必须为每个新的任务和数据集仔细设计、实施和测试增强方法集合。
- en: 6 Conclusion
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: With the development of deep learning, the requirements for training datasets
    are becoming increasingly stringent. Thus we emphasize that data augmentation
    is an effective solution for the shortage of limited labeled image data. In this
    paper, we present a comprehensive review on image data augmentation methods in
    various CV tasks. We propose a taxonomy, summarizing representative approaches
    in each category. We then compare the methods empirically in various CV tasks.
    Finally, we discuss the challenges and highlight future perspectives.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的发展，对训练数据集的要求变得越来越严格。因此，我们强调数据增强是解决有限标记图像数据短缺的有效方案。本文对各种计算机视觉任务中的图像数据增强方法进行了全面回顾。我们提出了一个分类法，总结了每个类别中的代表性方法。然后，我们在各种计算机视觉任务中对这些方法进行了实证比较。最后，我们讨论了挑战，并强调了未来的前景。
- en: References
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Algan and Ulusoy [2021] Görkem Algan and Ilkay Ulusoy. Image classification
    with deep learning in the presence of noisy labels: A survey. Knowledge-Based
    Systems, 215:106771, 2021.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Algan 和 Ulusoy [2021] Görkem Algan 和 Ilkay Ulusoy。在存在噪声标签的情况下进行图像分类：一项调查。《知识基系统》，215:106771，2021年。
- en: 'Cao et al. [2019] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu.
    Gcnet: Non-local networks meet squeeze-excitation networks and beyond. arXiv preprint
    arXiv:1904.11492, 2019.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao 等人 [2019] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei 和 Han Hu。Gcnet:
    非局部网络遇见挤压激励网络及其更远。arXiv 预印本 arXiv:1904.11492，2019年。'
- en: 'Chawla et al. [2002] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and
    W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal
    of artificial intelligence research, 16:321–357, 2002.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chawla 等人 [2002] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall 和 W Philip
    Kegelmeyer。Smote: 合成少数类过采样技术。《人工智能研究杂志》，16:321–357，2002年。'
- en: Chen et al. [2020] Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia.
    Gridmask data augmentation. arXiv preprint arXiv:2001.04086, 2020.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2020] Pengguang Chen, Shu Liu, Hengshuang Zhao 和 Jiaya Jia. Gridmask
    数据增强。arXiv 预印本 arXiv:2001.04086, 2020。
- en: 'Choi et al. [2018] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun
    Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain
    image-to-image translation. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.
    (CVPR), pages 8789–8797, 2018.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等 [2018] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim
    和 Jaegul Choo. Stargan：用于多领域图像到图像转换的统一生成对抗网络。发表于 IEEE 计算机视觉与模式识别会议（CVPR）论文集，页面
    8789–8797, 2018。
- en: 'Choi et al. [2020] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.
    Stargan v2: Diverse image synthesis for multiple domains. In Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit. (CVPR), pages 8185–8194\. IEEE, 2020.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等 [2020] Yunjey Choi, Youngjung Uh, Jaejun Yoo 和 Jung-Woo Ha. Stargan v2：多领域的多样化图像合成。发表于
    IEEE 计算机视觉与模式识别会议（CVPR）论文集，页面 8185–8194, 2020。
- en: 'Cubuk et al. [2019] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan,
    and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 113–123,
    2019.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cubuk 等 [2019] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan 和
    Quoc V Le. Autoaugment：从数据中学习增强策略。发表于 IEEE/CVF 计算机视觉与模式识别大会论文集，页面 113–123, 2019。
- en: 'Cubuk et al. [2020] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
    Le. Randaugment: Practical automated data augmentation with a reduced search space.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops, pages 702–703, 2020.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cubuk 等 [2020] Ekin D Cubuk, Barret Zoph, Jonathon Shlens 和 Quoc V Le. Randaugment：一种实用的自动化数据增强方法，具有简化的搜索空间。发表于
    IEEE/CVF 计算机视觉与模式识别大会研讨会论文集，页面 702–703, 2020。
- en: DeVries and Taylor [2017a] Terrance DeVries and Graham W Taylor. Dataset augmentation
    in feature space. arXiv preprint arXiv:1702.05538, 2017.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeVries 和 Taylor [2017a] Terrance DeVries 和 Graham W Taylor. 特征空间中的数据集增强。arXiv
    预印本 arXiv:1702.05538, 2017。
- en: DeVries and Taylor [2017b] Terrance DeVries and Graham W Taylor. Improved regularization
    of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552,
    2017.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeVries 和 Taylor [2017b] Terrance DeVries 和 Graham W Taylor. 改进的卷积神经网络正则化方法。arXiv
    预印本 arXiv:1708.04552, 2017。
- en: 'Duan et al. [2019] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming
    Huang, and Qi Tian. Centernet: Keypoint triplets for object detection. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pages 6569–6578,
    2019.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan 等 [2019] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang
    和 Qi Tian. Centernet：用于目标检测的关键点三元组。发表于 IEEE/CVF 国际计算机视觉大会论文集，页面 6569–6578, 2019。
- en: 'Everingham et al. [2015] Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher
    K. I. Williams, John M. Winn, and Andrew Zisserman. The pascal visual object classes
    challenge: A retrospective. Int. J. Comput. Vis., 111(1):98–136, Jan 2015.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Everingham 等 [2015] Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher
    K. I. Williams, John M. Winn 和 Andrew Zisserman. Pascal 视觉目标类别挑战：回顾。Int. J. Comput.
    Vis., 111(1):98–136, 2015年1月。
- en: Gastaldi [2017] Xavier Gastaldi. Shake-shake regularization. arXiv preprint
    arXiv:1705.07485, 2017.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gastaldi [2017] Xavier Gastaldi. Shake-shake 正则化。arXiv 预印本 arXiv:1705.07485,
    2017。
- en: 'Gong et al. [2021] Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang
    Liu. Keepaugment: A simple information-preserving data augmentation approach.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 1055–1064, 2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong 等 [2021] Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra 和 Qiang Liu.
    Keepaugment：一种简单的信息保留数据增强方法。发表于 IEEE/CVF 计算机视觉与模式识别大会论文集，页面 1055–1064, 2021。
- en: Goodfellow et al. [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. In Proc. NIPS, pages 2672–2680, 2014.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,
    David Warde-Farley, Sherjil Ozair, Aaron Courville 和 Yoshua Bengio. 生成对抗网络。发表于
    NIPS 会议论文集，页面 2672–2680, 2014。
- en: 'Harris et al. [2020] Ethan Harris, Antonia Marcu, Matthew Painter, Mahesan
    Niranjan, Adam Prügel-Bennett, and Jonathon Hare. Fmix: Enhancing mixed sample
    data augmentation. arXiv preprint arXiv:2002.12047, 2020.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harris 等 [2020] Ethan Harris, Antonia Marcu, Matthew Painter, Mahesan Niranjan,
    Adam Prügel-Bennett 和 Jonathon Hare. Fmix：增强混合样本数据增强。arXiv 预印本 arXiv:2002.12047,
    2020。
- en: 'Hassaballah and Awad [2020] Mahmoud Hassaballah and Ali Ismail Awad. Deep learning
    in computer vision: principles and applications. CRC Press, 2020.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassaballah 和 Awad [2020] Mahmoud Hassaballah 和 Ali Ismail Awad. 计算机视觉中的深度学习：原理与应用。CRC
    Press, 2020。
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit. (CVPR), pages 770–778, 2016.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren 和 Jian Sun. 图像识别的深度残差学习.
    发表在 IEEE 计算机视觉与模式识别会议 (CVPR) 论文集，页面 770–778，2016年。
- en: 'Hendrycks et al. [2019] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph,
    Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method
    to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hendrycks 等人 [2019] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin
    Gilmer 和 Balaji Lakshminarayanan. Augmix: 一种简单的数据处理方法以提高鲁棒性和不确定性. arXiv 预印本 arXiv:1912.02781，2019年。'
- en: 'Ho et al. [2019] Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel.
    Population based augmentation: Efficient learning of augmentation policy schedules.
    In International Conference on Machine Learning, pages 2731–2741\. PMLR, 2019.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等人 [2019] Daniel Ho, Eric Liang, Xi Chen, Ion Stoica 和 Pieter Abbeel. 基于人群的增强：有效的增强策略调度学习.
    发表在国际机器学习会议，页面 2731–2741。PMLR，2019年。
- en: Huang et al. [2017] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q
    Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 4700–4708, 2017.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2017] Gao Huang, Zhuang Liu, Laurens Van Der Maaten 和 Kilian Q Weinberger.
    密集连接卷积网络. 发表在 IEEE 计算机视觉与模式识别会议论文集，页面 4700–4708，2017年。
- en: Huang et al. [2019] Lang Huang, Yuhui Yuan, Jianyuan Guo, Chao Zhang, Xilin
    Chen, and Jingdong Wang. Interlaced sparse self-attention for semantic segmentation.
    arXiv preprint arXiv:1907.12273, 2019.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2019] Lang Huang, Yuhui Yuan, Jianyuan Guo, Chao Zhang, Xilin Chen
    和 Jingdong Wang. 用于语义分割的交织稀疏自注意力. arXiv 预印本 arXiv:1907.12273，2019年。
- en: Inoue [2018] Hiroshi Inoue. Data augmentation by pairing samples for images
    classification. CoRR, abs/1801.02929, 2018.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inoue [2018] Hiroshi Inoue. 通过样本配对进行图像分类的数据增强. CoRR, abs/1801.02929，2018年。
- en: Isola et al. [2017] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.
    Image-to-image translation with conditional adversarial networks. In Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 1125–1134, 2017.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isola 等人 [2017] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou 和 Alexei A Efros. 条件对抗网络的图像到图像翻译.
    发表在 IEEE 计算机视觉与模式识别会议 (CVPR) 论文集，页面 1125–1134，2017年。
- en: 'Khosla and Saini [2020] Cherry Khosla and Baljit Singh Saini. Enhancing performance
    of deep learning models with different data augmentation techniques: A survey.
    In 2020 International Conference on Intelligent Engineering and Management (ICIEM),
    pages 79–85, 2020.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khosla 和 Saini [2020] Cherry Khosla 和 Baljit Singh Saini. 通过不同的数据增强技术提升深度学习模型性能的调查.
    发表在 2020 国际智能工程与管理会议 (ICIEM)，页面 79–85，2020年。
- en: Krizhevsky and Hinton [2010] Alex Krizhevsky and Geoff Hinton. Convolutional
    deep belief networks on cifar-10. Unpublished manuscript, 40(7):1–9, 2010.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 和 Hinton [2010] Alex Krizhevsky 和 Geoff Hinton. CIFAR-10 上的卷积深度信念网络.
    未发表手稿，40(7):1–9，2010年。
- en: '[27] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (canadian
    institute for advanced research).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Alex Krizhevsky, Vinod Nair 和 Geoffrey Hinton. CIFAR-100（加拿大高级研究院）。'
- en: 'Kuo et al. [2020] Chia-Wen Kuo, Chih-Yao Ma, Jia-Bin Huang, and Zsolt Kira.
    Featmatch: Feature-based augmentation for semi-supervised learning. In European
    Conference on Computer Vision, pages 479–495. Springer, 2020.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kuo 等人 [2020] Chia-Wen Kuo, Chih-Yao Ma, Jia-Bin Huang 和 Zsolt Kira. Featmatch:
    基于特征的半监督学习增强. 发表在欧洲计算机视觉会议，页面 479–495。Springer，2020年。'
- en: 'Li et al. [2020] Pu Li, Xiangyang Li, and Xiang Long. Fencemask: A data augmentation
    approach for pre-extracted image features. arXiv preprint arXiv:2006.07877, 2020.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2020] Pu Li, Xiangyang Li 和 Xiang Long. Fencemask: 一种针对预提取图像特征的数据增强方法.
    arXiv 预印本 arXiv:2006.07877，2020年。'
- en: Li et al. [2021] Boyi Li, Felix Wu, Ser-Nam Lim, Serge Belongie, and Kilian Q.
    Weinberger. On feature normalization and data augmentation. In Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
    12383–12392, June 2021.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2021] Boyi Li, Felix Wu, Ser-Nam Lim, Serge Belongie 和 Kilian Q. Weinberger.
    关于特征标准化和数据增强. 发表在 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集，页面 12383–12392，2021年6月。
- en: Lim et al. [2019] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong
    Kim. Fast autoaugment. Advances in Neural Information Processing Systems, 32:6665–6675,
    2019.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lim 等人 [2019] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim 和 Sungwoong Kim.
    快速自动增强. 神经信息处理系统进展，32:6665–6675，2019年。
- en: 'Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft
    COCO: common objects in context. In Proc. ECCV, volume 8693 of Lecture Notes in
    Computer Science, pages 740–755\. Springer, 2014.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, 和 C. Lawrence Zitnick. Microsoft COCO:
    背景中的常见对象。ECCV 会议论文集, 计算机科学讲义系列第8693卷, 页码 740–755, Springer, 2014年。'
- en: Lin et al. [2019] Chen Lin, Minghao Guo, Chuming Li, Xin Yuan, Wei Wu, Junjie
    Yan, Dahua Lin, and Wanli Ouyang. Online hyper-parameter learning for auto-augmentation
    strategy. In Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pages 6579–6588, 2019.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. [2019] Chen Lin, Minghao Guo, Chuming Li, Xin Yuan, Wei Wu, Junjie
    Yan, Dahua Lin, 和 Wanli Ouyang. 用于自动增强策略的在线超参数学习。IEEE/CVF 国际计算机视觉会议论文集, 页码 6579–6588,
    2019年。
- en: 'Liu et al. [2020] Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen,
    Xinwang Liu, and Matti Pietikäinen. Deep learning for generic object detection:
    A survey. International journal of computer vision, 128(2):261–318, 2020.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2020] Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen,
    Xinwang Liu, 和 Matti Pietikäinen. 通用目标检测的深度学习：综述。国际计算机视觉期刊, 128(2):261–318, 2020年。
- en: 'Liu et al. [2021a] Baichuan Liu, Qingtao Zeng, Likun Lu, Yeli Li, and Fucheng
    You. A survey of recommendation systems based on deep learning. Journal of Physics:
    Conference Series, 1754(1):012148, feb 2021.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2021a] Baichuan Liu, Qingtao Zeng, Likun Lu, Yeli Li, 和 Fucheng
    You. 基于深度学习的推荐系统综述。物理学杂志：会议系列, 1754(1):012148, 2021年2月。
- en: 'Liu et al. [2021b] Baichuan Liu, Qingtao Zeng, Likun Lu, Yeli Li, and Fucheng
    You. A survey of recommendation systems based on deep learning. Journal of Physics:
    Conference Series, 1754(1):012148, Feb 2021.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2021b] Baichuan Liu, Qingtao Zeng, Likun Lu, Yeli Li, 和 Fucheng
    You. 基于深度学习的推荐系统综述。物理学杂志：会议系列, 1754(1):012148, 2021年2月。
- en: 'Minaee et al. [2021] Shervin Minaee, Yuri Y Boykov, Fatih Porikli, Antonio J
    Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos. Image segmentation using deep
    learning: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence,
    2021.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minaee et al. [2021] Shervin Minaee, Yuri Y Boykov, Fatih Porikli, Antonio J
    Plaza, Nasser Kehtarnavaz, 和 Demetri Terzopoulos. 使用深度学习的图像分割：综述。IEEE 计算机视觉与模式分析汇刊,
    2021年。
- en: Mirza and Osindero [2014] Mehdi Mirza and Simon Osindero. Conditional generative
    adversarial nets. CoRR, abs/1411.1784, 2014.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirza and Osindero [2014] Mehdi Mirza 和 Simon Osindero. 条件生成对抗网络。CoRR, abs/1411.1784,
    2014年。
- en: Netzer et al. [2011] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco,
    Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature
    learning. 2011.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Netzer et al. [2011] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco,
    Bo Wu, 和 Andrew Y Ng. 使用无监督特征学习读取自然图像中的数字。2011年。
- en: Pawara et al. [2017] Pornntiwa Pawara, Emmanuel Okafor, Lambert Schomaker, and
    Marco Wiering. Data augmentation for plant classification. In Proc. International
    Conference on Advanced Concepts for Intelligent Vision Systems, pages 615–626\.
    Springer, 2017.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pawara et al. [2017] Pornntiwa Pawara, Emmanuel Okafor, Lambert Schomaker, 和
    Marco Wiering. 用于植物分类的数据增强。国际智能视觉系统高级概念会议论文集, 页码 615–626, Springer, 2017年。
- en: Perez and Wang [2017] Luis Perez and Jason Wang. The effectiveness of data augmentation
    in image classification using deep learning. CoRR, abs/1712.04621, 2017.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez and Wang [2017] Luis Perez 和 Jason Wang. 使用深度学习进行图像分类的数据增强效果。CoRR, abs/1712.04621,
    2017年。
- en: 'Ren et al. [2016] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster
    r-cnn: Towards real-time object detection with region proposal networks. IEEE
    Trans. Pattern Anal. Mach. Intell., 39(6):1137–1149, Jan 2016.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren et al. [2016] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster
    r-cnn: 通过区域提议网络实现实时目标检测。IEEE Trans. Pattern Anal. Mach. Intell., 39(6):1137–1149,
    2016年1月。'
- en: Russakovsky et al. [2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
    Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael
    Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition
    challenge. Int. J. Comput. Vis., 115(3):211–252, 2015.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russakovsky et al. [2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
    Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael
    Bernstein, Alexander C. Berg, 和 Li Fei-Fei. ImageNet 大规模视觉识别挑战。国际计算机视觉期刊, 115(3):211–252,
    2015年。
- en: Salimans et al. [2016] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
    Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Proc.
    NIPS, pages 2234–2242, 2016.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salimans et al. [2016] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
    Cheung, Alec Radford, 和 Xi Chen. 改进的生成对抗网络训练技术。NIPS 会议论文集, 页码 2234–2242, 2016年。
- en: Shorten and Khoshgoftaar [2019] Connor Shorten and Taghi M Khoshgoftaar. A survey
    on image data augmentation for deep learning. Journal of Big Data, 6(1):1–48,
    2019.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shorten 和 Khoshgoftaar [2019] Connor Shorten 和 Taghi M Khoshgoftaar. 关于深度学习图像数据增强的调查。大数据期刊，6(1):1–48，2019。
- en: 'Singh et al. [2018] Krishna Kumar Singh, Hao Yu, Aron Sarmasi, Gautam Pradeep,
    and Yong Jae Lee. Hide-and-seek: A data augmentation technique for weakly-supervised
    localization and beyond. arXiv preprint arXiv:1811.02545, 2018.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等 [2018] Krishna Kumar Singh, Hao Yu, Aron Sarmasi, Gautam Pradeep 和 Yong
    Jae Lee. 躲猫猫：一种用于弱监督定位及其他的数据显示增强技术。arXiv 预印本 arXiv:1811.02545，2018。
- en: 'Sun et al. [2009] Yanmin Sun, Andrew KC Wong, and Mohamed S Kamel. Classification
    of imbalanced data: A review. International journal of pattern recognition and
    artificial intelligence, 23(04):687–719, 2009.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 [2009] Yanmin Sun, Andrew KC Wong 和 Mohamed S Kamel. 不平衡数据分类：综述。国际模式识别与人工智能期刊，23(04):687–719，2009。
- en: 'Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. Reinforcement
    learning: An introduction. MIT press, 2018.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto [2018] Richard S Sutton 和 Andrew G Barto. 强化学习：导论。MIT press，2018。
- en: Tian et al. [2020] Keyu Tian, Chen Lin, Ming Sun, Luping Zhou, Junjie Yan, and
    Wanli Ouyang. Improving auto-augment via augmentation-wise weight sharing. arXiv
    preprint arXiv:2009.14737, 2020.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等 [2020] Keyu Tian, Chen Lin, Ming Sun, Luping Zhou, Junjie Yan 和 Wanli
    Ouyang. 通过增强-wise 权重共享改进自动增强。arXiv 预印本 arXiv:2009.14737，2020。
- en: 'Torfi et al. [2020] Amirsina Torfi, Rouzbeh A Shirvani, Yaser Keneshloo, Nader
    Tavaf, and Edward A Fox. Natural language processing advancements by deep learning:
    A survey. arXiv preprint arXiv:2003.01200, 2020.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torfi 等 [2020] Amirsina Torfi, Rouzbeh A Shirvani, Yaser Keneshloo, Nader Tavaf
    和 Edward A Fox. 深度学习推动的自然语言处理进展：综述。arXiv 预印本 arXiv:2003.01200，2020。
- en: 'Verma et al. [2019] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi,
    Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better
    representations by interpolating hidden states. In International Conference on
    Machine Learning, pages 6438–6447\. PMLR, 2019.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Verma 等 [2019] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis
    Mitliagkas, David Lopez-Paz 和 Yoshua Bengio. 流形 mixup：通过插值隐藏状态来获得更好的表示。在国际机器学习大会论文集，页码
    6438–6447。PMLR，2019。
- en: Wang et al. [2017] Jason Wang, Luis Perez, et al. The effectiveness of data
    augmentation in image classification using deep learning. Convolutional Neural
    Networks Vis. Recognit, 11:1–8, 2017.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2017] Jason Wang, Luis Perez 等. 使用深度学习进行图像分类的数据增强效果。卷积神经网络视觉识别，11:1–8，2017。
- en: Wang et al. [2020] Xiang Wang, Kai Wang, and Shiguo Lian. A survey on face data
    augmentation for the training of deep neural networks. Neural computing and applications,
    pages 1–29, 2020.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2020] Xiang Wang, Kai Wang 和 Shiguo Lian. 关于面部数据增强以训练深度神经网络的调查。神经计算与应用，页码
    1–29，2020。
- en: 'Yun et al. [2019] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun,
    Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong
    classifiers with localizable features. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pages 6023–6032, 2019.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yun 等 [2019] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk
    Choe 和 Youngjoon Yoo. Cutmix：一种用于训练具有可定位特征的强分类器的正则化策略。见 IEEE/CVF 国际计算机视觉大会论文集，页码
    6023–6032，2019。
- en: Zagoruyko and Komodakis [2016] Sergey Zagoruyko and Nikos Komodakis. Wide residual
    networks. arXiv preprint arXiv:1605.07146, 2016.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zagoruyko 和 Komodakis [2016] Sergey Zagoruyko 和 Nikos Komodakis. 宽残差网络。arXiv
    预印本 arXiv:1605.07146，2016。
- en: 'Zhang et al. [2017] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David
    Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412,
    2017.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 [2017] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin 和 David Lopez-Paz.
    mixup: 超越经验风险最小化。arXiv 预印本 arXiv:1710.09412，2017。'
- en: Zhao et al. [2017] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang,
    and Jiaya Jia. Pyramid scene parsing network. In Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit. (CVPR), 2017.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 [2017] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang 和 Jiaya
    Jia. 金字塔场景解析网络。见 IEEE 计算机视觉与模式识别会议论文集 (CVPR)，2017。
- en: Zhong et al. [2020] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang.
    Random erasing data augmentation. In Proc. The Thirty-Fourth AAAI Conference on
    Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications
    of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on
    Educational Advances in Artificial Intelligence, EAAI, pages 13001–13008\. AAAI
    Press, 2020.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong 等 [2020] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li 和 Yi Yang.
    随机擦除数据增强。见 第三十四届 AAAI 人工智能会议论文集，AAAI 2020，第三十二届人工智能创新应用会议，IAAI 2020，第十届 AAAI 教育进展研讨会，EAAI，页码
    13001–13008。AAAI Press，2020。
- en: Zhu et al. [2017] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
    Unpaired image-to-image translation using cycle-consistent adversarial networks.
    In Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 2223–2232, 2017.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2017] Jun-Yan Zhu、Taesung Park、Phillip Isola 和 Alexei A Efros。使用循环一致性对抗网络的无配对图像到图像转换。发表于《IEEE计算机视觉与模式识别会议》（CVPR），第2223–2232页，2017年。
