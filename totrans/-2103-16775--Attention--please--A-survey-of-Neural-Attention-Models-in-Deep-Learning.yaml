- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:56:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:56:11'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2103.16775] Attention, please! A survey of Neural Attention Models in Deep
    Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2103.16775] 注意力，请！深度学习中神经注意力模型的调研'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2103.16775](https://ar5iv.labs.arxiv.org/html/2103.16775)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2103.16775](https://ar5iv.labs.arxiv.org/html/2103.16775)
- en: Attention, please! A survey of Neural Attention Models in Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力，请！深度学习中神经注意力模型的调研
- en: Alana de Santana Correia, and Esther Luna Colombini,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Alana de Santana Correia 和 Esther Luna Colombini，
- en: Institute of Computing, University of Campinas, Av. Albert Einstein, 1251 -
    Campinas, SP - Brazil
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 坎皮纳斯大学计算机学院，阿尔伯特·爱因斯坦大道 1251号 - 坎皮纳斯，SP - 巴西
- en: 'e-mail: {alana.correia, esther}@ic.unicamp.br Laboratory of Robotics and Cognitive
    Systems (LaRoCS)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '电子邮件: {alana.correia, esther}@ic.unicamp.br 机器人与认知系统实验室 (LaRoCS)'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In humans, Attention is a core property of all perceptual and cognitive operations.
    Given our limited ability to process competing sources, attention mechanisms select,
    modulate, and focus on the information most relevant to behavior. For decades,
    concepts and functions of attention have been studied in philosophy, psychology,
    neuroscience, and computing. For the last six years, this property has been widely
    explored in deep neural networks. Currently, the state-of-the-art in Deep Learning
    is represented by neural attention models in several application domains. This
    survey provides a comprehensive overview and analysis of developments in neural
    attention models. We systematically reviewed hundreds of architectures in the
    area, identifying and discussing those in which attention has shown a significant
    impact. We also developed and made public an automated methodology to facilitate
    the development of reviews in the area. By critically analyzing 650 works, we
    describe the primary uses of attention in convolutional, recurrent networks and
    generative models, identifying common subgroups of uses and applications. Furthermore,
    we describe the impact of attention in different application domains and their
    impact on neural networks’ interpretability. Finally, we list possible trends
    and opportunities for further research, hoping that this review will provide a
    succinct overview of the main attentional models in the area and guide researchers
    in developing future approaches that will drive further improvements.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在人类中，注意力是所有感知和认知操作的核心属性。由于我们处理竞争来源的能力有限，注意力机制选择、调节并专注于对行为最相关的信息。几十年来，注意力的概念和功能在哲学、心理学、神经科学和计算领域得到了研究。在过去六年中，这一属性在深度神经网络中得到了广泛探索。目前，深度学习的前沿技术由多个应用领域的神经注意力模型代表。本文综述了神经注意力模型的发展情况，并进行了全面的概述和分析。我们系统地审查了该领域数百种架构，识别并讨论了在注意力显著影响的那些架构。我们还开发并公开了一种自动化的方法论，以便于该领域综述的发展。通过对650篇研究工作的批判性分析，我们描述了卷积、递归网络和生成模型中注意力的主要应用，识别了常见的使用和应用子组。此外，我们还描述了注意力在不同应用领域的影响及其对神经网络可解释性的影响。最后，我们列出了进一步研究的可能趋势和机会，希望本文综述能够提供该领域主要注意力模型的简明概述，并指导研究人员开发未来的方法，从而推动进一步的改进。
- en: '*K*eywords Survey  $\cdot$ Attention Mechanism  $\cdot$ Neural Networks  $\cdot$
    Deep Learning  $\cdot$ Attention Models.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 调研  $\cdot$ 注意力机制  $\cdot$ 神经网络  $\cdot$ 深度学习  $\cdot$ 注意力模型'
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Attention is a behavioral and cognitive process of focusing selectively on a
    discrete aspect of information, whether subjective or objective, while ignoring
    other perceptible information [[1](#bib.bib1)], playing an essential role in human
    cognition and the survival of living beings in general. In animals of lower levels
    in the evolutionary scale, it provides perceptual resource allocation allowing
    these beings to respond correctly to the environment’s stimuli to escape predators
    and capture preys efficiently. In human beings, attention acts on practically
    all mental processes, from reactive responses to unexpected stimuli in the environment
    - guaranteeing our survival in the presence of danger - to complex mental processes,
    such as planning, reasoning, and emotions. Attention is necessary because, at
    any moment, the environment presents much more perceptual information than can
    be effectively processed, the memory contains more competing traits than can be
    remembered, and the choices, tasks, or motor responses available are much greater
    than can be dealt with [[2](#bib.bib2)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力**是一个行为和认知过程，专注于信息的一个离散方面，无论是主观的还是客观的，同时忽略其他可感知的信息[[1](#bib.bib1)]，在人的认知和生物体的生存中发挥着重要作用。在进化较低水平的动物中，它提供了感知资源分配，使这些生物能够正确响应环境刺激，以有效逃避捕食者和捕捉猎物。在人类中，注意力作用于几乎所有的心理过程，从对环境中意外刺激的反应——保证我们在危险面前的生存——到复杂的心理过程，如规划、推理和情感。注意力是必要的，因为在任何时刻，环境呈现的感知信息远远超过了有效处理的信息，记忆中包含的竞争特征远远超过了能够记住的内容，选择、任务或可用的运动反应远远超过了能够处理的范围[[2](#bib.bib2)]。'
- en: At early sensorial processing stages, data is separated between sight, hearing,
    touch, smell, and taste. At this level, Attention selects and modulates processing
    within each of the five modalities and directly impacts processing in the relevant
    cortical regions. For example, attention to visual stimuli increases discrimination
    and activates the relevant topographic areas in the retinotopic visual cortex [[3](#bib.bib3)],
    allowing observers to detect contrasting stimuli or make more precise discriminations.
    In hearing, attention allows listeners to detect weaker sounds or differences
    in extremely subtle tones but essential for recognizing emotions and feelings [[4](#bib.bib4)].
    Similar effects of attention operate on the somatosensory cortex [[5](#bib.bib5)],
    olfactory cortex [[6](#bib.bib6)], and gustatory cortex [[7](#bib.bib7)]. In addition
    to sensory perception, our cognitive control is intrinsically attentional. Our
    brain has severe cognitive limitations - the number of items that can be kept
    in working memory, the number of choices that can be selected, and the number
    of responses that can be generated at any time are limited. Hence, evolution has
    favored selective attention concepts as the brain has to prioritize.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期感官处理阶段，数据被分离为视觉、听觉、触觉、嗅觉和味觉。在这一层次上，**注意力**选择并调节每一种感官模式的处理，并直接影响相关皮层区域的处理。例如，对视觉刺激的注意会增加分辨率，并激活视网膜视觉皮层中的相关拓扑区域[[3](#bib.bib3)]，使观察者能够检测到对比刺激或做出更精确的区分。在听觉方面，注意力使听者能够检测到较弱的声音或极其微妙的音调差异，这对识别情绪和感受至关重要[[4](#bib.bib4)]。类似的注意力效应也作用于躯体感觉皮层[[5](#bib.bib5)]、嗅觉皮层[[6](#bib.bib6)]和味觉皮层[[7](#bib.bib7)]。除了感官感知外，我们的认知控制本质上是注意力驱动的。我们的大脑有着严重的认知限制——可以在工作记忆中保持的项数、可以选择的选项数以及可以在任何时刻生成的反应数都是有限的。因此，进化选择了选择性注意的概念，因为大脑必须进行优先排序。
- en: Long before contemporary psychologists entered the discussion on Attention,
    William James [[8](#bib.bib8)] offered us a precise definition that has been,
    at least, partially corroborated more than a century later by neurophysiological
    studies. According to James, “Attention implies withdrawal from some things in
    order to deal effectively with others… Millions of items of the outward order
    are present to my senses which never properly enter into my experience. Why? Because
    they have no interest for me. My experience is what I agree to attend to. Only
    those items which I notice shape my mind — without selective interest, experience
    is an utter chaos.” Indeed, the first scientific studies of Attention have been
    reported by Herman Von Helmholtz (1821-1894) and William James (1890-1950) in
    the nineteenth century. They both conducted experiments to understand the role
    of Attention.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 早在现代心理学家参与注意力讨论之前，威廉·詹姆斯 [[8](#bib.bib8)] 就给出了一个精确的定义，该定义至少在一个世纪后被神经生理学研究部分验证了。根据詹姆斯的说法，“注意力意味着从某些事物中撤离，以便有效地处理其他事物……大量外界的事物存在于我的感官中，但从未真正进入我的经验中。为什么？因为它们对我没有兴趣。我的经验就是我决定关注的事物。只有那些我注意到的事物塑造了我的思想——没有选择性的兴趣，经验就是完全的混乱。”
    确实，关于注意力的第一次科学研究由赫尔曼·冯·赫尔姆霍茨（1821-1894）和威廉·詹姆斯（1890-1950）在十九世纪报告。他们都进行了实验以理解注意力的作用。
- en: For the past decades, the concept of attention has permeated most aspects of
    research in perception and cognition, being considered as a property of multiple
    and different perceptual and cognitive operations [[1](#bib.bib1)]. Thus, to the
    extent that these mechanisms are specialized and decentralized, attention reflects
    this organization. These mechanisms are in wide communication, and the executive
    control processes help set priorities for the system. Selection mechanisms operate
    throughout the brain and are involved in almost every stage, from sensory processing
    to decision making and awareness. Attention has become a broad term to define
    how the brain controls its information processing, and its effects can be measured
    through conscious introspection, electrophysiology, and brain imaging. Attention
    has been studied from different perspectives for a long time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年里，注意力的概念渗透了感知和认知研究的大多数方面，被认为是多种不同感知和认知操作的属性 [[1](#bib.bib1)]。因此，考虑到这些机制是专业化和去中心化的，注意力反映了这种组织。这些机制广泛地相互通信，执行控制过程帮助为系统设定优先级。选择机制在整个大脑中运作，涉及几乎每个阶段，从感官处理到决策和意识。注意力已成为定义大脑如何控制其信息处理的广泛术语，其效果可以通过自觉内省、电生理学和脑成像来测量。长期以来，注意力从不同的角度进行了研究。
- en: 1.1 Pre-Deep Learning Models of Attention
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 深度学习前的注意力模型
- en: Computational attention systems based on psychophysical models, supported by
    neurobiological evidence, have existed for at least three decades [[9](#bib.bib9)].
    Treisman’s Feature Integration Theory (FIT)  [[10](#bib.bib10)], Wolfe’s Guides
    Search  [[11](#bib.bib11)], Triadic architecture  [[12](#bib.bib12)], Broadbent’s
    Model  [[13](#bib.bib13)], Norman Attentional Model  [[14](#bib.bib14)]  [[15](#bib.bib15)],
    Closed-loop Attention Model  [[16](#bib.bib16)], SeLective Attention Model  [[17](#bib.bib17)],
    among several other models, introduced the theoretical basis of computational
    attention systems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基于心理物理模型的计算注意力系统，得到神经生物学证据的支持，至少存在了三十年 [[9](#bib.bib9)]。特雷斯曼的特征整合理论（FIT） [[10](#bib.bib10)]，沃尔夫的引导搜索 [[11](#bib.bib11)]，三元架构 [[12](#bib.bib12)]，布罗德本特模型 [[13](#bib.bib13)]，诺曼注意模型 [[14](#bib.bib14)] [[15](#bib.bib15)]，闭环注意模型 [[16](#bib.bib16)]，选择性注意模型 [[17](#bib.bib17)]，以及其他若干模型，介绍了计算注意力系统的理论基础。
- en: 'Initially, attention was mainly studied with visual experiments where a subject
    looks at a scene that changes in time [[18](#bib.bib18)]. In these models, the
    attentional system was restricted only to the selective attention component in
    visual search tasks, focusing on the extraction of multiple features through a
    sensor. Therefore, most of the attentional computational models occurred in computer
    vision to select important image regions. Koch and Ullman  [[19](#bib.bib19)]
    introduced the area’s first visual attention architecture based on FIT  [[10](#bib.bib10)].
    The idea behind it is that several features are computed in parallel, and their
    conspicuities are collected on a salience map. Winner-Take-All (WTA) determines
    the most prominent region on the map, which is finally routed to the central representation.
    From then on, only the region of interest proceeds to more specific processing.
    Neuromorphic Vision Toolkit (NVT), derived from the Koch-Ullman  [[20](#bib.bib20)]
    model, was the basis for developing research in computational visual attention
    for several years. Navalpakkam and Itti introduce a derivative of NVT which can
    deal with top-down cues  [[21](#bib.bib21)]. The idea is to learn the target’s
    feature values from a training image in which a binary mask indicates the target.
    The attention system of Hamker  [[22](#bib.bib22)]  [[23](#bib.bib23)] calculates
    various features and contrast maps and turns them into perceptual maps. With target
    information influencing processing, they combine detection units to determine
    whether a region on the perceptual map is a candidate for eye movement. VOCUS
     [[24](#bib.bib24)] introduced a way to combine bottom-up and top-down attention,
    overcoming the limitations of the time. Several other models have emerged in the
    literature, each with peculiarities according to the task. Many computational
    attention systems focus on the computation of mainly three features: intensity,
    orientation, and color. These models employed neural networks or filter models
    that use classical linear filters to compute features.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，注意力主要通过视觉实验来研究，其中受试者观察一个随时间变化的场景[[18](#bib.bib18)]。在这些模型中，注意力系统仅限于视觉搜索任务中的选择性注意成分，专注于通过传感器提取多个特征。因此，大多数注意力计算模型出现在计算机视觉领域，用于选择重要的图像区域。Koch
    和 Ullman [[19](#bib.bib19)] 提出了基于 FIT [[10](#bib.bib10)] 的该领域首个视觉注意力架构。其背后的理念是多个特征并行计算，并将它们的显著性收集到显著性图上。Winner-Take-All
    (WTA) 确定图上最突出的区域，最终将其传递到中央表示。从那时起，只有感兴趣的区域才会继续进行更具体的处理。Neuromorphic Vision Toolkit
    (NVT)，源自 Koch-Ullman [[20](#bib.bib20)] 模型，是开发计算视觉注意力研究的基础。Navalpakkam 和 Itti
    引入了 NVT 的一种衍生版本，能够处理自上而下的提示 [[21](#bib.bib21)]。其理念是从训练图像中学习目标的特征值，其中一个二进制掩码表示目标。Hamker
    [[22](#bib.bib22)] [[23](#bib.bib23)] 的注意力系统计算各种特征和对比度图，并将它们转化为感知图。随着目标信息影响处理，它们结合检测单元来判断感知图上的区域是否是眼动的候选区域。VOCUS
    [[24](#bib.bib24)] 引入了一种将自下而上和自上而下的注意力结合的方法，克服了当时的限制。文献中出现了其他几个模型，每个模型根据任务具有其特定性。许多计算注意力系统集中于计算主要的三个特征：强度、方向和颜色。这些模型采用神经网络或使用经典线性滤波器来计算特征的滤波模型。
- en: Computational attention systems were used successfully before Deep Learning
    (DL) in object recognition  [[25](#bib.bib25)], image compression  [[26](#bib.bib26)],
    image matching  [[27](#bib.bib27)], image segmentation  [[26](#bib.bib26)], object
    tracking  [[28](#bib.bib28)], active vision  [[29](#bib.bib29)], human-robot interaction
     [[30](#bib.bib30)], object manipulation in robotics  [[31](#bib.bib31)], robotic
    navigation  [[32](#bib.bib32)], and SLAM  [[33](#bib.bib33)]. In mid-1997, Scheier
    and Egner  [[34](#bib.bib34)] presented a mobile robot that uses attention for
    navigation. Still, in the 90s, Baluja and Pomerleau  [[35](#bib.bib35)] used an
    attention system to navigate an autonomous car, which followed relevant regions
    of a projection map. Walther  [[27](#bib.bib27)] combined an attentional system
    with an object recognizer based on SIFT features and demonstrated that the attentional
    front-end enhanced the recognition results. Salah et al.  [[25](#bib.bib25)] combined
    attention with neural networks in an Observable Markov model for handwritten digit
    recognition and face recognition. Ouerhani et al.  [[26](#bib.bib26)] proposed
    the focused image compression, which determines the number of bits to be allocated
    for encoding regions of an image according to their salience. High saliency regions
    have a high quality of reconstruction concerning the rest of the image.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 计算注意力系统在深度学习（DL）之前在物体识别[[25](#bib.bib25)]、图像压缩[[26](#bib.bib26)]、图像匹配[[27](#bib.bib27)]、图像分割[[26](#bib.bib26)]、物体跟踪[[28](#bib.bib28)]、主动视觉[[29](#bib.bib29)]、人机交互[[30](#bib.bib30)]、机器人中的物体操作[[31](#bib.bib31)]、机器人导航[[32](#bib.bib32)]和SLAM[[33](#bib.bib33)]中成功应用。1997年中期，Scheier和Egner[[34](#bib.bib34)]提出了一种使用注意力进行导航的移动机器人。然而，在90年代，Baluja和Pomerleau[[35](#bib.bib35)]使用注意力系统来导航一辆自主汽车，该汽车跟踪了投影地图上的相关区域。Walther[[27](#bib.bib27)]将注意力系统与基于SIFT特征的物体识别器结合，展示了注意力前端如何增强识别结果。Salah等人[[25](#bib.bib25)]将注意力与神经网络结合在一个可观察的马尔可夫模型中用于手写数字识别和面部识别。Ouerhani等人[[26](#bib.bib26)]提出了聚焦图像压缩，根据图像区域的显著性确定分配给编码区域的比特数。高显著性区域的重建质量相较于图像的其他部分更高。
- en: '1.2 Deep Learning Models of Attention: the beginning'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 注意力的深度学习模型：起始
- en: 'By 2014, the DL community noticed attention as a fundamental concept for advancing
    deep neural networks. Currently, the state-of-the-art in the field uses neural
    attention models. As shown in figure [1](#S1.F1 "Figure 1 ‣ 1.2 Deep Learning
    Models of Attention: the beginning ‣ 1 Introduction ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning"), the number of published works grows
    each year significantly in the leading repositories. In neural networks, attention
    mechanisms dynamically manage the flow of information, the features, and the resources
    available, improving learning. These mechanisms filter out irrelevant stimuli
    for the task and help the network to deal with long-time dependencies simply.
    Many neural attentional models are simple, scalable, flexible, and with promising
    results in several application domains [[36](#bib.bib36)] [[37](#bib.bib37)] [[38](#bib.bib38)].
    Given the current research extent, interesting questions related to neural attention
    models arise in the literature: how these mechanisms help improve neural networks’
    performance, which classes of problems benefit from this approach, and how these
    benefits arise.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 到2014年，深度学习（DL）社区注意到注意力机制作为推动深度神经网络发展的基本概念。当前，该领域的最先进技术使用神经注意力模型。如图[1](#S1.F1
    "图 1 ‣ 1.2 注意力的深度学习模型：起始 ‣ 1 引言 ‣ 注意力，请！神经注意力模型在深度学习中的调查")所示，领先存储库中发表的作品数量每年显著增长。在神经网络中，注意力机制动态管理信息流、特征和可用资源，提升学习效果。这些机制过滤掉与任务无关的刺激，并帮助网络简单处理长期依赖性。许多神经注意力模型简单、可扩展、灵活，并在多个应用领域中取得了有希望的结果[[36](#bib.bib36)]
    [[37](#bib.bib37)] [[38](#bib.bib38)]。考虑到目前的研究范围，文献中出现了与神经注意力模型相关的有趣问题：这些机制如何帮助提升神经网络的性能，哪些问题类别从这种方法中受益，以及这些益处是如何产生的。
- en: '![Refer to caption](img/24f120d18f94a4e1ff8f1a9dbda04564.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/24f120d18f94a4e1ff8f1a9dbda04564.png)'
- en: 'Figure 1: Works published by year between 01/01/2014 to 15/02/2021\. The main
    sources collected are ArXiv, CVPR, ICCV, ICLR, IJCNN, NIPS, and AAAI. The other
    category refers mainly to the following publishing vehicles: ICML, ACL, ACM, EMNLP,
    ICRA, ICPR, ACCV, CORR, ECCV, ICASSP, ICLR, IEEE ACCESS, Neurocomputing, and several
    other magazines.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：2014年01月01日至2021年02月15日期间发表的作品。主要来源包括ArXiv、CVPR、ICCV、ICLR、IJCNN、NIPS和AAAI。其他类别主要指以下出版渠道：ICML、ACL、ACM、EMNLP、ICRA、ICPR、ACCV、CORR、ECCV、ICASSP、ICLR、IEEE
    ACCESS、Neurocomputing及其他几本杂志。
- en: To the best of our knowledge, most surveys available in the literature do not
    address all of these questions or are more specific to some domain. Wang et al. [[39](#bib.bib39)]
    propose a review on recurrent networks and applications in computer vision, Hu [[40](#bib.bib40)],
    and Galassi et al. [[41](#bib.bib41)] offer surveys on attention in natural language
    processing (NLP). Lee et al. [[42](#bib.bib42)] present a review on attention
    in graph neural networks, and Chaudhari et al. [[43](#bib.bib43)] presented a
    more general, yet short, review.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，大多数文献中的调查没有解决所有这些问题，或者更专注于某些领域。Wang 等人 [[39](#bib.bib39)] 提出了关于递归网络和计算机视觉应用的综述，Hu
    [[40](#bib.bib40)] 和 Galassi 等人 [[41](#bib.bib41)] 提供了关于自然语言处理（NLP）中注意力的调查。Lee
    等人 [[42](#bib.bib42)] 提出了关于图神经网络中注意力的综述，而 Choudhari 等人 [[43](#bib.bib43)] 则提供了更为一般但简短的综述。
- en: 1.3 Contributions
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 贡献
- en: To assess the breadth of attention applications in deep neural networks, we
    present a systemic review of the field in this survey. Throughout our review,
    we critically analyzed 650 papers while addressing quantitatively 6,567.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估深度神经网络中注意力应用的广度，我们在本调查中提供了对该领域的系统综述。在我们的审查过程中，我们批判性地分析了650篇论文，同时定量地解决了6,567个问题。
- en: 'As the main contributions of our work, we highlight:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要贡献如下：
- en: '1.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: A replicable research methodology. We provide, in the Appendix, the detailed
    process conducted to collect our data and we make available the scripts to collect
    the papers and create the graphs we use;
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可复制的研究方法。我们在附录中提供了收集数据的详细过程，并提供了收集论文和创建图表的脚本；
- en: '2.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: An in-depth overview of the field. We critically analyzed 650 papers and extracted
    different metrics from 6,567, employing various visualization techniques to highlight
    overall trends in the area;
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 领域的深入概述。我们批判性地分析了650篇论文，并从6,567篇中提取了不同的指标，采用各种可视化技术突出该领域的总体趋势；
- en: '3.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We describe the main attentional mechanisms;
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们描述了主要的注意力机制；
- en: '4.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: We present the main neural architectures that employ attention mechanisms, describing
    how they have contributed to the NN field;
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了主要的神经网络架构，这些架构采用了注意力机制，并描述了它们如何对神经网络领域作出贡献；
- en: '5.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: We introduce how attentional modules or interfaces have been used in classic
    DL architectures extending the Neural Network Zoo diagrams;
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了注意力模块或接口如何在经典的深度学习架构中使用，扩展了神经网络动物园图示；
- en: '6.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Finally, we present a broad description of application domains, trends, and
    research opportunities.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们提供了应用领域、趋势和研究机会的广泛描述。
- en: This survey is structured as follows. In Section [2](#S2 "2 Overview ‣ Attention,
    please! A survey of Neural Attention Models in Deep Learning") we present the
    field overview reporting the main events from 2014 to the present. Section [3](#S3
    "3 Attention Mechanisms ‣ Attention, please! A survey of Neural Attention Models
    in Deep Learning") contains a description of attention main mechanisms. In Section [4](#S4
    "4 Attention-based Classic Deep Learning Architectures ‣ Attention, please! A
    survey of Neural Attention Models in Deep Learning") we analyze how attentional
    modules are used in classic DL architectures. Section [5](#S5 "5 Applications
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning") explains
    the main classes of problems and applications of attention. Finally, in Section [6](#S6
    "6 Trends and Opportunities ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning") we discuss limitations, open challenges, current trends,
    and future directions in the area, concluding our work in section [7](#S7 "7 Conclusions
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning") with
    directions for further improvements.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的结构如下。在第[2](#S2 "2 概述 ‣ 注意力，请！深度学习中的神经注意力模型调查")节中，我们介绍了该领域的概况，报告了2014年至今的主要事件。第[3](#S3
    "3 注意力机制 ‣ 注意力，请！深度学习中的神经注意力模型调查")节包含了注意力主要机制的描述。在第[4](#S4 "4 基于注意力的经典深度学习架构 ‣
    注意力，请！深度学习中的神经注意力模型调查")节中，我们分析了注意力模块在经典深度学习架构中的应用。在第[5](#S5 "5 应用 ‣ 注意力，请！深度学习中的神经注意力模型调查")节中，我们解释了注意力的主要问题类别和应用。最后，在第[6](#S6
    "6 趋势和机遇 ‣ 注意力，请！深度学习中的神经注意力模型调查")节中，我们讨论了该领域的局限性、未解挑战、当前趋势以及未来方向，并在第[7](#S7 "7
    结论 ‣ 注意力，请！深度学习中的神经注意力模型调查")节中总结了进一步改进的方向。
- en: 2 Overview
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 概述
- en: Historically, research in computational attention systems has existed since
    the 1980s. Only in mid-2014, the Neural Attentional Networks (NANs) emerged in
    Natural Language Processing (NLP), where attention provided significant advances,
    bringing promising results through scalable and straightforward networks. Attention
    allowed us to move towards the complex tasks of conversational machine comprehension,
    sentiment analysis, machine translation, question-answering, and transfer learning,
    previously challenging. Subsequently, NANs appeared in other fields equally important
    for artificial intelligence, such as computer vision, reinforcement learning,
    and robotics. There are currently numerous attentional architectures, but few
    of them have a significantly higher impact, as shown in Figure [2](#S2.F2 "Figure
    2 ‣ 2 Overview ‣ Attention, please! A survey of Neural Attention Models in Deep
    Learning"). In this image, we depict the most relevant group of works organized
    according to citation levels and innovations where RNNSearch [[44](#bib.bib44)],
    Transformer [[37](#bib.bib37)], Memory Networks [[38](#bib.bib38)], “show, attend
    and tell” [[45](#bib.bib45)], and RAM [[46](#bib.bib46)] stand out as key developments.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，计算注意力系统的研究自1980年代以来就已存在。直到2014年中期，神经注意力网络（NANs）在自然语言处理（NLP）中出现，注意力机制带来了显著的进展，通过可扩展且简洁的网络提供了有前景的结果。注意力使我们能够向对话机器理解、情感分析、机器翻译、问答系统和迁移学习等复杂任务迈进，这些任务以前颇具挑战。随后，NANs也出现在对人工智能同样重要的其他领域，如计算机视觉、强化学习和机器人技术。目前存在众多注意力架构，但其中很少有显著影响力的，如图[2](#S2.F2
    "图 2 ‣ 2 概述 ‣ 注意力，请！深度学习中的神经注意力模型调查")所示。在此图像中，我们描绘了按引用水平和创新组织的最相关工作组，其中RNNSearch [[44](#bib.bib44)]、Transformer [[37](#bib.bib37)]、Memory
    Networks [[38](#bib.bib38)]、“show, attend and tell” [[45](#bib.bib45)]和RAM [[46](#bib.bib46)]作为关键发展脱颖而出。
- en: '![Refer to caption](img/f446c62d2808514ee63dab2c00b96b4b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f446c62d2808514ee63dab2c00b96b4b.png)'
- en: 'Figure 2: Main Neural Attention Networks (NAN). Each circle corresponds to
    an architecture. The radius of the circles is defined based on the impact of the
    NAN on the field. The impact was defined by the citation number and the architecture
    innovation level. The greater the radius of the circle, the more significant the
    impact of architecture, and vice versa. Architectures labels are color-coded as
    follows: orange - natural language processing, red - computer vision, dark brown
    - computer vision and natural language processing, dark yellow - reinforcement
    learning and computer vision, light yellow - reinforcement learning and natural
    language processing, blue - imitation learning and robotics, and purple - others.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：主要神经注意力网络（NAN）。每个圆圈对应一个架构。圆圈的半径根据NAN对领域的影响力来定义。影响力由引用次数和架构创新水平决定。圆圈的半径越大，架构的影响越显著，反之亦然。架构标签的颜色编码如下：橙色
    - 自然语言处理，红色 - 计算机视觉，深棕色 - 计算机视觉和自然语言处理，深黄色 - 强化学习和计算机视觉，浅黄色 - 强化学习和自然语言处理，蓝色 -
    模仿学习和机器人技术，紫色 - 其他。
- en: The bottleneck problem in the classic encoder-decoder framework worked as the
    initial motivation for attention research in Deep Learning. In this framework,
    the encoder encodes a source sentence into a fixed-length vector from which a
    decoder generates the translation. The main issue is that a neural network needs
    to compress all the necessary information from a source sentence into a fixed-length
    vector. Cho et al.  [[47](#bib.bib47)] showed that the performance of the classic
    encoder-decoder deteriorates rapidly as the size of the input sentence increases.
    To minimize this bottleneck, Bahdanau et al. [[44](#bib.bib44)] proposed RNNSearch,
    an extension to the encoder-decoder model that learns to align and translate together.
    RNNSearch generates a translated word at each time-step, looking for a set of
    positions in the source sentence with the most relevant words. The model predicts
    a target word based on the context vectors associated with those source positions
    and all previously generated target words. The main advantage is that RNNSearch
    does not encode an entire input sentence into a single fixed-length vector. Instead,
    it encodes the input sentence into a sequence of vectors, choosing a subset of
    these vectors adaptively while generating the translation. The attention mechanism
    allows extra information to be propagated through the network, eliminating the
    fixed-size context vector’s information bottleneck. This approach demonstrated
    that the attentive model outperforms classic encoder-decoder frameworks for long
    sentences for the first time.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 经典编码器-解码器框架中的瓶颈问题成为深度学习中注意力研究的初始动机。在这个框架中，编码器将源句子编码为固定长度的向量，解码器从中生成翻译。主要问题是神经网络需要将所有必要的信息从源句子压缩成一个固定长度的向量。Cho等人
    [[47](#bib.bib47)] 表明，经典编码器-解码器的性能随着输入句子的大小增加而迅速下降。为了最小化这个瓶颈，Bahdanau等人 [[44](#bib.bib44)]
    提出了RNNSearch，这是对编码器-解码器模型的扩展，学习同时对齐和翻译。RNNSearch在每个时间步生成一个翻译词，寻找源句子中最相关词汇的一组位置。模型基于这些源位置和所有先前生成的目标词汇相关的上下文向量来预测目标词汇。主要优点是RNNSearch不将整个输入句子编码为单一的固定长度向量，而是将输入句子编码为向量序列，在生成翻译时自适应地选择这些向量的子集。注意力机制允许额外的信息通过网络传播，消除了固定大小上下文向量的信息瓶颈。这种方法首次证明了在长句子中，注意力模型优于经典编码器-解码器框架。
- en: RNNSearch was instrumental in introducing the first attention mechanism, soft
    attention (Section [3](#S3 "3 Attention Mechanisms ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning")). This mechanism has the main characteristic
    of smoothly selecting the network’s most relevant elements. Based on RNNSearch,
    there have been numerous attempts to augment neural networks with new properties.
    Two research directions stand out as particularly interesting - attentional interfaces
    and end-to-end attention. Attentional interfaces treat attention as a module or
    set of elective modules, easily plugged into classic Deep Learning neural networks,
    just like RNNSearch. So far, this is the most explored research direction in the
    area, mainly for simplicity, general use, and the good results of generalization
    that the attentional interfaces bring. End-to-end attention is a younger research
    direction, where the attention block covers the entire neural network. High and
    low-level attentional layers act recursively or cascaded at all network abstraction
    levels to produce the desired output in these models. End-to-end attention models
    introduce a new class of neural networks in Deep Learning. End-to-end attention
    research makes sense since no isolated attention center exists in the human brain,
    and its mechanisms are used in different cognitive processes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: RNNSearch 在引入第一个注意力机制——软注意力（第[3节](#S3 "3 Attention Mechanisms ‣ Attention, please!
    A survey of Neural Attention Models in Deep Learning")）方面发挥了重要作用。该机制的主要特点是平滑地选择网络中最相关的元素。基于
    RNNSearch，已经有很多尝试在神经网络中增加新的属性。其中两个研究方向特别引人注目——注意力接口和端到端注意力。注意力接口将注意力视为一个模块或一组可选模块，可以轻松地插入到经典的深度学习神经网络中，就像
    RNNSearch 一样。到目前为止，这是该领域最为深入的研究方向，主要因为其简单性、通用性和注意力接口所带来的良好泛化结果。端到端注意力是一个较新的研究方向，其中注意力模块覆盖整个神经网络。高层和低层的注意力层在所有网络抽象级别上递归或级联地作用，以生成这些模型所需的输出。端到端注意力模型在深度学习中引入了一类新的神经网络。端到端注意力的研究是有意义的，因为人脑中没有孤立的注意力中心，并且其机制被应用于不同的认知过程。
- en: 2.1 Attentional interfaces
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 注意力接口
- en: RNNSearch is the basis for research on attentional interfaces. The attentional
    module of this architecture is widely used in several other applications. In voice
    recognition [[48](#bib.bib48)], allowing one RNN to process the audio while another
    examines it focusing on the relevant parts as it generates a description. In-text
    analysis [[49](#bib.bib49)], it allows a model to look at the words as it generates
    an analysis tree. In conversational modeling [[50](#bib.bib50)], it allows the
    model to focus on the last parts of the conversation as it generates its response.
    There are also important extensions to deal with other information bottlenecks
    in addition to the classic encoder-decoder problem. BiDAF [[51](#bib.bib51)] proposes
    a multi-stage hierarchical process to question-answering. It uses the bidirectional
    attention flow to build a multi-stage hierarchical network with context paragraph
    representations at different granularity levels. The attention layer does not
    summarize the context paragraph in a fixed-length vector. Instead, attention is
    calculated for each step, and the vector assisted at each step, along with representations
    of previous layers, can flow to the subsequent modeling layer. This reduces the
    loss of information caused by the early summary. At each stage of time, attention
    is only a function of the query and the paragraph of the context in the current
    stage and does not depend directly on the previous stage’s attention. The hypothesis
    is that this simplification leads to a work division between the attention layer
    and the modeling layer, forcing the attention layer to focus on learning attention
    between the query and the context.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: RNNSearch 是研究注意力接口的基础。该架构的注意力模块在多个其他应用中得到了广泛使用。在语音识别中 [[48](#bib.bib48)]，允许一个
    RNN 处理音频，而另一个 RNN 在生成描述时专注于相关部分。在文本分析中 [[49](#bib.bib49)]，它允许模型在生成分析树时查看单词。在对话建模中 [[50](#bib.bib50)]，它允许模型在生成响应时专注于对话的最后部分。还有一些重要的扩展，用于处理经典的编码器-解码器问题之外的其他信息瓶颈。BiDAF [[51](#bib.bib51)]
    提出了一个多阶段的层次化过程来进行问答。它使用双向注意力流来构建一个多阶段的层次化网络，在不同的粒度级别上有上下文段落表示。注意力层不会将上下文段落总结成一个固定长度的向量。相反，注意力在每一步都进行计算，每一步的辅助向量以及前面层的表示可以流入后续建模层。这减少了早期总结带来的信息丢失。在每个阶段，注意力仅是当前阶段查询和上下文段落的函数，不直接依赖于前一个阶段的注意力。假设这种简化导致了注意力层和建模层之间的工作分配，迫使注意力层专注于学习查询和上下文之间的注意力。
- en: 'Yang et al. [[52](#bib.bib52)] proposed the Hierarchical Attention Network
    (HAN) to capture two essential insights about document structure. Documents have
    a hierarchical structure: words form sentences, sentences form a document. Humans,
    likewise, construct a document representation by first building representations
    of sentences and then aggregating them into a document representation. Different
    words and sentences in a document are differentially informative. Moreover, the
    importance of words and sentences is highly context-dependent, i.e., the same
    word or sentence may have different importance in different contexts. To include
    sensitivity to this fact, HAN consists of two levels of attention mechanisms -
    one at the word level and one at the sentence level - that let the model pay more
    or less attention to individual words and sentences when constructing the document’s
    representation. Xiong et al. [[53](#bib.bib53)] created a coattentive encoder
    that captures the interactions between the question and the document with a dynamic
    pointing decoder that alternates between estimating the start and end of the answer
    span. To learn approximate solutions to computationally intractable problems,
    Ptr-Net [[54](#bib.bib54)] modifies the RNNSearch’s attentional mechanism to represent
    variable-length dictionaries. It uses the attention mechanism as a pointer.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 杨等人[[52](#bib.bib52)] 提出了层次注意网络（HAN），以捕捉关于文档结构的两个关键见解。文档具有层次结构：词语形成句子，句子形成文档。人类同样通过首先建立句子的表示，然后将其聚合成文档表示来构建文档表示。文档中的不同词语和句子具有不同的信息量。此外，词语和句子的重视程度高度依赖于上下文，即相同的词语或句子在不同的上下文中可能具有不同的重要性。为了包含对这一事实的敏感性，HAN
    包含两个层次的注意机制——一个在词语层次，另一个在句子层次——让模型在构建文档表示时对个别词语和句子给予更多或更少的关注。熊等人[[53](#bib.bib53)]
    创建了一种共同注意编码器，通过动态指向解码器捕捉问题与文档之间的交互，动态指向解码器在估计答案跨度的开始和结束之间交替进行。为了学习计算上难以处理的问题的近似解，Ptr-Net[[54](#bib.bib54)]
    修改了 RNNSearch 的注意机制，以表示可变长度的词典。它将注意机制用作指针。
- en: See et. al. [[55](#bib.bib55)] used a hybrid between classic sequence-to-sequence
    attentional models and a Ptr-Net [[54](#bib.bib54)] to abstractive text summarization.
    The hybrid pointer-generator [[55](#bib.bib55)] copies words from the source text
    via pointing, which aids accurate reproduction of information while retaining
    the ability to produce novel words through the generator. Finally, it uses a mechanism
    to keep track of what has been summarized, which discourages repetition. FusionNet [[56](#bib.bib56)]
    presents a novel concept of "history-of-word" to characterize attention information
    from the lowest word-embedding level up to the highest semantic-level representation.
    This concept considers that data input is gradually transformed into a more abstract
    representation, forming each word’s history in human mental flow. FusionNet employs
    a fully-aware multi-level attention mechanism and an attention score-function
    that takes advantage of the history-of-word. Rocktäschel et al. [[57](#bib.bib57)]
    introduce two-away attention for recognizing textual entailment (RTE). The mechanism
    allows the model to attend over past output vectors, solving the LSTM’s cell state
    bottleneck. The LSTM with attention does not need to capture the premise’s whole
    semantics in the LSTM cell state. Instead, attention generates output vectors
    while reading the premise and accumulating a representation in the cell state
    that informs the second LSTM which of the premises’ output vectors to attend to
    determine the RTE class. Luong, et al. [[58](#bib.bib58)], proposed global and
    local attention in machine translation. Global attention is similar to soft attention,
    while local is an improvement to make hard attention differentiable - the model
    first provides for a single position aligned to the current target word, and a
    window centered around the position is used to calculate a vector of context.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: See et. al. [[55](#bib.bib55)] 使用了一种经典的序列到序列注意力模型和 Ptr-Net [[54](#bib.bib54)]
    的混合模型来进行抽象文本摘要。混合指针生成器 [[55](#bib.bib55)] 通过指向复制源文本中的词，这有助于准确再现信息，同时保留通过生成器生成新词的能力。最后，它使用一种机制来跟踪已被总结的内容，从而避免重复。FusionNet
    [[56](#bib.bib56)] 提出了“词历史”这一新概念，以描述从最低的词嵌入级别到最高的语义级表示的注意力信息。这个概念考虑到数据输入逐渐转化为更抽象的表示，形成每个词在人的思维流中的历史。FusionNet
    采用了完全感知的多级注意力机制和一种利用词历史的注意力评分函数。Rocktäschel et al. [[57](#bib.bib57)] 引入了双向注意力机制来识别文本蕴涵（RTE）。这一机制允许模型关注过去的输出向量，解决了
    LSTM 的单元状态瓶颈。带有注意力的 LSTM 不需要在 LSTM 单元状态中捕捉前提的全部语义。相反，注意力在阅读前提时生成输出向量，并在单元状态中累积一个表示，告知第二个
    LSTM 应该关注前提的哪个输出向量来确定 RTE 类别。Luong et al. [[58](#bib.bib58)] 提出了机器翻译中的全局注意力和局部注意力。全局注意力类似于软注意力，而局部注意力则是对硬注意力的改进——模型首先为当前目标词提供一个对齐的单一位置，并使用以该位置为中心的窗口来计算上下文向量。
- en: Attentional interfaces have also emerged in architectures for computer vision
    tasks. Initially, they are based on human saccadic movements and robustness to
    change. The human visual attention mechanism can explore local differences in
    an image while highlighting the relevant parts. One person focuses attention on
    parts of the image simultaneously, glimpsing to quickly scan the entire image
    to find the main areas during the recognition process. In this process, the different
    regions’ internal relationship guides the eyes’ movement to find the next area
    to focus. Ignoring the irrelevant parts makes it easier to learn in the presence
    of disorder. Another advantage of glimpse and visual attention is its robustness.
    Our eyes can see an object in a real-world scene but ignore irrelevant parts.
    Convolutional neural networks (CNNs) are extremely different. CNNs are rigid,
    and the number of parameters grows linearly with the size of the image. Also,
    for the network to capture long-distance dependencies between pixels, the architecture
    needs to have many layers, compromising the model’s convergence. Besides, the
    network treats all pixels in the same way. This process does not resemble the
    human visual system that contains visual attention mechanisms and a glimpse structure
    that provides unmatched performance in object recognition.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力接口也在计算机视觉任务的架构中出现。最初，它们基于人类的眼球跳动运动和对变化的鲁棒性。人类的视觉注意机制可以探索图像中的局部差异，同时突出相关部分。一个人会同时将注意力集中在图像的某些部分，通过快速浏览整幅图像来在识别过程中找到主要区域。在这个过程中，不同区域的内部关系引导眼睛的运动，找到下一个需要关注的区域。忽略不相关的部分使得在存在混乱的情况下更容易学习。另一个优势是视觉注意的鲁棒性。我们的眼睛可以在现实场景中看到一个物体，但忽略不相关的部分。卷积神经网络（CNN）则完全不同。CNN是刚性的，参数数量随着图像大小线性增长。而且，为了让网络捕捉到像素之间的长距离依赖，架构需要有许多层，这会妥协模型的收敛性。此外，网络对所有像素的处理是相同的。这一过程与包含视觉注意机制和提供无与伦比的目标识别性能的眼动结构的人类视觉系统并不相似。
- en: RAM [[46](#bib.bib46)] and STN are pioneering architectures with attentional
    interfaces based on human visual attention. RAM [[46](#bib.bib46)] can extract
    information from an image or video by adaptively selecting a sequence of regions,
    glimpses, only processing the selected areas at high resolution. The model is
    a Recurrent Neural Network that processes different parts of the images (or video
    frames) at each instant of time t, building a dynamic internal representation
    of the scene via Reinforcement Learning training. The main model advantages are
    the reduced number of parameters and the architecture’s independence to the input
    image size, which does not occur in convolutional neural networks. This approach
    is generic. It can use static images, videos, or a perceptual module of an agent
    that interacts with the environment. STN (Spatial Transformer Network) [[59](#bib.bib59)]
    is a module robust to spatial transformation changes. In STN, if the input is
    transformed, the model must generate the correct classification label, even if
    it is distorted in unusual ways. STN works as an attentional module attachable
    – with few modifications – to any neural network to actively spatially transform
    feature maps. STN learns transformation during the training process. Unlike pooling
    layers, where receptive fields are fixed and local, a Spatial Transformer is a
    dynamic mechanism that can spatially transform an image, or feature map, producing
    the appropriate transformation for each input sample. The transformation is performed
    across the map and may include changes in scale, cut, rotations, and non-rigid
    body deformations. This approach allows the network to select the most relevant
    image regions (attention) and transform them into a desired canonical position
    by simplifying recognition in the following layers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: RAM [[46](#bib.bib46)] 和 STN 是具有基于人类视觉注意力的前沿架构。RAM [[46](#bib.bib46)] 可以通过自适应选择区域序列（*glimpses*），仅对选定区域以高分辨率进行处理，从而从图像或视频中提取信息。该模型是一个递归神经网络，在每个时间点
    t 处理图像（或视频帧）的不同部分，通过强化学习训练构建场景的动态内部表示。主要模型优点是参数数量减少以及架构对输入图像大小的独立性，这在卷积神经网络中并不存在。这种方法是通用的。它可以使用静态图像、视频或与环境交互的代理的感知模块。STN（空间变换网络）[[59](#bib.bib59)]
    是一个对空间变换变化具有鲁棒性的模块。在 STN 中，如果输入经过变换，模型必须生成正确的分类标签，即使它以不寻常的方式扭曲。STN 作为一个注意力模块，可以附加
    – 仅需少量修改 – 到任何神经网络，以主动空间变换特征图。STN 在训练过程中学习变换。与池化层不同，池化层的感受野是固定的和局部的，而空间变换器是一种动态机制，可以空间变换图像或特征图，为每个输入样本产生适当的变换。变换在整个图上进行，可能包括尺度变化、裁剪、旋转和非刚性变形。这种方法使网络能够选择最相关的图像区域（*attention*）并将其变换为所需的标准位置，从而简化后续层中的识别。
- en: Following the RAM approach, the Deep Recurrent Attentive Writer (DRAW) [[36](#bib.bib36)]
    represents a change to a more natural way of constructing the image in which parts
    of a scene are created independently of the others. This process is how human
    beings draw a scene by recreating a visual scene sequentially, refining all parts
    of the drawing for several iterations, and reevaluating their work after each
    modification. Although natural to humans, most approaches to automatic image generation
    aim to generate complete scenes at once. This means that all pixels are conditioned
    in a single latent distribution, making it challenging to scale large image approaches.
    DRAW belongs to the family of variational autoencoders. It has an encoder that
    compresses the images presented during training and a decoder that reconstructs
    the images. Unlike other generative models, DRAW iteratively constructs the scenes
    by accumulating modifications emitted by the decoder, each observed by the encoder.
    DRAW uses RAM attention mechanisms to attend to parts of the scene while ignoring
    others selectively. This mechanism’s main challenge is to learn where to look,
    which is usually addressed by reinforcement learning techniques. However, at DRAW,
    the attention mechanism is differentiable, making it possible to use backpropagation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 根据RAM方法，深度递归注意力写作者（DRAW）[[36](#bib.bib36)]代表了一种更自然的图像构建方式，其中场景的部分是独立创建的。这个过程类似于人类通过顺序重建视觉场景来绘制图像，不断细化图像的各个部分，并在每次修改后重新评估作品。尽管对人类来说很自然，但大多数自动图像生成方法旨在一次生成完整场景。这意味着所有像素都在单一潜在分布中进行条件化，使得扩展大型图像方法具有挑战性。DRAW属于变分自编码器家族。它有一个在训练过程中压缩图像的编码器和一个重建图像的解码器。与其他生成模型不同，DRAW通过累积解码器发出的修改来迭代构建场景，每次修改都被编码器观察到。DRAW使用RAM注意力机制来选择性地关注场景的部分，同时忽略其他部分。这个机制的主要挑战是学习关注的区域，这通常通过强化学习技术来解决。然而，在DRAW中，注意力机制是可微分的，使得可以使用反向传播。
- en: 2.2 Multimodality
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 多模态性
- en: The first attention interfaces’ use in DL were limited to NLP and computer vision
    domains to solve isolated tasks. Currently, attentional interfaces are studied
    in multimodal learning. Sensory multimodality in neural networks is a historical
    problem widely discussed by the scientific community [[60](#bib.bib60)] [[61](#bib.bib61)].
    Multimodal data improves the robustness of perception through complementarity
    and redundancy. The human brain continually deals with multimodal data and integrates
    it into a coherent representation of the world. However, employing different sensors
    present a series of challenges computationally, such as incomplete or spurious
    data, different properties (i.e. dimensionality or range of values), and the need
    for data alignment association. The integration of multiple sensors depends on
    a reasoning structure over the data to build a common representation, which does
    not exist in classical neural networks. Attentional interfaces adapted for multimodal
    perception are an efficient alternative for reasoning about misaligned data from
    different sensory sources.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首次在深度学习中使用注意力接口的领域仅限于自然语言处理和计算机视觉，用于解决孤立任务。目前，注意力接口在多模态学习中得到了研究。神经网络中的感觉多模态性是一个历史性的问题，科学界对此进行了广泛讨论[[60](#bib.bib60)]
    [[61](#bib.bib61)]。多模态数据通过互补性和冗余性提高了感知的鲁棒性。人脑不断处理多模态数据，并将其整合为对世界的连贯表征。然而，使用不同传感器会面临一系列计算挑战，如数据不完整或虚假、不同属性（即维度或数值范围）以及数据对齐关联的需求。多个传感器的集成依赖于对数据的推理结构，以建立共同的表征，这在经典神经网络中并不存在。适用于多模态感知的注意力接口是对不同传感器源的错位数据进行推理的有效替代方案。
- en: The first widespread use of attention for multimodality occurs with the attentional
    interface between a convolutional neural network and an LSTM in image captioning [[45](#bib.bib45)].
    In this model, a CNN processes the image, extracting high-level features, whereas
    the LSTM consumes the features to produce descriptive words, one by one. The attention
    mechanism guides the LSTM to relevant image information for each word’s generation,
    equivalent to the human visual attention mechanism. The visualization of attention
    weights in multimodal tasks improved the understanding of how architecture works.
    This approach derived from countless other works with attentional interfaces that
    deal with video-text data [[62](#bib.bib62)] [[63](#bib.bib63)] [[64](#bib.bib64)],
    image-text data [[65](#bib.bib65)] [[66](#bib.bib66)], monocular/RGB-D images [[67](#bib.bib67)] [[68](#bib.bib68)] [[69](#bib.bib69)],
    RADAR [[68](#bib.bib68)], remote sensing data [[70](#bib.bib70)] [[71](#bib.bib71)] [[72](#bib.bib72)] [[73](#bib.bib73)],
    audio-video [[74](#bib.bib74)] [[75](#bib.bib75)], and diverse sensors [[76](#bib.bib76)] [[77](#bib.bib77)] [[78](#bib.bib78)],
    as shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Multimodality ‣ 2 Overview ‣ Attention,
    please! A survey of Neural Attention Models in Deep Learning").
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对多模态的广泛应用首次出现在卷积神经网络与长短期记忆网络（LSTM）之间的注意力接口，用于图像字幕生成[[45](#bib.bib45)]。在这一模型中，CNN处理图像，提取高层特征，而LSTM则利用这些特征逐个生成描述性词语。注意力机制引导LSTM对每个词的生成定位到相关的图像信息，相当于人类的视觉注意机制。多模态任务中注意力权重的可视化提高了对架构工作的理解。这种方法源于无数其他处理视频-文本数据[[62](#bib.bib62)]
    [[63](#bib.bib63)] [[64](#bib.bib64)]、图像-文本数据[[65](#bib.bib65)] [[66](#bib.bib66)]、单目/RGB-D
    图像[[67](#bib.bib67)] [[68](#bib.bib68)] [[69](#bib.bib69)]、雷达[[68](#bib.bib68)]、遥感数据[[70](#bib.bib70)]
    [[71](#bib.bib71)] [[72](#bib.bib72)] [[73](#bib.bib73)]、音视频[[74](#bib.bib74)]
    [[75](#bib.bib75)]、以及各种传感器[[76](#bib.bib76)] [[77](#bib.bib77)] [[78](#bib.bib78)]的注意力接口，如图[3](#S2.F3
    "Figure 3 ‣ 2.2 Multimodality ‣ 2 Overview ‣ Attention, please! A survey of Neural
    Attention Models in Deep Learning")所示。
- en: Zhang et al. [[79](#bib.bib79)] used an adaptive attention mechanism to learn
    to emphasize different visual and textual sources for dialogue systems for fashion
    retail. An adaptive attention scheme automatically decided the evidence source
    for tracking dialogue states based on visual and textual context. Dual Attention
    Networks [[80](#bib.bib80)] presented attention mechanisms to capture the fine-grained
    interplay between images and textual information. The mechanism allows visual
    and textual attention to guide each other during collaborative inference. HATT [[63](#bib.bib63)]
    presented a new attention-based hierarchical fusion to explore the complementary
    features of multimodal features progressively, fusing temporal, motion, audio,
    and semantic label features for video representation. The model consists of three
    attention layers. First, the low-level attention layer deals with temporal, motion,
    and audio features inside each modality and across modalities. Second, high-level
    attention selectively focuses on semantic label features. Finally, the sequential
    attention layer incorporates hidden information generated by encoded low-level
    attention and high-level attention. Hori et. al. [[74](#bib.bib74)] extended simple
    attention multimodal fusion. Unlike the simple multimodal fusion method, the feature-level
    attention weights can change according to the decoder state and the context vectors,
    enabling the decoder network to pay attention to a different set of features or
    modalities when predicting each subsequent word in the description. Memory Fusion
    Network [[76](#bib.bib76)] presented the Delta-memory Attention module for multi-view
    sequential learning. First, an LSTM system, one for each of the modalities, encodes
    the modality-specific dynamics and interactions. Delta-memory attention discovers
    both cross-modality and temporal interactions in different memory dimensions of
    LSTMs. Finally, Multi-view Gated Memory (unifying memory) stores the cross-modality
    interactions over time.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等人 [[79](#bib.bib79)] 使用了一种自适应注意力机制来学习在时尚零售对话系统中强调不同的视觉和文本来源。自适应注意力方案会根据视觉和文本上下文自动决定用于跟踪对话状态的证据来源。Dual
    Attention Networks [[80](#bib.bib80)] 提出了注意力机制来捕捉图像和文本信息之间的细粒度相互作用。该机制允许视觉和文本注意力在协同推理过程中相互引导。HATT
    [[63](#bib.bib63)] 提出了基于注意力的分层融合方法，以逐步探索多模态特征的互补特征，将时间、运动、音频和语义标签特征融合用于视频表示。该模型由三层注意力层组成。首先，低级注意力层处理每种模态内部及模态间的时间、运动和音频特征。其次，高级注意力层选择性地关注语义标签特征。最后，序列注意力层结合由编码的低级注意力和高级注意力生成的隐藏信息。Hori
    等人 [[74](#bib.bib74)] 扩展了简单的注意力多模态融合。与简单的多模态融合方法不同，特征级别的注意力权重可以根据解码器状态和上下文向量变化，使得解码器网络在预测描述中的每个后续词时能够关注不同的特征或模态。Memory
    Fusion Network [[76](#bib.bib76)] 提出了 Delta-memory Attention 模块用于多视角序列学习。首先，每种模态的
    LSTM 系统对模态特定的动态和交互进行编码。Delta-memory attention 发现了 LSTM 的不同记忆维度中的跨模态和时间交互。最后，多视角门控记忆（统一记忆）存储了随时间变化的跨模态交互。
- en: Huang et al. [[81](#bib.bib81)] investigated the problem of matching image-text
    by exploiting the bi-directional attention with fine-granularity correlations
    between visual regions and textual words. Bi-directional attention connects the
    word to regions and objects to words for learning mage-text matching. Li et. al. [[82](#bib.bib82)]
    introduced Long Short-Term Memory with Pointing (LSTM-P) inspired by humans pointing
    behavior [[83](#bib.bib83)], and Pointer Networks [[54](#bib.bib54)]. The pointing
    mechanism encapsulates dynamic contextual information (current input word and
    LSTM cell output) to deal with the image captioning scenario’s novel objects.
    Liu et. al. [[84](#bib.bib84)] proposed a cross-modal attention-guided erasing
    approach for referring expressions. Previous attention models focus on only the
    most dominant features of both modalities and neglect textual-visual correspondences
    between images and referring expressions. To tackle this issue, cross-modal attention
    discards the most dominant information from either textual or visual domains to
    generate difficult training samples and drive the model to discover complementary
    textual-visual correspondences. Abolghasemi et al. [[85](#bib.bib85)] demonstrated
    an approach for augmenting a deep visuomotor policy trained through demonstrations
    with Task Focused Visual Attention (TFA). Attention receives as input a manipulation
    task specified in natural language text, an image with the environment, and returns
    as output the area with an object that the robot needs to manipulate. TFA allows
    the policy to be significantly more robust from the baseline policy, i.e., no
    visual attention. Pu et al. [[66](#bib.bib66)] adaptively select features from
    the multiple CNN layers for video captioning. Previous models often use the output
    from a specific layer of a CNN as video features. However, this attention model
    adaptively and sequentially focuses on different layers of CNN features.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 黄等人[[81](#bib.bib81)] 通过利用双向注意力来研究图像-文本匹配问题，该注意力具有视觉区域和文本词汇之间的细粒度关联。双向注意力将词汇与区域以及对象与词汇相连接，以学习图像-文本匹配。李等人[[82](#bib.bib82)]
    引入了受人类指向行为启发的长短期记忆网络（LSTM-P）[[83](#bib.bib83)]，以及指针网络[[54](#bib.bib54)]。指向机制封装了动态上下文信息（当前输入词汇和LSTM单元输出），以处理图像描述场景中的新颖对象。刘等人[[84](#bib.bib84)]
    提出了一个跨模态注意力引导的擦除方法，用于参考表达。之前的注意力模型仅关注两种模态中最具主导性的特征，忽略了图像和参考表达之间的文本-视觉对应关系。为了解决这个问题，跨模态注意力从文本或视觉领域中丢弃最主导的信息，以生成困难的训练样本，并推动模型发现互补的文本-视觉对应关系。阿布尔哈塞米等人[[85](#bib.bib85)]
    展示了一种通过演示增强深度视觉运动策略的方法，该方法结合了任务专注视觉注意力（TFA）。注意力机制接收自然语言文本中指定的操作任务、带有环境的图像作为输入，并返回机器人需要操作的对象所在区域作为输出。TFA使策略比基线策略（即无视觉注意力）显著更强健。蒲等人[[66](#bib.bib66)]
    自适应地从多个CNN层中选择特征用于视频描述。之前的模型通常使用CNN的特定层的输出作为视频特征。然而，这种注意力模型自适应地、顺序地关注CNN特征的不同层。
- en: '![Refer to caption](img/2f08ad1d0e77dd8154ae11e9ed5d3d71.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2f08ad1d0e77dd8154ae11e9ed5d3d71.png)'
- en: 'Figure 3: A diagram showing sensory modalities of neural attention models.
    Radial segments correspond to attention architectures, and each track corresponds
    to a modality. Modalities are: (A) audio, (B) biomedical signals, (I) image, (O)
    other sensors, (L) LiDAR, (R) remote sensing data, (T) text, and (V) video. The
    following coloring convention is used for the individual segments: white (the
    modality is not implemented), light yellow (CNN), light orange (RNN), orange (Self-attentive
    networks), red (Memory networks), dark red (framework), and brown (GNN). This
    diagram emphasizes multimodal architectures so that only the most representative
    single modality (i.e., text or image) architectures are shown. Most multimodal
    architectures use the image/text or video/text modalities.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：展示神经注意力模型的感官模态的图示。径向分段对应于注意力架构，每条轨迹对应于一种模态。模态包括：（A）音频，（B）生物医学信号，（I）图像，（O）其他传感器，（L）激光雷达，（R）遥感数据，（T）文本，以及（V）视频。个别分段使用以下颜色约定：白色（该模态未实现）、浅黄色（CNN）、浅橙色（RNN）、橙色（自注意网络）、红色（记忆网络）、深红色（框架）和棕色（GNN）。此图强调多模态架构，因此仅显示最具代表性的单一模态（即文本或图像）架构。大多数多模态架构使用图像/文本或视频/文本模态。
- en: 2.3 Attention-augmented memory
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 注意力增强记忆
- en: Attentional interfaces also allow the neural network iteration with other cognitive
    elements (i.e., memories, working memory). Memory control and logic flow are essential
    for learning. However, they are elements that do not exist in classical architectures.
    The memory of classic RNNs, encoded by hidden states and weights, is usually minimal
    and is not sufficient to remember facts from the past accurately. Most Deep Learning
    models do not have a simple way to read and write data to an external memory component.
    The Neural Turing Machine (NTM) [[86](#bib.bib86)] and Memory Networks (MemNN) [[38](#bib.bib38)]
    - a new class of neural networks - introduced the possibility for a neural network
    dealing with addressable memory. NTM is a differentiable approach that can be
    trained with gradient descent algorithms, producing a practical learning program
    mechanism. NTM memory is a short-term storage space for information with its rules-based
    manipulation. Computationally, these rules are simple programs, where data are
    those programs’ arguments. Therefore, an NTM resembles a working memory designed
    to solve tasks that require rules, where variables are quickly linked to memory
    slots. NTMs use an attentive process to read and write elements to memory selectively.
    This attentional mechanism makes the network learn to use working memory instead
    of implementing a fixed set of symbolic data rules.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力接口还允许神经网络与其他认知元素（即，记忆、工作记忆）进行迭代。记忆控制和逻辑流对于学习至关重要。然而，这些是经典架构中不存在的元素。经典RNN的记忆通过隐藏状态和权重进行编码，通常是最小的，不足以准确记住过去的事实。大多数深度学习模型没有简单的方法来读写外部记忆组件。神经图灵机（NTM）[[86](#bib.bib86)]和记忆网络（MemNN）[[38](#bib.bib38)]——一种新的神经网络类别——引入了神经网络处理可寻址记忆的可能性。NTM是一种可微分的方法，可以通过梯度下降算法进行训练，产生一个实用的学习程序机制。NTM记忆是一个用于信息的短期存储空间，其规则基础的操作。计算上，这些规则是简单的程序，其中数据是这些程序的参数。因此，NTM类似于一个工作记忆，旨在解决需要规则的任务，其中变量迅速与记忆槽关联。NTM使用注意过程选择性地读写内存中的元素。这个注意机制使网络学会使用工作记忆，而不是实现固定的数据符号规则集。
- en: Memory Networks [[38](#bib.bib38)] are a relatively new framework of models
    designed to alleviate the problem of learning long-term dependencies in sequential
    data by providing an explicit memory representation for each token in the sequence.
    Instead of forgetting the past, Memory Networks explicitly consider the input
    history, with a dedicated vector representation for each history element, effectively
    removing the chance to forget. The limit on memory size becomes a hyper-parameter
    to tune, rather than an intrinsic limitation of the model itself. This model was
    used in question-answering tasks where the long-term memory effectively acts as
    a (dynamic) knowledge base, and the output is a textual response. Large-scale
    question-answer tests were performed, and the reasoning power of memory networks
    that answer questions that require an in-depth analysis of verb intent was demonstrated.
    Mainly due to the success of MemNN, networks with external memory are a growing
    research direction in DL, with several branches under development as shown in
    figure [4](#S2.F4 "Figure 4 ‣ 2.3 Attention-augmented memory ‣ 2 Overview ‣ Attention,
    please! A survey of Neural Attention Models in Deep Learning").
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆网络[[38](#bib.bib38)]是一种相对较新的模型框架，旨在通过为序列中的每个标记提供明确的记忆表示来缓解学习长期依赖性的问题。记忆网络不会忘记过去，而是明确考虑输入历史，为每个历史元素提供专门的向量表示，有效地消除了遗忘的可能性。内存大小的限制变成了一个超参数进行调整，而不是模型本身的固有限制。该模型在问答任务中得到了应用，其中长期记忆有效地充当了一个（动态的）知识库，输出是一个文本响应。进行了大规模的问答测试，证明了记忆网络在回答需要深入分析动词意图的问题时的推理能力。主要由于MemNN的成功，带有外部记忆的网络成为了深度学习中的一个发展方向，多个分支正在开发中，如图[4](#S2.F4
    "Figure 4 ‣ 2.3 Attention-augmented memory ‣ 2 Overview ‣ Attention, please! A
    survey of Neural Attention Models in Deep Learning")所示。
- en: '![Refer to caption](img/bbd23bfe7b9f7e2d17ee5010481a8343.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bbd23bfe7b9f7e2d17ee5010481a8343.png)'
- en: 'Figure 4: Memory-based neural networks (MemNN). Architectures labels are color-coded
    as follows: orange - natural language processing, red - computer vision, purple
    - others. The end-to-End Memory networks is the first end-to-end differentiable
    version of MemNN. GMN [[87](#bib.bib87)] and MemGNN [[87](#bib.bib87)] are the
    first graph networks with memory. DMN [[88](#bib.bib88)], MemGNN [[87](#bib.bib87)],
    Episodic graph memory networks [[89](#bib.bib89)], Episodic CAMN [[90](#bib.bib90)],
    are the first instances of the episodic memory framework.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 基于记忆的神经网络（MemNN）。架构标签的颜色编码如下：橙色 - 自然语言处理，红色 - 计算机视觉，紫色 - 其他。端到端记忆网络是 MemNN
    的第一个端到端可微版本。GMN [[87](#bib.bib87)] 和 MemGNN [[87](#bib.bib87)] 是第一个具有记忆的图网络。DMN [[88](#bib.bib88)]、MemGNN [[87](#bib.bib87)]、情节图记忆网络 [[89](#bib.bib89)]、情节
    CAMN [[90](#bib.bib90)]，是情节记忆框架的第一个实例。'
- en: End-to-end Memory Networks [[91](#bib.bib91)] is the first version of MemNN
    applicable to realistic, trainable end-to-end scenarios, which requires low supervision
    during training. Aug Oh. et al. [[92](#bib.bib92)] extends Memory Networks to
    suit the task of semi-supervised segmentation of video objects. Frames with object
    masks are placed in memory, and a frame to be segmented acts as a query. The memory
    is updated with the new masks provided and faces challenges such as changes, occlusions,
    and accumulations of errors without online learning. The algorithm acts as an
    attentional space-time system calculating when and where to meet each query pixel
    to decide whether the pixel belongs to a foreground object or not. Kumar et al. [[93](#bib.bib93)]
    propose the first network with episodic memory - a type of memory extremely relevant
    to humans - to iterate over representations emitted by the input module updating
    its internal state through an attentional interface. In [[89](#bib.bib89)], an
    episodic memory with a key-value retrieval mechanism chooses which parts of the
    input to focus on thorough attention. The module then produces a summary representation
    of the memory, taking into account the query and the stored memory. Finally, the
    latest research has invested in Graph Memory Networks (GMN), which are memories
    in GNNs [[94](#bib.bib94)], to better handle unstructured data using key-value
    structured memories [[95](#bib.bib95)] [[87](#bib.bib87)] [[87](#bib.bib87)] [[96](#bib.bib96)].
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端记忆网络 [[91](#bib.bib91)] 是适用于现实、可训练端到端场景的第一版 MemNN，这需要在训练过程中低监督。Aug Oh. 等人 [[92](#bib.bib92)]
    扩展了记忆网络以适应半监督视频对象分割任务。具有对象掩码的帧被放置在记忆中，而需要分割的帧作为查询。记忆通过提供的新掩码进行更新，并面临如变化、遮挡和错误累积等挑战，且没有在线学习。该算法作为一个注意力时空系统，计算何时何地满足每个查询像素，以决定该像素是否属于前景对象。Kumar
    等人 [[93](#bib.bib93)] 提出了第一个具有情节记忆的网络——一种与人类极为相关的记忆——通过注意力接口迭代输入模块发出的表示并更新其内部状态。在 [[89](#bib.bib89)]
    中，具有键值检索机制的情节记忆选择通过全面注意力关注输入的哪些部分。该模块然后产生记忆的总结表示，考虑查询和存储的记忆。最后，最新的研究投入于图记忆网络（GMN），即
    GNNs 中的记忆 [[94](#bib.bib94)]，以利用键值结构化记忆 [[95](#bib.bib95)] [[87](#bib.bib87)] [[87](#bib.bib87)] [[96](#bib.bib96)]
    更好地处理非结构化数据。
- en: 2.4 End-to-end attention models
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 端到端注意力模型
- en: In mid-2017, research aiming at end-to-end attention models appeared in the
    area. The Neural Transformer (NT) [[37](#bib.bib37)] and Graph Attention Networks [[97](#bib.bib97)]
    - purely attentional architectures - demonstrated to the scientific community
    that attention is a key element for the future development in Deep Learning. The
    Transformer’s goal is to use self-attention (Section [3](#S3 "3 Attention Mechanisms
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")) to
    minimize traditional recurrent neural networks’ difficulties. The Neural Transformer
    is the first neural architecture that uses only attentional modules and fully-connected
    neural networks to process sequential data successfully. It dispenses recurrences
    and convolutions, capturing the relationship between the sequence elements regardless
    of their distance. Attention allows the Transformer to be simple, parallelizable,
    and low training cost [[37](#bib.bib37)]. Graph Attention Networks (GATs) are
    an end-to-end attention version of GNNs [[94](#bib.bib94)]. They have stacks of
    attentional layers that help the model focus on the unstructured data’s most relevant
    parts to make decisions. The main purpose of attention is to avoid noisy parts
    of the graph by improving the signal-to-noise ratio (SNR) while also reducing
    the structure’s complexity. Furthermore, they provide a more interpretable structure
    for solving the problem. For example, when analyzing the Attention of a model
    under different components in a graph, it is possible to identify the main factors
    contributing to achieving a particular response condition.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2017 年中期，旨在端到端注意力模型的研究在该领域出现。神经 Transformer (NT) [[37](#bib.bib37)] 和图注意力网络 [[97](#bib.bib97)]
    - 完全基于注意力的架构 - 向科学界展示了注意力是深度学习未来发展的关键元素。Transformer 的目标是使用自注意力（第 [3](#S3 "3 Attention
    Mechanisms ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")节）来最小化传统递归神经网络的困难。神经
    Transformer 是第一种仅使用注意力模块和全连接神经网络成功处理序列数据的神经架构。它摒弃了递归和卷积，能够捕捉序列元素之间的关系，而不受距离的限制。注意力使得
    Transformer 简单、可并行化且训练成本低 [[37](#bib.bib37)]。图注意力网络 (GATs) 是图神经网络 (GNNs) 的端到端注意力版本 [[94](#bib.bib94)]。它们具有多层注意力机制，帮助模型关注无结构数据中最相关的部分以作出决策。注意力的主要目的是通过提高信噪比
    (SNR) 来避免图的噪声部分，同时减少结构的复杂性。此外，它们为解决问题提供了更具可解释性的结构。例如，在分析图中不同组件下模型的注意力时，可以识别出对实现特定响应条件的主要因素。
- en: '![Refer to caption](img/2774271a31f8db4247b4a9e80d88da4b.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/2774271a31f8db4247b4a9e80d88da4b.png)'
- en: 'Figure 5: Transformer-based neural networks. Architectures labels are color-coded
    as follows: orange - natural language processing, red - computer vision, purple
    - others.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基于 Transformer 的神经网络。架构标签的颜色编码如下：橙色 - 自然语言处理，红色 - 计算机视觉，紫色 - 其他。
- en: 'There is a growing interest in NT and GATs, and some extensions have been proposed [[98](#bib.bib98)] [[99](#bib.bib99)] [[100](#bib.bib100)] [[101](#bib.bib101)],
    with numerous Transformer-based architectures as shown figure [5](#S2.F5 "Figure
    5 ‣ 2.4 End-to-end attention models ‣ 2 Overview ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning"). These architectures and all that
    use self-attention belong to a new category of neural networks, called Self-Attentive
    Neural Networks. They aim to explore self-attention in various tasks and improve
    the following drawbacks: 1) a Large number of parameters and training iterations
    to converge; 2) High memory cost per layer and quadratic growth of memory according
    to sequence length; 3) Auto-regressive model; 4) Low parallelization in the decoder
    layers. Specifically, Weighted Transformer [[102](#bib.bib102)] proposes modifications
    in the attention layers achieving a 40 % faster convergence. The multi-head attention
    modules are replaced by modules called branched attention that the model learns
    to match during the training process. The Star-transformer [[103](#bib.bib103)]
    proposes a lightweight alternative to reduce the model’s complexity with a star-shaped
    topology. To reduce the cost of memory, Music Transformer [[104](#bib.bib104)],
    and Sparse Transformer [[105](#bib.bib105)] introduces relative self-attention
    and factored self-attention, respectively. Lee et al. [[106](#bib.bib106)] also
    features an attention mechanism that reduces self-attention from quadratic to
    linear, allowing scaling for high inputs and data sets.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NT和GAT的兴趣日益增长，已经提出了一些扩展[[98](#bib.bib98)] [[99](#bib.bib99)] [[100](#bib.bib100)]
    [[101](#bib.bib101)]，如图[5](#S2.F5 "Figure 5 ‣ 2.4 End-to-end attention models
    ‣ 2 Overview ‣ Attention, please! A survey of Neural Attention Models in Deep
    Learning")所示的众多基于Transformer的架构。这些架构以及所有使用自注意力的网络属于一种新的神经网络类别，称为自注意力神经网络。它们旨在在各种任务中探索自注意力，并改善以下缺点：1)
    大量的参数和训练迭代以收敛；2) 每层的高内存成本和根据序列长度的内存二次增长；3) 自回归模型；4) 解码器层的低并行性。具体而言，Weighted Transformer
    [[102](#bib.bib102)] 提出了在注意力层中进行修改，从而实现了40%更快的收敛。多头注意力模块被称为分支注意力的模块所取代，模型在训练过程中学习如何匹配这些模块。Star-transformer
    [[103](#bib.bib103)] 提出了一个轻量级的替代方案，通过星形拓扑结构来减少模型的复杂性。为了降低内存成本，Music Transformer
    [[104](#bib.bib104)] 和 Sparse Transformer [[105](#bib.bib105)] 分别引入了相对自注意力和分解自注意力。Lee等人
    [[106](#bib.bib106)] 还展示了一种注意力机制，将自注意力从二次降低为线性，允许对高输入和数据集进行扩展。
- en: Some approaches adapt the Transformer to new applications and areas. In natural
    language processing, several new architectures have emerged, mainly in multimodal
    learning. Doubly Attentive Transformer [[107](#bib.bib107)] proposes a multimodal
    machine-translation method, incorporating visual information. It modifies the
    attentional decoder, allowing textual features from a pre-trained CNN encoder
    and visual features. The Multi-source Transformer [[108](#bib.bib108)] explores
    four different strategies for combining input into the multi-head attention decoder
    layer for multimodal translation. Style Transformer [[109](#bib.bib109)], Hierarchical
    Transformer [[110](#bib.bib110)], HighWay Recurrent Transformer [[111](#bib.bib111)],
    Lattice-Based Transformer [[112](#bib.bib112)], Transformer TTS Network [[113](#bib.bib113)],
    Phrase-Based Attention [[114](#bib.bib114)] are some important architectures in
    style transfer, document summarization and machine translation. Transfer Learning
    in NLP is one of Transformer’s major contribution areas. BERT [[115](#bib.bib115)],
    GPT-2 [[116](#bib.bib116)], and GPT-3 [[117](#bib.bib117)] based NT architecture
    to solve the problem of Transfer Learning in NLP because current techniques restrict
    the power of pre-trained representations. In computer vision, the generation of
    images is one of the Transformer’s great news. Image Transformer [[118](#bib.bib118)],
    SAGAN [[119](#bib.bib119)], and Image GPT [[120](#bib.bib120)] uses self-attention
    mechanism to attend the local neighborhoods. The size of the images that the model
    can process in practice significantly increases, despite maintaining significantly
    larger receptive fields per layer than the typical convolutional neural networks.
    Recently, at the beginning of 2021, OpenAi introduced the scientific community
    to DALL·E [[121](#bib.bib121)], the Newest language model based on Transformer
    and GPT-3, capable of generating images from texts extending the knowledge of
    GPT-3 for viewing with only 12 billions of parameters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法将 Transformer 适应于新的应用和领域。在自然语言处理领域，几种新的架构出现了，主要集中在多模态学习上。Doubly Attentive
    Transformer [[107](#bib.bib107)] 提出了一个多模态机器翻译方法，结合了视觉信息。它修改了注意力解码器，允许使用来自预训练 CNN
    编码器的文本特征和视觉特征。Multi-source Transformer [[108](#bib.bib108)] 探索了四种不同的策略，将输入合并到多头注意力解码层中，以实现多模态翻译。Style
    Transformer [[109](#bib.bib109)]、Hierarchical Transformer [[110](#bib.bib110)]、HighWay
    Recurrent Transformer [[111](#bib.bib111)]、Lattice-Based Transformer [[112](#bib.bib112)]、Transformer
    TTS Network [[113](#bib.bib113)]、Phrase-Based Attention [[114](#bib.bib114)] 是风格迁移、文档摘要和机器翻译中的一些重要架构。NLP
    中的迁移学习是 Transformer 的主要贡献领域之一。BERT [[115](#bib.bib115)]、GPT-2 [[116](#bib.bib116)]
    和 GPT-3 [[117](#bib.bib117)] 基于 NT 架构来解决 NLP 中迁移学习的问题，因为当前技术限制了预训练表示的能力。在计算机视觉中，图像生成是
    Transformer 的一个重要进展。Image Transformer [[118](#bib.bib118)]、SAGAN [[119](#bib.bib119)]
    和 Image GPT [[120](#bib.bib120)] 使用自注意力机制来关注局部邻域。尽管每层的感受野比典型的卷积神经网络大得多，但模型在实际应用中处理图像的大小显著增加。最近，在
    2021 年初，OpenAI 向科学界介绍了基于 Transformer 和 GPT-3 的最新语言模型 DALL·E [[121](#bib.bib121)]，它能够从文本生成图像，将
    GPT-3 的知识扩展到仅有 120 亿参数的视图生成能力。
- en: 2.5 Attention today
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 今日关注
- en: Currently, hybrid models that employ the main key developments in attention’s
    use in Deep Learning (Figure [6](#S2.F6 "Figure 6 ‣ 2.5 Attention today ‣ 2 Overview
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")) have
    aroused the scientific community’s interest. Mainly, hybrid models based on Transformer,
    GATs, and Memory Networks have emerged for multimodal learning and several other
    application domains. Hyperbolic Attention Networks (HAN) [[122](#bib.bib122)],
    Hyperbolic Graph Attention Networks (GHN) [[123](#bib.bib123)], Temporal Graph
    Networks (TGN) [[124](#bib.bib124)] and Memory-based Graph Networks (MGN) [[87](#bib.bib87)]
    are some of the most promising developments. Hyperbolic networks are a new class
    of architecture that combine the benefits of self-attention, memory, graphs, and
    hyperbolic geometry in activating neural networks to reason with high capacity
    over embeddings produced by deep neural networks. Since 2019 these networks have
    stood out as a new research branch because they represent state-of-the-art generalization
    on neural machine translation, learning on graphs, and visual question answering
    tasks while keeping the neural representations compact. Since 2019, GATs have
    also received much attention due to their ability to learn complex relationships
    or interactions in a wide spectrum of problems ranging from biology, particle
    physics, social networks to recommendation systems. To improve the representation
    of nodes and expand the capacity of GATs to deal with data of a dynamic nature
    (i.e. evolving features or connectivity over time), architectures that combine
    memory modules and the temporal dimension, like MGNs and TGNs, were proposed.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，采用深度学习中注意力机制主要关键发展的混合模型（图 [6](#S2.F6 "Figure 6 ‣ 2.5 Attention today ‣ 2
    Overview ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")）引起了科学界的兴趣。主要是基于
    Transformer、GATs 和 Memory Networks 的混合模型已出现，用于多模态学习和其他多个应用领域。Hyperbolic Attention
    Networks (HAN) [[122](#bib.bib122)]、Hyperbolic Graph Attention Networks (GHN)
    [[123](#bib.bib123)]、Temporal Graph Networks (TGN) [[124](#bib.bib124)] 和基于记忆的图网络
    (MGN) [[87](#bib.bib87)] 是一些最具前景的发展。超曲面网络是一类新架构，结合了自注意力、记忆、图和超曲面几何的优势，用于激活神经网络，以高容量推理深度神经网络生成的嵌入。从
    2019 年以来，这些网络作为一种新的研究方向脱颖而出，因为它们在神经机器翻译、图学习和视觉问答任务上代表了最新的泛化，同时保持了神经表示的紧凑性。自 2019
    年以来，GATs 也因其在生物学、粒子物理学、社交网络和推荐系统等广泛问题中的能力而受到广泛关注。为了提高节点表示并扩展 GATs 处理动态数据（即随时间变化的特征或连接）的能力，提出了结合记忆模块和时间维度的架构，如
    MGNs 和 TGNs。
- en: '![Refer to caption](img/7a0b658889059f94cf0ded2ad38364c7.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7a0b658889059f94cf0ded2ad38364c7.png)'
- en: 'Figure 6: Key developments in Attention in DL Timeline. RNNSearch presented
    the first attention mechanism. Neural Turing machine and Memory networks introduced
    memory and dynamic flow control. RAM and DRAW learned to combine multi-glimpse,
    visual attention, and sequential processing. Spatial Transformer introduced a
    module to increase the robustness of CNNs to variations in spatial transformations.
    Show, attend and tell created attention for multimodality. The Pointer network
    used attention as a pointer. BiDAF, HAN, and DCN presented attentional techniques
    to align data with different hierarchical levels. ACT introduced the computation
    time topic. Transformer [[37](#bib.bib37)] was the first self-attentive neural
    network with an end-to-end attention approach. GATs introduced attention in GNNs.
    BERT [[115](#bib.bib115)], GPT-2 [[116](#bib.bib116)], GPT-3 [[117](#bib.bib117)],
    and DALL·E [[121](#bib.bib121)] are the state-of-the-art in language models and
    text-to-image generation. Finally, BRIMs [[125](#bib.bib125)] learned to combine
    bottom-up and top-down signals.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：深度学习中注意力机制的关键发展时间线。RNNSearch 提出了第一个注意力机制。Neural Turing 机和 Memory networks
    引入了记忆和动态流控制。RAM 和 DRAW 学会了结合多视角、视觉注意力和顺序处理。Spatial Transformer 引入了一个模块，以提高 CNN
    对空间变换的鲁棒性。Show, attend and tell 为多模态创建了注意力机制。Pointer network 使用注意力作为指针。BiDAF、HAN
    和 DCN 提出了对齐不同层次数据的注意力技术。ACT 引入了计算时间主题。Transformer [[37](#bib.bib37)] 是第一个采用端到端注意力方法的自注意力神经网络。GATs
    在 GNNs 中引入了注意力。BERT [[115](#bib.bib115)]、GPT-2 [[116](#bib.bib116)]、GPT-3 [[117](#bib.bib117)]
    和 DALL·E [[121](#bib.bib121)] 是语言模型和文本到图像生成领域的最新技术。最后，BRIMs [[125](#bib.bib125)]
    学会了结合自下而上和自上而下的信号。
- en: 'At the end of 2020, two research branches still little explored in the literature
    were strengthened: 1) explicit combination of bottom-up and top-down stimuli in
    bidirectional recurrent neural networks and 2) adaptive computation time. Classic
    recurrent neural networks perform recurring iteration within a particular level
    of representation instead of using a top-down iteration, in which higher levels
    act at lower levels. However, Mittal et al. [[125](#bib.bib125)] revisited the
    bidirectional recurrent layers with attentional mechanisms to explicitly route
    the flow of bottom-up and top-down information, promoting selection iteration
    between the two levels of stimuli. The approach separates the hidden state into
    several modules so that upward iterations between bottom-up and top-down signals
    can be appropriately focused. The layer structure has concurrent modules so that
    each hierarchical layer can send information both in the bottom-up and top-down
    directions.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到2020年底，文献中仍然未被充分探讨的两个研究分支得到了加强：1）在双向递归神经网络中明确结合自下而上的刺激和自上而下的刺激，2）自适应计算时间。经典的递归神经网络在特定的表示层次内执行递归迭代，而不是使用自上而下的迭代，其中高层在低层上起作用。然而，Mittal等人[[125](#bib.bib125)]重新审视了具有注意力机制的双向递归层，以明确路由自下而上的信息流和自上而下的信息流，促进了两个刺激层次之间的选择迭代。这种方法将隐藏状态分成几个模块，以便自下而上和自上而下的信号之间的上升迭代可以得到适当的集中。层次结构具有并发模块，以便每个层次层可以在自下而上和自上而下的方向上发送信息。
- en: The adaptive computation time is an interesting little-explored topic in the
    literature that began to expand only in 2020 despite initial studies emerging
    in 2017\. ACT applies to different neural networks (e.g. RNNs, CNNs, LSTMs, Transformers).
    The general idea is that complex data might require more computation to produce
    a final result, while some unimportant or straightforward data might require less.
    The attention mechanism dynamically decides how long to process network training
    data. The seminal approach by Graves et al. [[126](#bib.bib126)] made minor modifications
    to an RNN, allowing the network to perform a variable number of state transitions
    and a variable number of outputs at each stage of the input. The resulting output
    is a weighted sum of the intermediate outputs, i.e., soft attention. A halting
    unit decides when the network should stop or continue. To limit computation time,
    attention adds a time penalty to the cost function by preventing the network from
    processing data for unnecessary amounts of time. This approach has recently been
    updated and expanded to other architectures. Spatially Adaptive Computation Time
    (SACT) [[127](#bib.bib127)] adapts ACT to adjust the per-position amount of computation
    to each spatial position of the block in convolutional layers, learning to focus
    computing on the regions of interest and to stop when the features maps are "good
    enough". Finally, Differentiable Adaptive Computation Time (DACT) [[128](#bib.bib128)]
    introduced the first differentiable end-to-end approach to computation time on
    recurring networks.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应计算时间是一个有趣且未被充分探讨的主题，尽管最初的研究始于2017年，但在2020年才开始扩展。ACT适用于不同的神经网络（例如RNN、CNN、LSTM、Transformers）。一般来说，复杂的数据可能需要更多的计算才能生成最终结果，而一些不重要或简单的数据则可能需要更少。注意力机制动态决定处理网络训练数据的时间长度。Graves等人的开创性方法[[126](#bib.bib126)]对RNN进行了小幅修改，使网络能够在每个输入阶段执行可变数量的状态转换和输出。结果输出是中间输出的加权和，即软注意力。一个停止单元决定网络何时停止或继续。为了限制计算时间，注意力通过阻止网络处理不必要的数据时间来给成本函数添加时间惩罚。这种方法最近已更新并扩展到其他架构。空间自适应计算时间（SACT）[[127](#bib.bib127)]将ACT调整为根据卷积层中块的每个空间位置调整计算量，学习将计算集中在感兴趣的区域，并在特征图“足够好”时停止。最后，可微分自适应计算时间（DACT）[[128](#bib.bib128)]引入了首个可微分的端到端计算时间方法，适用于递归网络。
- en: 3 Attention Mechanisms
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 注意力机制
- en: Deep attention mechanisms can be categorized into soft attention (global attention),
    hard attention (local attention), and self-attention (intra-attention).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 深度注意力机制可以分为软注意力（全局注意力）、硬注意力（局部注意力）和自注意力（内部注意力）。
- en: Soft Attention. Soft attention assigns a weight of 0 to 1 for each input element.
    It decides how much attention should be focused on each element, considering the
    interdependence between the input of the deep neural network’s mechanism and target.
    It uses softmax functions in the attention layers to calculate weights so that
    the entire attentional model is deterministic and differentiable. Soft attention
    can act in the spatial and temporal context. The spatial context operates mainly
    to extract the features or the weighting of the most relevant features. For the
    temporal context, it works by adjusting the weights of all samples in sliding
    time windows, as samples at different times have different contributions. Despite
    being deterministic and differentiable, soft mechanisms have a high computational
    cost for large inputs. Figure [7](#S3.F7 "Figure 7 ‣ 3 Attention Mechanisms ‣
    Attention, please! A survey of Neural Attention Models in Deep Learning") shows
    an intuitive example of a soft attention mechanism.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 软注意力。软注意力为每个输入元素分配0到1之间的权重。它决定了每个元素应该集中多少注意力，考虑到深度神经网络机制和目标之间的相互依赖。它在注意力层中使用softmax函数来计算权重，以确保整个注意力模型是确定性的和可微分的。软注意力可以在空间和时间上下文中发挥作用。空间上下文主要用于提取最相关特征的特征或加权。对于时间上下文，它通过调整滑动时间窗口中所有样本的权重来工作，因为不同时间的样本有不同的贡献。尽管是确定性和可微分的，但对于大输入，软机制具有较高的计算成本。图[7](#S3.F7
    "Figure 7 ‣ 3 Attention Mechanisms ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")展示了一个软注意力机制的直观示例。
- en: '![Refer to caption](img/394d9990a688a2c8d9b9f80ba53095c8.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/394d9990a688a2c8d9b9f80ba53095c8.png)'
- en: 'Figure 7: An intuitive example of Soft Attention. Visual QA architecture outputs
    an answer given an image and a textual question as input. It uses a soft attention
    mechanism that weighted visual features for the task for further processing. The
    premise is that the norm of the visual features correlates with their relevance.
    Besides, those feature vectors with high magnitudes correspond to image regions
    that contain relevant semantic content.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：一个直观的Soft Attention示例。视觉QA架构根据输入的图像和文本问题输出答案。它使用软注意力机制对任务的视觉特征进行加权，以便进一步处理。前提是视觉特征的范数与其相关性相关。此外，那些具有高幅度的特征向量对应于包含相关语义内容的图像区域。
- en: Hard Attention. Hard attention determines whether a part of the mechanism’s
    input should be considered or not, reflecting the interdependence between the
    input of the mechanism and the target of the deep neural network. The weight assigned
    to an input part is either 0 or 1\. Hence, as input elements are either seen,
    the objective is non-differentiable. The process involves making a sequence of
    selections on which part to attend. In the temporal context, for example, the
    model attends to a part of the input to obtain information, decidinng where to
    attend in the next step based on the known information. A neural network can make
    a selection based on this information. However, as there is no ground truth to
    indicate the correct selection policy, the hard-attention type mechanisms are
    represented by stochastic processes. As the model is not differentiable, reinforcement
    learning techniques are necessary to train models with hard attention. Inference
    time and computational costs are reduced compared to soft mechanisms once the
    entire input is not being stored or processed. Figure [8](#S3.F8 "Figure 8 ‣ 3
    Attention Mechanisms ‣ Attention, please! A survey of Neural Attention Models
    in Deep Learning") shows an intuitive example of a hard attention mechanism.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 硬注意力。硬注意力确定机制的输入部分是否应该被考虑，反映了机制的输入与深度神经网络目标之间的相互依赖。分配给输入部分的权重要么是0，要么是1。因此，由于输入元素要么被看到，目标是不可微分的。该过程涉及选择关注的部分。例如，在时间上下文中，模型关注输入的某一部分以获取信息，并根据已知信息决定下一步关注的位置。神经网络可以根据这些信息做出选择。然而，由于没有明确的标准来指示正确的选择策略，硬注意力类型机制由随机过程表示。由于模型不可微分，需要使用强化学习技术来训练具有硬注意力的模型。一旦整个输入不被存储或处理，相比软机制，推理时间和计算成本会降低。图[8](#S3.F8
    "Figure 8 ‣ 3 Attention Mechanisms ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")展示了一个硬注意力机制的直观示例。
- en: '![Refer to caption](img/1d9cf79e6dddc51cb1ec6ba55ec31a69.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1d9cf79e6dddc51cb1ec6ba55ec31a69.png)'
- en: 'Figure 8: An intuitive example of Hard Attention. Given an image and a textual
    question as input, the Visual QA architecture outputs an answer. It uses a hard
    attention mechanism that selects only the important visual features for the task
    for further processing.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：一个直观的**Hard Attention**示例。给定一张图像和一个文本问题作为输入，Visual QA架构会输出一个答案。它使用一种硬注意力机制，只选择对任务重要的视觉特征进行进一步处理。
- en: Self-Attention. Self-attention quantifies the interdependence between the input
    elements of the mechanism. This mechanism allows the inputs to interact with each
    other "self" and determine what they should pay more attention to. The self-attention
    layer’s main advantages compared to soft and hard mechanisms are parallel computing
    ability for a long input. This mechanism layer checks the attention with all the
    same input elements using simple and easily parallelizable matrix calculations.
    Figure [9](#S3.F9 "Figure 9 ‣ 3 Attention Mechanisms ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning") shows an intuitive example of a
    self-attention mechanism.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力。自注意力量化机制的输入元素之间的相互依赖关系。该机制允许输入之间进行“自我”互动，并确定应该更加关注什么。与软机制和硬机制相比，自注意力层的主要优势在于对长输入的并行计算能力。该机制层使用简单且易于并行化的矩阵计算来检查所有相同输入元素的注意力。图 [9](#S3.F9
    "Figure 9 ‣ 3 Attention Mechanisms ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning") 展示了自注意力机制的直观示例。
- en: '![Refer to caption](img/9a706b342a86b8e26b5448286807187f.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/9a706b342a86b8e26b5448286807187f.png)'
- en: 'Figure 9: Self-Attention examples. a) Self-attention in sentences b) Self-attention
    in images. The first image shows five representative query locations with color-coded
    dots with the corresponding color-coded arrows summarizing the most-attended regions.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：自注意力示例。a) 句子中的自注意力 b) 图像中的自注意力。第一张图展示了五个代表性的查询位置，带有颜色编码的点和对应的颜色编码箭头，总结了最受关注的区域。
- en: 4 Attention-based Classic Deep Learning Architectures
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基于注意力的经典深度学习架构
- en: This section introduces details about attentional interfaces in classic DL architectures.
    Specifically, we present the uses of attention in convolutional, recurrent networks
    and generative models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了经典深度学习架构中的注意力接口的详细信息。具体来说，我们展示了注意力在卷积、递归网络和生成模型中的应用。
- en: 4.1 Attention-based Convolutional Neural Networks (CNNs)
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于注意力的卷积神经网络（CNNs）
- en: 'Attention emerges in CNNs to filter information and allocate resources to the
    neural network efficiently. There are numerous ways to use attention on CNNs,
    which makes it very difficult to summarize how this occurs and the impacts of
    each use. We divided the uses of attention into six distinct groups (Figure [10](#S4.F10
    "Figure 10 ‣ 4.1 Attention-based Convolutional Neural Networks (CNNs) ‣ 4 Attention-based
    Classic Deep Learning Architectures ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")): 1) DCN attention pool – attention replaces the classic
    CNN pooling mechanism; 2) DCN attention input – the attentional modules are filter
    masks for the input data. This mask assigns low weights to regions irrelevant
    to neural network processing and high weights to relevant areas; 3) DCN attention
    layer – attention is between the convolutional layers; 4) DCN attention prediction
    – attentional mechanisms assist the model directly in the prediction process;
    5) DCN residual attention – extracts information from the feature maps and presents
    a residual input connection to the next layer; 6) DCN attention out – attention
    captures important stimuli of feature maps for other architectures, or other instances
    of the same architecture. To maintain consistency with the Deep Neural Network’s
    area, we extend The Neural Network Zoo schematics ¹¹1https://www.asimovinstitute.org/neural-network-zoo/
    to accommodate attention elements.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中，注意力机制用于过滤信息并高效分配资源到神经网络中。使用注意力机制的方法多种多样，这使得总结其发生方式和每种使用的影响变得非常困难。我们将注意力机制的应用分为六个不同的组（图
    [10](#S4.F10 "图 10 ‣ 4.1 基于注意力的卷积神经网络 (CNNs) ‣ 4 基于注意力的经典深度学习架构 ‣ 注意力，请！深度学习中的神经注意力模型调研")）：1)
    DCN注意力池 – 注意力机制替代了经典的CNN池化机制；2) DCN注意力输入 – 注意力模块作为输入数据的过滤掩模。该掩模对与神经网络处理无关的区域分配低权重，对相关区域分配高权重；3)
    DCN注意力层 – 注意力机制位于卷积层之间；4) DCN注意力预测 – 注意力机制直接协助模型进行预测过程；5) DCN残差注意力 – 从特征图中提取信息，并呈现给下一层的残差输入连接；6)
    DCN注意力输出 – 注意力机制捕捉特征图的重要刺激，供其他架构或同一架构的其他实例使用。为了保持与深度神经网络领域的一致性，我们扩展了神经网络动物园示意图¹¹1https://www.asimovinstitute.org/neural-network-zoo/以适应注意力元素。
- en: '![Refer to caption](img/a5ba90de2a125a34761113a3b6405fc2.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a5ba90de2a125a34761113a3b6405fc2.png)'
- en: 'Figure 10: Attention-based convolutional neural networks. DCN attention pool
    group uses an attention pool, instead of regular pooling, as a strategy to determine
    the importance of each individual in a given feature map window. The premise is
    that only a few of these windows are significant and must be selected concerning
    a particular objective. DCN attention input group uses structures similar to the
    human’s visual attention. DCN attention layer group collects important stimuli
    of high (semantic level) and low level (saliency) for subsequent layers of architecture.
    DCN attention prediction group uses attention in the final stages of prediction,
    sometimes as an ensemble element. DCN residual attention group uses attention
    as a residual module between any convolutional layers to mitigate the vanishing
    problem, capturing only the relevant stimuli from each feature map. DCN attention
    out-group can represent the category of recurrent attention processes.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 基于注意力的卷积神经网络。DCN注意力池组使用注意力池，而不是常规池化，作为确定给定特征图窗口中每个个体重要性的策略。前提是这些窗口中只有少数几个是重要的，必须根据特定目标进行选择。DCN注意力输入组使用类似于人类视觉注意力的结构。DCN注意力层组收集高（语义级别）和低级别（显著性）的重要刺激，以供架构的后续层使用。DCN注意力预测组在预测的最后阶段使用注意力，有时作为集成元素。DCN残差注意力组将注意力作为任何卷积层之间的残差模块使用，以减轻消失问题，仅捕捉每个特征图中的相关刺激。DCN注意力输出组可以代表递归注意力过程的类别。'
- en: 'DCN attention input mainly uses attention to filter input data - a structure
    similar to the multi-glimpse mechanism and visual attention of human beings. Multi-glimpse
    refers to the ability to quickly scan the entire image and find the main areas
    relevant to the recognition process, while visual attention focuses on a critical
    area by extracting key features to understand the scene. When a person focuses
    on one part of the image, the different regions’ internal relationship is captured,
    guiding eye movement to find the next relevant area—ignoring the irrelevant parts
    easy learning in the presence of disorder. For this reason, human vision has an
    incomparable performance in object recognition. The main contribution of attention
    at the CNNs’ input is robustness. If our eyes see an object in a real-world scene,
    parts far from the object are ignored. Therefore, the distant background of the
    fixed object does not interfere in recognition. However, CNNs treat all parts
    of the image equally. The irrelevant regions confuse the classification and make
    it sensitive to visual disturbances, including background, changes in camera views,
    and lighting conditions. Attention in CNNs’ input contributes to increasing robustness
    in several ways: 1) It makes architectures more scalable, in which the number
    of parameters does not vary linearly with the size of the input image; 2) Eliminates
    distractors; 3) Minimizes the effects of changing camera lighting, scale, and
    views. 4) It allows the extension of models for more complex tasks, i.e., fine-grained
    classification or segmentation. 5) Simplifies CNN encoding. 6) Facilitates learning
    by including relevant priorities for architecture.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DCN注意力输入主要使用注意力来过滤输入数据 - 这种结构类似于多视觉窥探机制和人类的视觉注意力。多视觉窥探是指快速扫描整个图像并找到与识别过程相关的主要区域的能力，而视觉注意力则通过提取关键特征来聚焦于关键区域以理解场景。当一个人关注图像的某一部分时，捕捉到了不同区域的内在关系，引导眼睛移动以找到下一个相关区域
    - 忽略不相关的部分，容易在混乱环境中学习。因此，人类视觉在物体识别方面具有无与伦比的性能。CNN的输入在注意力方面的主要贡献是鲁棒性。如果我们的眼睛在现实场景中看到一个物体，那么远离物体的部分就会被忽略。因此，固定物体的远处背景不会干扰识别。然而，CNN对图像的所有部分都一视同仁。不相关的区域会混淆分类，并使其对视觉干扰变得敏感，包括背景、摄像机视角变化和光照条件。CNN输入中的注意力在多个方面增加了鲁棒性：1)
    使体系结构更具可扩展性，其中参数的数量与输入图像的大小不成线性变化；2) 消除干扰物；3) 最小化改变摄像机光照、比例和视图的影响。4) 它允许扩展模型以执行更复杂的任务，例如细粒度分类或分割。5)
    简化了CNN编码。6) 通过为体系结构包括相关的优先级，有助于学习。
- en: 'Zhao et al. [[129](#bib.bib129)] used visual attention-based image processing
    to generate the focused image. Then, the focused image is input into CNN to be
    classified. According to the classification, the information entropy guides reinforcement
    learning agents to achieve a better image classification policy. Wang et al. [[130](#bib.bib130)]
    used attention to create representations rich in motion information for action
    recognition. The attention extracts saliency maps using both motion and appearance
    information to calculate the objectness scores. For a video, attention process
    frame by frame to generate a saliency-aware map for each frame. The classic pipeline
    uses only CNN sequence features as input for LSTMs, failing to capture adjacent
    frames’ motion information. The saliency-aware maps capture only regions with
    relevant movements making CNN encoding simple and representative for the task.
    Liu et al. [[131](#bib.bib131)] used attention as input of a CNN to provide important
    priors in counting crowded tasks. An attention map generator first provides two
    types of priors for the system: candidate crowd regions and crowd regions’ congestion
    degree. The priors guide subsequent CNNs to pay more attention to those regions
    with crowds and improving their capacity to be resistant to noise. Specifically,
    the congestion degree prior provides fine-grained density estimation for a system.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 赵等人[[129](#bib.bib129)] 使用基于视觉注意力的图像处理生成焦点图像。然后，将焦点图像输入CNN进行分类。根据分类结果，信息熵指导强化学习代理达到更好的图像分类策略。王等人[[130](#bib.bib130)]
    利用注意力创建富含运动信息的表示用于动作识别。注意力通过运动和外观信息提取显著性图来计算对象分数。对于视频，注意力逐帧处理生成每一帧的显著性感知图。经典的流程仅使用CNN序列特征作为LSTM的输入，未能捕捉相邻帧的运动信息。显著性感知图仅捕捉相关运动区域，使CNN编码对任务变得简单且具有代表性。刘等人[[131](#bib.bib131)]
    使用注意力作为CNN的输入，以提供在拥挤任务中重要的先验信息。注意力图生成器首先为系统提供两种类型的先验：候选拥挤区域和拥挤区域的拥堵程度。先验指导后续CNN更关注拥挤区域，提高其抗噪能力。具体来说，拥堵程度先验为系统提供细粒度的密度估计。
- en: In classic CNNs, the size of the receptive fields is relatively small. Most
    of them extract features locally with convolutional operations, which fail to
    capture long-range dependencies between pixels throughout the image. However,
    larger receptive fields allow for better use of training inputs, and much more
    context information is available at the expense of instability or even convergence
    in training. Also, traditional CNNs treat channel features equally. This naive
    treatment lacks the flexibility to deal with low and high-frequency information.
    Some frequencies may contain more relevant information for a task than others,
    but equal treatment by the network makes it difficult to converge the models.
    To mitigate such problems, most literature approaches use attention between convolutional
    layers (i.e., DCN attention layer and DCN residual attention), as shown in figure [11](#S4.F11
    "Figure 11 ‣ 4.1 Attention-based Convolutional Neural Networks (CNNs) ‣ 4 Attention-based
    Classic Deep Learning Architectures ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning"). Between layers, attention acts mainly for feature recalibration,
    capturing long-term dependencies, internalizing, and correctly using past experiences.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典CNN中，感受野的大小相对较小。它们大多通过卷积操作局部提取特征，这无法捕捉图像中像素之间的长期依赖关系。然而，更大的感受野允许更好地利用训练输入，能够获取更多的上下文信息，但这也可能导致训练的不稳定甚至无法收敛。此外，传统CNN对通道特征的处理是平等的。这种简单处理缺乏处理低频和高频信息的灵活性。一些频率可能对任务更相关，但网络的平等处理使得模型难以收敛。为了缓解这些问题，大多数文献方法在卷积层之间使用注意力（即DCN注意力层和DCN残差注意力），如图[11](#S4.F11
    "Figure 11 ‣ 4.1 Attention-based Convolutional Neural Networks (CNNs) ‣ 4 Attention-based
    Classic Deep Learning Architectures ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")所示。在层之间，注意力主要用于特征重校准，捕捉长期依赖关系，内化并正确使用过去的经验。
- en: The pioneering approach to adopting attention between convolutional layers is
    the Squeeze-and-Excitation Networks [[132](#bib.bib132)] created in 2016 and winner
    of the ILSVRC in 2017\. It is also the first architecture to model channel interdependencies
    to recalibrate filter responses in two steps, squeeze and excitation, i.e., SE
    blocks. To explore local dependencies, the squeeze module encodes spatial information
    into a channel descriptor. The output is a collection of local descriptors with
    expressive characteristics for the entire image. To make use of the information
    aggregated by the squeeze operation, excitation captures channel-wise dependencies
    by learning a non-linear and non-mutually exclusive relationship between channels,
    ensuring that multiple channels can be emphasized. In this sense, SE blocks intrinsically
    introduce attentional dynamics to boost feature discrimination between convolutional
    layers.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 采用卷积层间注意力的开创性方法是2016年创建的**Squeeze-and-Excitation Networks** [[132](#bib.bib132)]，并在2017年赢得了ILSVRC。这也是首个通过两个步骤——压缩和激励，即SE块，来建模通道间依赖关系以重新校准滤波器响应的架构。为了探索局部依赖性，压缩模块将空间信息编码为通道描述符。输出是整个图像的具有表达特征的局部描述符集合。为了利用压缩操作聚合的信息，激励通过学习通道之间的非线性且非互斥关系来捕捉通道级依赖性，确保多个通道可以被强调。从这个意义上讲，SE块本质上引入了注意力动态，以提升卷积层之间的特征区分度。
- en: '![Refer to caption](img/ef7fd56020fc0f6e213fbc74693a4153.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ef7fd56020fc0f6e213fbc74693a4153.png)'
- en: 'Figure 11: Attention-based convolutional neural networks main architectures
    by task and attention use.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：基于注意力的卷积神经网络主架构按任务和注意力使用分类。
- en: The inter-channel and intra-channel attention to capturing long-term dependencies
    and simultaneously taking advantage of high and low-level stimuli are widely explored
    in the literature. Zhang. et al. [[133](#bib.bib133)] proposed residual local
    and non-local attention blocks consisting of trunk and mask branches. Their attention
    mechanism helps to learn local and non-local information from the hierarchical
    features, further preserving low-level features while maintaining a representational
    quality of high-level features. The Cbam [[134](#bib.bib134)] infers attentional
    maps in two separate dimensions, channel and spatial, for adaptive feature refinement.
    The double attention block in [[135](#bib.bib135)] aggregates and propagates global
    informational features considering the entire spatio-temporal context of images
    and videos, allowing subsequent convolution layers to access resources from across
    space efficiently. In the first stage, attention gathers features from all space
    into a compact set employing groupings. In the second stage, it selects and adaptively
    distributes the resources for each architectural location. Following similar exploration
    proposals, several attentional modules can be easily plugged into classic CNNs [[136](#bib.bib136)] [[137](#bib.bib137)] [[138](#bib.bib138)] [[139](#bib.bib139)].
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中广泛探讨了捕捉长期依赖的通道间和通道内注意力，同时利用高低级刺激。Zhang等人[[133](#bib.bib133)]提出了由主干和掩码分支组成的残差局部和非局部注意力块。他们的注意力机制有助于从层次特征中学习局部和非局部信息，进一步保持低级特征的同时维护高级特征的表示质量。Cbam[[134](#bib.bib134)]在通道和空间两个独立维度上推断注意力图，用于自适应特征细化。[[135](#bib.bib135)]中的双重注意力块聚合并传播全球信息特征，考虑图像和视频的整个时空背景，允许后续卷积层高效地访问跨空间的资源。在第一阶段，注意力通过分组将所有空间的特征汇集到一个紧凑的集合中。在第二阶段，它选择并自适应地分配每个架构位置的资源。遵循类似的探索提案，几个注意力模块可以轻松地插入经典CNN中[[136](#bib.bib136)]
    [[137](#bib.bib137)] [[138](#bib.bib138)] [[139](#bib.bib139)]。
- en: 'Hackel et al. [[140](#bib.bib140)] explored attention to preserving sparsity
    in convolutional operations. Convolutions with kernels greater than $1\times 1$
    generate fill-in, reducing feature maps’ sparse nature. Generally, the change
    in data sparsity has little influence on the network output, but memory consumption
    and execution time considerably increase when it occurs in many layers. To guarantee
    low memory consumption, attention acts as a $k-selection$ filter, which has two
    different versions of selection: 1) it acts on the output of the convolution,
    preferring the largest $k$ positive responses similar to a rectified linear unit;
    2) it chooses the $k$ highest absolute values, expressing a preference for responses
    of great magnitude. The parameter $k$ controls the level of sparse data and, consequently,
    computational resources during training and inference. Results point out that
    training with attentional control of data sparsity can reduce in more than $200\%$
    the forward pass runtime in one layer.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Hackel 等人 [[140](#bib.bib140)] 探讨了在卷积操作中保持稀疏性的注意力机制。大于 $1\times 1$ 的卷积核会产生填充，从而降低特征图的稀疏性。通常，数据稀疏性的变化对网络输出影响不大，但当这种变化发生在多个层中时，内存消耗和执行时间会显著增加。为了保证低内存消耗，注意力机制充当
    $k$-选择过滤器，有两种不同的选择方式：1) 它作用于卷积的输出，倾向于选择最大的 $k$ 个正响应，类似于修正线性单元；2) 它选择 $k$ 个绝对值最大的响应，表达对大幅度响应的偏好。参数
    $k$ 控制稀疏数据的级别，因此在训练和推理过程中影响计算资源。结果显示，使用注意力控制数据稀疏性的训练可以将单层的前向传播运行时间减少超过 $200\%$。
- en: To previous aggregate information and dynamically point to past experiences,
    SNAIL [[141](#bib.bib141)] - a pioneering class of meta-learner based attention
    architectures - has proposed combining temporal convolutions with soft attention.
    This approach demonstrates that attention acts as a complement to the disadvantages
    of convolution. Attention allows precise access in an infinitely large context,
    while convolutions provide high-bandwidth access at the expense of a finite context.
    By merging convolutional layers with attentional layers, SNAIL can have unrestricted
    access to the number of previous experiences effectively, as well as the model
    can learn a more efficient representation of features. As additional benefits,
    SNAIL architectures become simpler to train than classic RNNs.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结先前的聚合信息并动态地指向过去的经验，SNAIL [[141](#bib.bib141)]——一种基于注意力的元学习器的开创性类别——提出了将时间卷积与软注意力相结合。这种方法表明，注意力机制可以补充卷积的缺点。注意力机制允许在无限大的上下文中进行精确访问，而卷积则以有限上下文为代价提供高带宽访问。通过将卷积层与注意力层合并，SNAIL
    可以有效地访问前面的经验，并且模型可以学习到更高效的特征表示。作为额外的好处，SNAIL 架构比经典的 RNN 更容易训练。
- en: The DCN attention out-group uses attention to share relevant feature maps with
    other architectures or even with instances of the current architecture. Usually,
    the main objective is to facilitate the fusion of features, multimodality, and
    external knowledge. In some cases, attention regularly works by turning classic
    CNNs into recurrent convolutional neural networks - a new trend in Deep Learning
    to deal with challenging images’ problems. RA-CNN [[142](#bib.bib142)] is a pioneering
    framework for recurrent convolutional networks. In their framework, attention
    proceeds along two dimensions, i.e., discriminative feature learning and sophisticated
    part localization. Given an input image, a classic CNN extracts feature maps,
    and the attention proposal network maps convolutional features to a feature vector
    that could be matched with the category entries. Then, attention estimates the
    focus region for the next CNN instance, i.e., the next finer scale. Once the focus
    region is located, the system cuts and enlarges the region to a finer scale with
    higher resolution to extract more refined features. Thus, each CNN in the stack
    generates a prediction so that the stack’s deepest layers generate more accurate
    predictions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: DCN 注意力组使用注意力机制与其他架构或当前架构的实例共享相关特征图。通常，主要目标是促进特征融合、多模态以及外部知识。在某些情况下，注意力机制通过将经典
    CNN 转变为递归卷积神经网络——一种应对具有挑战性的图像问题的新趋势。RA-CNN [[142](#bib.bib142)] 是递归卷积网络的开创性框架。在他们的框架中，注意力机制沿两个维度进行，即判别特征学习和复杂的部分定位。给定输入图像，经典
    CNN 提取特征图，注意力提议网络将卷积特征映射到一个特征向量，这个特征向量可以与类别条目匹配。然后，注意力机制估计下一个 CNN 实例的关注区域，即下一个更精细的尺度。一旦确定了关注区域，系统会将该区域切割并放大到更高分辨率的精细尺度，以提取更精细的特征。因此，堆叠中的每个
    CNN 都生成一个预测，以使堆叠的最深层生成更准确的预测。
- en: For merging features, Cheng. et. al. [[143](#bib.bib143)] presented Feature-fusion
    Encoder-Decoder Network (FED-net) to image segmentation. Their model uses attention
    to fuse features of different levels of an encoder. At each encoder level, the
    attention module merges features from its current level with features from later
    levels. After the merger, the decoder performs convolutional upsampling with the
    information from each attention level, which contributes by modulating the most
    relevant stimuli for segmentation. Tian et al. [[144](#bib.bib144)] used feature
    pyramid-based attention to combine meaningful semantic features with semantically
    weak but visually strong features in a face detection task. Their goal is to learn
    more discriminative hierarchical features with enriched semantics and details
    at all levels to detect hard-to-detect faces, like tiny or partially occluded
    faces. Their attention mechanism can fuse different feature maps from top to bottom
    recursively by combining transposed convolutions and element-wise multiplication
    maximizing mutual information between the lower and upper-level representations.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了融合特征，Cheng 等人 [[143](#bib.bib143)] 提出了用于图像分割的特征融合编码器-解码器网络（FED-net）。他们的模型使用注意力机制来融合编码器中不同级别的特征。在每个编码器级别，注意力模块将当前级别的特征与后续级别的特征进行合并。合并后，解码器使用来自每个注意力级别的信息执行卷积上采样，这有助于调节最相关的刺激以进行分割。Tian
    等人 [[144](#bib.bib144)] 在人脸检测任务中使用基于特征金字塔的注意力机制，将有意义的语义特征与语义较弱但视觉上强的特征进行结合。他们的目标是在所有级别上学习更多具有丰富语义和细节的判别层次特征，以检测难以检测的人脸，如微小或部分遮挡的人脸。他们的注意力机制通过结合转置卷积和逐元素相乘递归地融合从上到下的不同特征图，从而最大化低层次和高层次表示之间的互信息。
- en: Delta [[145](#bib.bib145)] framework presented an efficient strategy for transfer
    learning. Their attention system acts as a behavior regulator between the source
    model and the target model. The attention identifies the source model’s completely
    transferable channels, preserving their responses and identifying the non-transferable
    channels to dynamically modulate their signals, increasing the target model’s
    generalization capacity. Specifically, the attentional system characterizes the
    distance between the source/target model through the feature maps’ outputs and
    incorporates that distance to regularize the loss function. Optimization normally
    affects the weights of the neural network and assigns generalization capacity
    to the target model. Regularization modulated by attention on high and low semantic
    stimuli manages to take important steps in the semantic problem to plug in external
    knowledge.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Delta [[145](#bib.bib145)] 框架提出了一种高效的迁移学习策略。他们的注意力系统充当源模型和目标模型之间的行为调节器。注意力系统识别源模型的完全可迁移通道，保留其响应，并识别不可迁移的通道以动态调节其信号，从而提高目标模型的泛化能力。具体而言，注意力系统通过特征图的输出表征源/目标模型之间的距离，并将该距离纳入损失函数的正则化中。优化通常影响神经网络的权重，并将泛化能力分配给目标模型。由注意力在高语义和低语义刺激上调节的正则化管理了语义问题中的重要步骤，以引入外部知识。
- en: The DCN attention prediction group uses attention directly in the prediction
    process. Various attentional systems capture features from different convolutional
    layers as input and generate a prediction as an output. Voting between different
    predictors generates the final prediction. Reusing activations of CNNs feature
    maps to find the most informative parts of the image at different depths makes
    prediction tasks more discriminative. Each attentional system learns to relate
    stimuli and part-based fine-grained features, which, although correlated, are
    not explored together in classical approaches. Zheng et al. [[146](#bib.bib146)]
    proposed a multi-attention mechanism to group channels, creating part classification
    sub-networks. The mechanism takes as input feature maps from convolutional layers
    and generates multiple single clusters spatially-correlated subtle patterns as
    a compact representation. The sub-network classifies an image by each individual
    part. The attention mechanism proposed in [[147](#bib.bib147)] uses a similar
    approach. However, instead of grouping features into clusters, the attentional
    system has the most relevant feature map regions selected by the attention heads.
    The output heads generate a hypothesis given the attended information, and the
    confidence gates generate a confidence score for each attention head.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: DCN 注意力预测组直接在预测过程中使用注意力。各种注意力系统从不同的卷积层捕获特征作为输入，并生成预测作为输出。不同预测器之间的投票生成最终预测。重新使用
    CNN 特征图的激活来在不同深度找到图像中最具信息量的部分，使预测任务更具判别力。每个注意力系统学习将刺激与基于部分的细粒度特征相关联，这些特征虽然相关，但在经典方法中未被一起探索。Zheng
    等人 [[146](#bib.bib146)] 提出了一个多注意力机制来对通道进行分组，创建部分分类子网络。该机制以卷积层的特征图作为输入，并生成多个空间上相关的细微模式作为紧凑表示。子网络通过每个单独的部分对图像进行分类。[[147](#bib.bib147)]
    提出的注意力机制使用了类似的方法。然而，注意力系统不是将特征分组为簇，而是通过注意力头选择最相关的特征图区域。输出头根据关注的信息生成一个假设，置信度门为每个注意力头生成置信度分数。
- en: Finally, the DCN attention pool group replaces classic pooling strategies with
    attention-based pooling. The objective is to create a non-linear encoding to select
    only stimuli relevant to the task, given that classical strategies select only
    the most contrasting stimuli. To modulate the resulting stimuli, attentional pooling
    layers generally capture different relationships between feature maps or between
    different layers. For example, Wang et al. [[148](#bib.bib148)] created an attentional
    mechanism that captures pertinent relationships between convoluted context windows
    and the relation class embedding through a correlation matrix learned during training.
    The correlation matrix modulates the convolved windows, and finally, the mechanism
    selects only the most salient stimuli. A similar approach is also followed in [[149](#bib.bib149)]
    for modeling sentence pairs.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，DCN注意力池组用基于注意力的池化替代经典的池化策略。其目标是创建一个非线性编码，以仅选择与任务相关的刺激，因为经典策略仅选择最对比的刺激。为了调节结果刺激，注意力池化层通常捕捉特征图之间或不同层之间的不同关系。例如，Wang等人[[148](#bib.bib148)]
    创建了一种注意力机制，通过训练过程中学习的相关矩阵捕捉卷积上下文窗口与关系类嵌入之间的相关关系。相关矩阵调节卷积窗口，最后机制选择最突出的刺激。类似的方法也在[[149](#bib.bib149)]中用于建模句子对。
- en: 4.2 Attention-based Recurrent Neural Networks (RNNs)
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于注意力的递归神经网络（RNNs）
- en: 'Attention in RNNs is mainly responsible for capturing long-distance dependencies.
    Currently, there are not many ways to use attention on RNNs. RNNSearch’s mechanism
    for encoder-decoder frameworks inspires most approaches [[44](#bib.bib44)]. We
    divided the uses of attention into three distinct groups (Figure [12](#S4.F12
    "Figure 12 ‣ 4.2 Attention-based Recurrent Neural Networks (RNNs) ‣ 4 Attention-based
    Classic Deep Learning Architectures ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")): 1) Recurrent attention input – the first stage of
    attention to select elementary input stimulus, i.e., elementary features, 2) recurrent
    memory attention – the first stage of attention to historical weight components,
    3) Recurrent hidden attention – the second stage of attention to select categorical
    information to the decode stage.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNN中，注意力主要负责捕捉长距离依赖关系。目前，用于RNN的注意力方法不多。RNNSearch的编码器-解码器框架机制激发了大多数方法[[44](#bib.bib44)]。我们将注意力的用途分为三个不同的组（图[12](#S4.F12
    "图 12 ‣ 4.2 基于注意力的递归神经网络（RNNs） ‣ 4 基于注意力的经典深度学习架构 ‣ 注意力，请！深度学习中神经注意力模型的调查")）：1)
    递归注意力输入——注意力选择基本输入刺激，即基本特征的第一阶段，2) 递归记忆注意力——注意力历史权重成分的第一阶段，3) 递归隐藏注意力——注意力选择解码阶段的分类信息的第二阶段。
- en: '![Refer to caption](img/a07dc2c9f018b878efa80b1427d51d9c.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a07dc2c9f018b878efa80b1427d51d9c.png)'
- en: 'Figure 12: Attention-based recurrent neural networks. The architecture is a
    classic recurrent network or a Bi-RNN when hidden layers are recurrent cells.
    When the hidden layer is a recurrent memory, the architecture is an LSTM or Bi-LSTM.
    Finally, the architecture is a GRU when the hidden layer is a gated memory cell.
    The recurrent attention input group uses attention to filter input data. Recurrent
    hidden attention groups automatically select relevant encoder hidden states across
    all time steps. Usually, this group implements attention in encoder-decoder frameworks.
    The recurrent memory attention group implements attention within the memory cell.
    There are not many architectures in this category as far as we know, but the main
    uses are related to filtering the input data and the weighting of different historical
    components for predicting the current time step.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 基于注意力的递归神经网络。该架构是经典的递归网络，或者当隐藏层是递归单元时是Bi-RNN。当隐藏层是递归记忆时，该架构是LSTM或Bi-LSTM。最后，当隐藏层是门控记忆单元时，该架构是GRU。递归注意力输入组使用注意力来过滤输入数据。递归隐藏注意力组自动选择所有时间步长中的相关编码器隐藏状态。通常，这个组在编码器-解码器框架中实现注意力。递归记忆注意力组在记忆单元内实现注意力。据我们所知，这一类别中的架构并不多，但主要用途与过滤输入数据和加权不同历史成分以预测当前时间步长有关。'
- en: The recurrent attention input group main uses are item-wise hard, local-wise
    hard, item-wise soft, and local-wise soft selection. Item-wise hard selects discretely
    relevant input data for further processing, whereas location-wise hard discretely
    focuses only on the most relevant features for the task. Item-wise soft assigns
    a continuous weight to each input data given a sequence of items as input, and
    location-wise soft assigns a continuous weight between input features. Location-wise
    soft estimates high weights for features more correlated with the global context
    of the task. Hard selection for input elements are applied more frequently in
    computer vision approaches [[46](#bib.bib46)] [[150](#bib.bib150)]. On the other
    hand, soft mechanisms are often applied in other fields, mainly in Natural Language
    Processing. The soft selection normally weighs relevant parts of the series or
    input features, and the attention layer is a feed-forward network differentiable
    and with a low computational cost. Soft approaches are interesting to filter noise
    from time series and to dynamically learn the correlation between input features
    and output [[151](#bib.bib151)] [[152](#bib.bib152)] [[153](#bib.bib153)]. Besides,
    this approach is useful for addressing graph-to-sequence learning problems that
    learn a mapping between graph-structured inputs to sequence outputs, which current
    Seq2Seq and Tree2Seq may be inadequate to handle [[154](#bib.bib154)].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 循环注意力输入组的主要用途包括项目级硬选择、本地级硬选择、项目级软选择和本地级软选择。项目级硬选择为进一步处理选择离散相关的输入数据，而本地级硬选择仅离散地关注任务中最相关的特征。项目级软选择为每个输入数据分配一个连续权重，给定一系列项目作为输入，而本地级软选择在输入特征之间分配一个连续权重。本地级软选择对与任务的全局上下文相关性较高的特征估计较高的权重。硬选择在计算机视觉方法中使用得更为频繁[[46](#bib.bib46)]
    [[150](#bib.bib150)]。另一方面，软机制通常应用于其他领域，主要是自然语言处理。软选择通常对系列或输入特征的相关部分进行加权，而注意力层是一个前馈网络，具有可微分性和低计算成本。软方法对过滤时间序列中的噪声以及动态学习输入特征与输出之间的相关性非常有趣[[151](#bib.bib151)]
    [[152](#bib.bib152)] [[153](#bib.bib153)]。此外，这种方法对于解决图到序列学习问题非常有用，该问题学习图结构输入与序列输出之间的映射，而当前的Seq2Seq和Tree2Seq可能不足以处理[[154](#bib.bib154)]。
- en: Hard mechanisms take inspiration from how humans perform visual sequence recognition
    tasks, such as reading by continually moving the fovea to the next relevant object
    or character, recognizing the individual entity, and adding the knowledge to our
    internal representation. A deep recurrent neural network, at each step, processes
    a multi-resolution crop of the input image, called a glimpse. The network uses
    information from the glimpse to update its internal representation and outputs
    the next glimpse location. The Glimpse network captures salient information about
    the input image at a specific position and region size. The internal state is
    formed by the hidden units $h_{t}$ of the recurrent neural network, which is updated
    over time by the core network. At each step, the location network estimates the
    next focus localization, and action networks depend on the task (e.g., for the
    classification task, the action network’s outputs are a prediction for the class
    label.). Hard attention is not entirely differentiable and therefore uses reinforcement
    learning.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 硬机制受到人类执行视觉序列识别任务的启发，例如通过不断移动中央凹点到下一个相关对象或字符来阅读，识别个体实体，并将知识添加到我们的内部表征中。深度循环神经网络在每一步处理输入图像的多分辨率裁剪，称为“瞥视”。网络使用来自“瞥视”的信息来更新其内部表征，并输出下一个“瞥视”位置。瞥视网络捕获关于输入图像特定位置和区域大小的显著信息。内部状态由循环神经网络的隐藏单元
    $h_{t}$ 形成，随着时间的推移由核心网络进行更新。在每一步，位置网络估计下一个焦点定位，动作网络依赖于任务（例如，对于分类任务，动作网络的输出是对类别标签的预测）。硬注意力并非完全可微分，因此使用强化学习。
- en: 'RAM [[46](#bib.bib46)] was the first architecture to use a recurrent network
    implementing hard selection for image classification tasks. While this model has
    learned successful strategies in various image data sets, it only uses several
    static glimpse sizes. CRAM [[150](#bib.bib150)] uses an additional sub-network
    to dynamically change the glimpse size, with the assumption to increase the performance,
    and in Jimmy et al. [[155](#bib.bib155)] explore modifications in RAM for real-world
    image tasks and multiple objects classification. CRAM is a similar RAM model except
    for two key differences: Firstly, a dynamically updated attention mechanism restrains
    the input region observed by the glimpse network and the next output region prediction
    from the emission network – a network that incorporates the location and capacity
    information as well as past information. In a more straightforward way, the sub-network
    decides at each time-step what the focus region’s capacity should be. Secondly,
    the capacity sub-network outputs are successively added to the emission network’s
    input that will ultimately generate the information for the next focus region—allowing
    the emission network to combine the information from the location and the capacity
    networks.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: RAM [[46](#bib.bib46)] 是首个使用递归网络实现硬选择进行图像分类任务的架构。虽然该模型在各种图像数据集上学习了成功的策略，但它仅使用了几种静态的视角大小。CRAM [[150](#bib.bib150)]
    使用了一个额外的子网络来动态调整视角大小，目的是提高性能，而Jimmy等人 [[155](#bib.bib155)] 探索了在实际图像任务和多物体分类中对RAM进行的修改。CRAM
    是一个类似于RAM的模型，但有两个主要区别：首先，动态更新的注意力机制限制了视角网络观察到的输入区域和发射网络预测的下一个输出区域——发射网络综合了位置和容量信息以及过去的信息。更直接地说，子网络在每个时间步决定焦点区域的容量应是多少。其次，容量子网络的输出会依次添加到发射网络的输入中，这将最终生成下一个焦点区域的信息——允许发射网络将来自位置和容量网络的信息结合起来。
- en: Nearly all important works in the field belong to the recurrent hidden attention
    group, as shown in Figure [13](#S4.F13 "Figure 13 ‣ 4.2 Attention-based Recurrent
    Neural Networks (RNNs) ‣ 4 Attention-based Classic Deep Learning Architectures
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning"). In
    this category, the attention mechanism selects elements that are in the RNN’s
    hidden layers for inter-alignment, contextual embedding, multiple-input processing,
    memory management, and capturing long-term dependencies, a typical problem with
    recurrent neural networks. Inter-alignment involves the encoder-decoder framework,
    and the attention module between these two networks is the most common approach.
    This mechanism builds a context vector dynamically from all previous decoder hidden
    states and the current encoder hidden state. Attention in inter-alignment helps
    minimize the bottleneck problem, with RNNSearch [[44](#bib.bib44)] for machine
    translation tasks as its first representative. Further, several other architectures
    implemented the same approach in other tasks [[156](#bib.bib156)] [[52](#bib.bib52)] [[51](#bib.bib51)].
    For example, Zichao Yang et al. [[52](#bib.bib52)] extended the soft selection
    to the hierarchical attention structure, which allows the calculation of soft
    attention at the word level and the sentence level in the GRU networks encoder
    for document classification.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有该领域的重要工作都属于递归隐藏注意力组，如图[13](#S4.F13 "Figure 13 ‣ 4.2 Attention-based Recurrent
    Neural Networks (RNNs) ‣ 4 Attention-based Classic Deep Learning Architectures
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")所示。在这一类别中，注意力机制选择RNN隐藏层中的元素进行对齐、上下文嵌入、多输入处理、记忆管理和捕捉长期依赖，这是递归神经网络的一个典型问题。对齐涉及编码器-解码器框架，这两个网络之间的注意力模块是最常见的方法。该机制动态构建一个上下文向量，基于所有先前的解码器隐藏状态和当前的编码器隐藏状态。在对齐中，注意力机制帮助最小化瓶颈问题，RNNSearch [[44](#bib.bib44)]
    是其在机器翻译任务中的第一个代表。此外，其他几个架构在其他任务中也实现了相同的方法 [[156](#bib.bib156)] [[52](#bib.bib52)] [[51](#bib.bib51)]。例如，Zichao
    Yang 等人 [[52](#bib.bib52)] 将软选择扩展到层次注意力结构，这允许在GRU网络编码器中在词级别和句子级别计算软注意力，用于文档分类。
- en: '![Refer to caption](img/b8405bed190fb524f4cabed0992081e7.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/b8405bed190fb524f4cabed0992081e7.png)'
- en: 'Figure 13: Attention-based recurrent neural networks main architectures by
    task and attention use.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：基于注意力的递归神经网络主要架构按任务和注意力使用情况分类。
- en: To create contextual embeddings and to manipulate multimodal inputs, co-attention
    is highly effective for text matching applications. Co-attention enables the learning
    of pairwise attention, i.e., learning to attend based on computing word-level
    affinity scores between two documents. Such a mechanism is designed for architectures
    comprised of queries and context, such as questions and answers and emotions analysis.
    Co-attention models can be fine-grained or coarse-grained. Fine-grained models
    consider each element of input concerning each element of the other input. Coarse-grained
    models calculate attention for each input, using an embedding of the other input
    as a query. Although efficient, co-attention suffers from information loss from
    the target and the context due to the anticipated summary. Attention flow emerges
    as an alternative to summary problems. Unlike co-attention, attention flow links
    and merges context and query information at each stage of time, allowing embeddings
    from previous layers to flow to subsequent modeling layers. The attention flow
    layer is not used to summarize the query and the context in vectors of unique
    features, reducing information loss. Attention is calculated in two directions,
    from the context to the query and from the query to the context. The output is
    the query-aware representations of context words. Attention flow allows a hierarchical
    process of multiple stages to represent the context at different granularity levels
    without an anticipated summary.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建上下文嵌入并处理多模态输入，协同注意力在文本匹配应用中非常有效。协同注意力使得可以学习成对的注意力，即基于计算两个文档之间的词级亲和度分数来进行关注。这种机制设计用于由查询和上下文组成的架构，例如问题和答案及情感分析。协同注意力模型可以是细粒度的，也可以是粗粒度的。细粒度模型会考虑输入的每个元素与另一个输入的每个元素之间的关系。粗粒度模型则为每个输入计算注意力，使用另一个输入的嵌入作为查询。尽管高效，协同注意力由于预期的总结会导致目标和上下文的信息丢失。注意力流作为解决总结问题的替代方案出现。与协同注意力不同，注意力流在每个时间阶段链接和合并上下文与查询信息，允许来自前一层的嵌入流向后续的建模层。注意力流层并不用于将查询和上下文总结成唯一特征的向量，从而减少信息丢失。注意力在两个方向上计算：从上下文到查询，以及从查询到上下文。输出的是查询感知的上下文词表示。注意力流允许一个多阶段的层级过程以不同的粒度水平表示上下文，而无需预期的总结。
- en: Hard attention mechanisms do not often occur on recurrent hidden attention networks.
    However, Nan Rosemary et al. [[157](#bib.bib157)] demonstrate that hard selection
    to retrieve past hidden states based on the current state mimics an effect similar
    to the brain’s ability. Humans use a very sparse subset of past experiences and
    can access them directly and establish relevance with the present, unlike classic
    RNNs and self-attentive networks. Hard attention is an efficient mechanism for
    RNNs to recover sparse memories. It determines which memories will be selected
    on the forward pass, and therefore which will receive gradient updates. At time
    $t$, RNN receives a vector of hidden states $h^{t-1}$, a vector of cell states
    $c^{t-1}$, and an input $x^{t}$, and computes new cell states $c^{t}$ and a provisional
    hidden state vector $\widetilde{h}^{t}$ that also serves as a provisional output.
    First, the provisional hidden state vector $\widetilde{h}^{t}$ is concatenated
    to each memory vector $m_{i}$ in the memory $M$. MLP maps each vector to an attention
    weight $a^{t}_{i}$, representing memory relevance $i$ in current moment $t$. With
    attention weights $a^{t}_{i}$ sparse attention computes a hard decision. The attention
    mechanism is differentiable but implements a hard selection to forget memories
    with no prominence over others. This is quite different from typical approaches
    as the mechanism does not allow the gradient to flow directly to a previous step
    in the training process. Instead, it propagates to some local timesteps as a type
    of local credit given to a memory.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 硬注意力机制通常不会出现在递归隐藏注意力网络中。然而，Nan Rosemary等人[[157](#bib.bib157)]证明了基于当前状态检索过去隐藏状态的硬选择模拟了类似于大脑能力的效果。与经典的RNN和自注意力网络不同，人类使用非常稀疏的过去经验子集，能够直接访问这些经验并与当前情况建立关联。硬注意力是RNN恢复稀疏记忆的高效机制。它在前向传播中决定哪些记忆会被选择，从而决定哪些记忆会接收梯度更新。在时间$t$，RNN接收一个隐藏状态向量$h^{t-1}$、一个细胞状态向量$c^{t-1}$和一个输入$x^{t}$，并计算新的细胞状态$c^{t}$和一个临时隐藏状态向量$\widetilde{h}^{t}$，后者也作为临时输出。首先，将临时隐藏状态向量$\widetilde{h}^{t}$与记忆$M$中的每个记忆向量$m_{i}$进行连接。MLP将每个向量映射到一个注意力权重$a^{t}_{i}$，表示记忆$i$在当前时刻$t$的相关性。利用注意力权重$a^{t}_{i}$，稀疏注意力计算出一个硬决策。注意力机制是可微分的，但实施了一个硬选择，以忘记那些在其他记忆中不显著的记忆。这与典型的方法大相径庭，因为该机制不允许梯度直接流向训练过程中的前一步。而是将其传播到某些局部时间步，作为对记忆的局部信用。
- en: Finally, recurrent memory attention groups implement attention within the memory
    cell. As far as our research goes, there are not many architectures in this category.
    Pengfei et al. [[69](#bib.bib69)] proposed an approach that modulates the input
    adaptively within the memory cell by assigning different levels of importance
    to each element/dimension of the input shown in Figure [14](#S4.F14 "Figure 14
    ‣ 4.2 Attention-based Recurrent Neural Networks (RNNs) ‣ 4 Attention-based Classic
    Deep Learning Architectures ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")a. Dilruk et al. [[158](#bib.bib158)] proposed mechanisms
    of attention within memory cell to improve the past encoding history in the cell’s
    state vector since all parts of the data history are not equally relevant to the
    current prediction. As shown in Figure [14](#S4.F14 "Figure 14 ‣ 4.2 Attention-based
    Recurrent Neural Networks (RNNs) ‣ 4 Attention-based Classic Deep Learning Architectures
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")b,
    the mechanism uses additional gates to update LSTM’s current cell.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，递归记忆注意力组在记忆单元内实现了注意力。根据我们的研究，这一类别的架构不多。Pengfei等人[[69](#bib.bib69)]提出了一种方法，通过为每个输入元素/维度分配不同的重视程度，在记忆单元内自适应地调节输入，如图[14](#S4.F14
    "Figure 14 ‣ 4.2 Attention-based Recurrent Neural Networks (RNNs) ‣ 4 Attention-based
    Classic Deep Learning Architectures ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")a所示。Dilruk等人[[158](#bib.bib158)]提出了在记忆单元内的注意力机制，以改善细胞状态向量中的过去编码历史，因为数据历史的所有部分对当前预测的相关性并不相等。如图[14](#S4.F14
    "Figure 14 ‣ 4.2 Attention-based Recurrent Neural Networks (RNNs) ‣ 4 Attention-based
    Classic Deep Learning Architectures ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")b所示，该机制使用额外的门来更新LSTM的当前细胞。
- en: '![Refer to caption](img/d1ac967d064461b56e74d8557fb5383e.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d1ac967d064461b56e74d8557fb5383e.png)'
- en: 'Figure 14: recurrent memory attention approaches. a) Illustration of Element-wise-Attention
    Gate in GRU. Specifically, the input modulation is adaptable to the content and
    is performed in fine granularity, element-wise rather than input-wise. b) Gate
    attention in LSTM. The model calculates attention scores to weigh the relevance
    of different parts of history. There are two additional gates for updating the
    current cell using the previous state $h_{A}^{t-1}$. The first, input attention
    gate layer, analyzes the current input $i^{\triangle t}$ and $h_{A}^{t-1}$ to
    determine which values to be updated in current cell $C^{t}$. The second, the
    modulation attention gate, analyzes the current input $i^{\triangle t}$ and $h_{A}^{t-1}$.
    Then computes the set of candidate values that must be added when updating the
    current cell state $C^{t}$. The attention mechanisms in the memory cell help to
    more easily capture long-term dependencies and the problem of data scarcity.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：递归记忆注意力方法。a) GRU中逐元素注意力门的示意图。具体来说，输入调制适应内容，并以细粒度、逐元素而非输入为单位执行。b) LSTM中的门控注意力。该模型计算注意力分数，以加权不同历史部分的相关性。还有两个额外的门，用于使用之前的状态$h_{A}^{t-1}$更新当前单元。第一个，输入注意力门层，分析当前输入$i^{\triangle
    t}$和$h_{A}^{t-1}$，以确定在当前单元$C^{t}$中更新哪些值。第二个，调制注意力门，分析当前输入$i^{\triangle t}$和$h_{A}^{t-1}$，然后计算在更新当前单元状态$C^{t}$时必须添加的一组候选值。记忆单元中的注意力机制有助于更容易地捕捉长期依赖关系和数据稀缺问题。
- en: 4.3 Attention-based Generative Models
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基于注意力的生成模型
- en: 'Attention emerges in generative models essentially to augmented memory. Currently,
    there are not many ways to use attention on generative models. Since GANs are
    not a neural network architecture but a framework, we do not discuss the use of
    attention in GANs but autoencoders. We divided the uses of attention into three
    distinct groups (Figure [15](#S4.F15 "Figure 15 ‣ 4.3 Attention-based Generative
    Models ‣ 4 Attention-based Classic Deep Learning Architectures ‣ Attention, please!
    A survey of Neural Attention Models in Deep Learning")): 1) Autoencoder input
    attention – attention provides spatial masks corresponding to all the parts for
    a given input, while a component autoencoder (e.g., AE, VAE, SAE) independently
    models each of the parts indicated by the masks. 2) Autoencoder memory attention
    – attention module acts as a layer between the encoder-decoder to augmented memory.,
    3) Autoencoder attention encoder-decoder – a fully attentive architecture acts
    on the encoder, decoder, or both.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力在生成模型中的出现本质上是为了增强记忆。目前，关于在生成模型中使用注意力的方法不多。由于GANs不是神经网络架构，而是一个框架，因此我们不讨论GANs中的注意力使用，而是自编码器。我们将注意力的使用分为三组（见图[15](#S4.F15
    "Figure 15 ‣ 4.3 Attention-based Generative Models ‣ 4 Attention-based Classic
    Deep Learning Architectures ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")）：1) 自编码器输入注意力——注意力为给定输入的所有部分提供空间掩码，而组件自编码器（如AE、VAE、SAE）独立建模掩码指示的每个部分。2)
    自编码器记忆注意力——注意力模块作为编码器-解码器之间的一个层，以增强记忆。3) 自编码器注意力编码器-解码器——完全注意力架构作用于编码器、解码器或两者。
- en: '![Refer to caption](img/6107590c04baa9edd76c3e4bbb2cd674.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6107590c04baa9edd76c3e4bbb2cd674.png)'
- en: 'Figure 15: Attention-based generative models. The Autoencoder input attention
    group uses attention to facilitate the decomposition of scenes in abstract building
    blocks. The input components extracted by the masks share significant properties
    and help to imagine new scenarios. This approach is very efficient for the network
    to learn to decompose challenging scenarios between semantically significant components.
    Autoencoder memory attention group handles a memory buffer that allows read/writes
    operations and is persistent over time. Such models generally handle input and
    output to the memory buffer using write/read operations guided by the attention
    system. The use of attention and memory history in autoencoders helps to increase
    the generalizability of architecture. Autoencoder attention encoder-decoder using
    a model (usually self-attentive model) to increase the ability to generalize.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：基于注意力的生成模型。自编码器输入注意力组使用注意力来促进场景在抽象构建块中的分解。掩码提取的输入组件具有显著的属性，并帮助想象新的场景。这种方法对于网络学习在语义上重要的组件之间分解具有挑战性的场景非常高效。自编码器记忆注意力组处理一个允许读/写操作并且随着时间保持持久的记忆缓冲区。这些模型通常通过受注意力系统指导的写/读操作来处理输入和输出到记忆缓冲区。在自编码器中使用注意力和记忆历史有助于提高架构的泛化能力。自编码器注意力编码器-解码器使用模型（通常是自注意力模型）来增强泛化能力。
- en: MONet [[159](#bib.bib159)] is one of the few architectures to implement attention
    at the VAE input. A VAE is a neural network with an encoder parameterized by $\phi$
    and a decoder parameterized by $\theta$. The encoder parameterizes a distribution
    over the component latent $z_{k}$, conditioned on both the input data x and an
    attention mask $m_{k}$. The mask indicates which regions of the input the VAE
    should focus on representing via its latent posterior distribution, $q\phi(z_{k}|x,m_{k})$.
    During training, the VAE’s decoder likelihood term in the loss $p_{\theta}(x|z_{k})$
    is weighted according to the mask, such that it is unconstrained outside of the
    masked regions. In [[160](#bib.bib160)], the authors use soft attention with learned
    memory contents to augment models to have more parameters in the autoencoder.
    In [[161](#bib.bib161)], Generative Matching Networks use attention to access
    the exemplar memory, with the address weights computed based on a learned similarity
    function between an observation at the address and a function of the latent state
    of the generative model. In [[162](#bib.bib162)], external memory and attention
    work as a way of implementing one-shot generalization by treating the exemplars
    conditioned on as memory entries accessed through a soft attention mechanism at
    each step of the incremental generative process similar to DRAW [[36](#bib.bib36)].
    Although most approaches use soft attention to address the memory, in [[163](#bib.bib163)]
    the authors use a stochastic, hard attention approach, which allows using variational
    inference about it in a context of few-shot learning.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: MONet [[159](#bib.bib159)] 是少数在 VAE 输入处实现注意力机制的架构之一。VAE 是一个神经网络，其中编码器由$\phi$参数化，解码器由$\theta$参数化。编码器对组件潜变量
    $z_{k}$ 进行参数化，该参数化依赖于输入数据 x 和注意力掩码 $m_{k}$。该掩码指示了 VAE 应该专注于通过其潜在后验分布 $q\phi(z_{k}|x,m_{k})$
    表示输入的哪些区域。在训练过程中，VAE 的解码器似然项在损失 $p_{\theta}(x|z_{k})$ 中根据掩码加权，使得在掩码区域外没有约束。在 [[160](#bib.bib160)]
    中，作者使用带有学习记忆内容的软注意力来增强模型，使其在自编码器中具有更多的参数。在 [[161](#bib.bib161)] 中，生成匹配网络使用注意力访问示例记忆，地址权重基于观察在地址上的学习相似性函数与生成模型潜在状态的函数进行计算。在
    [[162](#bib.bib162)] 中，外部记忆和注意力机制作为一种实现一次性泛化的方法，通过在增量生成过程中每一步通过软注意力机制访问条件示例作为记忆条目，类似于
    DRAW [[36](#bib.bib36)]。尽管大多数方法使用软注意力来处理记忆，但在 [[163](#bib.bib163)] 中，作者使用了一种随机硬注意力方法，这使得在少样本学习的背景下可以对其进行变分推断。
- en: In [[164](#bib.bib164)], self-attentive networks increase the autoencoder ability
    to generalize. The advantage of using this model instead of other alternatives,
    such as recurrent or convolutional encoders, is that this model is based only
    on self-attention and traditional attention over the whole representation created
    by the encoder. This approach allows us to easily employ the different components
    of the networks (encoder and decoder) as modules that, during inference, can be
    used with other parts of the network without the need for previous step information.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[164](#bib.bib164)] 中，自注意力网络提高了自编码器的泛化能力。使用这种模型而不是其他替代方案，如递归编码器或卷积编码器的优势在于该模型仅基于自注意力和传统注意力来处理编码器创建的整体表示。这种方法使我们能够轻松地将网络的不同组件（编码器和解码器）作为模块，在推理过程中可以与网络的其他部分一起使用，而无需先前步骤的信息。
- en: 5 Applications
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个应用
- en: In a few years, neural attention networks have been used in numerous domains
    due to versatility, interpretability, and significance of results. These networks
    have been explored mainly in computer vision, natural language processing, and
    multi-modal tasks, as shown in figure [16](#S5.F16 "Figure 16 ‣ 5 Applications
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning"). In
    some applications, these models transformed the area entirely (i.e., question-answering,
    machine translation, document representations/embeddings, graph embeddings), mainly
    due to significant performance impacts on the task in question. In others, they
    helped learn better representations and deal with temporal dependencies over long
    distances. This section explores a list of application domains and subareas, mainly
    discussing each domain’s main models and how it benefits from attention. We also
    present the most representative instances within each area and list them with
    reference approaches in a wide range of applications.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 几年来，由于其多功能性、可解释性和结果的重要性，神经注意力网络在多个领域得到了应用。这些网络主要在计算机视觉、自然语言处理和多模态任务中得到了探索，如图[16](#S5.F16
    "图 16 ‣ 5 应用 ‣ 注意力，请！深度学习中神经注意力模型的调研")所示。在一些应用中，这些模型完全改变了领域（即问答系统、机器翻译、文档表示/嵌入、图嵌入），主要是由于对相关任务的显著性能影响。在其他应用中，它们帮助学习更好的表示，并处理长距离的时间依赖性。本节探讨了应用领域和子领域的列表，主要讨论每个领域的主要模型及其如何受益于注意力。我们还展示了每个领域内最具代表性的实例，并列出了一系列应用中的参考方法。
- en: '![Refer to caption](img/fc7f7d487fb4bc02d3567dbbc473cf9a.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fc7f7d487fb4bc02d3567dbbc473cf9a.png)'
- en: 'Figure 16: Diagram showing the main existing applications of neural attention
    networks. The main areas are Natural language processing (NLP), Computer Vision
    (CV), multimodal tasks (mainly with images and texts - CV/NLP), reinforcement
    learning (RL), robotics, recommendation systems, and others (e.i., graph embeddings,
    interpretability.).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：展示神经注意力网络主要现有应用的示意图。主要领域包括自然语言处理 (NLP)、计算机视觉 (CV)、多模态任务（主要是图像和文本 - CV/NLP）、强化学习
    (RL)、机器人技术、推荐系统及其他（如图嵌入、可解释性）。
- en: 5.1 Natural Language Processing (NLP)
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 自然语言处理 (NLP)
- en: In the NLP domain, attention plays a vital role in many sub-areas, as shown
    in figure [16](#S5.F16 "Figure 16 ‣ 5 Applications ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning"). There are several state-of-the-art
    approaches, mainly in language modeling, machine translation, natural language
    inference, question answering, sentiment analysis, semantic analysis, speech recognition,
    and text summarization. Table [1](#S5.T1 "Table 1 ‣ 5.1 Natural Language Processing
    (NLP) ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention Models
    in Deep Learning") groups works developed in each of these areas. Several applications
    have been facing an increasing expansion, with few representative works, such
    as emotion recognition, speech classification, sequence prediction, semantic matching,
    and grammatical correction, as shown in Table [2](#S5.T2 "Table 2 ‣ 5.1 Natural
    Language Processing (NLP) ‣ 5 Applications ‣ Attention, please! A survey of Neural
    Attention Models in Deep Learning").
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理领域，注意力在许多子领域中发挥着至关重要的作用，如图[16](#S5.F16 "图 16 ‣ 5 应用 ‣ 注意力，请！深度学习中神经注意力模型的调研")所示。有几种最先进的方法，主要涉及语言建模、机器翻译、自然语言推理、问答系统、情感分析、语义分析、语音识别和文本摘要。表[1](#S5.T1
    "表 1 ‣ 5.1 自然语言处理 (NLP) ‣ 5 应用 ‣ 注意力，请！深度学习中神经注意力模型的调研")总结了这些领域中开发的工作。几个应用正在面临不断扩展，少数有代表性的工作包括情感识别、语音分类、序列预测、语义匹配和语法修正，如表[2](#S5.T2
    "表 2 ‣ 5.1 自然语言处理 (NLP) ‣ 5 应用 ‣ 注意力，请！深度学习中神经注意力模型的调研")所示。
- en: '| Task | References |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 参考文献 |'
- en: '| --- | --- |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Language Modeling |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模 |'
- en: '&#124; [[165](#bib.bib165)] [[166](#bib.bib166)] [[157](#bib.bib157)] [[156](#bib.bib156)] [[167](#bib.bib167)] [[168](#bib.bib168)] [[169](#bib.bib169)] [[170](#bib.bib170)] [[171](#bib.bib171)]
    &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[165](#bib.bib165)] [[166](#bib.bib166)] [[157](#bib.bib157)] [[156](#bib.bib156)]
    [[167](#bib.bib167)] [[168](#bib.bib168)] [[169](#bib.bib169)] [[170](#bib.bib170)]
    [[171](#bib.bib171)] &#124;'
- en: '|'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Machine Translation |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 机器翻译 |'
- en: '&#124; [[172](#bib.bib172)] [[37](#bib.bib37)] [[122](#bib.bib122)] [[173](#bib.bib173)] [[165](#bib.bib165)] [[44](#bib.bib44)] [[58](#bib.bib58)] [[174](#bib.bib174)] [[175](#bib.bib175)]
    &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[172](#bib.bib172)] [[37](#bib.bib37)] [[122](#bib.bib122)] [[173](#bib.bib173)]
    [[165](#bib.bib165)] [[44](#bib.bib44)] [[58](#bib.bib58)] [[174](#bib.bib174)]
    [[175](#bib.bib175)] &#124;'
- en: '&#124; [[176](#bib.bib176)] [[177](#bib.bib177)] [[178](#bib.bib178)] [[179](#bib.bib179)] [[180](#bib.bib180)] [[181](#bib.bib181)] [[182](#bib.bib182)] [[183](#bib.bib183)] [[184](#bib.bib184)]
    &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[176](#bib.bib176)] [[177](#bib.bib177)] [[178](#bib.bib178)] [[179](#bib.bib179)] [[180](#bib.bib180)] [[181](#bib.bib181)] [[182](#bib.bib182)] [[183](#bib.bib183)] [[184](#bib.bib184)]
    &#124;'
- en: '&#124; [[185](#bib.bib185)] [[186](#bib.bib186)] [[187](#bib.bib187)] [[188](#bib.bib188)] [[189](#bib.bib189)] [[190](#bib.bib190)] [[191](#bib.bib191)]
    &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[185](#bib.bib185)] [[186](#bib.bib186)] [[187](#bib.bib187)] [[188](#bib.bib188)] [[189](#bib.bib189)] [[190](#bib.bib190)] [[191](#bib.bib191)]
    &#124;'
- en: '|'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Natural Language Inference |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言推断 |'
- en: '&#124; [[172](#bib.bib172)] [[192](#bib.bib192)] [[193](#bib.bib193)] [[156](#bib.bib156)] [[194](#bib.bib194)] [[103](#bib.bib103)] [[195](#bib.bib195)] [[196](#bib.bib196)]
    &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[172](#bib.bib172)] [[192](#bib.bib192)] [[193](#bib.bib193)] [[156](#bib.bib156)] [[194](#bib.bib194)] [[103](#bib.bib103)] [[195](#bib.bib195)] [[196](#bib.bib196)]
    &#124;'
- en: '|'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Question Answering |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 |'
- en: '&#124; [[172](#bib.bib172)] [[197](#bib.bib197)] [[198](#bib.bib198)] [[165](#bib.bib165)] [[38](#bib.bib38)] [[93](#bib.bib93)] [[50](#bib.bib50)] [[199](#bib.bib199)]
    &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[172](#bib.bib172)] [[197](#bib.bib197)] [[198](#bib.bib198)] [[165](#bib.bib165)] [[38](#bib.bib38)] [[93](#bib.bib93)] [[50](#bib.bib50)] [[199](#bib.bib199)]
    &#124;'
- en: '&#124; [[200](#bib.bib200)] [[51](#bib.bib51)] [[201](#bib.bib201)] [[202](#bib.bib202)] [[203](#bib.bib203)] [[204](#bib.bib204)] [[205](#bib.bib205)] [[206](#bib.bib206)] [[207](#bib.bib207)]
    &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[200](#bib.bib200)] [[51](#bib.bib51)] [[201](#bib.bib201)] [[202](#bib.bib202)] [[203](#bib.bib203)] [[204](#bib.bib204)] [[205](#bib.bib205)] [[206](#bib.bib206)] [[207](#bib.bib207)]
    &#124;'
- en: '&#124; [[208](#bib.bib208)] [[209](#bib.bib209)] [[210](#bib.bib210)] [[211](#bib.bib211)] [[91](#bib.bib91)] [[212](#bib.bib212)] [[213](#bib.bib213)] [[214](#bib.bib214)] [[215](#bib.bib215)]
    &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[208](#bib.bib208)] [[209](#bib.bib209)] [[210](#bib.bib210)] [[211](#bib.bib211)] [[91](#bib.bib91)] [[212](#bib.bib212)] [[213](#bib.bib213)] [[214](#bib.bib214)] [[215](#bib.bib215)]
    &#124;'
- en: '&#124; [[216](#bib.bib216)] [[56](#bib.bib56)] [[217](#bib.bib217)] [[218](#bib.bib218)] [[219](#bib.bib219)]
    &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[216](#bib.bib216)] [[56](#bib.bib56)] [[217](#bib.bib217)] [[218](#bib.bib218)] [[219](#bib.bib219)]
    &#124;'
- en: '|'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Sentiment Analysis |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 |'
- en: '&#124; [[220](#bib.bib220)] [[193](#bib.bib193)] [[156](#bib.bib156)] [[221](#bib.bib221)] [[222](#bib.bib222)] [[223](#bib.bib223)] [[224](#bib.bib224)] [[225](#bib.bib225)] [[226](#bib.bib226)]
    &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[220](#bib.bib220)] [[193](#bib.bib193)] [[156](#bib.bib156)] [[221](#bib.bib221)] [[222](#bib.bib222)] [[223](#bib.bib223)] [[224](#bib.bib224)] [[225](#bib.bib225)] [[226](#bib.bib226)]
    &#124;'
- en: '&#124; [[227](#bib.bib227)] [[228](#bib.bib228)] [[229](#bib.bib229)] [[230](#bib.bib230)] [[231](#bib.bib231)] [[232](#bib.bib232)] [[233](#bib.bib233)] [[234](#bib.bib234)]
    &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[227](#bib.bib227)] [[228](#bib.bib228)] [[229](#bib.bib229)] [[230](#bib.bib230)] [[231](#bib.bib231)] [[232](#bib.bib232)] [[233](#bib.bib233)] [[234](#bib.bib234)]
    &#124;'
- en: '|'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Semantic Analysis |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 语义分析 |'
- en: '&#124; [[57](#bib.bib57)] [[235](#bib.bib235)] [[236](#bib.bib236)] [[237](#bib.bib237)] [[238](#bib.bib238)] [[239](#bib.bib239)] [[240](#bib.bib240)]
    &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[57](#bib.bib57)] [[235](#bib.bib235)] [[236](#bib.bib236)] [[237](#bib.bib237)] [[238](#bib.bib238)] [[239](#bib.bib239)] [[240](#bib.bib240)]
    &#124;'
- en: '|'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Speech Recognition |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 语音识别 |'
- en: '&#124; [[241](#bib.bib241)] [[175](#bib.bib175)] [[242](#bib.bib242)] [[243](#bib.bib243)] [[244](#bib.bib244)] [[245](#bib.bib245)] [[246](#bib.bib246)] [[247](#bib.bib247)] [[248](#bib.bib248)]
    &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[241](#bib.bib241)] [[175](#bib.bib175)] [[242](#bib.bib242)] [[243](#bib.bib243)] [[244](#bib.bib244)] [[245](#bib.bib245)] [[246](#bib.bib246)] [[247](#bib.bib247)] [[248](#bib.bib248)]
    &#124;'
- en: '&#124; [[249](#bib.bib249)] [[250](#bib.bib250)] [[251](#bib.bib251)] [[252](#bib.bib252)] [[253](#bib.bib253)] [[254](#bib.bib254)] [[255](#bib.bib255)] [[256](#bib.bib256)] [[257](#bib.bib257)]
    &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[249](#bib.bib249)] [[250](#bib.bib250)] [[251](#bib.bib251)] [[252](#bib.bib252)] [[253](#bib.bib253)] [[254](#bib.bib254)] [[255](#bib.bib255)] [[256](#bib.bib256)] [[257](#bib.bib257)]
    &#124;'
- en: '|'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Text Summarization |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 文本摘要 |'
- en: '&#124; [[258](#bib.bib258)] [[259](#bib.bib259)] [[260](#bib.bib260)] [[55](#bib.bib55)] [[175](#bib.bib175)] [[261](#bib.bib261)] [[262](#bib.bib262)] [[263](#bib.bib263)] [[264](#bib.bib264)]
    &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[258](#bib.bib258)] [[259](#bib.bib259)] [[260](#bib.bib260)] [[55](#bib.bib55)] [[175](#bib.bib175)] [[261](#bib.bib261)] [[262](#bib.bib262)] [[263](#bib.bib263)] [[264](#bib.bib264)]
    &#124;'
- en: '|'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 1: Summary state-of-the-art approaches in several natural language processing
    sub-areas.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：总结了多个自然语言处理子领域的最前沿方法。
- en: For machine translation (MT), question answering (QA), and automatic speech
    recognition (ASR), attention works mainly in the alignment input and output sequences
    capturing long-range dependencies. For example, in ASR tasks, attention aligns
    acoustic frames extracting information from anchor words to recognize the main
    speaker while ignoring background noise and interfering speech. Hence, only information
    on the desired speech is used for the decoder as it provides a straightforward
    way to align each output symbol with different input frames with selective noise
    decoding. In MT, automatic alignment translates long sentences more efficiently.
    It is a powerful tool for multilingual machine translation (NMT), efficiently
    capturing subjects, verbs, and nouns in sentences of different languages that
    differ significantly in their syntactic structure and semantics.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器翻译（MT）、问答（QA）和自动语音识别（ASR），注意力机制主要在对齐输入和输出序列中起作用，捕捉长期依赖关系。例如，在ASR任务中，注意力机制对齐声学帧，从锚词中提取信息，以识别主要发言者，同时忽略背景噪声和干扰语音。因此，解码器仅使用所需语音的信息，因为它提供了一种直接的方式将每个输出符号与具有选择性噪声解码的不同输入帧对齐。在MT中，自动对齐使得长句子的翻译更加高效。它是多语言机器翻译（NMT）的强大工具，能够高效捕捉不同语言中句子的主语、动词和名词，这些语言在句法结构和语义上差异很大。
- en: In QA, alignment usually occurs between a query and the content, looking for
    key terms to answer the question. The classic QA approaches do not support very
    long sequences and fail to correctly model the meaning of context-dependent words.
    Different words can have different meanings, which increases the difficulty of
    extracting the essential semantic logical flow of each sentence in different paragraphs
    of context. These models are unable to address uncertain situations that require
    additional information to answer a particular question. In contrast, attention
    networks allow rich dialogues through addressing mechanisms for explicit memories
    or alignment structures in the query-context and context-query directions.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在QA中，对齐通常发生在查询和内容之间，寻找关键术语以回答问题。经典的QA方法不支持非常长的序列，无法正确建模依赖上下文的词语的意义。不同的词可能有不同的意义，这增加了在不同段落的上下文中提取每句话的核心语义逻辑流的难度。这些模型无法处理需要额外信息来回答特定问题的不确定情况。相比之下，注意力网络通过在查询-上下文和上下文-查询方向上对显式记忆或对齐结构的机制，允许丰富的对话。
- en: 'Attention also contributes to summarize or classify texts/documents. It mainly
    helps build more effective embeddings that generally consider contextual, semantic,
    and hierarchical information between words, phrases, and paragraphs. Specifically,
    in summarization tasks, attention minimizes critical problems involving: 1) modeling
    of keywords; 2) summary of abstract sentences; 3) capture of the sentence’s hierarchical
    structure; 4) repetitions of inconsistent phrases; and 5) generation of short
    sentences preserving their meaning.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制也有助于总结或分类文本/文档。它主要帮助构建更有效的嵌入，通常考虑词汇、短语和段落之间的上下文、语义和层次信息。具体而言，在摘要任务中，注意力机制减少了以下关键问题：1）关键词建模；2）抽象句子的总结；3）句子的层次结构捕捉；4）不一致短语的重复；5）生成保留其意义的短句。
- en: '![Refer to caption](img/5feda58974fd04590605d70a82fe7441.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5feda58974fd04590605d70a82fe7441.png)'
- en: 'Figure 17: Illustration of RNNSearch [[44](#bib.bib44)] for machine translation,
    and End-to-End Memory Networks [[91](#bib.bib91)] for question answering. a) The
    RNNSearch architecture. The attention guided by the decoder’s previous state dynamically
    searches for important source words for the next time step. b) The End-to-End
    Memory Networks. The architecture consists of external memory and several stacked
    attention modules. To generate a response, the model makes several hops in memory
    using only attentional layers.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：RNNSearch [[44](#bib.bib44)] 机器翻译的示意图，以及 End-to-End Memory Networks [[91](#bib.bib91)]
    的问答示意图。a) RNNSearch 架构。由解码器的先前状态指导的注意力机制动态地搜索下一个时间步骤的重要源词。b) End-to-End Memory
    Networks。该架构由外部记忆和几个堆叠的注意力模块组成。为了生成响应，模型在记忆中进行多个跳跃，仅使用注意力层。
- en: '![Refer to caption](img/900a646ad01ef15cdd245da44bd1287a.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/900a646ad01ef15cdd245da44bd1287a.png)'
- en: 'Figure 18: Illustration of Neural Transformer [[37](#bib.bib37)] for machine
    translation. The architecture consists of several stacked attentional encoders
    and decoders. The encoder process is massively parallel and eliminates recurrences,
    and the decoder generates the translated words sequentially. Each encoder uses
    multiple heads of the self-attention mechanism followed by fusion and normalization
    layers. Similarly, to generate the output, each decoder has multiple heads of
    self-attention and masked-self attention to mask words not yet generated.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：神经转换器的示意图 [[37](#bib.bib37)] 用于机器翻译。该架构由多个堆叠的注意力编码器和解码器组成。编码器处理过程是大规模并行的，消除了递归，而解码器则按顺序生成翻译后的单词。每个编码器使用多个自注意力机制的头部，之后是融合和归一化层。类似地，为了生成输出，每个解码器都有多个自注意力头和掩码自注意力，用于掩盖尚未生成的单词。
- en: 'Figure [17](#S5.F17 "Figure 17 ‣ 5.1 Natural Language Processing (NLP) ‣ 5
    Applications ‣ Attention, please! A survey of Neural Attention Models in Deep
    Learning") illustrates two models working in NLP tasks: RNNSearch [[44](#bib.bib44)],
    in machine translation, and End-to-End Memory Networks [[91](#bib.bib91)] in question
    answering. In RNNSearch, the attention guided by the decoder’s previous state
    dynamically searches for important source words for the next time step. It consists
    of an encoder followed by a decoder. The encoder is a bidirectional RNN (BiRNN) [[265](#bib.bib265)]
    that consists of forward and backward RNN’s. The forward RNN reads the input sequence
    in order and calculates the forward hidden state sequence. The backward RNN reads
    the sequence in the reverse order, resulting in the backward hidden states sequence.
    The decoder has an RNN and an attention system that calculates a probability distribution
    for all possible output symbols from a context vector.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [17](#S5.F17 "图 17 ‣ 5.1 自然语言处理 (NLP) ‣ 5 应用 ‣ 注意力，请！深度学习中的神经注意力模型调查") 说明了两种在
    NLP 任务中工作的模型：RNNSearch [[44](#bib.bib44)]，用于机器翻译，以及端到端记忆网络 [[91](#bib.bib91)]，用于问答。在
    RNNSearch 中，由解码器的前一个状态指导的注意力机制动态地搜索下一个时间步的重要源词。它由一个编码器和一个解码器组成。编码器是一个双向 RNN (BiRNN) [[265](#bib.bib265)]，包括前向和后向
    RNN。前向 RNN 按顺序读取输入序列，并计算前向隐藏状态序列。后向 RNN 以相反的顺序读取序列，得到后向隐藏状态序列。解码器具有一个 RNN 和一个注意力系统，该系统计算从上下文向量中所有可能输出符号的概率分布。
- en: In End-to-End Memory Networks, attention looks for the memory elements most
    related to query $q$ using an alignment function that dispenses the RNNs’ complex
    structure. It consists of a memory and a stack of identical attentional systems.
    Each layer $i$ takes as input set $\left\{x_{1},...,x_{N}\right\}$ to store in
    the memory. The input set is converted in memory vectors $\left\{m_{1},...,m_{N}\right\}$
    and $\left\{h_{1},...,h_{N}\right\}$, in the simplest case using the embedding
    matrix $A^{i}\in\mathbb{R}^{d\times V}$ to generate each $m_{i}\in\mathbb{R}^{d}$,
    and the matrix $C^{i}\in\mathbb{R}^{d\times V}$ to generate each $h_{i}\in\mathbb{R}^{d}$.
    In the first, layer the query $q$ is also embedded, via embedding matrix $B^{1}$
    to obtain an internal state $u^{1}$. From the second layer, the internal state
    $u^{i+1}$ is the sum of the $i$ layer output and the internal state $u^{i}$. Finally,
    the last layer generates $\hat{a}$.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在端到端记忆网络中，注意力机制通过对齐函数寻找与查询 $q$ 最相关的记忆元素，从而简化了 RNN 的复杂结构。它由一个记忆体和一系列相同的注意力系统组成。每一层
    $i$ 以输入集 $\left\{x_{1},...,x_{N}\right\}$ 作为存储在记忆中的内容。输入集被转换为记忆向量 $\left\{m_{1},...,m_{N}\right\}$
    和 $\left\{h_{1},...,h_{N}\right\}$，在最简单的情况下，使用嵌入矩阵 $A^{i}\in\mathbb{R}^{d\times
    V}$ 生成每个 $m_{i}\in\mathbb{R}^{d}$，使用矩阵 $C^{i}\in\mathbb{R}^{d\times V}$ 生成每个 $h_{i}\in\mathbb{R}^{d}$。在第一层中，查询
    $q$ 也通过嵌入矩阵 $B^{1}$ 嵌入以获得内部状态 $u^{1}$。从第二层开始，内部状态 $u^{i+1}$ 是 $i$ 层输出和内部状态 $u^{i}$
    的总和。最后，最后一层生成 $\hat{a}$。
- en: The Neural Transformer [[37](#bib.bib37)], illustrated in figure [18](#S5.F18
    "Figure 18 ‣ 5.1 Natural Language Processing (NLP) ‣ 5 Applications ‣ Attention,
    please! A survey of Neural Attention Models in Deep Learning"), is the basis model
    for state-of-the-art results in NLP. The architecture consists of an arbitrary
    amount of stacked encoders and decoders. Each encoder has linear layers, an attention
    system, feed-forward neural networks, and normalization layers. The attention
    system has several parallel heads. Each head has $N$ attentional subsystems that
    perform the same task but have different contextual inputs. The encoder receives
    a word embedding matrix $I=\left\{x_{1},...,x_{N}\right\}$, $I$ $\in\mathbb{R}^{N\times
    d_{emb}}$ as input. As the architecture does not use recurrences, the input tokens’
    position information is not explicit, but it is necessary. To represent the spatial
    position information, the Transformer adds a positional encoding to each embedding
    vector. Positional encoding is fixed and uses sinusoidal functions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 神经变换器 [[37](#bib.bib37)]，如图 [18](#S5.F18 "图 18 ‣ 5.1 自然语言处理 (NLP) ‣ 5 应用 ‣ 注意力，请注意！深度学习中神经注意力模型的调查")
    所示，是 NLP 最先进结果的基础模型。该架构由任意数量的堆叠编码器和解码器组成。每个编码器具有线性层、一个注意力系统、前馈神经网络和归一化层。注意力系统有多个并行头。每个头有
    $N$ 个注意力子系统，它们执行相同的任务但具有不同的上下文输入。编码器接收一个词嵌入矩阵 $I=\left\{x_{1},...,x_{N}\right\}$，$I$
    $\in\mathbb{R}^{N\times d_{emb}}$ 作为输入。由于该架构不使用递归，因此输入标记的位置信息不是显式的，但它是必要的。为了表示空间位置信息，变换器为每个嵌入向量添加了位置编码。位置编码是固定的，并使用正弦函数。
- en: The input $I$ goes through linear layers and generates, for each word, a query
    vector ($q_{i}$), a key vector ($k_{i}$), and a value vector ($v_{i}$).The attentional
    system receives all $Q$, $K$, and $V$ arrays as input and uses several parallel
    attention heads. The motivation for using a multi-head structure is to explore
    multiple subspaces since each head gets a different projection of the data. Each
    head learns a different aspect of attention to the input, calculating different
    attentional distributions. Having multiple heads on the Transformer is similar
    to having multiple feature extraction filters on CNNs. The head outputs an attentional
    mask that relates all queries to a certain key. In a simplified way, the operation
    performed by a head is a matrix multiplication between a matrix of queries and
    keys.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 $I$ 经过线性层，为每个词生成一个查询向量 ($q_{i}$)、一个键向量 ($k_{i}$) 和一个值向量 ($v_{i}$)。注意力系统接收所有
    $Q$、$K$ 和 $V$ 数组作为输入，并使用多个并行注意力头。使用多头结构的动机是探索多个子空间，因为每个头获得数据的不同投影。每个头学习输入的不同注意力方面，计算不同的注意力分布。变换器上具有多个头类似于
    CNN 上的多个特征提取滤波器。头输出一个注意力掩码，将所有查询与某个键相关联。简化地说，头执行的操作是查询矩阵与键矩阵之间的矩阵乘法。
- en: Finally, the data is added to the residual output from the previous layer and
    normalized, representing the encoder output. This data is input to the next encoder.
    The last encoder’s data are transformed into the attention matrices $K_{encdec}$
    and $V_{encdec}$. They are input to all decoder layers. This data help the decoder
    to focus on the appropriate locations in the input sequence. The decoder has two
    layers of attention, Feed-Foward layers and normalization layers. The attentional
    layers are the masked multi-head attention and the decoder multi-head attention.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据被加到来自前一层的残差输出中并进行归一化，表示编码器输出。这些数据作为输入传递给下一个编码器。最后一个编码器的数据被转换为注意力矩阵 $K_{encdec}$
    和 $V_{encdec}$。它们作为输入传递给所有解码器层。这些数据帮助解码器专注于输入序列中的适当位置。解码器具有两个注意力层、前馈层和归一化层。注意力层包括掩码多头注意力和解码器多头注意力。
- en: The masked multi-head attention is very similar to the encoder multi-head attention,
    with the difference that the attention matrices $Q$, $K$, and $V$ are created
    only with the previous data words, masking future positions with $-\infty$ values
    before the softmax step. The decoder multi-head attention is equal to the encoder
    multi-head attention, except it creates the Q matrix from the data of the previous
    layer and uses the $K_{encdec}$ and $V_{encdec}$ matrices of the encoder output.
    The $K_{encdec}$ and $V_{encdec}$ matrices are the memory structure of the network,
    storing context information of the input sequence, and given the previous words
    in the output decoder, the relevant information is selected in memory for the
    prediction of the next word. Finally, a linear layer followed by a softmax function
    projects the decoder vector by the last decoder into a probability vector in which
    each position defines the probability of the output word being a given vocabulary
    word. At each time step $t$, the position with the highest probability value is
    chosen, and the word associated with it is the output.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 掩蔽的多头注意力与编码器的多头注意力非常相似，不同之处在于注意力矩阵 $Q$、$K$ 和 $V$ 仅使用先前的数据词创建，在 softmax 步骤之前用
    $-\infty$ 值掩蔽未来的位置。解码器多头注意力与编码器多头注意力相等，只不过它从上一层的数据创建 Q 矩阵，并使用编码器输出的 $K_{encdec}$
    和 $V_{encdec}$ 矩阵。$K_{encdec}$ 和 $V_{encdec}$ 矩阵是网络的记忆结构，存储输入序列的上下文信息，并且根据解码器中的先前词，选择记忆中的相关信息来预测下一个词。最后，一个线性层加上
    softmax 函数将最后的解码器向量投影成一个概率向量，其中每个位置定义了输出词是给定词汇中的一个词的概率。在每个时间步 $t$，选择概率值最高的位置，并且与之相关联的词即为输出。
- en: '| Task | References |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 参考文献 |'
- en: '| --- | --- |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Code Summarization | [[266](#bib.bib266)] |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 代码总结 | [[266](#bib.bib266)] |'
- en: '| Language Recognition | [[267](#bib.bib267)] |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 语言识别 | [[267](#bib.bib267)] |'
- en: '| Emotion Recognition |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 情感识别 |'
- en: '&#124; [[268](#bib.bib268)] [[269](#bib.bib269)] [[270](#bib.bib270)] [[271](#bib.bib271)] [[272](#bib.bib272)] [[273](#bib.bib273)] [[274](#bib.bib274)]
    &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[268](#bib.bib268)] [[269](#bib.bib269)] [[270](#bib.bib270)] [[271](#bib.bib271)] [[272](#bib.bib272)] [[273](#bib.bib273)] [[274](#bib.bib274)]
    &#124;'
- en: '|'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Text Classification | [[275](#bib.bib275)] [[103](#bib.bib103)] |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 文本分类 | [[275](#bib.bib275)] [[103](#bib.bib103)] |'
- en: '| Speech Classification | [[276](#bib.bib276)] [[277](#bib.bib277)] |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 语音分类 | [[276](#bib.bib276)] [[277](#bib.bib277)] |'
- en: '| Relation Classification |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 关系分类 |'
- en: '&#124; [[278](#bib.bib278)] [[279](#bib.bib279)] [[148](#bib.bib148)] [[280](#bib.bib280)] [[281](#bib.bib281)] [[282](#bib.bib282)]
    &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[278](#bib.bib278)] [[279](#bib.bib279)] [[148](#bib.bib148)] [[280](#bib.bib280)] [[281](#bib.bib281)] [[282](#bib.bib282)]
    &#124;'
- en: '|'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Intent Classification | [[283](#bib.bib283)] |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 意图分类 | [[283](#bib.bib283)] |'
- en: '| Document Classification | [[284](#bib.bib284)] [[52](#bib.bib52)] |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 文档分类 | [[284](#bib.bib284)] [[52](#bib.bib52)] |'
- en: '| Audio Classification | [[285](#bib.bib285)] |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 音频分类 | [[285](#bib.bib285)] |'
- en: '| Transfer Learning | [[286](#bib.bib286)] [[287](#bib.bib287)] |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 迁移学习 | [[286](#bib.bib286)] [[287](#bib.bib287)] |'
- en: '| Text-to-Speech | [[288](#bib.bib288)] [[289](#bib.bib289)] [[113](#bib.bib113)]
    |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 文本转语音 | [[288](#bib.bib288)] [[289](#bib.bib289)] [[113](#bib.bib113)] |'
- en: '| Syntax Analysis | [[290](#bib.bib290)] |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 语法分析 | [[290](#bib.bib290)] |'
- en: '| Speech Translation | [[291](#bib.bib291)] |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 语音翻译 | [[291](#bib.bib291)] |'
- en: '| Speech Transcription | [[48](#bib.bib48)] |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 语音转录 | [[48](#bib.bib48)] |'
- en: '| Speech Production | [[292](#bib.bib292)] |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 语音生成 | [[292](#bib.bib292)] |'
- en: '| Sequence Prediction | [[293](#bib.bib293)] |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 序列预测 | [[293](#bib.bib293)] |'
- en: '| Semantic Matching | [[294](#bib.bib294)] |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 语义匹配 | [[294](#bib.bib294)] |'
- en: '| Relation Extraction | [[295](#bib.bib295)] |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 关系抽取 | [[295](#bib.bib295)] |'
- en: '| Reading Comprehension | [[194](#bib.bib194)] [[202](#bib.bib202)] [[296](#bib.bib296)] [[297](#bib.bib297)] [[298](#bib.bib298)]
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 阅读理解 | [[194](#bib.bib194)] [[202](#bib.bib202)] [[296](#bib.bib296)] [[297](#bib.bib297)] [[298](#bib.bib298)]
    |'
- en: '| Natural Language Understanding | [[299](#bib.bib299)] |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言理解 | [[299](#bib.bib299)] |'
- en: '| Natural Language Transduction | [[300](#bib.bib300)] |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言转导 | [[300](#bib.bib300)] |'
- en: '| Natural Language Generation | [[154](#bib.bib154)] |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言生成 | [[154](#bib.bib154)] |'
- en: '| Machine Reading | [[301](#bib.bib301)] |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 机器阅读 | [[301](#bib.bib301)] |'
- en: '| Intent Detection | [[302](#bib.bib302)] |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 意图检测 | [[302](#bib.bib302)] |'
- en: '| Grammatical Correction | [[303](#bib.bib303)] |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 语法纠错 | [[303](#bib.bib303)] |'
- en: '| Entity Resolution | [[304](#bib.bib304)] [[305](#bib.bib305)] |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 实体解析 | [[304](#bib.bib304)] [[305](#bib.bib305)] |'
- en: '| Entity Description | [[138](#bib.bib138)] |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 实体描述 | [[138](#bib.bib138)] |'
- en: '| Embedding | [[306](#bib.bib306)] [[307](#bib.bib307)] [[308](#bib.bib308)]
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入 | [[306](#bib.bib306)] [[307](#bib.bib307)] [[308](#bib.bib308)] |'
- en: '| Dependency Parsing | [[309](#bib.bib309)] [[310](#bib.bib310)] |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 依存句法分析 | [[309](#bib.bib309)] [[310](#bib.bib310)] |'
- en: '| Conversation Model | [[311](#bib.bib311)] [[312](#bib.bib312)] |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 对话模型 | [[311](#bib.bib311)] [[312](#bib.bib312)] |'
- en: '| Automatic Question Tagging | [[313](#bib.bib313)] |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 自动问题标记 | [[313](#bib.bib313)] |'
- en: 'Table 2: Summary others applications of attention in natural language processing.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：总结了自然语言处理中的注意力其他应用。
- en: 5.2 Computer Vision (CV)
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 计算机视觉 (CV)
- en: Visual attention has become popular in many CV tasks. Action recognition, counting
    crowds, image classification, image generation, object detection, person recognition,
    segmentation, saliency detection, text recognition, and tracking targets are the
    most explored sub-areas, as shown in Table [3](#S5.T3 "Table 3 ‣ 5.2 Computer
    Vision (CV) ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning"). Applications in other sub-areas still have few representative
    works, such as clustering, compression, deblurring, depth estimation, image restoration,
    among others, as shown in Table [4](#S5.T4 "Table 4 ‣ 5.2 Computer Vision (CV)
    ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention Models in Deep
    Learning").
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉注意力在许多计算机视觉任务中变得流行。动作识别、人群计数、图像分类、图像生成、物体检测、人物识别、分割、显著性检测、文本识别和目标跟踪是最为深入探讨的子领域，如表[3](#S5.T3
    "Table 3 ‣ 5.2 Computer Vision (CV) ‣ 5 Applications ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning")所示。其他子领域的应用仍有少数代表性工作，如聚类、压缩、去模糊、深度估计、图像恢复等，如表[4](#S5.T4
    "Table 4 ‣ 5.2 Computer Vision (CV) ‣ 5 Applications ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning")所示。
- en: Visual attention in image classification tasks was first addressed by Graves
    et al. [[46](#bib.bib46)]. In this domain, there are sequential approaches inspired
    by human saccadic movements [[46](#bib.bib46)] and feedforward-augmented structures
    CNNs (Section [4](#S4 "4 Attention-based Classic Deep Learning Architectures ‣
    Attention, please! A survey of Neural Attention Models in Deep Learning")). The
    general goal is usually to amplify fine-grained recognition, improve classification
    in the presence of occlusions, sudden variations in points of view, lighting,
    and rotation. Some approaches aim to learn to look at the most relevant parts
    of the input image, while others try to discern between discriminating regions
    through feature recalibration and ensemble predictors via attention. To fine-grained
    recognition, important advances have been achieved through recurrent convolutional
    networks in the classification of bird subspecies [[142](#bib.bib142)] and architectures
    trained via RL to classify vehicle subtypes [[129](#bib.bib129)].
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类任务中的视觉注意力最早由Graves等人提出[[46](#bib.bib46)]。在这个领域，有一些受人类眼跳运动启发的顺序方法[[46](#bib.bib46)]和增强型前馈结构CNN（第[4](#S4
    "4 Attention-based Classic Deep Learning Architectures ‣ Attention, please! A
    survey of Neural Attention Models in Deep Learning")节）。一般目标通常是放大细粒度识别，改进在遮挡、视角突变、光照和旋转存在时的分类。一些方法旨在学习关注输入图像中最相关的部分，而其他方法则通过特征重新校准和通过注意力的集成预测器来区分辨别区域。对于细粒度识别，通过递归卷积网络在鸟类亚种分类[[142](#bib.bib142)]和通过强化学习训练的架构来分类车辆亚型[[129](#bib.bib129)]取得了重要进展。
- en: '| Task | References |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 参考文献 |'
- en: '| --- | --- |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Action Recognition |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 动作识别 |'
- en: '&#124; [[314](#bib.bib314)] [[315](#bib.bib315)] [[316](#bib.bib316)] [[317](#bib.bib317)] [[69](#bib.bib69)] [[135](#bib.bib135)] [[318](#bib.bib318)] [[319](#bib.bib319)] [[153](#bib.bib153)] [[320](#bib.bib320)]
    &#124;'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[314](#bib.bib314)] [[315](#bib.bib315)] [[316](#bib.bib316)] [[317](#bib.bib317)]
    [[69](#bib.bib69)] [[135](#bib.bib135)] [[318](#bib.bib318)] [[319](#bib.bib319)]
    [[153](#bib.bib153)] [[320](#bib.bib320)] &#124;'
- en: '&#124; [[321](#bib.bib321)] [[322](#bib.bib322)] [[323](#bib.bib323)] [[324](#bib.bib324)] [[325](#bib.bib325)] [[326](#bib.bib326)] [[130](#bib.bib130)] [[327](#bib.bib327)] [[328](#bib.bib328)]
    &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[321](#bib.bib321)] [[322](#bib.bib322)] [[323](#bib.bib323)] [[324](#bib.bib324)]
    [[325](#bib.bib325)] [[326](#bib.bib326)] [[130](#bib.bib130)] [[327](#bib.bib327)]
    [[328](#bib.bib328)] &#124;'
- en: '&#124; [[329](#bib.bib329)] [[330](#bib.bib330)] &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[329](#bib.bib329)] [[330](#bib.bib330)] &#124;'
- en: '|'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Counting Crowds |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 人群计数 |'
- en: '&#124; [[331](#bib.bib331)] [[332](#bib.bib332)] [[333](#bib.bib333)] [[334](#bib.bib334)] [[131](#bib.bib131)]
    &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[331](#bib.bib331)] [[332](#bib.bib332)] [[333](#bib.bib333)] [[334](#bib.bib334)]
    [[131](#bib.bib131)] &#124;'
- en: '|'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Image Classification |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 图像分类 |'
- en: '&#124; [[142](#bib.bib142)] [[335](#bib.bib335)] [[336](#bib.bib336)] [[337](#bib.bib337)] [[140](#bib.bib140)] [[338](#bib.bib338)] [[46](#bib.bib46)] [[135](#bib.bib135)] [[157](#bib.bib157)] [[147](#bib.bib147)]
    &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[142](#bib.bib142)] [[335](#bib.bib335)] [[336](#bib.bib336)] [[337](#bib.bib337)] [[140](#bib.bib140)] [[338](#bib.bib338)] [[46](#bib.bib46)] [[135](#bib.bib135)] [[157](#bib.bib157)] [[147](#bib.bib147)]
    &#124;'
- en: '&#124; [[339](#bib.bib339)] [[59](#bib.bib59)] [[340](#bib.bib340)] [[167](#bib.bib167)] [[132](#bib.bib132)] [[341](#bib.bib341)] [[342](#bib.bib342)] [[343](#bib.bib343)] [[344](#bib.bib344)] [[344](#bib.bib344)]
    &#124;'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[339](#bib.bib339)] [[59](#bib.bib59)] [[340](#bib.bib340)] [[167](#bib.bib167)] [[132](#bib.bib132)] [[341](#bib.bib341)] [[342](#bib.bib342)] [[343](#bib.bib343)] [[344](#bib.bib344)] [[344](#bib.bib344)]
    &#124;'
- en: '&#124; [[345](#bib.bib345)] [[346](#bib.bib346)] [[73](#bib.bib73)] [[347](#bib.bib347)] [[348](#bib.bib348)] [[349](#bib.bib349)] [[350](#bib.bib350)] [[134](#bib.bib134)] [[351](#bib.bib351)] [[352](#bib.bib352)]
    &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[345](#bib.bib345)] [[346](#bib.bib346)] [[73](#bib.bib73)] [[347](#bib.bib347)] [[348](#bib.bib348)] [[349](#bib.bib349)] [[350](#bib.bib350)] [[134](#bib.bib134)] [[351](#bib.bib351)] [[352](#bib.bib352)]
    &#124;'
- en: '&#124; [[71](#bib.bib71)] [[353](#bib.bib353)] [[354](#bib.bib354)] [[355](#bib.bib355)] [[356](#bib.bib356)] [[357](#bib.bib357)] [[72](#bib.bib72)] [[358](#bib.bib358)] [[359](#bib.bib359)]
    &#124;'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[71](#bib.bib71)] [[353](#bib.bib353)] [[354](#bib.bib354)] [[355](#bib.bib355)] [[356](#bib.bib356)] [[357](#bib.bib357)] [[72](#bib.bib72)] [[358](#bib.bib358)] [[359](#bib.bib359)]
    &#124;'
- en: '|'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Image Generation |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 图像生成 |'
- en: '&#124; [[36](#bib.bib36)] [[119](#bib.bib119)] [[360](#bib.bib360)] [[361](#bib.bib361)] [[163](#bib.bib163)] [[362](#bib.bib362)] [[118](#bib.bib118)] [[363](#bib.bib363)] [[105](#bib.bib105)] [[364](#bib.bib364)]
    &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[36](#bib.bib36)] [[119](#bib.bib119)] [[360](#bib.bib360)] [[361](#bib.bib361)] [[163](#bib.bib163)] [[362](#bib.bib362)] [[118](#bib.bib118)] [[363](#bib.bib363)] [[105](#bib.bib105)] [[364](#bib.bib364)]
    &#124;'
- en: '&#124; [[365](#bib.bib365)] &#124;'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[365](#bib.bib365)] &#124;'
- en: '|'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Object Recognition |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 对象识别 |'
- en: '&#124;  [[366](#bib.bib366)] [[146](#bib.bib146)] [[367](#bib.bib367)] [[368](#bib.bib368)] [[369](#bib.bib369)] [[370](#bib.bib370)] [[371](#bib.bib371)] [[372](#bib.bib372)] [[373](#bib.bib373)]
    &#124;'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[366](#bib.bib366)] [[146](#bib.bib146)] [[367](#bib.bib367)] [[368](#bib.bib368)] [[369](#bib.bib369)] [[370](#bib.bib370)] [[371](#bib.bib371)] [[372](#bib.bib372)] [[373](#bib.bib373)]
    &#124;'
- en: '|'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Object Detection |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 对象检测 |'
- en: '&#124;  [[374](#bib.bib374)] [[375](#bib.bib375)] [[376](#bib.bib376)] [[377](#bib.bib377)] [[378](#bib.bib378)] [[379](#bib.bib379)]
    &#124;'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[374](#bib.bib374)] [[375](#bib.bib375)] [[376](#bib.bib376)] [[377](#bib.bib377)] [[378](#bib.bib378)] [[379](#bib.bib379)]
    &#124;'
- en: '|'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Person Recognition |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 人物识别 |'
- en: '&#124; [[380](#bib.bib380)] [[381](#bib.bib381)] [[382](#bib.bib382)] [[383](#bib.bib383)] [[384](#bib.bib384)] [[385](#bib.bib385)] [[386](#bib.bib386)] [[387](#bib.bib387)] [[388](#bib.bib388)] [[389](#bib.bib389)]
    &#124;'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[380](#bib.bib380)] [[381](#bib.bib381)] [[382](#bib.bib382)] [[383](#bib.bib383)] [[384](#bib.bib384)] [[385](#bib.bib385)] [[386](#bib.bib386)] [[387](#bib.bib387)] [[388](#bib.bib388)] [[389](#bib.bib389)]
    &#124;'
- en: '&#124; [[390](#bib.bib390)] [[391](#bib.bib391)] [[392](#bib.bib392)] &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[390](#bib.bib390)] [[391](#bib.bib391)] [[392](#bib.bib392)] &#124;'
- en: '|'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Segmentation |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 分割 |'
- en: '&#124; [[393](#bib.bib393)] [[394](#bib.bib394)] [[395](#bib.bib395)] [[396](#bib.bib396)] [[397](#bib.bib397)] [[398](#bib.bib398)] [[399](#bib.bib399)] [[400](#bib.bib400)] [[401](#bib.bib401)] [[402](#bib.bib402)]
    &#124;'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[393](#bib.bib393)] [[394](#bib.bib394)] [[395](#bib.bib395)] [[396](#bib.bib396)] [[397](#bib.bib397)] [[398](#bib.bib398)] [[399](#bib.bib399)] [[400](#bib.bib400)] [[401](#bib.bib401)] [[402](#bib.bib402)]
    &#124;'
- en: '&#124; [[403](#bib.bib403)] [[404](#bib.bib404)] [[405](#bib.bib405)] [[406](#bib.bib406)] [[143](#bib.bib143)] [[407](#bib.bib407)] [[408](#bib.bib408)] [[409](#bib.bib409)] [[410](#bib.bib410)]
    &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[403](#bib.bib403)] [[404](#bib.bib404)] [[405](#bib.bib405)] [[406](#bib.bib406)] [[143](#bib.bib143)] [[407](#bib.bib407)] [[408](#bib.bib408)] [[409](#bib.bib409)] [[410](#bib.bib410)]
    &#124;'
- en: '|'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Saliency Detection | [[411](#bib.bib411)] [[412](#bib.bib412)] [[412](#bib.bib412)] [[413](#bib.bib413)] [[414](#bib.bib414)]
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 显著性检测 | [[411](#bib.bib411)] [[412](#bib.bib412)] [[412](#bib.bib412)] [[413](#bib.bib413)] [[414](#bib.bib414)]
    |'
- en: '| Text Recognition | [[415](#bib.bib415)] [[416](#bib.bib416)] [[417](#bib.bib417)] [[418](#bib.bib418)] [[419](#bib.bib419)] [[420](#bib.bib420)]
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 文本识别 | [[415](#bib.bib415)] [[416](#bib.bib416)] [[417](#bib.bib417)] [[418](#bib.bib418)] [[419](#bib.bib419)] [[420](#bib.bib420)]
    |'
- en: '| Tracking Targets |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 目标跟踪 |'
- en: '&#124; [[393](#bib.bib393)] [[394](#bib.bib394)] [[395](#bib.bib395)] [[396](#bib.bib396)] [[397](#bib.bib397)] [[398](#bib.bib398)] [[399](#bib.bib399)] [[400](#bib.bib400)] [[401](#bib.bib401)] [[402](#bib.bib402)]
    &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[393](#bib.bib393)] [[394](#bib.bib394)] [[395](#bib.bib395)] [[396](#bib.bib396)] [[397](#bib.bib397)] [[398](#bib.bib398)] [[399](#bib.bib399)] [[400](#bib.bib400)] [[401](#bib.bib401)] [[402](#bib.bib402)]
    &#124;'
- en: '&#124; [[403](#bib.bib403)] [[404](#bib.bib404)] [[405](#bib.bib405)] [[406](#bib.bib406)] [[143](#bib.bib143)]
    &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[403](#bib.bib403)] [[404](#bib.bib404)] [[405](#bib.bib405)] [[406](#bib.bib406)] [[143](#bib.bib143)]
    &#124;'
- en: '|'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 3: Summary state-of-art approaches in computer vision sub-areas.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：计算机视觉子领域的最新技术总结。
- en: 'Visual attention also provides significant benefits for action recognition
    tasks by capturing spatio-temporal relationships. The biggest challenge’s classical
    approaches are capturing discriminative features of movement in the sequences
    of images or videos. The attention allows the network to focus the processing
    only on the relevant joints or on the movement features easily. Generally, the
    main approaches use the following strategies: 1) saliency maps: spatiotemporal
    attention models learn where to look in video directly human fixation data. These
    models express the probability of saliency for each pixel. Deep 3D CNNs extract
    features only high saliency regions to represent spatial and short time relations
    at clip level, and LSTMs expand the temporal domain from few frames to seconds [[319](#bib.bib319)];
    2) self-attention: modeling context-dependencies. The person being classified
    is the Query (Q), and the clip around the person is the memory, represented by
    keys (K) and values (V) vectors. The network process the query and memory to generate
    an updated query vector. Intuitively self-attention adds context to other people
    and objects in the clip to assist in subsequent classification [[315](#bib.bib315)];
    3) recurrent attention mechanisms: captures relevant positions of joints or movement
    features and, through a recurring structure, refines the attentional focus at
    each time step [[67](#bib.bib67)] [[153](#bib.bib153)]; and 4) temporal attention:
    captures relevant spatial-temporal locations [[320](#bib.bib320)] [[320](#bib.bib320)] [[317](#bib.bib317)] [[421](#bib.bib421)] [[422](#bib.bib422)] [[423](#bib.bib423)].'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉注意力在动作识别任务中也提供了显著的好处，通过捕捉时空关系来实现。经典方法面临的最大挑战是捕捉图像或视频序列中的运动区分特征。注意力机制允许网络仅关注相关的关节或运动特征。通常，主要方法使用以下策略：1）显著性图：时空注意力模型直接从人类注视数据学习在视频中观察的位置。这些模型表达了每个像素的显著性概率。深度3D
    CNNs仅从高显著性区域提取特征，以在片段级别表示空间和短时间关系，LSTMs将时间域从几帧扩展到几秒钟[[319](#bib.bib319)]；2）自注意力：建模上下文依赖性。被分类的人是查询（Q），而围绕此人的片段是记忆，由键（K）和值（V）向量表示。网络处理查询和记忆以生成更新的查询向量。直观上，自注意力将上下文添加到片段中的其他人和物体，以辅助后续分类[[315](#bib.bib315)]；3）递归注意力机制：捕捉关节或运动特征的相关位置，并通过递归结构在每个时间步精炼注意力焦点[[67](#bib.bib67)]
    [[153](#bib.bib153)]；4）时间注意力：捕捉相关的时空位置[[320](#bib.bib320)] [[320](#bib.bib320)]
    [[317](#bib.bib317)] [[421](#bib.bib421)] [[422](#bib.bib422)] [[423](#bib.bib423)]。
- en: Liu et al. [[67](#bib.bib67)] model is a recurrent attention approach to capturing
    the person’s relevant positions. This model presented a pioneering approach using
    two layers of LSTMs and the context memory cell that recurrently interact with
    each other, as shown in figure [19](#S5.F19 "Figure 19 ‣ 5.2 Computer Vision (CV)
    ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention Models in Deep
    Learning")a. First, a layer of LSTMs generates an encoding of a skeleton sequence,
    initializing the context memory cell. The memory representation is input to the
    second layer of LSTMs and helps the network selectively focus on each frame’s
    informational articulations. Finally, attentional representation feeds back the
    context memory cell to refine the focus’s orientation again by paying attention
    more reliably. Similarly, Du et al. [[153](#bib.bib153)] proposed RPAN - a recurrent
    attention approach between sequentially modeling by LSTMs and convolutional features
    extractors. First, CNNs extract features from the current frame, and the attentional
    mechanism guided by the LSTM’s previous hidden state estimates a series of features
    related to human articulations related to the semantics of movements of interest.
    Then, these highly discriminative features feed LSTM time sequences.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人[[67](#bib.bib67)]的模型是一种递归注意力方法，用于捕捉人的相关位置。该模型提出了一种开创性的方法，使用两层LSTM和上下文记忆单元，这些单元相互递归交互，如图[19](#S5.F19
    "Figure 19 ‣ 5.2 Computer Vision (CV) ‣ 5 Applications ‣ Attention, please! A
    survey of Neural Attention Models in Deep Learning")a所示。首先，一层LSTM生成骨架序列的编码，初始化上下文记忆单元。记忆表示作为输入传递给第二层LSTM，并帮助网络选择性地关注每一帧的信息表达。最后，注意力表示将上下文记忆单元反馈回去，再次通过更可靠的注意力精炼焦点的方向。类似地，杜等人[[153](#bib.bib153)]提出了RPAN——一种递归注意力方法，结合了LSTM与卷积特征提取器进行序列建模。首先，CNN从当前帧中提取特征，而由LSTM先前隐藏状态指导的注意力机制估计一系列与运动语义相关的人体表达特征。然后，这些高度区分的特征被输入到LSTM时间序列中。
- en: '![Refer to caption](img/b74eec800b750624e915345f217f1f1c.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b74eec800b750624e915345f217f1f1c.png)'
- en: 'Figure 19: Illustration of Global Context-Aware Attention [[67](#bib.bib67)]
    for action recognition, and DRAW [[36](#bib.bib36)] for image generation. a) The
    Context-Aware Attention Network. The first LSTM layer encodes the skeleton sequence
    and generates an initial global context memory. The second layer performs attention
    over the inputs with global context memory assistance and generates a refined
    representation. The refine representation is then used back to the global context.
    Multiple attention iterations are carried out to refine the global context progressively.
    Finally, memory information is used for classification. b) The DRAW architecture.
    At each time step $t$, the input is read by attention and passed to the encoder
    RNN. The encoder’s output is used to compute the approximate posterior over the
    latent variables. On the right, an illustration shows the iterative process of
    generating some images. Each row shows successive stages in the generation of
    a single digit. The network guided by attention draws and refine regions successively.
    The red rectangle delimits the area attended to by the network at each time-step.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：全球上下文感知注意力[[67](#bib.bib67)]在动作识别中的应用，以及DRAW[[36](#bib.bib36)]在图像生成中的应用。a)
    上下文感知注意力网络。第一个LSTM层对骨架序列进行编码，并生成初始的全球上下文记忆。第二层在全球上下文记忆的帮助下对输入进行注意力操作，并生成精炼的表示。精炼的表示随后被反馈回全球上下文。多次注意力迭代被执行，以逐步精炼全球上下文。最后，记忆信息被用于分类。b)
    DRAW架构。在每个时间步$t$，输入通过注意力读取并传递给编码器RNN。编码器的输出用于计算潜在变量的近似后验。在右侧，插图展示了生成一些图像的迭代过程。每一行显示了单个数字生成的连续阶段。网络在注意力的指导下，逐步绘制和精炼区域。红色矩形标记了网络在每个时间步关注的区域。
- en: In image generation, there were also notable benefits. DRAW [[36](#bib.bib36)]
    introduced visual attention with an innovative approach - image patches are generated
    sequentially and gradually refined, in which to generate the entire image in a
    single pass (figure [19](#S5.F19 "Figure 19 ‣ 5.2 Computer Vision (CV) ‣ 5 Applications
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")b).
    Subsequently, attentional mechanisms emerged in generative adversarial networks
    (GANs) to minimize the challenges in modeling images with structural constraints.
    Naturally, GANs efficiently synthesize elements differentiated by texture (i.e.,
    oceans, sky, natural landscapes) but suffer to generate geometric patterns (i.e.,
    faces, animals, people, fine details). The central problem is the convolutions
    that fail to model dependencies between distant regions. Besides, the statistical
    and computational efficiency of the model suffers from the stacking of many layers.
    The attentional mechanisms, especially self-attention, offered a computationally
    inexpensive alternative to model long-range dependencies easily. Self-attention
    as a complement to convolution contributes significantly to the advancement of
    the area with approaches capable of generating fine details [[119](#bib.bib119)],
    high-resolution images, and with intricate geometric patterns [[120](#bib.bib120)].
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像生成中，也有显著的好处。DRAW [[36](#bib.bib36)] 引入了视觉注意力，采用创新的方法 - 图像块被顺序生成并逐步细化，从而在一次传递中生成整个图像（图[19](#S5.F19
    "Figure 19 ‣ 5.2 Computer Vision (CV) ‣ 5 Applications ‣ Attention, please! A
    survey of Neural Attention Models in Deep Learning")b）。随后，注意力机制出现在生成对抗网络（GANs）中，以最小化在建模具有结构约束的图像时的挑战。自然地，GANs有效合成了按纹理区分的元素（即海洋、天空、自然景观），但在生成几何图案（即面孔、动物、人、细节）时表现较差。核心问题在于卷积未能建模远程区域之间的依赖关系。此外，模型的统计和计算效率受层叠多层的影响。注意力机制，特别是自注意力，提供了一种计算成本低的替代方案，轻松建模长程依赖关系。自注意力作为卷积的补充，显著推动了该领域的发展，能够生成细节丰富的图像[[119](#bib.bib119)]、高分辨率图像，以及复杂的几何图案[[120](#bib.bib120)]。
- en: In expression recognition, attention optimizes the entire segmentation process
    by scanning input as a whole sequence, choosing the most relevant region to describe
    a segmented symbol or implicit space operator [[424](#bib.bib424)]. In information
    retriever, attention helps obtain appropriate semantic resources using individual
    class semantic resources to progressively orient visual aids to generate an attention
    map to ponder the importance of different local regions. [[425](#bib.bib425)].
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在表达识别中，注意力优化了整个分割过程，通过扫描输入作为整体序列，选择最相关的区域来描述分割符号或隐式空间操作符[[424](#bib.bib424)]。在信息检索中，注意力有助于使用单独的类别语义资源获取适当的语义资源，逐步定向视觉辅助工具以生成注意力图，权衡不同局部区域的重要性[[425](#bib.bib425)]。
- en: In medical image analysis, attention helps implicitly learn to suppress irrelevant
    areas in an input image while highlighting useful resources for a specific task.
    This allows us to eliminate the need to use explicit external tissue/organ localization
    modules using convolutional neural networks (CNNs). Besides, it allows generating
    both images and maps of attention in unsupervised learning useful for data annotation.
    For this, there are the ATA-GANS [[360](#bib.bib360)] and attention gate [[426](#bib.bib426)]
    modules that work with unsupervised and supervised learning, respectively.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学图像分析中，注意力机制有助于隐式地学习抑制输入图像中的无关区域，同时突出特定任务的有用资源。这使得我们能够消除使用显式外部组织/器官定位模块的需要，利用卷积神经网络（CNNs）。此外，它还允许在无监督学习中生成图像和注意力图，这对于数据注释非常有用。为此，有ATA-GANS
    [[360](#bib.bib360)] 和注意力门[[426](#bib.bib426)]模块，分别用于无监督和监督学习。
- en: '| Task | References |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 参考文献 |'
- en: '| --- | --- |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Clustering |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 |'
- en: '&#124; [[106](#bib.bib106)] &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[106](#bib.bib106)] &#124;'
- en: '|'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Compression | [[427](#bib.bib427)] |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 压缩 | [[427](#bib.bib427)] |'
- en: '| Deblurring | [[428](#bib.bib428)] |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 去模糊 | [[428](#bib.bib428)] |'
- en: '| Depth Estimation | [[429](#bib.bib429)] [[430](#bib.bib430)] |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 深度估计 | [[429](#bib.bib429)] [[430](#bib.bib430)] |'
- en: '| Image Restoration | [[133](#bib.bib133)] [[431](#bib.bib431)] [[432](#bib.bib432)]
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 图像恢复 | [[133](#bib.bib133)] [[431](#bib.bib431)] [[432](#bib.bib432)] |'
- en: '| Image-to-Image Translation | [[433](#bib.bib433)] [[434](#bib.bib434)] |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 图像到图像翻译 | [[433](#bib.bib433)] [[434](#bib.bib434)] |'
- en: '| Information Retriever | [[425](#bib.bib425)] [[435](#bib.bib435)] [[436](#bib.bib436)]
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 信息检索 | [[425](#bib.bib425)] [[435](#bib.bib435)] [[436](#bib.bib436)] |'
- en: '| Medical Image Analysis | [[360](#bib.bib360)] [[426](#bib.bib426)] [[437](#bib.bib437)]
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 医学图像分析 | [[360](#bib.bib360)] [[426](#bib.bib426)] [[437](#bib.bib437)] |'
- en: '| Multiple Instance Learning | [[438](#bib.bib438)] |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 多实例学习 | [[438](#bib.bib438)] |'
- en: '| Ocr | [[439](#bib.bib439)] |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 光学字符识别（Ocr） | [[439](#bib.bib439)] |'
- en: '| One-Shot Learning | [[162](#bib.bib162)] |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 一次学习 | [[162](#bib.bib162)] |'
- en: '| Pose Estimation | [[440](#bib.bib440)] |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 姿态估计 | [[440](#bib.bib440)] |'
- en: '| Super-Resolution | [[441](#bib.bib441)] |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 超分辨率 | [[441](#bib.bib441)] |'
- en: '| Transfer Learning | [[442](#bib.bib442)] [[443](#bib.bib443)] [[145](#bib.bib145)]
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 转移学习 | [[442](#bib.bib442)] [[443](#bib.bib443)] [[145](#bib.bib145)] |'
- en: '| Video Classification | [[444](#bib.bib444)] [[445](#bib.bib445)] |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 视频分类 | [[444](#bib.bib444)] [[445](#bib.bib445)] |'
- en: '| Facial Detection | [[144](#bib.bib144)] [[446](#bib.bib446)] [[447](#bib.bib447)]
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 面部检测 | [[144](#bib.bib144)] [[446](#bib.bib446)] [[447](#bib.bib447)] |'
- en: '| Fall Detection | [[448](#bib.bib448)] |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 跌倒检测 | [[448](#bib.bib448)] |'
- en: '| Person Detection | [[449](#bib.bib449)] [[450](#bib.bib450)] |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 人物检测 | [[449](#bib.bib449)] [[450](#bib.bib450)] |'
- en: '| Sign Detection | [[451](#bib.bib451)] |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 手势检测 | [[451](#bib.bib451)] |'
- en: '| Text Detection | [[415](#bib.bib415)] [[452](#bib.bib452)] [[453](#bib.bib453)] [[454](#bib.bib454)]
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 文本检测 | [[415](#bib.bib415)] [[452](#bib.bib452)] [[453](#bib.bib453)] [[454](#bib.bib454)]
    |'
- en: '| Face Recognition | [[139](#bib.bib139)] |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 人脸识别 | [[139](#bib.bib139)] |'
- en: '| Facial Expression Recognition | [[455](#bib.bib455)] [[456](#bib.bib456)] [[457](#bib.bib457)]
    |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 面部表情识别 | [[455](#bib.bib455)] [[456](#bib.bib456)] [[457](#bib.bib457)] |'
- en: '| Sequence Recognition | [[458](#bib.bib458)] |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 序列识别 | [[458](#bib.bib458)] |'
- en: 'Table 4: Summary others applications of attention in computer vision.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：计算机视觉中注意力的其他应用总结。
- en: For person recognition, attention has become essential in in-person re-identification
    (re-id) [[336](#bib.bib336)] [[388](#bib.bib388)] [[380](#bib.bib380)]. Re-id
    aims to search for people seen from a surveillance camera implanted in different
    locations. In classical approaches, the bounding boxes of detected people were
    not optimized for re-identification suffering from misalignment problems, background
    disorder, occlusion, and absent body parts. Misalignment is one of the biggest
    challenges, as people are often captured in various poses, and the system needs
    to compare different images. In this sense, neural attention models started to
    lead the developments mainly with multiple attentional mechanisms of alignment
    between different bounding boxes.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人物识别，注意力机制在现场重新识别（re-id）中变得至关重要[[336](#bib.bib336)] [[388](#bib.bib388)] [[380](#bib.bib380)]。重新识别旨在搜索从不同地点安装的监控摄像头中看到的人。在经典方法中，检测到的人的边界框未针对重新识别进行优化，存在对齐问题、背景杂乱、遮挡和缺失身体部位。对齐问题是最大的挑战之一，因为人们通常以不同的姿势被捕捉，系统需要比较不同的图像。在这方面，神经注意力模型开始引领发展，主要通过不同边界框之间的多重注意力机制来解决对齐问题。
- en: There are still less popular applications, but for which attention plays an
    essential role. Self-attention models iterations between the input set for clustering
    tasks [[106](#bib.bib106)]. Attention refines and merges multi-scale feature maps
    in-depth estimation and edge detection [[459](#bib.bib459)] [[429](#bib.bib429)].
    In video classification, attention helps capture global and local resources generating
    a comprehensive representation [[460](#bib.bib460)]. It also measures each time
    interval’s relevance in a sequence [[423](#bib.bib423)], promoting a more intuitive
    interpretation of the impact of content on the video’s popularity, providing the
    regions that contribute the most to the prediction [[444](#bib.bib444)]. In face
    detection, attention dynamically selects the main reference points of the face [[447](#bib.bib447)].
    It improves deblurring in each convolutional layer in deblurring, preserving fine
    details [[428](#bib.bib428)]. Finally, in emotion recognition, it captures complex
    relationships between audio and video data by obtaining regions where both signals
    relate to emotion [[75](#bib.bib75)].
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 仍有一些较少被关注的应用，但其中注意力机制起着至关重要的作用。自注意力模型在聚类任务中对输入集合进行迭代[[106](#bib.bib106)]。注意力机制在深度估计和边缘检测中对多尺度特征图进行细化和融合[[459](#bib.bib459)]
    [[429](#bib.bib429)]。在视频分类中，注意力机制帮助捕捉全局和局部资源，生成全面的表示[[460](#bib.bib460)]。它还衡量序列中每个时间间隔的相关性[[423](#bib.bib423)]，促进对内容对视频受欢迎程度影响的更直观解释，提供对预测贡献最大的区域[[444](#bib.bib444)]。在人脸检测中，注意力机制动态选择脸部的主要参考点[[447](#bib.bib447)]。它改善了每个卷积层的去模糊效果，保留了细节[[428](#bib.bib428)]。最后，在情感识别中，它通过获取音频和视频数据中与情感相关的区域，捕捉音频和视频数据之间的复杂关系[[75](#bib.bib75)]。
- en: 5.3 Multimodal Tasks (CV/NLP)
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 多模态任务（计算机视觉/自然语言处理）
- en: 'Attention has been used extensively in multimodal learning, mainly for mapping
    complex relationships between different sensory modalities. In this domain, the
    importance of attention is quite intuitive, given that communication and human
    sensory processing are completely multimodal. The first approaches emerged from
    2015 inspired by an attentive encoder-decoder framework entitled “Show, attend
    and tell: Neural image caption generation with visual attention” by Xu et al. [[45](#bib.bib45)].
    In this framework, depicted in figure [20](#S5.F20 "Figure 20 ‣ 5.3 Multimodal
    Tasks (CV/NLP) ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")a at each time step $t$, attention generates a vector
    with a dynamic context of visual features based on the words previously generated
    - a principle very similar to that presented in RNNSearch [[44](#bib.bib44)].
    Later, more elaborate methods using visual and textual sources were developed
    mainly in image captioning, video captioning, and visual question answering, as
    shown in the Table [5](#S5.T5 "Table 5 ‣ 5.3 Multimodal Tasks (CV/NLP) ‣ 5 Applications
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning").'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力已被广泛应用于多模态学习，主要用于映射不同感官模态之间的复杂关系。在这一领域，注意力的重要性是相当直观的，因为交流和人类感官处理是完全的多模态的。首个方法出现在2015年，灵感来自一个名为“展示、关注和讲述：带有视觉注意力的神经图像字幕生成”的注意力编码-解码框架，由Xu等人提出[[45](#bib.bib45)]。在该框架中，如图[20](#S5.F20
    "图 20 ‣ 5.3 多模态任务 (CV/NLP) ‣ 5 应用 ‣ 注意力，请！深度学习中的神经注意力模型调查")a所示，在每个时间步$t$，注意力基于之前生成的单词生成一个具有动态视觉特征上下文的向量——这一原理与RNNSearch中提出的非常相似[[44](#bib.bib44)]。后来，使用视觉和文本来源的更复杂方法主要在图像字幕生成、视频字幕生成和视觉问答中得到发展，如表[5](#S5.T5
    "表 5 ‣ 5.3 多模态任务 (CV/NLP) ‣ 5 应用 ‣ 注意力，请！深度学习中的神经注意力模型调查")所示。
- en: '| Task | References |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 参考文献 |'
- en: '| --- | --- |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Emotion Recognition | [[461](#bib.bib461)] [[76](#bib.bib76)] [[77](#bib.bib77)]
    |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 情感识别 | [[461](#bib.bib461)] [[76](#bib.bib76)] [[77](#bib.bib77)] |'
- en: '| Expression Comprehension | [[462](#bib.bib462)] |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 表情理解 | [[462](#bib.bib462)] |'
- en: '| Image Classification | [[463](#bib.bib463)] |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 图像分类 | [[463](#bib.bib463)] |'
- en: '| Text-to-Image Generation | [[464](#bib.bib464)] |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 文本到图像生成 | [[464](#bib.bib464)] |'
- en: '| Image-to-Text Generation | [[465](#bib.bib465)] |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 图像到文本生成 | [[465](#bib.bib465)] |'
- en: '| Image Captioning |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 图像字幕生成 |'
- en: '&#124; [[466](#bib.bib466)] [[467](#bib.bib467)] [[45](#bib.bib45)] [[468](#bib.bib468)] [[469](#bib.bib469)] [[470](#bib.bib470)] [[471](#bib.bib471)] [[472](#bib.bib472)]
    &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[466](#bib.bib466)] [[467](#bib.bib467)] [[45](#bib.bib45)] [[468](#bib.bib468)] [[469](#bib.bib469)] [[470](#bib.bib470)] [[471](#bib.bib471)] [[472](#bib.bib472)]
    &#124;'
- en: '&#124; [[81](#bib.bib81)] [[473](#bib.bib473)] [[474](#bib.bib474)] [[475](#bib.bib475)] [[476](#bib.bib476)] [[82](#bib.bib82)] [[477](#bib.bib477)] [[70](#bib.bib70)] [[478](#bib.bib478)]
    &#124;'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[81](#bib.bib81)] [[473](#bib.bib473)] [[474](#bib.bib474)] [[475](#bib.bib475)] [[476](#bib.bib476)] [[82](#bib.bib82)] [[477](#bib.bib477)] [[70](#bib.bib70)] [[478](#bib.bib478)]
    &#124;'
- en: '&#124; [[479](#bib.bib479)] [[480](#bib.bib480)] [[481](#bib.bib481)] [[466](#bib.bib466)]
    &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[479](#bib.bib479)] [[480](#bib.bib480)] [[481](#bib.bib481)] [[466](#bib.bib466)]
    &#124;'
- en: '|'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Video Captioning |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 视频字幕生成 |'
- en: '&#124; [[482](#bib.bib482)] [[63](#bib.bib63)] [[483](#bib.bib483)] [[484](#bib.bib484)] [[66](#bib.bib66)] [[62](#bib.bib62)] [[485](#bib.bib485)] [[74](#bib.bib74)] [[486](#bib.bib486)]
    &#124;'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[482](#bib.bib482)] [[63](#bib.bib63)] [[483](#bib.bib483)] [[484](#bib.bib484)] [[66](#bib.bib66)] [[62](#bib.bib62)] [[485](#bib.bib485)] [[74](#bib.bib74)] [[486](#bib.bib486)]
    &#124;'
- en: '&#124; [[321](#bib.bib321)] [[487](#bib.bib487)] [[483](#bib.bib483)] [[488](#bib.bib488)] [[489](#bib.bib489)] [[490](#bib.bib490)] [[491](#bib.bib491)] [[492](#bib.bib492)] [[493](#bib.bib493)]
    &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[321](#bib.bib321)] [[487](#bib.bib487)] [[483](#bib.bib483)] [[488](#bib.bib488)] [[489](#bib.bib489)] [[490](#bib.bib490)] [[491](#bib.bib491)] [[492](#bib.bib492)] [[493](#bib.bib493)]
    &#124;'
- en: '&#124; [[494](#bib.bib494)] &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[494](#bib.bib494)] &#124;'
- en: '|'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Visual Question Answering |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 视觉问答 |'
- en: '&#124; [[495](#bib.bib495)] [[496](#bib.bib496)] [[497](#bib.bib497)] [[498](#bib.bib498)] [[122](#bib.bib122)] [[499](#bib.bib499)] [[500](#bib.bib500)]
    &#124;'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[495](#bib.bib495)] [[496](#bib.bib496)] [[497](#bib.bib497)] [[498](#bib.bib498)] [[122](#bib.bib122)] [[499](#bib.bib499)] [[500](#bib.bib500)]
    &#124;'
- en: '&#124; [[80](#bib.bib80)] [[190](#bib.bib190)] [[501](#bib.bib501)] [[502](#bib.bib502)] [[503](#bib.bib503)] [[504](#bib.bib504)] [[505](#bib.bib505)] [[88](#bib.bib88)] [[506](#bib.bib506)]
    &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[80](#bib.bib80)] [[190](#bib.bib190)] [[501](#bib.bib501)] [[502](#bib.bib502)] [[503](#bib.bib503)] [[504](#bib.bib504)] [[505](#bib.bib505)] [[88](#bib.bib88)] [[506](#bib.bib506)]
    &#124;'
- en: '&#124; [[507](#bib.bib507)] [[508](#bib.bib508)] [[509](#bib.bib509)] [[510](#bib.bib510)] [[511](#bib.bib511)] [[512](#bib.bib512)] [[513](#bib.bib513)] [[514](#bib.bib514)] [[79](#bib.bib79)]
    &#124;'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[507](#bib.bib507)] [[508](#bib.bib508)] [[509](#bib.bib509)] [[510](#bib.bib510)] [[511](#bib.bib511)] [[512](#bib.bib512)] [[513](#bib.bib513)] [[514](#bib.bib514)] [[79](#bib.bib79)]
    &#124;'
- en: '&#124; [[515](#bib.bib515)] [[516](#bib.bib516)] [[517](#bib.bib517)] [[518](#bib.bib518)] [[466](#bib.bib466)] [[519](#bib.bib519)]
    &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[515](#bib.bib515)] [[516](#bib.bib516)] [[517](#bib.bib517)] [[518](#bib.bib518)] [[466](#bib.bib466)] [[519](#bib.bib519)]
    &#124;'
- en: '|'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 5: Summary of state-of-art approaches in multimodal tasks (CV/NLP).'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：多模态任务（CV/NLP）中的最新方法总结。
- en: 'For image captioning, Yan et al. [[474](#bib.bib474)] extended the seminal
    framework by Xu et al. [[45](#bib.bib45)] with review attention, a sequence of
    modules that capture global information in various stages of reviewing hidden
    states and generate more compact, abstract, and global context vectors. Zhu et
    al. [[467](#bib.bib467)] presented a triple attention model which enhances object
    information at the text generation stage. Two attention mechanisms capture semantic
    visual information in input, and a mechanism in the prediction stage integrates
    word and image information better. Lu et al. [[469](#bib.bib469)] presented an
    adaptive attention encoder-decoder framework that decides when to trust visual
    signals and when to trust only the language model. Specifically, their mechanism
    has two complementary elements: the visual sentinel vector decides when to look
    at the image, and the sentinel gate decides how much new information the decoder
    wants from the image. Recently, Pan et al. [[481](#bib.bib481)] created attentional
    mechanisms based on bilinear pooling capable of capturing high order interactions
    between multi-modal features, unlike the classic mechanisms that capture only
    first-order feature interactions.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像字幕生成，Yan等人[[474](#bib.bib474)]扩展了Xu等人的划时代框架[[45](#bib.bib45)]，引入了review
    attention，一系列模块用于捕捉隐藏状态中不同阶段的全局信息，并生成更加精简、抽象和全局的上下文向量。Zhu等人[[467](#bib.bib467)]提出了一种三重注意力模型，增强了文本生成阶段的对象信息。两种注意力机制在输入中捕捉语义视觉信息，而预测阶段的机制更好地集成了单词和图像信息。Lu等人[[469](#bib.bib469)]提出了一种自适应注意力编码器-解码器框架，该框架决定何时信任视觉信号，何时仅信任语言模型。具体而言，他们的机制具有两个互补的元素：视觉哨兵向量决定何时查看图像，哨兵门决定解码器从图像中想要获得多少新信息。最近，Pan等人[[481](#bib.bib481)]基于双线性池化提出了基于注意力机制的方法，能够捕捉多模式特征之间的高阶交互，与仅捕捉一阶特征交互的经典机制不同。
- en: Similarly, in visual question-answering tasks, methods seek to align salient
    textual features with visual features via feedforward or recurrent soft attention
    methods [[466](#bib.bib466)] [[499](#bib.bib499)] [[500](#bib.bib500)] [[517](#bib.bib517)].
    More recent approaches aim to generate complex inter-modal representations. In
    this line, Kim et al. [[516](#bib.bib516)] proposed Hypergraph Attention Networks
    (HANs), a solution to minimize the disparity between different levels of abstraction
    from different sensory sources. So far, HAN is the first approach to define a
    common semantic space with symbolic graphs of each modality and extract an inter-modal
    representation based on co-attention maps in the constructed semantic space, as
    shown in figure [20](#S5.F20 "Figure 20 ‣ 5.3 Multimodal Tasks (CV/NLP) ‣ 5 Applications
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")b.
    Liang et al. [[497](#bib.bib497)] used attention to capture hierarchical relationships
    between sequences of image-text pairs not directly related. The objective is to
    answer questions and justify what results in the system were based on answers.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在视觉问答任务中，方法通过前向或循环的软注意力机制[[466](#bib.bib466)] [[499](#bib.bib499)] [[500](#bib.bib500)] [[517](#bib.bib517)]来使显著的文本特征与视觉特征对齐。最新的方法旨在生成复杂的跨模态表示。在这方面，Kim等人[[516](#bib.bib516)]提出了超图注意力网络（HANs），用于减小不同感官来源的不同抽象层次之间的差异。到目前为止，HAN是首个使用每种模态的符号图定义共同语义空间，并基于构建的语义空间中的协同注意力图提取跨模态表示的方法，如图[20](#S5.F20
    "Figure 20 ‣ 5.3 Multimodal Tasks (CV/NLP) ‣ 5 Applications ‣ Attention, please!
    A survey of Neural Attention Models in Deep Learning")b所示。Liang等人[[497](#bib.bib497)]利用注意力捕捉不直接相关的图像-文本对序列之间的层次关系。目标是回答问题并解释系统结果基于答案的依据。
- en: For video captioning most approaches generally align textual features and spatio-temporal
    representations of visual features via simple soft attention mechanisms [[482](#bib.bib482)] [[485](#bib.bib485)] [[62](#bib.bib62)] [[74](#bib.bib74)].
    For example, Pu et al. [[66](#bib.bib66)] design soft attention to adaptively
    emphasize different CNN layers while also imposing attention within local spatiotemporal
    regions of the feature maps at particular layers. These mechanisms define the
    importance of regions and layers to produce a word based on word-history information.
    Recently, self-attention mechanisms have also been used to capture more complex
    and explicit relationships between different modalities. Zhu et al. [[484](#bib.bib484)]
    introduced ActBERT, a transformer-based approach trained via self-supervised learning
    to encode complex relations between global actions and local, regional objects
    and linguistic descriptions. Zhou et al. [[483](#bib.bib483)] proposed a multimodal
    transformer via supervised learning, which employs a masking network to restrict
    its attention to the proposed event over the encoding feature.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 对于视频字幕生成，大多数方法通常通过简单的软注意力机制对齐文本特征和视觉特征的时空表示[[482](#bib.bib482)] [[485](#bib.bib485)]
    [[62](#bib.bib62)] [[74](#bib.bib74)]。例如，Pu等人[[66](#bib.bib66)]设计了软注意力，以自适应地强调不同的CNN层，同时在特定层的特征图的局部时空区域内施加注意力。这些机制定义了区域和层的重要性，以基于词历史信息生成单词。最近，self-attention机制也被用来捕捉不同模态之间更复杂和明确的关系。Zhu等人[[484](#bib.bib484)]提出了ActBERT，一种基于transformer的方法，通过自监督学习训练，用于编码全球动作与局部、区域对象及语言描述之间的复杂关系。Zhou等人[[483](#bib.bib483)]提出了一种通过监督学习的多模态transformer，该方法利用掩码网络将注意力限制在编码特征上的提议事件上。
- en: '![Refer to caption](img/5a2b0a4b5b4b418123e986ed3986d842.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5a2b0a4b5b4b418123e986ed3986d842.png)'
- en: 'Figure 20: Illustration of classic attentive encoder-decoder framework by Xu
    et al. [[45](#bib.bib45)] and Hypergraph Attention Network [[516](#bib.bib516)]
    for question answering tasks. a) The show, attend and tell framework. At each
    time step $t$, attention takes as input visual feature maps and previous hidden
    state of the decoder and produces a dynamic context vector with essential visual
    features to predict the next word. b) The Hypergraph Attention Network. For a
    given pair of images and questions, two symbolic graphs $G_{i}$, and $G_{q}$ are
    constructed. After, two hypergraphs $HG_{i}$, and $HG_{q}$ with random-walk hyperedge
    are constructed and combined via co-attention map $A$. Finally, the final representation
    $Z_{s}$ is used to predict an answer for the given question.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：Xu等人提出的经典注意力编码器-解码器框架[[45](#bib.bib45)]和用于问答任务的超图注意力网络[[516](#bib.bib516)]的示意图。a)
    显示、关注和讲述框架。在每个时间步$t$，注意力将视觉特征图和解码器的先前隐藏状态作为输入，并生成一个动态上下文向量，该向量包含预测下一个词所需的关键视觉特征。b)
    超图注意力网络。对于给定的图像和问题对，构建两个符号图$G_{i}$和$G_{q}$。然后，构建两个带有随机游走超边的超图$HG_{i}$和$HG_{q}$，并通过共同注意力图$A$将它们结合起来。最后，最终表示$Z_{s}$用于预测给定问题的答案。
- en: 'Other applications also benefit from the attention. In the emotion recognition
    domain, the main approaches use memory fusion structures inspired by the human
    brain’s communication understanding mechanisms. Biologically, different regions
    process and understand different modalities connected via neural links to integrate
    multimodal information over time. Similarly, in existing approaches, an attentional
    component models view-specific dynamics within each modality via recurrent neural
    networks, and a second component simultaneously finds multiple cross-view dynamics
    in each recurrence timestep by storing them in hybrid memories. Memory updates
    occur based on all the sequential data seen. Finally, to generate the output,
    the predictor integrates the two levels of information: view-specific and multiple
    cross-view memory information [[76](#bib.bib76)] [[77](#bib.bib77)].'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 其他应用也受益于注意力。在情感识别领域，主要方法使用受人脑通信理解机制启发的记忆融合结构。从生物学角度看，不同的区域通过神经链接处理和理解不同的模态，以在时间上整合多模态信息。类似地，在现有方法中，一个注意力组件通过递归神经网络建模每种模态内的视角特定动态，另一个组件同时在每个递归时间步找到多个视角交叉动态，并将其存储在混合记忆中。记忆更新基于所见的所有顺序数据。最后，为生成输出，预测器整合了两个层次的信息：视角特定信息和多个视角记忆信息[[76](#bib.bib76)]
    [[77](#bib.bib77)]。
- en: There are still few multimodal methods for classification. Whang et al. [[463](#bib.bib463)]
    presented a pioneering framework for classifying and describing image regions
    simultaneously from textual and visual sources. Their framework detects, classifies,
    and generates explanatory reports regarding abnormalities observed in chest X-ray
    images through multi-level attentional modules end-to-end in LSTMs and CNNs. In
    LSTMs, attention combines all hidden states and generates a dynamic context vector,
    then a spatial mechanism guided by a textual mechanism highlights the regions
    of the image with more meaningful information. Intuitively, the salient features
    of the image are extracted based on high-relevance textual regions.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 目前针对分类的多模态方法仍然较少。Whang 等人 [[463](#bib.bib463)] 提出了一个开创性的框架，用于同时从文本和视觉来源分类和描述图像区域。他们的框架通过
    LSTM 和 CNN 的多层关注模块端到端地检测、分类并生成关于胸部 X 射线图像中观察到的异常的解释性报告。在 LSTM 中，注意力结合所有隐藏状态并生成动态上下文向量，然后由文本机制引导的空间机制突出图像中更有意义的信息区域。直观上，图像的显著特征是基于高相关文本区域提取的。
- en: 5.4 Recommender Systems (RS)
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 推荐系统 (RS)
- en: Attention has also been used in recommender systems for behavioral modeling
    of users. Capturing user interests is a challenging problem for neural networks,
    as some iterations are transient, some clicks are unintentional, and interests
    can change quickly in the same session. Classical approaches (i.e., Markov Chains
    and RNNs) have limited performance predicting the user’s next actions, present
    different performances in sparse and dense datasets, and long-term memory problems.
    In this sense, attention has been used mainly to assign weights to a user’s interacted
    items capturing long and short-term interests more effectively than traditional
    ones. Self-attention and memory approaches have been explored to improve the area’s
    development. STAMP [[520](#bib.bib520)] model, based on attention and memory,
    manages users’ general interests in long-term memories and current interests in
    short-term memories resulting in behavioral representations that are more coherent.
    The Collaborative Filtering [[521](#bib.bib521)] framework, and SASRec [[338](#bib.bib338)]
    explored self-attention in capturing long-term semantics for finding the most
    relevant items in user’s history.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力也被用于推荐系统中的用户行为建模。捕捉用户兴趣对神经网络而言是一个具有挑战性的问题，因为有些迭代是暂时的，有些点击是无意的，并且兴趣在同一会话中可以迅速变化。经典方法（即马尔可夫链和
    RNN）在预测用户的下一步动作时性能有限，在稀疏和密集数据集中表现不同，并且存在长期记忆问题。在这方面，注意力主要用于为用户的交互项目分配权重，比传统方法更有效地捕捉长期和短期兴趣。自注意力和记忆方法已被探索以改善该领域的发展。基于注意力和记忆的
    STAMP [[520](#bib.bib520)] 模型管理用户的长期记忆中的一般兴趣和短期记忆中的当前兴趣，从而产生更连贯的行为表征。协同过滤 [[521](#bib.bib521)]
    框架和 SASRec [[338](#bib.bib338)] 探索了自注意力在捕捉长期语义方面，以便找到用户历史中最相关的项目。
- en: 5.5 Reinforcement Learning (RL)
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 强化学习 (RL)
- en: Attention has been gradually introduced in reinforcement learning to deal with
    unstructured environments in which rewards and actions depend on past states and
    where it is challenging guaranteeing the Markov property. Specifically, the goals
    are to increase the agent’s generalizability and minimize long-term memory problems.
    Currently, the main attentional reinforcement learning approaches are computer
    vision, graph reasoning, natural language processing, and virtual navigation,
    as shown in Table [6](#S5.T6 "Table 6 ‣ 5.5 Reinforcement Learning (RL) ‣ 5 Applications
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning").
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力也逐渐被引入到强化学习中，以处理奖励和行动依赖于过去状态的非结构化环境，并且保证马尔可夫性质是具有挑战性的。具体而言，目标是提高代理的泛化能力并最小化长期记忆问题。目前，主要的注意力强化学习方法包括计算机视觉、图形推理、自然语言处理和虚拟导航，如表 [6](#S5.T6
    "Table 6 ‣ 5.5 Reinforcement Learning (RL) ‣ 5 Applications ‣ Attention, please!
    A survey of Neural Attention Models in Deep Learning")所示。
- en: '| Task | References |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 参考文献 |'
- en: '| --- | --- |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Computer Vision |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 计算机视觉 |'
- en: '&#124; [[155](#bib.bib155)] [[522](#bib.bib522)] [[523](#bib.bib523)] [[524](#bib.bib524)] [[525](#bib.bib525)]
    &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[155](#bib.bib155)] [[522](#bib.bib522)] [[523](#bib.bib523)] [[524](#bib.bib524)] [[525](#bib.bib525)]
    &#124;'
- en: '&#124; [[129](#bib.bib129)] [[526](#bib.bib526)] [[150](#bib.bib150)] [[527](#bib.bib527)]
    &#124;'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[129](#bib.bib129)] [[526](#bib.bib526)] [[150](#bib.bib150)] [[527](#bib.bib527)]
    &#124;'
- en: '|'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Graph reasoning | [[528](#bib.bib528)] |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 图形推理 | [[528](#bib.bib528)] |'
- en: '| Natural Language Processing | [[78](#bib.bib78)] |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言处理 | [[78](#bib.bib78)] |'
- en: '| Navigation | [[141](#bib.bib141)] [[529](#bib.bib529)] [[530](#bib.bib530)] [[78](#bib.bib78)] [[531](#bib.bib531)]
    |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 导航 | [[141](#bib.bib141)] [[529](#bib.bib529)] [[530](#bib.bib530)] [[78](#bib.bib78)]
    [[531](#bib.bib531)] |'
- en: 'Table 6: Summary of main state-of-art approaches in attentional reinforcement
    learning tasks.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：注意力强化学习任务中主要前沿方法的总结。
- en: To increase the ability to generalize in partially observable environments,
    some approaches use attention in the policy network. Mishra et al. [[141](#bib.bib141)]
    used attention to easily capture long-term temporal dependencies in convolutions
    in an agent’s visual navigation task in random mazes. At each time step $t$, the
    model receives as input the current observation $o_{t}$ and previous sequences
    of observations, rewards, and actions so that attention allows the policy to maintain
    a long memory of past episodes. Other approaches implement attention directly
    to the representation of the state. State representation is a classic and critical
    problem in RL, given that state space is one of the major bottlenecks for speed,
    efficiency, and generalization of training techniques. In this sense, the importance
    of attention on this topic is quite intuitive.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高在部分可观测环境中的泛化能力，一些方法在策略网络中使用注意力。Mishra 等人[[141](#bib.bib141)] 使用注意力轻松捕捉在随机迷宫中的代理视觉导航任务中的长期时间依赖关系。在每个时间步$t$，模型接收当前观察$o_{t}$和先前的观察序列、奖励和动作作为输入，从而使注意力允许策略保持对过去情节的长期记忆。其他方法直接将注意力实现到状态的表示中。状态表示是强化学习中的经典且关键问题，因为状态空间是训练技术速度、效率和泛化的主要瓶颈之一。在这种意义上，注意力在这个主题上的重要性是相当直观的。
- en: However, there are still few approaches exploring the representation of states.
    The neural map [[529](#bib.bib529)] maintains an internal memory in the agent
    controlled via attention mechanisms. While the agent navigates the environment,
    an attentional mechanism alters the internal memory, dynamically constructing
    a history summary. At the same time, another generates a representation $o_{t}$,
    based on the contextual information of the memory and the state’s current observation.
    Then, the policy network receives $o_{t}$ as input and generates the distribution
    of shares. Some more recent approaches affect the representation of the current
    $o_{t}$ observation of the state via self attention in iterative reasoning between
    entities in the scene [[530](#bib.bib530)] [[531](#bib.bib531)], or between the
    current observation $o_{t}$ and memory units [[78](#bib.bib78)] to guide model-free
    policies.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前仍有少数方法探索状态的表示。神经图谱[[529](#bib.bib529)] 在代理中通过注意力机制保持内部记忆。当代理在环境中导航时，注意力机制会改变内部记忆，动态构建历史总结。同时，另一个机制基于记忆的上下文信息和状态的当前观察生成表示$o_{t}$。然后，策略网络接收$o_{t}$
    作为输入，并生成股份的分布。一些较新的方法通过场景中实体之间的自注意力的迭代推理[[530](#bib.bib530)] [[531](#bib.bib531)]，或在当前观察$o_{t}$
    和记忆单元之间[[78](#bib.bib78)]来影响状态当前的$o_{t}$观察，从而指导无模型政策。
- en: The most discussed topic is the use of the policy network to guide the attentional
    focus of the agent’s glimpses sensors on the environment so that the representation
    of the state refers to only a small portion of the entire operating environment.
    This approach emerged initially by Graves et al. [[46](#bib.bib46)] using policy
    gradient methods (i.e., REINFORCE algorithm) in the hybrid training of recurrent
    networks in image classification tasks. Their model consists of a glimpse sensor
    that captures only a portion of the input image, a core network that maintains
    a summary of the history of patches seen by the agent, an action network that
    estimates the class of the image seen, and a location network trained via RL which
    estimates the focus of the glimpse on the next time step, as shown in figure [21](#S5.F21
    "Figure 21 ‣ 5.5 Reinforcement Learning (RL) ‣ 5 Applications ‣ Attention, please!
    A survey of Neural Attention Models in Deep Learning")a. This structure considers
    the network as the agent, the image as the environment, and the reward is the
    number of correct network ratings in an episode. Stollenga et al. [[524](#bib.bib524)]
    proposed a similar approach, however directly focused on CNNs, as shown in figure [21](#S5.F21
    "Figure 21 ‣ 5.5 Reinforcement Learning (RL) ‣ 5 Applications ‣ Attention, please!
    A survey of Neural Attention Models in Deep Learning")b. The structure allows
    each layer to influence all the others through attentional bottom-up and top-down
    connections that modulate convolutional filters’ activity. After supervised training,
    the attentional connections’ weights implement a control policy via RL and SNES [[532](#bib.bib532)].
    The policy learns to suppress or enhance features at various levels by improving
    the classification of difficult cases not captured by the initial supervised training.
    Subsequently, variants similar to these approaches appeared in multiple image
    classification [[155](#bib.bib155)] [[150](#bib.bib150)] [[129](#bib.bib129)],
    action recognition [[527](#bib.bib527)], and face hallucination [[525](#bib.bib525)].
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 最常讨论的话题是使用策略网络来引导代理的窥视传感器在环境中关注的注意力焦点，以便状态的表示仅涉及整个操作环境的一小部分。这种方法最初由Graves等人提出
    [[46](#bib.bib46)]，在混合训练循环网络进行图像分类任务时采用了策略梯度方法（即REINFORCE算法）。他们的模型包括一个只捕获输入图像部分的窥视传感器，一个维护代理所看到的补丁历史摘要的核心网络，一个估计所看到图像类别的行动网络，以及一个通过强化学习训练的位置网络，估计下一个时间步的窥视焦点，如图[21](#S5.F21
    "Figure 21 ‣ 5.5 Reinforcement Learning (RL) ‣ 5 Applications ‣ Attention, please!
    A survey of Neural Attention Models in Deep Learning")a所示。该结构将网络视为代理，将图像视为环境，奖励是一集中网络评级的正确数量。Stollenga等人提出了类似的方法，但是直接针对卷积神经网络（CNNs），如图[21](#S5.F21
    "Figure 21 ‣ 5.5 Reinforcement Learning (RL) ‣ 5 Applications ‣ Attention, please!
    A survey of Neural Attention Models in Deep Learning")b所示。该结构允许每个层通过注意力自下而上和自上而下的连接来影响其他所有层的卷积滤波器活动。在监督训练之后，注意力连接的权重通过强化学习和SNES
    [[532](#bib.bib532)]实现了一个控制策略。该策略通过改进初始监督训练未捕获的难例的分类来抑制或增强各个层的特征。随后，类似于这些方法的变体出现在多个图像分类
    [[155](#bib.bib155)] [[150](#bib.bib150)] [[129](#bib.bib129)]、动作识别 [[527](#bib.bib527)]
    和面部图像增强 [[525](#bib.bib525)] 中。
- en: '![Refer to caption](img/00343537d2d383782b79c11e8627b808.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/00343537d2d383782b79c11e8627b808.png)'
- en: 'Figure 21: Illustration of RAM [[46](#bib.bib46)] and dasNet [[524](#bib.bib524)]
    for image classification tasks. a) The RAM framework. At each time step $t$, the
    glimpse networks extracts a retina-like representation based on input image and
    an focus location $l_{t-1}$. The core network takes the glimpse representation
    as input and combining it with the internal representation at the previous time
    step and produces the new internal state of the model $h_{t}$. The location network
    and action network use the $h_{t}$ to produce the next location $l_{t}$ to attend
    and the classification. b) The dasNet network. Each image is classified after
    $T$ passes through CNNs. After each forward propagation, the averages of feature
    maps are combined into an observation vector $o_{t}$ that is used by a deterministic
    policy to choose an action $a_{t}$ that changes the focus of all the feature maps
    for the next pass of the same image. Finally, after pass $T$ times, the output
    is used to classify the image.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：RAM [[46](#bib.bib46)] 和 dasNet [[524](#bib.bib524)] 在图像分类任务中的示意图。a) RAM
    框架。在每个时间步 $t$，glimpse 网络基于输入图像和焦点位置 $l_{t-1}$ 提取类似视网膜的表示。核心网络将 glimp 代表作为输入，并将其与上一个时间步的内部表示结合，产生模型的新内部状态
    $h_{t}$。位置网络和动作网络使用 $h_{t}$ 来生成下一个位置 $l_{t}$ 以进行关注和分类。b) dasNet 网络。每个图像在通过 CNNs
    经过 $T$ 次后被分类。在每次前向传播后，特征图的平均值被组合成观察向量 $o_{t}$，由确定性策略选择动作 $a_{t}$，该动作改变所有特征图的焦点，以便对同一图像进行下一次传递。最终，经过
    $T$ 次传递后，输出用于对图像进行分类。
- en: 5.6 Robotics
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 机器人技术
- en: In robotics, there are still few applications with neural attentional models.
    A small portion of current work is focused on control, visual odometry, navigation,
    and human-robot interaction, as shown in Table [7](#S5.T7 "Table 7 ‣ 5.6 Robotics
    ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention Models in Deep
    Learning").
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人技术中，神经注意模型的应用仍然很少。目前的工作小部分集中在控制、视觉里程计、导航和人机交互上，如表[7](#S5.T7 "Table 7 ‣ 5.6
    Robotics ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention Models
    in Deep Learning")所示。
- en: '| Task | References |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 参考文献 |'
- en: '| --- | --- |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Control | [[533](#bib.bib533)] [[85](#bib.bib85)] |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 控制 | [[533](#bib.bib533)] [[85](#bib.bib85)] |'
- en: '| Visual odometry | [[534](#bib.bib534)] [[535](#bib.bib535)] [[536](#bib.bib536)] [[537](#bib.bib537)] [[538](#bib.bib538)] [[539](#bib.bib539)]
    |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 视觉里程计 | [[534](#bib.bib534)] [[535](#bib.bib535)] [[536](#bib.bib536)] [[537](#bib.bib537)]
    [[538](#bib.bib538)] [[539](#bib.bib539)] |'
- en: '| Navigation | [[540](#bib.bib540)] [[541](#bib.bib541)] [[542](#bib.bib542)] [[543](#bib.bib543)]
    |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 导航 | [[540](#bib.bib540)] [[541](#bib.bib541)] [[542](#bib.bib542)] [[543](#bib.bib543)]
    |'
- en: '| Human-robot interaction | [[544](#bib.bib544)] |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 人机交互 | [[544](#bib.bib544)] |'
- en: 'Table 7: Summary state-of-art main approaches in robotics.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：机器人技术中前沿主流方法的总结。
- en: 'Navigation and visual odometry are the most explored domains, although still
    with very few published works. For classic DL approaches, navigating tasks in
    real or complex environments are still very challenging. These approaches have
    limited performance in dynamic and unstructured environments and over long horizon
    tasks. In real environments, the robot must deal with dynamic and unexpected changes
    in humans and other obstacles around it. Also, decision-making depends on the
    information received in the past and the ability to infer the future state of
    the environment. Some seminal approaches in the literature have demonstrated the
    potential of attention to minimizing these problems without compromising the techniques’
    computational cost. Sadeghian et al. [[540](#bib.bib540)] proposed Sophie: an
    interpretable framework based on GANs for robotic agents in environments with
    human crowds. Their framework via attention extracts two levels of information:
    a physical extractor learns spatial and physical constraints generating a $C$
    context vector that focuses on viable paths for each agent. In contrast, a social
    extractor learns the interactions between agents and their influence on each agent’s
    future path. Finally, LSTMs based on GAN generate realistic samples capturing
    the nature of future paths, and the attentional mechanisms allow the framework
    to predict physically and socially feasible paths for agents, achieving cutting-edge
    performances on several different trajectories.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 导航和视觉里程计是最受关注的领域，尽管仍有很少的公开研究。对于经典的深度学习方法，实际或复杂环境中的导航任务仍然非常具有挑战性。这些方法在动态和非结构化环境以及长期任务中的表现有限。在真实环境中，机器人必须处理周围人类和其他障碍物的动态和意外变化。此外，决策依赖于过去收到的信息以及推断环境未来状态的能力。文献中的一些开创性方法展示了注意力机制在不妨碍技术计算成本的情况下最小化这些问题的潜力。Sadeghian
    等人[[540](#bib.bib540)] 提出了 Sophie：一个基于生成对抗网络（GANs）的可解释框架，针对在人群环境中的机器人代理。该框架通过注意力机制提取两个层次的信息：物理提取器学习空间和物理约束，生成一个
    $C$ 上下文向量，专注于每个代理的可行路径。相对地，社会提取器学习代理之间的互动及其对每个代理未来路径的影响。最后，基于 GAN 的 LSTM 生成真实样本，捕捉未来路径的特性，注意力机制使框架能够预测代理的物理和社会上可行路径，在多条不同轨迹上实现了尖端性能。
- en: 'Vemula et al. [[541](#bib.bib541)] proposed a trajectory prediction model that
    captures each person’s relative importance when navigating in the crowd, regardless
    of their proximity via spatio-temporal graphs. Chen et al. [[542](#bib.bib542)]
    proposed the crowd-aware robot navigation with attention-based deep reinforcement
    learning. Specifically, a self-attention mechanism models interactions between
    human-robot and human-human pairs, improving the robot’s inference capacity of
    future environment states. It also captures how human-human interactions can indirectly
    affect decision-making, as shown in figure [22](#S5.F22 "Figure 22 ‣ 5.6 Robotics
    ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention Models in Deep
    Learning") in a). Fang et. al. [[543](#bib.bib543)] proposed the novel memory-based
    policy (i.e., scene memory transformer - SMT) for embodied agents in long-horizon
    tasks. The SMT policy consists of two modules: 1) scene memory which stores all
    past observations in an embedded form, and 2) an attention-based policy network
    that uses the updated scene memory to compute a distribution over actions. The
    SMT model is based on an encoder-decoder Transformer and showed strong performance
    as the agent moves in a large environment, and the number of observations grows
    rapidly.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: Vemula 等人[[541](#bib.bib541)] 提出了一个轨迹预测模型，该模型通过时空图捕捉每个人在拥挤环境中导航时的相对重要性，而不考虑他们的接近程度。Chen
    等人[[542](#bib.bib542)] 提出了基于注意力的深度强化学习的群体感知机器人导航。具体来说，自注意力机制建模了人机和人际对的互动，提升了机器人对未来环境状态的推理能力。它还捕捉了人际互动如何间接影响决策，如图[22](#S5.F22
    "Figure 22 ‣ 5.6 Robotics ‣ 5 Applications ‣ Attention, please! A survey of Neural
    Attention Models in Deep Learning") a)所示。Fang 等人[[543](#bib.bib543)] 提出了用于长时间任务的创新记忆基础策略（即场景记忆变换器
    - SMT）。SMT 策略由两个模块组成：1) 场景记忆，它以嵌入形式存储所有过去的观察；2) 基于注意力的策略网络，利用更新的场景记忆计算动作的分布。SMT
    模型基于编码器-解码器 Transformer，并在代理在大型环境中移动且观察数量迅速增加时表现出强大的性能。
- en: In visual odometry (VO), the classic learning-based methods consider the VO
    task a problem of pure tracking through the recovery of camera poses from fragments
    of the image, leading to the accumulation of errors. Such approaches often disregard
    crucial global information to alleviate accumulated errors. However, it is challenging
    to preserve this information in end-to-end systems effectively. Attention represents
    an alternative that is still little explored in this area to alleviate such disadvantages.
    Xue et al. [[534](#bib.bib534)] proposed an adaptive memory approach to avoid
    the network’s catastrophic forgetfulness. Their framework consists mainly of a
    memory, a remembering, and a refining module, as shown in figure  [22](#S5.F22
    "Figure 22 ‣ 5.6 Robotics ‣ 5 Applications ‣ Attention, please! A survey of Neural
    Attention Models in Deep Learning")b). First, it remembers to select the main
    hidden states based on camera movement while preserving selected hidden states
    in the memory slot to build a global map. The memory stores the global information
    of the entire sequence, allowing refinements on previous results. Finally, the
    refining module estimates each view’s absolute pose, allowing previously refined
    outputs to pass through recurrent units, thus improving the next estimate.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉里程计（VO）中，经典的基于学习的方法将VO任务视为纯粹的跟踪问题，通过从图像片段中恢复相机姿态来实现，这会导致错误的积累。这些方法往往忽视了缓解累积误差所需的关键全局信息。然而，在端到端系统中有效地保存这些信息是具有挑战性的。注意力机制代表了一种在该领域尚未被充分探索的替代方法，用于缓解这些不足。Xue等人[[534](#bib.bib534)]提出了一种自适应记忆方法，以避免网络的灾难性遗忘。他们的框架主要由一个记忆、一个记忆模块和一个细化模块组成，如图[22](#S5.F22
    "图22 ‣ 5.6 机器人 ‣ 5 应用 ‣ 注意力，请！深度学习中神经注意力模型的调查")b)所示。首先，它通过相机运动选择主要的隐藏状态，同时将选定的隐藏状态保存在记忆槽中以构建全局地图。记忆存储了整个序列的全局信息，允许对先前结果进行细化。最后，细化模块估计每个视图的绝对姿态，使得先前细化的输出可以通过递归单元，从而改进下一个估计。
- en: Another common problem in VO classical approaches is selecting the features
    to derive ego-motion between consecutive frames. In scenes, there are dynamic
    objects and non-textured surfaces that generate inconsistencies in the estimation
    of movement. Recently, self-attention mechanisms have been successfully employed
    in dynamic reweighting of features, and in the semantic selection of image regions
    to extract more refined egomotion [[536](#bib.bib536)] [[537](#bib.bib537)] [[538](#bib.bib538)].
    Additionally, self-attentive neural networks have been used to replace traditional
    recurrent networks that consume training time and are inaccurate in the temporal
    integration of long sequences [[539](#bib.bib539)].
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在VO经典方法中，另一个常见问题是选择特征以导出连续帧之间的自我运动。在场景中，动态物体和无纹理表面会导致运动估计的不一致。最近，自注意力机制在动态特征重加权和语义选择图像区域以提取更精细的自我运动方面取得了成功[[536](#bib.bib536)]
    [[537](#bib.bib537)] [[538](#bib.bib538)]。此外，自注意力神经网络已被用于替代传统的递归网络，这些传统网络在时间序列的长序列的训练时间上消耗较多且不准确[[539](#bib.bib539)]。
- en: '![Refer to caption](img/c3efb3033ac966491d220948288be9df.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c3efb3033ac966491d220948288be9df.png)'
- en: 'Figure 22: Illustration of deep visual odometry with Adaptive Memory [[534](#bib.bib534)]
    and Crowd-Robot Interaction [[542](#bib.bib542)] for navigation. a) The deep visual
    odometry with adaptive memory framework. This framework introduces two important
    components called remembering and refining, as opposed to classic frameworks that
    treat VO as just a tracking task. Remembering preserves long-time information
    by adopting an adaptive context selection, and refining improves previous outputs
    using spatial-temporal feature reorganization mechanism. b) Crowd-Robot Interaction
    network. The network consists of three modules: interaction, pooling, and planning.
    Interaction extracts robot-human interactions in crowds. Pooling aggregates all
    interaction information, and planning estimates the state’s value based on robots
    and humans for navigation in crowds.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：展示了用于导航的深度视觉里程计与自适应记忆[[534](#bib.bib534)]以及人群-机器人互动[[542](#bib.bib542)]。a)
    带有自适应记忆框架的深度视觉里程计。该框架引入了两个重要的组件：记忆和细化，而与传统框架只将视觉里程计视为跟踪任务不同。记忆通过采用自适应上下文选择来保存长期信息，而细化则利用时空特征重组机制改进先前的输出。b)
    人群-机器人互动网络。该网络由三个模块组成：互动、池化和规划。互动模块提取人群中的机器人-人类互动信息。池化模块聚合所有互动信息，而规划模块基于机器人和人类估计状态值，用于在人群中进行导航。
- en: In human-robot interaction, Zang et al. [[544](#bib.bib544)] proposed a framework
    that interprets navigation instructions in natural language and finds a mapping
    of commands in an executable navigation plan. The attentional mechanisms correlate
    navigation instructions very efficiently with the commands to be executed by the
    robot in only one trainable end-to-end model, unlike the classic approaches that
    use decoupled training and external interference during the system’s operation.
    In control, existing applications mainly use manipulator robots in visual-motor
    tasks. Duan et al. [[533](#bib.bib533)] used attention to improve the model’s
    generalization capacity in imitation learning approaches with a complex manipulator
    arm. The objective is to build a one-shot learning system capable of successfully
    performing instances of tasks not seen in the training. Thus, it employs soft
    attention mechanisms to process a long sequence of (states, actions) demonstration
    pairs. Finally, Abolghasemi et al. [[85](#bib.bib85)] proposed a deep visual engine
    policy through task-focused visual attention to make the policy more robust and
    to prevent the robot from releasing manipulated objects even under physical attacks.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在人机交互中，Zang 等人[[544](#bib.bib544)] 提出了一个框架，该框架解释了自然语言中的导航指令，并在可执行的导航计划中找到命令的映射。注意机制将导航指令与机器人在仅一个可训练的端到端模型中需要执行的命令高效关联，不同于经典方法中使用解耦训练和系统操作期间的外部干扰。在控制方面，现有应用主要使用在视觉-运动任务中的操作机器人。Duan
    等人[[533](#bib.bib533)] 使用注意力机制在模仿学习方法中改进了复杂操作臂的模型泛化能力。其目标是建立一个一-shot 学习系统，能够成功执行训练中未见过的任务实例。因此，它使用软注意力机制处理长序列的（状态，动作）演示对。最后，Abolghasemi
    等人[[85](#bib.bib85)] 提出了通过任务聚焦视觉注意力的深度视觉引擎策略，以使策略更具鲁棒性，并防止机器人在物理攻击下释放操作物体。
- en: 5.7 Interpretability
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7 可解释性
- en: 'A long-standing criticism of neural network models is their lack of interpretability [[545](#bib.bib545)].
    Academia and industry have a great interest in the development of interpretable
    models mainly for the following aspects: 1) critical decisions: when critical
    decisions need to be made (e.i., medical analysis, stock market, autonomous cars),
    it is essential to provide explanations to increase the confidence of the specialist
    human results; 2) failure analysis: an interpretable model can retrospectively
    inspect where bad decisions were made and understand how to improve the system;
    3) verification: there is no evidence of the models’ robustness and convergence
    even with small errors in the test set. It is difficult to explain the influence
    of spurious correlations on performance and why the models are sometimes excellent
    in some test cases and flawed in others; and 4) model improvements: interpretability
    can guide improvements in the model’s structure if the results are not acceptable.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 对神经网络模型的长期批评之一是其缺乏可解释性[[545](#bib.bib545)]。学术界和工业界对可解释模型的发展非常感兴趣，主要出于以下几个方面：1)
    关键决策：当需要做出关键决策时（例如医疗分析、股市、自动驾驶汽车），提供解释对于增加专家对结果的信心至关重要；2) 失败分析：一个可解释的模型可以回顾性地检查在哪里做出了不良决策，并了解如何改进系统；3)
    验证：即使在测试集上出现小错误，也没有证据表明模型的鲁棒性和收敛性。很难解释虚假关联对性能的影响，以及为什么模型在某些测试案例中表现出色，而在其他测试案例中却存在缺陷；4)
    模型改进：如果结果不可接受，可解释性可以指导模型结构的改进。
- en: Attention as an interpretability tool is still an open discussion. For some
    researchers, it allows to inspect the models’ internal dynamics – the hypothesis
    is that the attentional weights’ magnitude is correlated with the data’s relevance
    for predicting the output. Li et al. [[545](#bib.bib545)] proposed a general methodology
    to analyze the effect of erasing particular representations of neural networks’
    input. When analyzing the effects of erasure, they found that attentional focuses
    are essential to understand networks’ internal functioning. In [[546](#bib.bib546)]
    the results showed that higher attentional weights generally contribute with more
    significant impact to the model’s decision, but multiple weights generally do
    not fully identify the most relevant representations for the final decision. In
    this investigation, the researchers concluded that attention is an ideal tool
    to identify which elements are responsible for the output but do not yet fully
    explain the model’s decisions.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力作为一种可解释性工具仍然是一个开放讨论的话题。对于一些研究者来说，它可以用来检查模型的内部动态——假设是注意力权重的大小与数据在预测输出时的相关性相关。Li
    等人[[545](#bib.bib545)] 提出了一个通用的方法论来分析抹去神经网络输入特定表征的效果。在分析抹除效果时，他们发现注意力焦点对于理解网络的内部运作至关重要。在[[546](#bib.bib546)]的研究中，结果表明较高的注意力权重通常对模型的决策有更显著的影响，但多个权重通常不能完全识别出对最终决策最相关的表征。在这项研究中，研究人员得出结论，注意力是识别哪些元素对输出负有责任的理想工具，但尚未完全解释模型的决策。
- en: 'Some studies have also shown that attention encodes linguistic notions relevant
    to understanding NLP models [[547](#bib.bib547)] [[548](#bib.bib548)] [[549](#bib.bib549)].
    However, Jain et al. [[550](#bib.bib550)] showed that although attention improves
    NLP results, its ability to provide transparency or significant explanations for
    the model’s predictions is questionable. Specifically, the researchers investigated
    the relationship between attentional weights and model results by answering the
    following questions: (i) to what extent do weights of attention correlate with
    metrics of the importance of features, specifically those resulting from gradient?
    Moreover, (ii) do different attentional maps produce different predictions? The
    results showed that the correlation between intuitive metrics about the features’
    importance (e.i., gradient approaches, erasure of features) and attentional weights
    is low in recurrent encoders. Besides, the selection of features other than the
    attentional distribution did not significantly impact the output as attentional
    weights exchanged at random also induced minimal output changes. The researchers
    also concluded that such results depend significantly on the type of architecture,
    given that feedforward encoders obtained more coherent relationships between attentional
    weights and output than other models.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究还表明，注意力编码了理解自然语言处理模型相关的语言学概念[[547](#bib.bib547)] [[548](#bib.bib548)] [[549](#bib.bib549)]。然而，Jain
    等人[[550](#bib.bib550)] 表明，虽然注意力改善了自然语言处理的结果，但其提供模型预测透明性或重要解释的能力仍然值得怀疑。具体来说，研究人员通过回答以下问题来调查注意力权重与模型结果之间的关系：（i）注意力权重与特征重要性度量（特别是那些源于梯度的度量）相关的程度如何？此外，（ii）不同的注意力图是否会产生不同的预测？结果显示，在递归编码器中，关于特征重要性的直观度量（例如梯度方法、特征的抹除）与注意力权重之间的相关性较低。此外，除了注意力分布之外的特征选择对输出的影响不显著，因为注意力权重的随机交换也会引起最小的输出变化。研究人员还得出结论，这些结果在很大程度上取决于架构类型，因为前馈编码器在注意力权重和输出之间获得了比其他模型更一致的关系。
- en: Vashishth et al. [[551](#bib.bib551)] systematically investigated explanations
    for the researchers’ distinct views through experiments on NLP tasks with single
    sequence models, pair sequence models, and self-attentive neural networks. The
    experiments showed that attentional weights in single sequences tasks work like
    gates and do not reflect the reasoning behind the model’s prediction, justifying
    the observations made by Jain et al. [[550](#bib.bib550)]. However, for pair sequence
    tasks, attentional weights were essential to explaining the model’s reasoning.
    Manual tests have also shown that attentional weights are highly similar to the
    manual assessment of human observers’ attention. Recently, Wiegreffe et al. [[552](#bib.bib552)]
    also investigated these issues in depth through an extensive protocol of experiments.
    The authors observed that attention as an explanation depends on the definition
    of explainability considered. If the focus is on plausible explainability, the
    authors concluded that attention could help interpret model insights. However,
    if the focus is a faithful and accurate interpretation of the link that the model
    establishes between inputs and outputs, results are not always positive. These
    authors confirmed that good alternatives distributions could be found in LSTMs
    and classification tasks, as hypothesized by Jain et al. [[550](#bib.bib550)].
    However, in some experiments, adversarial training’s alternative distributions
    had poor performances concerning attention’s traditional mechanisms. These results
    indicate that the attention mechanisms trained mainly in RNNs learn something
    significant about the relationship between tokens and prediction, which cannot
    be easily hacked. In the end, they showed that attention efficiency as an explanation
    depends on the data set and the model’s properties.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: Vashishth 等人[[551](#bib.bib551)] 通过对 NLP 任务中的单序列模型、对序列模型和自注意力神经网络进行实验，系统地调查了研究人员不同观点的解释。实验表明，单序列任务中的注意力权重像门控一样工作，并不反映模型预测背后的推理，这也解释了
    Jain 等人[[550](#bib.bib550)] 的观察。然而，对于对序列任务，注意力权重对解释模型的推理至关重要。手动测试也显示，注意力权重与人工观察者的注意力评估高度相似。最近，Wiegreffe
    等人[[552](#bib.bib552)] 通过广泛的实验协议深入调查了这些问题。作者观察到，注意力作为解释的有效性取决于所考虑的可解释性的定义。如果重点是可信的可解释性，作者得出结论，注意力可以帮助解释模型的见解。然而，如果重点是模型在输入和输出之间建立的链接的真实和准确的解释，结果并不总是积极的。这些作者确认，LSTM
    和分类任务中可以找到好的替代分布，正如 Jain 等人[[550](#bib.bib550)] 所假设的。然而，在一些实验中，对抗训练的替代分布在注意力的传统机制方面表现不佳。这些结果表明，主要在
    RNN 中训练的注意力机制学习到了关于标记和预测之间关系的某些重要信息，这些信息不容易被破解。最终，他们显示出注意力作为解释的效率取决于数据集和模型的特性。
- en: 6 Trends and Opportunities
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 趋势与机会
- en: Attention has been one of the most influential ideas in the Deep Learning community
    in recent years, with several profound advances, mainly in computer vision and
    natural language processing. However, there is much space to grow, and many contributions
    are still to appear. In this section, we highlight some gaps and opportunities
    in this scenario.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力在近年来深度学习领域中一直是最具影响力的观点之一，主要在计算机视觉和自然语言处理方面取得了几项深刻的进展。然而，仍有很大的发展空间，许多贡献尚待出现。在这一部分，我们强调了这一情境中的一些差距和机遇。
- en: 6.1 End-To-End Attention models
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 端到端注意力模型
- en: Over the past eight years, most of the papers published in the literature have
    involved attentional mechanisms. Models that are state of the art in DL use attention.
    Specifically, we note that end-to-end attention networks, such as Transformers [[37](#bib.bib37)]
    and Graph Attention Networks [[97](#bib.bib97)], have been expanding significantly
    and have been used successfully in tasks across multiple domains (Section [2](#S2
    "2 Overview ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")).
    In particular, Transformer has introduced a new form of computing in which the
    neural network’s core is fully attentional. Transformer-based language models
    like BERT [[286](#bib.bib286)], GPT2 [[116](#bib.bib116)], and GPT3 [[117](#bib.bib117)]
    are the most advanced language models in NLP. Image GPT [[120](#bib.bib120)] has
    recently revolutionized the results of unsupervised learning in imaging. It is
    already a trend to propose Transfomer based models with sparse attentional mechanisms
    to reduce the Transformer’s complexity from quadratic to linear and use attentional
    mechanisms to deal with multimodality in GATs. However, Transformer is still an
    autoregressive architecture in the decoder and does not use other cognitive mechanisms
    such as memory. As research in attention and DL is still at early stages, there
    is still plenty of space in the literature for new attentional mechanisms, and
    we believe that end-to-end attention architectures might be very influential in
    Deep Learning’s future models.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去八年中，大多数文献中发表的论文涉及注意力机制。最先进的深度学习模型使用注意力机制。特别是，我们注意到端到端注意力网络，如Transformers[[37](#bib.bib37)]和图注意力网络[[97](#bib.bib97)]，已显著扩展，并在多个领域的任务中取得了成功（第[2](#S2
    "2 Overview ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")节）。特别是，Transformer引入了一种新形式的计算，其中神经网络的核心完全是注意力机制。基于Transformer的语言模型，如BERT[[286](#bib.bib286)]、GPT2[[116](#bib.bib116)]和GPT3[[117](#bib.bib117)]，是NLP中最先进的语言模型。Image
    GPT[[120](#bib.bib120)]最近在图像中的无监督学习结果上取得了革命性进展。提出基于Transformer的稀疏注意力机制模型，以将Transformer的复杂性从平方级降至线性级，并使用注意力机制处理GATs中的多模态性已经成为一种趋势。然而，Transformer仍然是解码器中的自回归架构，并未使用其他认知机制，如记忆。由于注意力和深度学习的研究仍处于早期阶段，文献中仍有大量空间用于新的注意力机制，我们相信端到端注意力架构可能在深度学习未来的模型中具有重要影响。
- en: 6.2 Learning Multimodality
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 学习多模态性
- en: Attention has played a crucial role in the growth of learning from multimodal
    data. Multimodality is extremely important for learning complex tasks. Human beings
    use different sensory signals all the time to interpret situations and decide
    which action to take. For example, while recognizing emotions, humans use visual
    data, gestures, and voice tones to analyze feelings. Attention allowed models
    to learn the synergistic relationship between the different sensory data, even
    if they are not synchronized, allowing the development of increasingly complex
    applications mainly in emotion recognition, [[75](#bib.bib75)], feelings [[461](#bib.bib461)],
    and language-based image generation [[121](#bib.bib121)]. We note that multimodal
    applications are continually growing in recent years. However, most research efforts
    are still focused on relating a pair of sensory data, mostly visual and textual
    data. Architectures that can scale easily to handle more than one pair of sensors
    are not yet widely explored. Multimodal learning exploring voice data, RGBD images,
    images from monocular cameras, data from various sensors, such as accelerometers,
    gyroscopes, GPS, RADAR, biomedical sensors, are still scarce in the literature.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力在从多模态数据学习的增长中发挥了关键作用。多模态性对学习复杂任务极为重要。人类在解释情况和决定采取何种行动时，始终使用不同的感官信号。例如，在识别情感时，人类使用视觉数据、手势和语音语调来分析情感。注意力使模型能够学习不同感官数据之间的协同关系，即使它们未同步，从而推动了主要在情感识别[[75](#bib.bib75)]、情感[[461](#bib.bib461)]和基于语言的图像生成[[121](#bib.bib121)]等领域的应用的发展。我们注意到，多模态应用近年来不断增长。然而，大多数研究工作仍然集中在关联一对感官数据，主要是视觉和文本数据。可以轻松扩展以处理多个传感器对的架构尚未被广泛探索。文献中对语音数据、RGBD图像、单目相机图像、来自各种传感器（如加速度计、陀螺仪、GPS、雷达、生物医学传感器）的数据进行多模态学习的研究仍然很少。
- en: 6.3 Cognitive Elements
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 认知元素
- en: Attention proposed a new way of thinking about the architecture of neural networks.
    For many years, the scientific community neglected using other cognitive elements
    in neural network architectures, such as memory and logic flow control. Attention
    has made possible including in neural networks other elements that are widely
    important in human cognition. Memory Networks [[38](#bib.bib38)], and Neural Turing
    Machine [[86](#bib.bib86)] are essential approaches in which attention makes updates
    and recoveries in external memory. However, research on this topic is at an early
    stage. The Neural Turing Machine has not yet been explored in several application
    domains, being used only in simple datasets for algorithmic tasks, with a slow
    and unstable convergence. We believe that there is plenty of room to explore the
    advantages of NTM in a wide range of problems and develop more stable and efficient
    models. Still, Memory Networks [[38](#bib.bib38)] presents some developments (Section [2](#S2
    "2 Overview ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")),
    but few studies explore the use of attention to managing complex and hierarchical
    structures of memory. Attention to managing different memory types simultaneously
    (i.e., working memory, declarative, non-declarative, semantic, and long and short
    term) is still absent in the literature. To the best of our knowledge, the most
    significant advances have been made in Dynamic Memory Networks [[93](#bib.bib93)]
    with the use of episodic memory. Another open challenge is how to use attention
    to plug external knowledge into memory and make training faster. Finally, undoubtedly
    one of the biggest challenges still lies in including other human cognition elements
    such as imagination, reasoning, creativity, and consciousness working in harmony
    with attentional structures.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '**Attention** 提出了一个新的思维方式来构建神经网络的架构。多年来，科学界忽视了在神经网络架构中使用其他认知元素，如记忆和逻辑流控制。**Attention**
    使得在神经网络中包含人类认知中广泛重要的其他元素成为可能。**Memory Networks** [[38](#bib.bib38)] 和 **Neural
    Turing Machine** [[86](#bib.bib86)] 是一些关键方法，其中 **Attention** 在外部记忆中进行更新和恢复。然而，这一领域的研究仍处于早期阶段。**Neural
    Turing Machine** 在多个应用领域尚未得到探索，只在简单的数据集上用于算法任务，且收敛速度慢且不稳定。我们相信在各种问题中探索 **NTM**
    的优势并开发更稳定高效的模型还有很大的空间。尽管 **Memory Networks** [[38](#bib.bib38)] 展示了一些进展（见第[2](#S2
    "2 Overview ‣ Attention, please! A survey of Neural Attention Models in Deep Learning)节），但很少有研究探索使用
    **Attention** 管理复杂和层次化的记忆结构。在同时管理不同记忆类型（即工作记忆、声明性记忆、非声明性记忆、语义记忆以及长期和短期记忆）方面的 **Attention**
    在文献中仍然缺失。据我们所知，最显著的进展是在 **Dynamic Memory Networks** [[93](#bib.bib93)] 中使用了情节记忆。另一个开放性挑战是如何利用
    **Attention** 将外部知识融入记忆并加快训练速度。最后，无疑，最大挑战之一仍然是将其他人类认知元素如想象力、推理、创造力和意识与 **Attention**
    结构和谐地结合起来。'
- en: 6.4 Computer Vision
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 计算机视觉
- en: Recurrent Attention Models (RAM) [[46](#bib.bib46)] introduced a new form of
    image computing using glimpses and hard attention. The architecture is simple,
    scalable, and flexible. Spatial Transformer (STN) [[59](#bib.bib59)] presented
    a simple module for learning image transformations that can be easily plugged
    into different architectures. We note that RAM has a high potential for many tasks
    in which convolutional neural networks have difficulties, such as large, high-resolution
    images. However, currently, RAM has been explored with simple datasets. We believe
    that it is interesting to validate RAM in complex classification and regression
    tasks. Another proposal is to add new modules to the architecture, such as memory,
    multimodal glimpses, and scaling. It is interesting to explore STN in conjunction
    with RAM in classification tasks or use STN to predict transformations between
    sets of images. RAM aligned with STN can help address robostusnees to spatial
    transformation, learn the system dynamics in Visual Odometry tasks, enhance multiple-instance
    learning, addressing multiple view-points.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '**Recurrent Attention Models (RAM)** [[46](#bib.bib46)] 引入了一种新的图像计算形式，使用了**glimpses**
    和 **hard attention**。该架构简单、可扩展且灵活。**Spatial Transformer (STN)** [[59](#bib.bib59)]
    提出了一个简单的模块，用于学习图像变换，可以轻松地集成到不同的架构中。我们注意到 **RAM** 在许多卷积神经网络难以处理的任务中具有很高的潜力，例如大型高分辨率图像。然而，目前
    **RAM** 主要在简单的数据集上进行研究。我们认为在复杂的分类和回归任务中验证 **RAM** 是有趣的。另一个建议是向架构中添加新模块，如记忆、多模态**glimpses**
    和缩放。将 **STN** 与 **RAM** 结合在分类任务中进行探索，或使用 **STN** 预测图像集之间的变换是很有趣的。**RAM** 与 **STN**
    的结合可以帮助解决对空间变换的鲁棒性问题，学习视觉里程计任务中的系统动态，增强多实例学习，处理多视角问题。'
- en: 6.5 Capsule Neural Network
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 胶囊神经网络
- en: Capsule networks (CapsNets), a new class of deep neural network architectures
    proposed recently by Hinton et al. [[553](#bib.bib553)], have shown excellent
    performance in many fields, particularly in image recognition and natural language
    processing. However, few studies in the literature implement attention in capsule
    networks. AR CapsNet [[554](#bib.bib554)] implements a dynamic routing algorithm
    where routing between capsules is made through an attention module. The attention
    routing is a fast forward-pass while keeping spatial information. DA-CapsNet [[555](#bib.bib555)]
    proposes a dual attention mechanism, the first layer is added after the convolution
    layer, and the second layer is added after the primary caps. SACN [[556](#bib.bib556)]
    is the first model that incorporates the self-attention mechanism as an integral
    layer. Recently, Tsai. et al. [[557](#bib.bib557)] introduced a new attentional
    routing mechanism in which a daughter capsule is routed to a parent capsule-based
    between the father’s state and the daughter’s vote. We particularly believe that
    attention is essential to improve the relational and hierarchical nature that
    CapsNets propose. The development of works aiming at the dynamic attentional routing
    of the capsules and incorporating attentional capsules of self-attention, soft
    and hard attention can bring significant results to current models.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 胶囊网络（CapsNets）是由Hinton等人最近提出的一类新的深度神经网络架构[[553](#bib.bib553)]，在许多领域，特别是图像识别和自然语言处理方面表现出色。然而，文献中很少有研究在胶囊网络中实现注意力机制。AR
    CapsNet[[554](#bib.bib554)]实现了一种动态路由算法，通过注意力模块在胶囊之间进行路由。注意力路由是一种快速的前向传递，同时保持空间信息。DA-CapsNet[[555](#bib.bib555)]提出了一种双重注意力机制，第一个层级添加在卷积层之后，第二个层级添加在主胶囊之后。SACN[[556](#bib.bib556)]是第一个将自注意力机制作为一个整体层的模型。最近，Tsai等人[[557](#bib.bib557)]介绍了一种新的注意力路由机制，其中子胶囊根据父胶囊的状态和子胶囊的投票来路由到父胶囊。我们特别认为注意力机制对于提高胶囊网络提出的关系性和层次性至关重要。旨在胶囊动态注意力路由的研究工作，以及将自注意力、软注意力和硬注意力的胶囊融入其中，可以为当前模型带来显著成果。
- en: 6.6 Neural-Symbolic Learning and Reasoning
  id: totrans-390
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 神经符号学习与推理
- en: According to LeCun [[558](#bib.bib558)] one of the great challenges of artificial
    intelligence is to combine the robustness of connectionist systems (i.e., neural
    networks) with symbolic representation to perform complex reasoning tasks. While
    symbolic representation is highly recursive and declarative, neural networks encode
    knowledge implicitly by adjusting weights. For many decades exploring the fusion
    between connectionist and symbolic systems has been overlooked by the scientific
    community. Only over the past decade, research with hybrid approaches using the
    two families of AI methodologies has grown again. Approaches such as statistical
    relational learning (SRL) [[559](#bib.bib559)] and neural-symbolic learning [[560](#bib.bib560)]
    were proposed. Recently, attention mechanisms have been integrated into some neural-symbolic
    models, the development of which is still at an early stage. Memory Networks [[38](#bib.bib38)]
    (Section [2](#S2 "2 Overview ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")) and Neural Turing Machine [[86](#bib.bib86)] (Section [2](#S2
    "2 Overview ‣ Attention, please! A survey of Neural Attention Models in Deep Learning"))
    were the first initiatives to include reasoning in deep connectionist models.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 根据LeCun[[558](#bib.bib558)]的说法，人工智能面临的一个重大挑战是将连接主义系统（即神经网络）的鲁棒性与符号表示结合起来，以执行复杂的推理任务。虽然符号表示高度递归和声明性，神经网络则通过调整权重隐式地编码知识。几十年来，科学界忽视了连接主义和符号系统之间融合的探索。只有在过去十年中，使用这两种AI方法的混合方法的研究才重新增长。提出了如统计关系学习（SRL）[[559](#bib.bib559)]和神经符号学习[[560](#bib.bib560)]等方法。最近，注意力机制已被集成到一些神经符号模型中，这些模型的发展仍处于初期阶段。记忆网络[[38](#bib.bib38)]（第[2](#S2
    "2 Overview ‣ Attention, please! A survey of Neural Attention Models in Deep Learning)节）和神经图灵机[[86](#bib.bib86)]（第[2](#S2
    "2 Overview ‣ Attention, please! A survey of Neural Attention Models in Deep Learning)节）是首次将推理纳入深度连接主义模型的倡议。
- en: In the context of neural logic programming, attention has been exploited to
    reason about knowledge graphs or memory structures to combine the learning of
    parameters and structures of logical rules. Neural Logic Programming [[561](#bib.bib561)]
    uses attention on a neural controller that learns to select a subset of operations
    and memory content to execute first-order rules. Logic Attention Networks [[562](#bib.bib562)]
    facilitates inductive KG embedding and uses attention to aggregate information
    coming from graph neighbors with rules and attention weights. A pGAT [[563](#bib.bib563)]
    uses attention to knowledge base completion, which involves the prediction of
    missing relations between entities in a knowledge graph. While producing remarkable
    advances, recent approaches to reasoning with deep networks do not adequately
    address the task of symbolic reasoning. Current efforts are only about using attention
    to ensure efficient memory management. We believe that attention can be better
    explored to understand which pieces of knowledge are relevant to formulate a hypothesis
    to provide a correct answer, which are rarely present in current neural systems
    of reasoning.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经逻辑编程的背景下，已利用**注意力**来推理知识图谱或记忆结构，以结合逻辑规则的参数和结构学习。神经逻辑编程 [[561](#bib.bib561)]
    在神经控制器上使用注意力，该控制器学习选择一组操作和记忆内容来执行一阶规则。逻辑注意力网络 [[562](#bib.bib562)] 促进了归纳KG嵌入，并使用注意力来聚合来自图邻居的信息以及规则和注意力权重。pGAT [[563](#bib.bib563)]
    利用注意力进行知识库补全，涉及预测知识图谱中实体之间缺失的关系。尽管取得了显著进展，但最近的深度网络推理方法尚未充分解决符号推理的任务。目前的工作仅集中于利用注意力确保高效的记忆管理。我们相信，**注意力**可以更好地探索，以理解哪些知识片段对形成假设提供正确答案是相关的，这在当前的神经推理系统中很少存在。
- en: 6.7 Incremental Learning
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7 增量学习
- en: Incremental learning is one of the challenges for the DL community in the coming
    years. Machine learning classifiers are trained to recognize a fixed set of classes.
    However, it is desirable to have the flexibility to learn additional classes with
    limited data without re-training in the complete training set. Attention can significantly
    contribute to advances in the area and has been little explored. Ren et al. [[356](#bib.bib356)]
    were the first to introduce seminal work in the area. They use Attention Attractor
    Networks to regularize the learning of new classes. In each episode, a set of
    new weights is trained to recognize new classes until they converge. Attention
    Attractor Networks helps recognize new classes while remembering the classes beforehand
    without revising the original training set.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 增量学习是未来几年DL社区面临的挑战之一。机器学习分类器被训练以识别固定的类别集合。然而，希望能够在不重新训练整个训练集的情况下，有灵活性地学习额外的类别。**注意力**可以显著推动这一领域的进展，但尚未得到充分探索。Ren等人 [[356](#bib.bib356)]
    是首批在这一领域引入开创性工作的研究者。他们使用注意力吸引网络来规范新类别的学习。在每个阶段，训练一组新的权重以识别新类别，直到它们收敛。注意力吸引网络帮助识别新类别，同时记住之前的类别，而无需修订原始训练集。
- en: 6.8 Credit Assignment Problem (CAP)
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8 信用分配问题 (CAP)
- en: In Reinforcement Learning (RL), an action that leads to a higher final cumulative
    reward should have more value. Therefore, more "credit" should be assigned to
    it than an action that leads to a lower final reward. However, measuring the individual
    contribution of actions to future rewards is not simple and has been studied by
    the RL community for years. There are at least three variations of the CAP problem
    that have been explored. The temporal CAP refers to identifying which actions
    were useful or useless in obtaining the final feedback. The structural CAP seeks
    to find the set of sensory situations in which a given sequence of actions will
    produce the same result. Transfer CAP refers to learning how to generalize a sequence
    of actions in tasks. Few works in the literature explore attention to the CAP
    problem. We believe that attention will be fundamental to advance credit assignment
    research. Recently, Ferret et al. [[564](#bib.bib564)] started the first research
    in the area by proposing a seminal work with attention to learn how to assign
    credit through a separate supervised problem and transfer credit assignment capabilities
    to new environments.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，导致最终累计奖励更高的行动应具有更高的价值。因此，应该为其分配比导致较低最终奖励的行动更多的“信用”。然而，衡量行动对未来奖励的个体贡献并不简单，RL
    社区已经研究了多年。至少有三种 CAP 问题的变体已被探讨。时间 CAP 指的是识别哪些行动在获得最终反馈中是有用的或无用的。结构 CAP 旨在寻找在某一感官情境中，给定行动序列将产生相同结果的情况。迁移
    CAP 指的是学习如何在任务中概括行动序列。文献中对 CAP 问题的关注很少。我们认为，注意力将是推动信用分配研究的基础。最近，Ferret 等人[[564](#bib.bib564)]
    开始了该领域的首项研究，提出了一个开创性的工作，通过一个独立的监督问题来学习如何分配信用，并将信用分配能力转移到新环境中。
- en: 6.9 Attention and Interpretability
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9 注意力与可解释性
- en: There are investigations to verify attention as an interpretability tool. Some
    recent studies suggest that attention can be considered reliable for this purpose.
    However, other researchers criticize the use of attention weights as an analytical
    tool. Jain and Wallace [[550](#bib.bib550)] proved that attention is not consistent
    with other explainability metrics and that it is easy to create distributions
    similar to those of the trained model but to produce a different result. Their
    conclusion is that changing attention weights does not significantly affect the
    model’s prediction, contrary to research by Rudin [[565](#bib.bib565)] and Riedl [[566](#bib.bib566)]
    (Section [5.7](#S5.SS7 "5.7 Interpretability ‣ 5 Applications ‣ Attention, please!
    A survey of Neural Attention Models in Deep Learning")). On the other hand, some
    studies have found how attention in neural models captures various notions of
    syntax and co-reference [[547](#bib.bib547)] [[549](#bib.bib549)] [[548](#bib.bib548)].
    Amid such confusion, Vashishth et al. [[551](#bib.bib551)] investigated attention
    more systematically. They attempted to justify the two types of observation (that
    is, when attention is interpretable and not), employing various experiments on
    various NLP tasks. The conclusion was that attention weights are interpretable
    and are correlated with metrics of the importance of features. However, this is
    only valid for cases where weights are essential for predicting models and cannot
    simply be reduced to a gating unit. Despite the existing studies, there are numerous
    research opportunities to develop systematic methodologies to analyze attention
    as an interpretability tool. The current conclusions are based on experiments
    with few architectures in a specific set of applications in NLP.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 有研究验证注意力作为可解释性工具的有效性。一些最近的研究表明，注意力可以被认为是可靠的。然而，其他研究者批评使用注意力权重作为分析工具。Jain 和 Wallace
    [[550](#bib.bib550)] 证明注意力与其他可解释性度量不一致，并且很容易创建与训练模型相似的分布，但产生不同的结果。他们的结论是，改变注意力权重不会显著影响模型的预测，这与
    Rudin [[565](#bib.bib565)] 和 Riedl [[566](#bib.bib566)] 的研究（第 [5.7](#S5.SS7 "5.7
    Interpretability ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning") 节）相反。另一方面，一些研究发现神经模型中的注意力如何捕捉各种句法和共指概念[[547](#bib.bib547)]
    [[549](#bib.bib549)] [[548](#bib.bib548)]。在这种困惑中，Vashishth 等人[[551](#bib.bib551)]
    更系统地研究了注意力。他们尝试通过在各种 NLP 任务中进行实验来证明两种观察类型（即，当注意力是可解释的和不可解释的）。结论是，注意力权重是可解释的，并与特征重要性的度量相关。然而，这仅适用于权重对预测模型至关重要的情况，不能简单地减少为一个门控单元。尽管已有研究，但仍有大量研究机会来开发系统的方法来分析注意力作为可解释性工具。目前的结论基于少量架构在特定的
    NLP 应用中的实验。
- en: 6.10 Unsupervised Learning
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.10 无监督学习
- en: In the last decade, unsupervised learning has also been recognized as one of
    the most critical challenges of machine learning since, in fact, human learning
    is mainly unsupervised [[558](#bib.bib558)]. Some works have recently successfully
    explored attention within purely unsupervised models. In GANs, attention has been
    used to improve the global perception of a model (i.e., the model learns which
    part of the image gives more attention to the others). SAGAN [[119](#bib.bib119)]
    was one of the pioneering efforts to incorporate self-attention in Convolutional
    Gans to improve the quality of the images generated. Image Transformer is an end-to-end
    attention network created to generate high-resolution images that significantly
    surpassed state-of-the-art in ImageNet in 2018\. AttGan [[567](#bib.bib567)] uses
    attention to easily take advantage of multimodality to improve the generation
    of images. Combining a region of the image with a corresponding part of the word-context
    vector helps to generate new features with more details in each stage.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，无监督学习也被认定为机器学习中最关键的挑战之一，因为实际上，人类学习主要是无监督的[[558](#bib.bib558)]。一些研究最近成功地在纯无监督模型中探索了注意力机制。在生成对抗网络（GANs）中，注意力被用来改善模型的全球感知（即模型学习图像的哪些部分比其他部分更受关注）。SAGAN
    [[119](#bib.bib119)] 是在卷积生成对抗网络中引入自注意力的开创性工作之一，以提高生成图像的质量。图像转换器（Image Transformer）是一个端到端的注意力网络，用于生成高分辨率图像，在2018年显著超越了ImageNet的现有技术水平。AttGan
    [[567](#bib.bib567)] 利用注意力机制轻松利用多模态性来改善图像生成。将图像的一个区域与相应的词语上下文向量结合，有助于在每个阶段生成更具细节的新特征。
- en: Attention has still been little explored to make generative models simpler,
    scalable, and more stable. Perhaps the only approach in the literature to explore
    such aspects more deeply is DRAW [[36](#bib.bib36)], which presents a sequential
    and straightforward way to generate images, being possible to refine image patches
    while more information is captured sequentially. However, the architecture was
    tested only in simple datasets, leaving open spaces for new developments. There
    is not much exploration of attention using autoencoders. Using VAEs, Bornschein
    et al. [[163](#bib.bib163)] increased the generative models with external memory
    and used an attentional system to address and retrieve the corresponding memory
    content.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在简化、扩展和提高生成模型稳定性方面仍然探索较少。也许文献中唯一较深入探索这些方面的方法是DRAW [[36](#bib.bib36)]，它提供了一种顺序而直接的图像生成方式，可以在捕捉到更多信息的同时细化图像补丁。然而，该架构仅在简单数据集上进行了测试，留下了新的发展空间。对使用自编码器的注意力机制的探索还不多。使用变分自编码器（VAEs），Bornschein等人[[163](#bib.bib163)]
    增强了具有外部记忆的生成模型，并使用注意力系统来处理和检索相应的记忆内容。
- en: In Natural Language Processing, attention is explored in unsupervised models
    mainly to extract aspects of sentiment analysis. It is also used within autoencoders
    to generate semantic representations of phrases [[568](#bib.bib568)][[569](#bib.bib569)].
    However, most studies still use supervised learning attention, and few approaches
    still focus on computer vision and NLP. Therefore, we believe that there is still
    a great path for research and exploration of attention in the unsupervised context,
    particularly we note that the construction of purely bottom-up attentional systems
    is not explored in the literature and especially in the context of unsupervised
    learning, these systems can great value, accompanied by inhibition and return
    mechanisms.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理领域，注意力机制在无监督模型中主要用于提取情感分析的方面。它还在自编码器中用于生成短语的语义表示[[568](#bib.bib568)][[569](#bib.bib569)]。然而，大多数研究仍然使用监督学习的注意力机制，且很少有方法关注计算机视觉和自然语言处理。因此，我们认为在无监督背景下，特别是在无监督学习的背景中，构建纯粹自下而上的注意力系统还有很大的研究和探索空间，这些系统可以带来巨大的价值，并伴有抑制和反馈机制。
- en: 6.11 New Tasks and Robotics
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.11 新任务与机器人技术
- en: Although attention has been used in several domains, there are still potential
    applications that can benefit from it. The prediction of time series, medical
    applications, and robotics applications are little-explored areas of the literature.
    Predicting time series becomes challenging as the size of the series increases.
    Attentional neural networks can contribute significantly to improving results.
    Specifically, we believe that exploring RAM [[46](#bib.bib46)] with multiple glimpses
    looking at different parts of the series or different frequency ranges can introduce
    a new way of computing time series. In medical applications, there are still few
    works that explore biomedical signals in attentional architectures. There are
    opportunities to apply attention to all applications, ranging from segmentation
    and image classification, support for disease diagnosis to support treatments
    such as Parkinson’s, Alzheimer’s, and other chronic diseases.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管注意力机制已在多个领域得到应用，但仍有潜在的应用可以从中受益。时间序列预测、医学应用和机器人应用是文献中尚未深入探索的领域。随着序列长度的增加，时间序列预测变得具有挑战性。注意力神经网络可以显著提高结果。具体而言，我们认为，探索RAM
    [[46](#bib.bib46)] 通过多个视角观察序列的不同部分或不同频率范围，可能会引入一种新的时间序列计算方法。在医学应用中，尚有少量工作探索了在注意力架构中的生物医学信号。将注意力应用于所有应用领域，包括图像分割和分类、疾病诊断支持以及帕金森病、阿尔茨海默病和其他慢性病等治疗支持，仍有很大机会。
- en: For robotics, there are countless opportunities. For years the robotics community
    has been striving for robots to perform tasks in a safe manner and with behaviors
    closer to humans. However, DL techniques need to cope well with multimodality,
    active learning, incremental learning, identify unknowns, uncertainty estimation,
    object and scene semantics, reasoning, awareness, and planning for this task.
    Architectures like RAM [[46](#bib.bib46)], DRAW [[36](#bib.bib36)] and Transformer [[37](#bib.bib37)]
    can contribute a lot by being applied to visual odometry, SLAM and mapping tasks.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器人技术来说，机会无处不在。多年来，机器人社区一直在努力使机器人以安全的方式执行任务，并展现出更接近人类的行为。然而，深度学习技术需要很好地处理多模态、主动学习、增量学习、识别未知事物、不确定性估计、物体和场景语义、推理、意识以及规划等任务。像RAM
    [[46](#bib.bib46)]、DRAW [[36](#bib.bib36)] 和 Transformer [[37](#bib.bib37)] 这样的架构可以通过应用于视觉里程计、SLAM
    和地图绘制任务做出巨大贡献。
- en: 7 Conclusions
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this survey, we presented a systematic review of the literature on attention
    in Deep Learning to overview the area from its main approaches, historical landmarks,
    uses of attention, applications, and research opportunities. In total, we critically
    analyzed more than 600 relevant papers published from 2014 to the present. To
    the best of our knowledge, this is the broadest survey in the literature, given
    that most of the existing reviews cover only particular domains with a slightly
    smaller number of reviewed works. Throughout the paper, we have identified and
    discussed the relationship between attention mechanisms in established deep neural
    network models, emphasizing CNNs, RNNs, and generative models. We discussed how
    attention led to performance gains, improvements in computational efficiency,
    and a better understanding of networks’ knowledge. We present an exhaustive list
    of application domains discussing the main benefits of attention, highlighting
    each domain’s most representative instances. We also showed recent discussions
    about attention on the explanation and interpretability of models, a branch of
    research that is widely discussed today. Finally, we present what we consider
    trends and opportunities for new developments around attentive models. We hope
    that this survey will help the audience understand the different existing research
    directions and provide significant scientific community background in generating
    future research.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们提供了关于深度学习中注意力机制的文献系统回顾，以概述该领域的主要方法、历史里程碑、注意力的使用、应用及研究机会。总的来说，我们批判性地分析了2014年至今发表的600多篇相关论文。根据我们的了解，这是文献中最广泛的调查，因为现有的大多数评论仅涵盖了特定领域，并且审阅的作品数量较少。在本文中，我们识别并讨论了在建立的深度神经网络模型中注意力机制的关系，重点是CNNs、RNNs和生成模型。我们讨论了注意力如何带来性能提升、计算效率改进以及对网络知识的更好理解。我们呈现了应用领域的详尽清单，讨论了注意力的主要好处，突出了每个领域中最具代表性的实例。我们还展示了关于模型解释性和可解释性的注意力机制的最新讨论，这是当前广泛讨论的研究分支。最后，我们呈现了我们认为围绕注意力模型的新发展趋势和机会。我们希望这项调查能帮助读者理解不同的研究方向，并为科学界在生成未来研究时提供重要背景。
- en: It is worth mentioning that our survey results from an extensive and exhaustive
    process of searching, filtering, and critical analysis of papers published between
    01/01/2014 until 15/02/2021 in the central publication repositories for machine
    learning and related areas. In total, we collected more than 20,000 papers. After
    successive automatic and manual filtering, we selected approximately 650 papers
    for critical analysis and more than 6,000 for quantitative analyses, which correspond
    mainly to identifying the main application domains, places of publication, and
    main architectures. For automatic filtering, we use keywords from the area and
    set up different combinations of filters to eliminate noise from psychology and
    classic computational visual attention techniques (i.e., saliency maps). In manual
    filtering, we separate the papers by year and define the originality and number
    of citations of the work as the main selection criteria. In the appendix, we provide
    our complete methodology and links to our search codes to facilitate improving
    future revisions on any topic in the area.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，我们的调查结果来源于对2014年01月01日到2021年02月15日之间在机器学习及相关领域的中央出版库中发表的论文进行的广泛而详尽的搜索、筛选和批判性分析。我们总共收集了超过20,000篇论文。经过连续的自动和手动筛选，我们选择了大约650篇论文进行批判性分析，并对超过6,000篇论文进行了定量分析，这些论文主要涉及识别主要应用领域、出版地点和主要架构。对于自动筛选，我们使用了来自该领域的关键词，并设置了不同的过滤器组合，以消除心理学和经典计算视觉注意力技术（即显著性图）的噪音。在手动筛选中，我们按年份分开论文，并将工作的原创性和引用数量作为主要选择标准。在附录中，我们提供了完整的方法论和搜索代码链接，以便于改进该领域任何主题的未来修订。
- en: We are currently complementing this survey with a theoretical analysis of the
    main neural attention models. This complementary survey will help to address an
    urgent need for an attentional framework supported by taxonomies based on theoretical
    aspects of attention, which predate the era of Deep Learning. The few existing
    taxonomies in the area do not yet use theoretical concepts and are challenging
    to extend to various architectures and application domains. Taxonomies inspired
    by classical concepts are essential to understand how attention has acted in deep
    neural networks and whether the roles played corroborate with theoretical foundations
    studied for more than 40 years in psychology and neuroscience. This study is already
    in the final stages of development by our team and will hopefully help researchers
    develop new attentional structures with functions still little explored in the
    literature. We hope to make it available to the scientific community as soon as
    possible.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前正在通过对主要神经注意力模型的理论分析来补充本调查。这项补充调查将帮助满足对基于理论的注意力框架的迫切需求，这些框架基于深度学习时代之前的注意力理论。现有的分类系统还未使用理论概念，并且难以扩展到各种架构和应用领域。受经典概念启发的分类系统对于理解注意力在深度神经网络中的作用至关重要，并且这些作用是否与心理学和神经科学中研究了超过40年的理论基础相符。这项研究已经进入我们团队的最后阶段，希望能帮助研究人员开发出在文献中尚未深入探索的新型注意力结构。我们希望尽快向科学界发布这项研究。
- en: Appendix
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: 'This survey employs a systematic review (SR) approach aiming to collect, critically
    evaluate, and synthesize the results of multiple primary studies concerning Attention
    in Deep Learning. The selection and evaluation of the works should be meticulous
    and easily reproducible. Also, SR should be objective, systematic, transparent,
    and replicable. Although recent, the use of attention in Deep Learning is extensive.
    Therefore, we systematically reviewed the literature, collecting works from a
    variety of sources. SR consists of the following steps: defining the scientific
    questions, identifying the databases, establishing the criteria for selecting
    papers, searching the databases, performing a critical analysis to choose the
    most relevant works, and preparing a critical summary of the most relevant papers,
    as shown Figure [23](#Sx1.F23 "Figure 23 ‣ Appendix ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning").'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查采用系统性综述（SR）方法，旨在收集、批判性评估并综合多个关于深度学习中注意力的主要研究结果。对文献的选择和评估应细致且易于重复。此外，SR 应该是客观的、系统的、透明的和可重复的。尽管近期使用注意力机制的深度学习研究广泛，但我们仍系统性地回顾了文献，从各种来源收集了相关研究。SR
    包含以下步骤：定义科学问题、识别数据库、建立选择论文的标准、搜索数据库、进行批判性分析以选择最相关的工作，以及准备最相关论文的批判性总结，如图[23](#Sx1.F23
    "图 23 ‣ 附录 ‣ 注意力，请！深度学习中神经注意力模型的调查")所示。
- en: '![Refer to caption](img/d6e8e24a057497c03af4fcbca0040ff1.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/d6e8e24a057497c03af4fcbca0040ff1.png)'
- en: 'Figure 23: Steps of the systematic review used in this survey.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23：本调查中使用的系统性综述步骤。
- en: 'This survey covers the following aspects: 1) The uses of attention in Deep
    Learning; 2) Attention mechanisms; 3) Uses of attention; 4) Attention applications;
    5) Attention and interpretability; 6) Trends and challenges. These aspects provide
    the main topics regarding attention in Deep Learning, which can help understand
    the field’s fundamentals. The second step identifies the main databases in the
    machine learning area, such as arXiv, DeepMind, Google AI, OpenAI, Facebook AI
    research, Microsoft research, Amazon research, Google Scholar, IEEE Xplore, DBLP,
    ACM, NIPS, ICML, ICLR, AAAI, CVPR, ICCV, CoRR, IJCNN, Neurocomputing, and Google
    general search (including blogs, distill, and Quora). Our searching period comprises
    01/01/2014 to 06/30/2019 (first stage) and 07/01/2019 to 02/15/2021 (second stage),
    and the search was performed via a Phyton script ²²2https://github.com/larocs/attention_dl.
    The papers’ title, abstract, year, DOI, and source publication were downloaded
    and stored in a JSON file. The most appropriate set of keywords to perform the
    searches was defined by partially searching the field with expert knowledge from
    our research group. The final set of keywords wore: attention, attentional, and
    attentive.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查涵盖以下方面：1) 深度学习中注意力的应用；2) 注意力机制；3) 注意力的用途；4) 注意力的应用；5) 注意力与可解释性；6) 趋势与挑战。这些方面提供了关于深度学习中注意力的主要主题，有助于理解该领域的基础知识。第二步识别了机器学习领域的主要数据库，如
    arXiv、DeepMind、Google AI、OpenAI、Facebook AI 研究、Microsoft 研究、Amazon 研究、Google Scholar、IEEE
    Xplore、DBLP、ACM、NIPS、ICML、ICLR、AAAI、CVPR、ICCV、CoRR、IJCNN、Neurocomputing 和 Google
    通用搜索（包括博客、Distill 和 Quora）。我们的搜索时间段包括 2014 年 01 月 01 日至 2019 年 06 月 30 日（第一阶段）和
    2019 年 07 月 01 日至 2021 年 02 月 15 日（第二阶段），搜索是通过 Phyton 脚本执行的 ²²2https://github.com/larocs/attention_dl。论文的标题、摘要、年份、DOI
    和来源出版物被下载并存储在 JSON 文件中。通过结合我们研究组的专家知识，部分搜索领域定义了最合适的关键词集合。最终的关键词集合为：attention、attentional
    和 attentive。
- en: 'However, these keywords are also relevant in psychology and visual attention.
    Hence, we performed a second selection to eliminate these papers and remmove duplicate
    papers unrelated to the DL field. After removing duplicates, 18,257 different
    papers remained. In the next selection step, we performed a sequential combination
    of three types of filters: 1) Filter I: Selecting the works with general terms
    of attention (i.e., attention, attentive, attentional, saliency, top-down, bottom-up,
    memory, focus, and mechanism); 2) Filter II: Selecting the works with terms related
    to DL (i.e. deep learning, neural network, ann, dnn deep neural, encoder, decoder,
    recurrent neural network, recurrent network, rnn, long short term memory, long
    short-term memory, lstm, gated recurrent unit, gru, autoencoder, ae, variational
    autoencoder, vae, denoising ae, dae, sparse ae, sae, markov chain, mc, hopfield
    network, boltzmann machine, em, restricted boltzmann machine, rbm, deep belief
    network, dbn, deep convolutional network, dcn, deconvolution network, dn, deep
    convolutional inverse graphics network, dcign, generative adversarial network,
    gan, liquid state machine, lsm, extreme learnng, machine, elm, echo state network,
    esn, deep residual network, drn, konohen network, kn, turing machine, ntm, convolutional
    network, cnn, and capsule network); 3) Filter III: Selecting the works with specific
    words of attention in Deep Learning (i.e., attention network, soft attention,
    hard attention, self-attention, self attention deep attention, hierarchical attention,
    transformer, local attention, global attention, coattention, co-attention, flow
    attention, attention-over-attention, way attention, intra-attention, self-attentive,
    and self attentive).'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些关键词在心理学和视觉注意力中也很相关。因此，我们进行了第二次筛选，以排除这些论文，并移除与 DL 领域无关的重复论文。去除重复项后，剩余 18,257
    篇不同的论文。在下一步筛选中，我们依次应用了三种类型的过滤器：1) 过滤器 I：选择具有一般注意力术语的工作（即 attention、attentive、attentional、saliency、top-down、bottom-up、memory、focus
    和 mechanism）；2) 过滤器 II：选择与 DL 相关的术语的工作（即 deep learning、neural network、ann、dnn
    deep neural、encoder、decoder、recurrent neural network、recurrent network、rnn、long
    short term memory、long short-term memory、lstm、gated recurrent unit、gru、autoencoder、ae、variational
    autoencoder、vae、denoising ae、dae、sparse ae、sae、markov chain、mc、hopfield network、boltzmann
    machine、em、restricted boltzmann machine、rbm、deep belief network、dbn、deep convolutional
    network、dcn、deconvolution network、dn、deep convolutional inverse graphics network、dcign、generative
    adversarial network、gan、liquid state machine、lsm、extreme learning、machine、elm、echo
    state network、esn、deep residual network、drn、konohen network、kn、turing machine、ntm、convolutional
    network、cnn 和 capsule network）；3) 过滤器 III：选择具有深度学习中特定注意力词的工作（即 attention network、soft
    attention、hard attention、self-attention、self attention deep attention、hierarchical
    attention、transformer、local attention、global attention、coattention、co-attention、flow
    attention、attention-over-attention、way attention、intra-attention、self-attentive
    和 self attentive）。
- en: '![Refer to caption](img/6ec84a129e9597cb685d2cc5f9ecdddb.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6ec84a129e9597cb685d2cc5f9ecdddb.png)'
- en: 'Figure 24: The filter strategies for selecting the relevant works. Search I
    corresponds to articles collected between 01/01/2014 to 06/30/2019 (first stage),
    and Search II corresponds to papers collected between 07/01/2019 to 02/15/2021
    (second stage).'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 图24：选择相关工作的过滤策略。搜索I对应于2014年01月01日至2019年06月30日收集的文章（第一阶段），搜索II对应于2019年07月01日至2021年02月15日收集的论文（第二阶段）。
- en: 'The decision tree with the second selection is shown in Figure [24](#Sx1.F24
    "Figure 24 ‣ Appendix ‣ Attention, please! A survey of Neural Attention Models
    in Deep Learning"). The third filtering selects works with at least one specific
    term of attention in deep learning. In the next filtering, we remove papers without
    abstract, the collection of filters verify if there is at least one specific term
    of Deep Learning and remove the works with the following keywords: visual attention,
    saliency, and eye-tracking. For the papers with abstract, the selection is more
    complex, requiring three cascade conditions: 1) First condition: Selecting the
    works that have more than five filter terms from filter II; 2) Second condition:
    selecting the works that have between three and five terms from filter II and
    where there is at least one of the following: attention model, attention mechanism,
    attention, or attentive; 3) Third condition: Selecting the works with one or two
    terms from filter II; without the terms: salience, visual attention, attentive,
    and attentional mechanism. A total of 6,338 works remained for manual selection.
    We manually excluded the papers without a title, abstract, or introduction related
    to the DL field. After manual selection, 3,567 works were stored in Zotero. Given
    the number of papers, we grouped them by year and chose those above a threshold
    (average citations in the group). Only works above average were read and classified
    as relevant or not for critical analysis. To find the number of citations, we
    automated the process with a Python script. 650 papers were considered relevant
    for this survey’s critical analysis, and 6,567 were used to perform quantitative
    analyzes.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 带有第二次选择的决策树如图[24](#Sx1.F24 "Figure 24 ‣ Appendix ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning")所示。第三次过滤选择了在深度学习中至少包含一个特定注意术语的作品。在下一次过滤中，我们删除了没有摘要的论文，过滤器集合验证是否至少有一个特定的深度学习术语，并移除包含以下关键词的作品：视觉注意、显著性和眼动追踪。对于有摘要的论文，选择过程更复杂，需要满足三个级联条件：1)
    第一条件：选择过滤器II中有五个以上术语的作品；2) 第二条件：选择过滤器II中有三到五个术语的作品，并且至少包含以下之一：注意模型、注意机制、注意或关注；3)
    第三条件：选择过滤器II中有一个或两个术语的作品；不包含以下术语：显著性、视觉注意、关注和注意机制。最终剩下6,338篇作品供手动选择。我们手动排除了没有标题、摘要或与DL领域相关的引言的论文。经过手动筛选后，共有3,567篇作品存储在Zotero中。考虑到论文数量，我们按年分组，并选择了超过阈值（组内平均引用）的论文。只有超过平均水平的作品被阅读和分类为相关或不相关以进行关键分析。为了查找引用数量，我们使用Python脚本自动化了这一过程。650篇论文被认为对本次调查的关键分析具有相关性，6,567篇用于执行定量分析。
- en: References
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Esther Luna Colombini, A da Silva Simoes, and CHC Ribeiro. An attentional
    model for intelligent robotics agents. PhD thesis, Instituto Tecnológico de Aeronáutica,
    São José dos Campos, Brazil, 2014.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 埃斯特·卢娜·科伦比尼、A·达·席尔瓦·西莫斯和CHC·里贝罗。《智能机器人代理的注意模型》。博士论文，巴西圣保罗州圣若泽·杜斯坎普斯航空技术学院，2014年。'
- en: '[2] Marvin M Chun, Julie D Golomb, and Nicholas B Turk-Browne. A taxonomy of
    external and internal attention. Annual review of psychology, 62:73–101, 2011.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] 马文·M·春、朱莉·D·戈隆布和尼古拉斯·B·特克-布朗。《外部和内部注意的分类》。心理学年评，62:73–101，2011年。'
- en: '[3] Roger BH Tootell, Nouchine Hadjikhani, E Kevin Hall, Sean Marrett, Wim
    Vanduffel, J Thomas Vaughan, and Anders M Dale. The retinotopy of visual spatial
    attention. Neuron, 21(6):1409–1422, 1998.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] 罗杰·BH·图特尔、努钦·哈吉基哈尼、E·凯文·霍尔、肖恩·马雷特、温姆·范杜费尔、J·托马斯·沃恩和安德斯·M·戴尔。《视觉空间注意的视网膜映射》。神经元，21(6):1409–1422，1998年。'
- en: '[4] Marty G Woldorff, Christopher C Gallen, Scott A Hampson, Steven A Hillyard,
    Christo Pantev, David Sobel, and Floyd E Bloom. Modulation of early sensory processing
    in human auditory cortex during auditory selective attention. Proceedings of the
    National Academy of Sciences, 90(18):8722–8726, 1993.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] 马蒂·G·沃尔多夫、克里斯托弗·C·加伦、斯科特·A·汉普森、史蒂文·A·希利亚德、克里斯托·潘特夫、大卫·索贝尔和弗洛伊德·E·布鲁姆。《在听觉选择注意期间对人类听觉皮层早期感觉处理的调制》。美国国家科学院院刊，90(18):8722–8726，1993年。'
- en: '[5] Heidi Johansen-Berg and Donna M Lloyd. The physiology and psychology of
    selective attention to touch. Front Biosci, 5:D894–D904, 2000.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Heidi Johansen-Berg 和 Donna M Lloyd。选择性触觉注意的生理学与心理学。《前沿生物学》，5:D894–D904，2000。'
- en: '[6] Christina Zelano, Moustafa Bensafi, Jess Porter, Joel Mainland, Brad Johnson,
    Elizabeth Bremner, Christina Telles, Rehan Khan, and Noam Sobel. Attentional modulation
    in human primary olfactory cortex. Nature neuroscience, 8(1):114–120, 2005.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Christina Zelano、Moustafa Bensafi、Jess Porter、Joel Mainland、Brad Johnson、Elizabeth
    Bremner、Christina Telles、Rehan Khan 和 Noam Sobel。人类初级嗅觉皮层中的注意力调节。《自然神经科学》，8(1):114–120，2005。'
- en: '[7] Maria G Veldhuizen, Genevieve Bender, R Todd Constable, and Dana M Small.
    Trying to detect taste in a tasteless solution: modulation of early gustatory
    cortex by attention to taste. Chemical Senses, 32(6):569–581, 2007.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Maria G Veldhuizen、Genevieve Bender、R Todd Constable 和 Dana M Small。在无味溶液中尝试检测味道：注意对味觉的早期调节。《化学感官》，32(6):569–581，2007。'
- en: '[8] William James. The Principles of Psychology. Dover Publications, 1890.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] William James。《心理学原理》。Dover Publications，1890。'
- en: '[9] Simone Frintrop, Erich Rome, and Henrik Christensen. Computational visual
    attention systems and their cognitive foundations: A survey. ACM Transactions
    on Applied Perception, 7, 01 2010.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Simone Frintrop、Erich Rome 和 Henrik Christensen。计算视觉注意系统及其认知基础：一项综述。《ACM应用感知学报》，7，2010年1月。'
- en: '[10] Anne M Treisman and Garry Gelade. A feature-integration theory of attention.
    Cognitive psychology, 12(1):97–136, 1980.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Anne M Treisman 和 Garry Gelade。注意的特征整合理论。《认知心理学》，12(1):97–136，1980。'
- en: '[11] Jeremy M Wolfe, Kyle R Cave, and Susan L Franzel. Guided search: an alternative
    to the feature integration model for visual search. Journal of Experimental Psychology:
    Human perception and performance, 15(3):419, 1989.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Jeremy M Wolfe、Kyle R Cave 和 Susan L Franzel。引导搜索：一种视觉搜索的特征整合模型替代方案。《实验心理学杂志：人类感知与表现》，15(3):419，1989。'
- en: '[12] Ronald A Rensink. The dynamic representation of scenes. Visual cognition,
    7(1-3):17–42, 2000.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Ronald A Rensink。《场景的动态表征》。视觉认知，7(1-3):17–42，2000。'
- en: '[13] Donald Eric Broadbent. Perception and communication. Elsevier, 2013.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Donald Eric Broadbent。《感知与交流》。Elsevier，2013。'
- en: '[14] Donald A Norman. Toward a theory of memory and attention. Psychological
    review, 75(6):522, 1968.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Donald A Norman。迈向记忆与注意的理论。《心理学评论》，75(6):522，1968。'
- en: '[15] Daniel Kahneman. Attention and effort, volume 1063. Citeseer, 1973.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Daniel Kahneman。《注意与努力》，第1063卷。Citeseer，1973。'
- en: '[16] Frank Van der Velde, Marc de Kamps, et al. Clam: Closed-loop attention
    model for visual search. Neurocomputing, 58:607–612, 2004.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Frank Van der Velde、Marc de Kamps 等。Clam：用于视觉搜索的闭环注意模型。《神经计算》，58:607–612，2004。'
- en: '[17] R Hans Phaf, AHC Van der Heijden, and Patrick TW Hudson. Slam: A connectionist
    model for attention in visual selection tasks. Cognitive psychology, 22(3):273–341,
    1990.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] R Hans Phaf、AHC Van der Heijden 和 Patrick TW Hudson。Slam：一个用于视觉选择任务的连接主义注意模型。《认知心理学》，22(3):273–341，1990。'
- en: '[18] Simone Frintrop, Erich Rome, and Henrik I Christensen. Computational visual
    attention systems and their cognitive foundations: A survey. ACM Transactions
    on Applied Perception (TAP), 7(1):1–39, 2010.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Simone Frintrop、Erich Rome 和 Henrik I Christensen。计算视觉注意系统及其认知基础：一项综述。《ACM应用感知学报》(TAP)，7(1):1–39，2010。'
- en: '[19] Christof Koch and Shimon Ullman. Shifts in selective visual attention:
    towards the underlying neural circuitry. In Matters of intelligence, pages 115–141\.
    Springer, 1987.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Christof Koch 和 Shimon Ullman。选择性视觉注意的转变：朝向潜在的神经回路。《智能问题》，第115–141页。Springer，1987。'
- en: '[20] Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based
    visual attention for rapid scene analysis. IEEE PAMI, 20(11):1254–1259, 1998.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Laurent Itti、Christof Koch 和 Ernst Niebur。基于显著性的视觉注意模型用于快速场景分析。《IEEE PAMI》，20(11):1254–1259，1998。'
- en: '[21] Vidhya Navalpakkam and Laurent Itti. An integrated model of top-down and
    bottom-up attention for optimizing detection speed. In 2006 IEEE CVPR), volume 2,
    pages 2049–2056\. IEEE, 2006.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Vidhya Navalpakkam 和 Laurent Itti。一个集成的自上而下和自下而上的注意模型用于优化检测速度。在2006 IEEE
    CVPR，第2卷，第2049–2056页。IEEE，2006。'
- en: '[22] Fred H Hamker. The emergence of attention by population-based inference
    and its role in distributed processing and cognitive control of vision. Computer
    Vision and Image Understanding, 100(1-2):64–106, 2005.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Fred H Hamker。通过基于人群的推理出现的注意及其在视觉分布处理和认知控制中的作用。《计算机视觉与图像理解》，100(1-2):64–106，2005。'
- en: '[23] Fred H Hamker. Modeling feature-based attention as an active top-down
    inference process. BioSystems, 86(1-3):91–99, 2006.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Fred H Hamker。将基于特征的注意建模为一个主动的自上而下推理过程。《生物系统》，86(1-3):91–99，2006。'
- en: '[24] Simone Frintrop. VOCUS: A visual attention system for object detection
    and goal-directed search, volume 3899. Springer, 2006.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Albert Ali Salah, Ethem Alpaydin, and Lale Akarun. A selective attention-based
    method for visual pattern recognition with application to handwritten digit recognition
    and face recognition. IEEE PAMI, 24(3):420–425, 2002.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Nabil Ouerhani. Visual attention: from bio-inspired modeling to real-time
    implementation. PhD thesis, Université de Neuchâtel, 2003.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Dirk Walther. Interactions of visual attention and object recognition:
    computational modeling, algorithms, and psychophysics. PhD thesis, California
    Institute of Technology, 2006.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Dirk Walther, Duane R Edgington, and Christof Koch. Detection and tracking
    of objects in underwater video. In Proc. of the IEEE CVPR, volume 1, pages I–I.
    IEEE, 2004.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] James J Clark and Nicola J Ferrier. Modal control of an attentive vision
    system. In IEEE ICCV, pages 514–523\. IEEE, 1988.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Cynthia Breazeal and Brian Scassellati. A context-dependent attention
    system for a social robot. rn, 255:3, 1999.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] A Rotenstein, Alexander Andreopoulos, Ehzan Fazl, David Jacob, Matt Robinson,
    Ksenia Shubina, Yuliang Zhu, and J Tsotsos. Towards the dream of intelligent,
    visually-guided wheelchairs. In Proc. 2nd Int’l Conf. on Technology and Aging,
    2007.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] James J Clark and Nicola J Ferrier. Attentive visual servoing. In Active
    vision. Citeseer, 1992.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Simone Frintrop and Patric Jensfelt. Attentional landmarks and active
    gaze control for visual slam. IEEE Transactions on Robotics, 24(5):1054–1065,
    2008.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Christian Scheier and Steffen Egner. Visual attention in a mobile robot.
    In ISIE’97 Proceeding of the IEEE International Symposium on Industrial Electronics,
    volume 1, pages SS48–SS52\. IEEE, 1997.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Shumeet Baluja and Dean A Pomerleau. Expectation-based selective attention
    for visual monitoring and control of a robot vehicle. Robotics and autonomous
    systems, 22(3-4):329–344, 1997.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and
    Daan Wierstra. Draw: A recurrent neural network for image generation. arXiv preprint
    arXiv:1502.04623, 2015.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    arXiv:1706.03762 [cs], June 2017. arXiv: 1706.03762.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv
    preprint arXiv:1410.3916, 2014.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Feng Wang and David MJ Tax. Survey on the attention based rnn model and
    its applications in computer vision. arXiv preprint arXiv:1601.06823, 2016.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Dichao Hu. An introductory survey on attention mechanisms in nlp problems.
    In Proceedings of SAI Intelligent Systems Conference, pages 432–448\. Springer,
    2019.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Andrea Galassi, Marco Lippi, and Paolo Torroni. Attention in natural language
    processing. IEEE Transactions on Neural Networks and Learning Systems, 2020.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] John Boaz Lee, Ryan A. Rossi, Sungchul Kim, Nesreen K. Ahmed, and Eunyee
    Koh. Attention models in graphs: A survey. arXiv:1807.07984 [cs], July 2018. arXiv:
    1807.07984.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] 约翰·博阿兹·李、瑞安·A·罗西、成柱·金、内斯林·K·艾哈迈德 和 尹熙。图中的注意力模型：综述。arXiv:1807.07984 [cs]，2018年7月。arXiv:
    1807.07984。'
- en: '[43] Sneha Chaudhari, Gungor Polatkan, Rohan Ramanath, and Varun Mithal. An
    attentive survey of attention models. arXiv preprint arXiv:1904.02874, 2019.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] 斯涅哈·乔杜哈里、贡戈尔·波拉特坎、罗汉·拉马纳斯 和 瓦伦·米萨尔。注意力模型的细致综述。arXiv预印本 arXiv:1904.02874，2019年。'
- en: '[44] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation
    by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] 德米特里·巴赫达瑙、京洪·赵 和 约书亚·本吉奥。通过联合学习对齐和翻译进行神经机器翻译。arXiv预印本 arXiv:1409.0473，2014年。'
- en: '[45] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
    Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image
    caption generation with visual attention. In International Conference on Machine
    Learning, pages 2048–2057, June 2015.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] 凯尔文·徐、吉米·巴、瑞安·基罗斯、京洪·赵、亚伦·库尔维尔、鲁斯兰·萨拉赫丁诺夫、理查德·泽梅尔 和 约书亚·本吉奥。展示、关注和叙述：带有视觉注意力的神经图像描述生成。发表于国际机器学习会议，第2048–2057页，2015年6月。'
- en: '[46] Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. Recurrent
    models of visual attention. arXiv preprint arXiv:1406.6247, 2014.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] 沃洛基米尔·姆尼赫、尼古拉斯·赫斯、亚历克斯·格雷夫斯 和 科雷·卡夫克丘。视觉注意力的递归模型。arXiv预印本 arXiv:1406.6247，2014年。'
- en: '[47] Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio.
    On the properties of neural machine translation: Encoder-decoder approaches. arXiv
    preprint arXiv:1409.1259, 2014.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] 京洪·赵、巴特·范·梅里恩博尔、德米特里·巴赫达瑙 和 约书亚·本吉奥。神经机器翻译的性质：编码器-解码器方法。arXiv预印本 arXiv:1409.1259，2014年。'
- en: '[48] William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals. Listen, attend
    and spell. arXiv preprint arXiv:1508.01211, 2015.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] 威廉·陈、纳夫迪普·贾特利、郭·V·李 和 奥里奥尔·维尼尔斯。倾听、关注和拼写。arXiv预印本 arXiv:1508.01211，2015年。'
- en: '[49] Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever,
    and Geoffrey Hinton. Grammar as a foreign language. In Advances in neural information
    processing systems, pages 2773–2781, 2015.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] 奥里奥尔·维尼尔斯、卢卡斯·凯瑟、特里·库、斯拉夫·彼得罗夫、伊利亚·苏茨克维 和 杰弗里·辛顿。语法作为外语。发表于神经信息处理系统进展，第2773–2781页，2015年。'
- en: '[50] Oriol Vinyals and Quoc Le. A neural conversational model. arXiv preprint
    arXiv:1506.05869, 2015.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] 奥里奥尔·维尼尔斯 和 郭·李。神经对话模型。arXiv预印本 arXiv:1506.05869，2015年。'
- en: '[51] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi.
    Bidirectional attention flow for machine comprehension. arXiv:1611.01603 [cs],
    November 2016. arXiv: 1611.01603.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] 闵俊·肖、安努鲁德哈·肯巴维、阿里·法尔哈迪 和 汉娜赫·哈吉什尔齐。双向注意力流用于机器理解。arXiv:1611.01603 [cs]，2016年11月。arXiv:
    1611.01603。'
- en: '[52] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard
    Hovy. Hierarchical attention networks for document classification. In Proceedings
    of the 2016 conference of the North American chapter of the association for computational
    linguistics: human language technologies, pages 1480–1489, 2016.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] 杨自超、杨第易、克里斯·戴尔、肖东·赫、亚历克斯·斯莫拉 和 埃杜阿德·霍维。用于文档分类的层次注意力网络。发表于2016年北美计算语言学协会会议：人类语言技术会议论文集，第1480–1489页，2016年。'
- en: '[53] Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic coattention networks
    for question answering. arXiv preprint arXiv:1611.01604, 2016.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] 蔡明·熊、维克托·钟 和 理查德·索彻。用于问题回答的动态共注意力网络。arXiv预印本 arXiv:1611.01604，2016年。'
- en: '[54] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks.
    In Advances in neural information processing systems, pages 2692–2700, 2015.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] 奥里奥尔·维尼尔斯、梅尔·福图纳托 和 纳夫迪普·贾特利。指针网络。发表于神经信息处理系统进展，第2692–2700页，2015年。'
- en: '[55] Abigail See, Peter J Liu, and Christopher D Manning. Get to the point:
    Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368,
    2017.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] 阿比盖尔·西、彼得·J·刘 和 克里斯托弗·D·曼宁。直达要点：使用指针生成网络的摘要。arXiv预印本 arXiv:1704.04368，2017年。'
- en: '[56] Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and Weizhu Chen. Fusionnet:
    Fusing via fully-aware attention with application to machine comprehension. arXiv
    preprint arXiv:1711.07341, 2017.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] 黄欣源、郑光、沈业龙 和 陈伟铸。Fusionnet：通过全知注意力融合，并应用于机器理解。arXiv预印本 arXiv:1711.07341，2017年。'
- en: '[57] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiskỳ,
    and Phil Blunsom. Reasoning about entailment with neural attention. arXiv preprint
    arXiv:1509.06664, 2015.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] 蒂姆·罗克特谢尔、爱德华·格雷芬斯特特、卡尔·莫里茨·赫尔曼、托马斯·科西基 和 菲尔·布伦索姆。通过神经注意力推理蕴涵关系。arXiv预印本
    arXiv:1509.06664，2015年。'
- en: '[58] Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches
    to attention-based neural machine translation. In Proceedings of the 2015 Conference
    on Empirical Methods in Natural Language Processing, pages 1412–1421, Lisbon,
    Portugal, 2015\. Association for Computational Linguistics.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Thang Luong, Hieu Pham 和 Christopher D. Manning. 基于注意力的神经机器翻译的有效方法。在 2015
    年自然语言处理经验方法会议论文集中，页面 1412–1421，葡萄牙里斯本，2015年\. 计算语言学协会。'
- en: '[59] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and koray kavukcuoglu.
    Spatial transformer networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,
    and R. Garnett, editors, Advances in Neural Information Processing Systems 28,
    pages 2017–2025\. Curran Associates, Inc., 2015.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Max Jaderberg, Karen Simonyan, Andrew Zisserman 和 Koray Kavukcuoglu. 空间变换网络。在
    C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama 和 R. Garnett 编辑的《神经信息处理系统进展
    28》中，页面 2017–2025\. Curran Associates, Inc.，2015年。'
- en: '[60] Dhanesh Ramachandram and Graham W Taylor. Deep multimodal learning: A
    survey on recent advances and trends. IEEE Signal Processing Magazine, 34(6):96–108,
    2017.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Dhanesh Ramachandram 和 Graham W Taylor. 深度多模态学习：关于近期进展和趋势的调查。《IEEE 信号处理杂志》，34(6):96–108，2017年。'
- en: '[61] Jing Gao, Peng Li, Zhikui Chen, and Jianing Zhang. A survey on deep learning
    for multimodal data fusion. Neural Computation, 32(5):829–864, 2020.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Jing Gao, Peng Li, Zhikui Chen 和 Jianing Zhang. 关于深度学习在多模态数据融合中的应用调查。《神经计算》，32(5):829–864，2020年。'
- en: '[62] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal,
    Hugo Larochelle, and Aaron Courville. Describing videos by exploiting temporal
    structure. In Proceedings of the IEEE international conference on computer vision,
    pages 4507–4515, 2015.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal,
    Hugo Larochelle 和 Aaron Courville. 利用时间结构描述视频。在 IEEE 国际计算机视觉会议论文集中，页面 4507–4515，2015年。'
- en: '[63] Chunlei Wu, Yiwei Wei, Xiaoliang Chu, Sun Weichen, Fei Su, and Leiquan
    Wang. Hierarchical attention-based multimodal fusion for video captioning. Neurocomputing,
    315:362–370, 2018.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Chunlei Wu, Yiwei Wei, Xiaoliang Chu, Sun Weichen, Fei Su 和 Leiquan Wang.
    基于层次注意力的多模态融合用于视频字幕生成。《神经计算》，315:362–370，2018年。'
- en: '[64] Rasool Fakoor, Abdel-rahman Mohamed, Margaret Mitchell, Sing Bing Kang,
    and Pushmeet Kohli. Memory-augmented attention modelling for videos. arXiv preprint
    arXiv:1611.02261, 2016.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Rasool Fakoor, Abdel-rahman Mohamed, Margaret Mitchell, Sing Bing Kang
    和 Pushmeet Kohli. 增强记忆的注意力建模用于视频。arXiv 预印本 arXiv:1611.02261，2016年。'
- en: '[65] Jiang Tian, Cong Li, Zhongchao Shi, and Feiyu Xu. A diagnostic report
    generator from ct volumes on liver tumor with semi-supervised attention mechanism.
    In International Conference on Medical Image Computing and Computer-Assisted Intervention,
    pages 702–710\. Springer, 2018.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Jiang Tian, Cong Li, Zhongchao Shi 和 Feiyu Xu. 基于半监督注意力机制的肝肿瘤 CT 图像诊断报告生成器。在国际医学图像计算与计算机辅助干预会议论文集中，页面
    702–710\. Springer，2018年。'
- en: '[66] Yunchen Pu, Martin Renqiang Min, Zhe Gan, and Lawrence Carin. Adaptive
    feature abstraction for translating video to text. In Thirty-Second AAAI Conference
    on Artificial Intelligence, April 2018.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Yunchen Pu, Martin Renqiang Min, Zhe Gan 和 Lawrence Carin. 自适应特征抽象用于视频到文本的转换。在第
    32 届 AAAI 人工智能大会上，2018年4月。'
- en: '[67] Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, and Alex C Kot. Global context-aware
    attention lstm networks for 3d action recognition. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 1647–1656, 2017.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan 和 Alex C Kot. 用于 3D 动作识别的全局上下文感知注意力
    LSTM 网络。在 IEEE 计算机视觉与模式识别会议论文集中，页面 1647–1656，2017年。'
- en: '[68] Liang Zhang, Guangming Zhu, Lin Mei, Peiyi Shen, Syed Afaq Ali Shah, and
    Mohammed Bennamoun. Attention in convolutional lstm for gesture recognition. In
    Advances in Neural Information Processing Systems, pages 1953–1962, 2018.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Liang Zhang, Guangming Zhu, Lin Mei, Peiyi Shen, Syed Afaq Ali Shah 和
    Mohammed Bennamoun. 卷积 LSTM 中的注意力用于手势识别。在《神经信息处理系统进展》中，页面 1953–1962，2018年。'
- en: '[69] Pengfei Zhang, Jianru Xue, Cuiling Lan, Wenjun Zeng, Zhanning Gao, and
    Nanning Zheng. Adding attentiveness to the neurons in recurrent neural networks.
    In Proceedings of the European Conference on Computer Vision (ECCV), pages 135–151,
    2018.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Pengfei Zhang, Jianru Xue, Cuiling Lan, Wenjun Zeng, Zhanning Gao 和 Nanning
    Zheng. 向递归神经网络中的神经元添加注意力。在欧洲计算机视觉会议 (ECCV) 论文集中，页面 135–151，2018年。'
- en: '[70] Xiangrong Zhang, Xin Wang, Xu Tang, Huiyu Zhou, and Chen Li. Description
    generation for remote sensing images using attribute attention mechanism. Remote
    Sensing, 11(6):612, 2019.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Xiangrong Zhang, Xin Wang, Xu Tang, Huiyu Zhou 和 Chen Li. 使用属性注意力机制生成遥感图像描述。《遥感》，11(6):612，2019年。'
- en: '[71] Bei Fang, Ying Li, Haokui Zhang, and Jonathan Cheung-Wai Chan. Hyperspectral
    images classification based on dense convolutional networks with spectral-wise
    attention mechanism. Remote Sensing, 11(2):159, 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] 方贝, 李颖, 张浩奎, 和 陈乔纳森·陈. 基于密集卷积网络与光谱级注意力机制的高光谱图像分类。遥感，11(2):159，2019年。'
- en: '[72] Qi Wang, Shaoteng Liu, Jocelyn Chanussot, and Xuelong Li. Scene classification
    with recurrent attention of vhr remote sensing images. IEEE Transactions on Geoscience
    and Remote Sensing, 57(2):1155–1167, 2018.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] 王琪, 刘少腾, 乔赛琳·查努索特, 和 李学龙. 基于递归注意力的VHR遥感图像场景分类。IEEE地球科学与遥感学报，57(2):1155–1167，2018年。'
- en: '[73] Xiaoguang Mei, Erting Pan, Yong Ma, Xiaobing Dai, Jun Huang, Fan Fan,
    Qinglei Du, Hong Zheng, and Jiayi Ma. Spectral-spatial attention networks for
    hyperspectral image classification. Remote Sensing, 11(8):963, 2019.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] 梅晓光, 潘尔婷, 马勇, 戴晓兵, 黄俊, 范凡, 杜庆磊, 郑洪, 和 马佳怡. 高光谱图像分类的光谱-空间注意力网络。遥感，11(8):963，2019年。'
- en: '[74] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Kazuhiro Sumi, John R. Hershey,
    and Tim K. Marks. Attention-based multimodal fusion for video description. arXiv:1701.03126
    [cs], January 2017. arXiv: 1701.03126.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] 堀千织, 堀高明, 李腾焰, 铃木和广, 约翰·R·赫希, 和 提姆·K·马克斯. 基于注意力的多模态融合用于视频描述。arXiv:1701.03126
    [cs]，2017年1月。arXiv: 1701.03126。'
- en: '[75] Yuanyuan Zhang, Zi-Rui Wang, and Jun Du. Deep fusion: An attention guided
    factorized bilinear pooling for audio-video emotion recognition. In 2019 International
    Joint Conference on Neural Networks (IJCNN), pages 1–8\. IEEE, 2019.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] 张媛媛, 王梓瑞, 和 杜军. 深度融合：一种注意力引导的分解双线性池化用于音视频情感识别。载于2019年国际神经网络联合会议（IJCNN），第1–8页。IEEE，2019年。'
- en: '[76] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria,
    and Louis-Philippe Morency. Memory fusion network for multi-view sequential learning.
    In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] 阿米尔·扎德赫, 保罗·普·梁, 纳沃尼尔·马祖姆德, 索佳娜·波里亚, 埃里克·坎布里亚, 和 路易斯-菲利普·莫伦西. 多视角序列学习的记忆融合网络。载于第三十二届AAAI人工智能大会，2018年。'
- en: '[77] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria,
    and Louis-Philippe Morency. Multi-attention recurrent network for human communication
    comprehension. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] 阿米尔·扎德赫, 保罗·普·梁, 索佳娜·波里亚, 普拉提克·维杰, 埃里克·坎布里亚, 和 路易斯-菲利普·莫伦西. 用于人类沟通理解的多重注意力递归网络。载于第三十二届AAAI人工智能大会，2018年。'
- en: '[78] Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski,
    Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap.
    Relational recurrent neural networks. arXiv preprint arXiv:1806.01822, 2018.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] 亚当·桑托罗, 瑞安·福克纳, 大卫·拉波索, 杰克·雷, 迈克·赫尔扎诺夫斯基, 特奥法内·韦伯, 丹·维尔斯特拉, 奥里奥尔·维尼亚尔斯,
    拉兹万·帕斯卡努, 和 提莫西·利利克拉普. 关系递归神经网络。arXiv预印本arXiv:1806.01822，2018年。'
- en: '[79] Zheng Zhang, Lizi Liao, Minlie Huang, Xiaoyan Zhu, and Tat-Seng Chua.
    Neural multimodal belief tracker with adaptive attention for dialogue systems.
    In The World Wide Web Conference, pages 2401–2412, 2019.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] 郑章, 廖丽子, 黄敏磊, 朱晓燕, 和 邱达生. 具有自适应注意力的神经多模态信念跟踪器用于对话系统。载于世界广网会议，第2401–2412页，2019年。'
- en: '[80] Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual attention networks
    for multimodal reasoning and matching. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 299–307, 2017.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] 南贤洙, 郑宇浩, 和 金晶熙. 用于多模态推理和匹配的双重注意力网络。载于IEEE计算机视觉与模式识别会议论文集，第299–307页，2017年。'
- en: '[81] Feiran Huang, Xiaoming Zhang, Zhonghua Zhao, and Zhoujun Li. Bi-directional
    spatial-semantic attention networks for image-text matching. IEEE Transactions
    on Image Processing, 28(4):2008–2020, 2018.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] 黄飞然, 张晓明, 赵中华, 和 李周俊. 双向空间-语义注意力网络用于图像-文本匹配。IEEE图像处理学报，28(4):2008–2020，2018年。'
- en: '[82] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei. Pointing
    novel objects in image captioning. In Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pages 12497–12506, 2019.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] 李叶豪, 姚婷, 潘英伟, 赵洪阳, 和 梅涛. 图像字幕中的新颖物体指点。载于IEEE/CVF计算机视觉与模式识别会议论文集，第12497–12506页，2019年。'
- en: '[83] Danielle Matthews, Tanya Behne, Elena Lieven, and Michael Tomasello. Origins
    of the human pointing gesture: a training study. Developmental science, 15(6):817–829,
    2012.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] 丹妮尔·马修斯, 谭雅·贝恩, 艾琳娜·利文, 和 迈克尔·托马塞洛. 人类指点手势的起源：一项训练研究。发展科学，15(6):817–829，2012年。'
- en: '[84] Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, and Hongsheng Li. Improving
    referring expression grounding with cross-modal attention-guided erasing. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1950–1959,
    2019.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang 和 Hongsheng Li。通过跨模态注意引导擦除来改进指代表达的定位。发表于IEEE/CVF计算机视觉与模式识别会议论文集，页码1950–1959，2019年。'
- en: '[85] Pooya Abolghasemi, Amir Mazaheri, Mubarak Shah, and Ladislau Boloni. Pay
    attention!-robustifying a deep visuomotor policy through task-focused visual attention.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 4254–4262, 2019.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Pooya Abolghasemi, Amir Mazaheri, Mubarak Shah 和 Ladislau Boloni。注意力！通过任务专注的视觉注意来增强深度视觉运动策略。发表于IEEE计算机视觉与模式识别会议论文集，页码4254–4262，2019年。'
- en: '[86] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv:1410.5401
    [cs], October 2014. arXiv: 1410.5401.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Alex Graves, Greg Wayne 和 Ivo Danihelka。神经图灵机。arXiv:1410.5401 [cs]，2014年10月。arXiv:
    1410.5401。'
- en: '[87] Amir Hosein Khasahmadi, Kaveh Hassani, Parsa Moradi, Leo Lee, and Quaid
    Morris. Memory-based graph networks. arXiv preprint arXiv:2002.09518, 2020.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Amir Hosein Khasahmadi, Kaveh Hassani, Parsa Moradi, Leo Lee 和 Quaid Morris。基于记忆的图网络。arXiv预印本
    arXiv:2002.09518，2020年。'
- en: '[88] Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks
    for visual and textual question answering. arXiv:1603.01417 [cs], March 2016.
    arXiv: 1603.01417.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Caiming Xiong, Stephen Merity 和 Richard Socher。用于视觉和文本问题回答的动态记忆网络。arXiv:1603.01417
    [cs]，2016年3月。arXiv: 1603.01417。'
- en: '[89] Xinkai Lu, Wenguan Wang, Martin Danelljan, Tianfei Zhou, Jianbing Shen,
    and Luc Van Gool. Video object segmentation with episodic graph memory networks.
    arXiv preprint arXiv:2007.07020, 2020.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Xinkai Lu, Wenguan Wang, Martin Danelljan, Tianfei Zhou, Jianbing Shen
    和 Luc Van Gool。具有情景图记忆网络的视频对象分割。arXiv预印本 arXiv:2007.07020，2020年。'
- en: '[90] Abrar H Abdulnabi, Bing Shuai, Stefan Winkler, and Gang Wang. Episodic
    camn: Contextual attention-based memory networks with iterative feedback for scene
    labeling. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 5561–5570, 2017.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Abrar H Abdulnabi, Bing Shuai, Stefan Winkler 和 Gang Wang。情景标记的情景记忆网络：基于上下文注意的记忆网络与迭代反馈。发表于IEEE计算机视觉与模式识别会议论文集，页码5561–5570，2017年。'
- en: '[91] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory
    networks. In Advances in neural information processing systems, pages 2440–2448,
    2015.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus 等。端到端记忆网络。发表于神经信息处理系统进展，页码2440–2448，2015年。'
- en: '[92] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object
    segmentation using space-time memory networks. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 9226–9235, 2019.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Seoung Wug Oh, Joon-Young Lee, Ning Xu 和 Seon Joo Kim。使用时空记忆网络的视频对象分割。发表于IEEE国际计算机视觉会议论文集，页码9226–9235，2019年。'
- en: '[93] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury,
    Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything:
    Dynamic memory networks for natural language processing. In International conference
    on machine learning, pages 1378–1387\. PMLR, 2016.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury,
    Ishaan Gulrajani, Victor Zhong, Romain Paulus 和 Richard Socher。问我任何事：用于自然语言处理的动态记忆网络。发表于国际机器学习会议论文集，页码1378–1387。PMLR，2016年。'
- en: '[94] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
    S Yu Philip. A comprehensive survey on graph neural networks. IEEE Transactions
    on Neural Networks and Learning Systems, 2020.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang 和 S
    Yu Philip。图神经网络的综合综述。IEEE神经网络与学习系统汇刊，2020年。'
- en: '[95] Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine
    Bordes, and Jason Weston. Key-value memory networks for directly reading documents.
    arXiv preprint arXiv:1606.03126, 2016.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine
    Bordes 和 Jason Weston。直接读取文档的键值记忆网络。arXiv预印本 arXiv:1606.03126，2016年。'
- en: '[96] Seungwhan Moon, Pararth Shah, Anuj Kumar, and Rajen Subba. Memory graph
    networks for explainable memory-grounded question answering. In Proceedings of
    the 23rd Conference on Computational Natural Language Learning (CoNLL), pages
    728–736, 2019.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Seungwhan Moon, Pararth Shah, Anuj Kumar 和 Rajen Subba。用于可解释记忆驱动问题回答的记忆图网络。发表于第23届计算自然语言学习会议（CoNLL）论文集，页码728–736，2019年。'
- en: '[97] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
    Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903,
    2017.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
    Pietro Lio 和 Yoshua Bengio。图注意力网络。arXiv预印本 arXiv:1710.10903，2017年。'
- en: '[98] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S
    Yu. Heterogeneous graph attention network. In The World Wide Web Conference, pages
    2022–2032, 2019.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Xiao Wang、Houye Ji、Chuan Shi、Bai Wang、Yanfang Ye、Peng Cui 和 Philip S Yu。异质图注意力网络。在世界范围的网络会议，页面
    2022–2032，2019年。'
- en: '[99] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and Jie Shan. Graph
    attention convolution for point cloud semantic segmentation. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pages 10296–10305,
    2019.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Lei Wang、Yuchun Huang、Yaolin Hou、Shenman Zhang 和 Jie Shan。用于点云语义分割的图注意力卷积。在
    IEEE 计算机视觉与模式识别会议论文集，页面 10296–10305，2019年。'
- en: '[100] Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alexander A Alemi.
    Watch your step: Learning node embeddings via graph attention. In Advances in
    Neural Information Processing Systems, pages 9180–9190, 2018.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Sami Abu-El-Haija、Bryan Perozzi、Rami Al-Rfou 和 Alexander A Alemi。小心脚下：通过图注意力学习节点嵌入。在神经信息处理系统进展，页面
    9180–9190，2018年。'
- en: '[101] Linjie Li, Zhe Gan, Yu Cheng, and Jingjing Liu. Relation-aware graph
    attention network for visual question answering. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 10313–10322, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Linjie Li、Zhe Gan、Yu Cheng 和 Jingjing Liu。关系感知图注意力网络用于视觉问答。在 IEEE 国际计算机视觉会议论文集，页面
    10313–10322，2019年。'
- en: '[102] Karim Ahmed, Nitish Shirish Keskar, and Richard Socher. Weighted transformer
    network for machine translation. arXiv preprint arXiv:1711.02132, 2017.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Karim Ahmed、Nitish Shirish Keskar 和 Richard Socher。用于机器翻译的加权变换器网络。arXiv
    预印本 arXiv:1711.02132，2017年。'
- en: '[103] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and
    Zheng Zhang. Star-transformer. arXiv preprint arXiv:1902.09113, 2019.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Qipeng Guo、Xipeng Qiu、Pengfei Liu、Yunfan Shao、Xiangyang Xue 和 Zheng Zhang。星形变换器。arXiv
    预印本 arXiv:1902.09113，2019年。'
- en: '[104] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer,
    Ian Simon, Curtis Hawthorne, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu,
    and Douglas Eck. Music transformer. arXiv preprint arXiv:1809.04281, 2018.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Cheng-Zhi Anna Huang、Ashish Vaswani、Jakob Uszkoreit、Noam Shazeer、Ian
    Simon、Curtis Hawthorne、Andrew M Dai、Matthew D Hoffman、Monica Dinculescu 和 Douglas
    Eck。音乐变换器。arXiv 预印本 arXiv:1809.04281，2018年。'
- en: '[105] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating
    long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Rewon Child、Scott Gray、Alec Radford 和 Ilya Sutskever。使用稀疏变换器生成长序列。arXiv
    预印本 arXiv:1904.10509，2019年。'
- en: '[106] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R Kosiorek, Seungjin Choi, and
    Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant
    neural networks. arXiv preprint arXiv:1810.00825, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Juho Lee、Yoonho Lee、Jungtaek Kim、Adam R Kosiorek、Seungjin Choi 和 Yee
    Whye Teh。集合变换器：一种基于注意力的排列不变神经网络框架。arXiv 预印本 arXiv:1810.00825，2018年。'
- en: '[107] Hasan Sait Arslan, Mark Fishel, and Gholamreza Anbarjafari. Doubly attentive
    transformer machine translation. arXiv preprint arXiv:1807.11605, 2018.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Hasan Sait Arslan、Mark Fishel 和 Gholamreza Anbarjafari。双重注意力变换器机器翻译。arXiv
    预印本 arXiv:1807.11605，2018年。'
- en: '[108] Jindřich Libovickỳ, Jindřich Helcl, and David Mareček. Input combination
    strategies for multi-source transformer decoder. arXiv preprint arXiv:1811.04716,
    2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Jindřich Libovickỳ、Jindřich Helcl 和 David Mareček。多源变换器解码器的输入组合策略。arXiv
    预印本 arXiv:1811.04716，2018年。'
- en: '[109] Ning Dai, Jianze Liang, Xipeng Qiu, and Xuanjing Huang. Style transformer:
    Unpaired text style transfer without disentangled latent representation. arXiv
    preprint arXiv:1905.05621, 2019.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Ning Dai、Jianze Liang、Xipeng Qiu 和 Xuanjing Huang。风格变换器：无需解缠结潜在表示的无配对文本风格转换。arXiv
    预印本 arXiv:1905.05621，2019年。'
- en: '[110] Yang Liu and Mirella Lapata. Hierarchical transformers for multi-document
    summarization. arXiv preprint arXiv:1905.13164, 2019.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Yang Liu 和 Mirella Lapata。用于多文档摘要的层次化变换器。arXiv 预印本 arXiv:1905.13164，2019年。'
- en: '[111] Ting-Rui Chiang, Chao-Wei Huang, Shang-Yu Su, and Yun-Nung Chen. Learning
    multi-level information for dialogue response selection by highway recurrent transformer.
    arXiv preprint arXiv:1903.08953, 2019.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Ting-Rui Chiang、Chao-Wei Huang、Shang-Yu Su 和 Yun-Nung Chen。通过高速公路递归变换器学习对话响应选择的多层次信息。arXiv
    预印本 arXiv:1903.08953，2019年。'
- en: '[112] Fengshun Xiao, Jiangtong Li, Hai Zhao, Rui Wang, and Kehai Chen. Lattice-based
    transformer encoder for neural machine translation. arXiv preprint arXiv:1906.01282,
    2019.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Fengshun Xiao、Jiangtong Li、Hai Zhao、Rui Wang 和 Kehai Chen。基于格的变换器编码器用于神经机器翻译。arXiv
    预印本 arXiv:1906.01282，2019年。'
- en: '[113] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural
    speech synthesis with transformer network. In Proceedings of the AAAI Conference
    on Artificial Intelligence, pages 6706–6713, 2019.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Phi Xuan Nguyen and Shafiq Joty. Phrase-based attentions. arXiv preprint
    arXiv:1810.03444, 2018.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805
    [cs], October 2018. arXiv: 1810.04805.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and
    Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog,
    1(8):9, 2019.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165,
    2020.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,
    Alexander Ku, and Dustin Tran. Image transformer. In International Conference
    on Machine Learning, pages 4055–4064\. PMLR, 2018.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention
    generative adversarial networks. In International conference on machine learning,
    pages 7354–7363\. PMLR, 2019.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal,
    David Luan, and Ilya Sutskever. Generative pretraining from pixels. In Proceedings
    of the 37th International Conference on Machine Learning, volume 1, 2020.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, and Scott Gray. Dall·e: Creating
    images from text, 2021.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan
    Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam
    Santoro, et al. Hyperbolic attention networks. arXiv preprint arXiv:1805.09786,
    2018.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Yiding Zhang, Xiao Wang, Xunqiang Jiang, Chuan Shi, and Yanfang Ye. Hyperbolic
    graph attention network. arXiv preprint arXiv:1912.03046, 2019.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
    Monti, and Michael Bronstein. Temporal graph networks for deep learning on dynamic
    graphs. arXiv preprint arXiv:2006.10637, 2020.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Sarthak Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan,
    Guillaume Lajoie, Michael Mozer, and Yoshua Bengio. Learning to combine top-down
    and bottom-up signals in recurrent neural networks with attention over modules.
    In International Conference on Machine Learning, pages 6972–6986\. PMLR, 2020.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Alex Graves. Adaptive computation time for recurrent neural networks.
    arXiv:1603.08983 [cs], March 2016. arXiv: 1603.08983.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang,
    Dmitry Vetrov, and Ruslan Salakhutdinov. Spatially adaptive computation time for
    residual networks. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 1039–1048, 2017.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] 迈克尔·费古尔诺夫, 麦克斯韦·D·柯林斯, 朱玉坤, 张力, 乔纳森·黄, 德米特里·维特罗夫, 和 鲁斯兰·萨拉胡丁诺夫. 残差网络的空间自适应计算时间.
    在 IEEE 计算机视觉与模式识别会议论文集中，第1039–1048页, 2017.'
- en: '[128] Cristobal Eyzaguirre and Alvaro Soto. Differentiable adaptive computation
    time for visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pages 12817–12825, 2020.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] 克里斯托瓦尔·埃萨圭雷和 阿尔瓦罗·索托. 用于视觉推理的可微自适应计算时间. 在 IEEE/CVF 计算机视觉与模式识别会议论文集中，第12817–12825页,
    2020.'
- en: '[129] Dongbin Zhao, Yaran Chen, and Le Lv. Deep reinforcement learning with
    visual attention for vehicle classification. IEEE Transactions on Cognitive and
    Developmental Systems, 9(4):356–367, 2016.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] 赵东宾, 陈亚然, 和 吕乐. 使用视觉注意力的深度强化学习进行车辆分类. IEEE 认知与发展系统汇刊, 9(4):356–367, 2016.'
- en: '[130] Xuanhan Wang, Lianli Gao, Jingkuan Song, and Hengtao Shen. Beyond frame-level
    cnn: saliency-aware 3-d cnn with lstm for video action recognition. IEEE Signal
    Processing Letters, 24(4):510–514, 2016.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] 王轩涵, 高连立, 宋景宽, 和 沈恒涛. 超越帧级CNN: 带LSTM的显著性意识3-D CNN用于视频动作识别. IEEE 信号处理快报,
    24(4):510–514, 2016.'
- en: '[131] Ning Liu, Yongchao Long, Changqing Zou, Qun Niu, Li Pan, and Hefeng Wu.
    Adcrowdnet: An attention-injective deformable convolutional network for crowd
    understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pages 3225–3234, 2019.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] 刘宁, 龙永超, 邹长青, 牛群, 潘力, 和 吴赫峰. Adcrowdnet: 一种用于人群理解的注意力注入可变形卷积网络. 在 IEEE/CVF
    计算机视觉与模式识别会议论文集中，第3225–3234页, 2019.'
- en: '[132] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. Squeeze-and-excitation
    networks. arXiv:1709.01507 [cs], September 2017. arXiv: 1709.01507.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] 胡杰, 沈李, 塞缪尔·阿尔巴尼, 孙刚, 和 吴恩华. 挤压-激励网络. arXiv:1709.01507 [cs], 2017年9月.
    arXiv: 1709.01507.'
- en: '[133] Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and Yun Fu. Residual non-local
    attention networks for image restoration. arXiv preprint arXiv:1903.10082, 2019.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] 张玉伦, 李坤鹏, 李凯, 钟秉能, 和 傅云. 用于图像恢复的残差非局部注意力网络. arXiv 预印本 arXiv:1903.10082,
    2019.'
- en: '[134] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional
    block attention module. In Proceedings of the European conference on computer
    vision (ECCV), pages 3–19, 2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] 宋贤浩, 朴钟灿, 李俊英, 和 崔仁硕. CBAM: 卷积块注意力模块. 在 欧洲计算机视觉会议（ECCV）论文集中，第3–19页, 2018.'
- en: '[135] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and Jiashi
    Feng. A^2-nets: Double attention networks. In S. Bengio, H. Wallach, H. Larochelle,
    K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information
    Processing Systems 31, pages 352–361\. Curran Associates, Inc., 2018.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] 陈云鹏, 雅尼斯·卡兰蒂迪斯, 李剑恕, 颜水成, 和 冯佳时. A^2-nets: 双重注意力网络. 在 S. Bengio, H. Wallach,
    H. Larochelle, K. Grauman, N. Cesa-Bianchi, 和 R. Garnett 编者的《神经信息处理系统进展 31》中，第352–361页。Curran
    Associates, Inc., 2018.'
- en: '[136] Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, and Hironobu Fujiyoshi.
    Attention branch network: Learning of attention mechanism for visual explanation.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 10705–10714, 2019.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] 福井博, 平川翼, 山下高义, 和 藤吉博信. 注意力分支网络: 视觉解释的注意力机制学习. 在 IEEE/CVF 计算机视觉与模式识别会议论文集中，第10705–10714页,
    2019.'
- en: '[137] Zhizhong Han, Xiyang Wang, Chi-Man Vong, Yu-Shen Liu, Matthias Zwicker,
    and CL Chen. 3dviewgraph: Learning global features for 3d shapes from a graph
    of unordered views with attention. arXiv preprint arXiv:1905.07503, 2019.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] 韩志忠, 王西洋, 方志满, 刘宇深, 马蒂亚斯·兹维克, 和 CL 陈. 3dviewgraph: 从无序视图图谱中学习3D形状的全局特征并引入注意力.
    arXiv 预印本 arXiv:1905.07503, 2019.'
- en: '[138] Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao. Distant supervision for
    relation extraction with sentence-level attention and entity descriptions. In
    Proceedings of the AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] 姜国亮, 刘康, 何世柱, 和 赵军. 利用句子级注意力和实体描述进行关系抽取的远程监督. 在 AAAI 人工智能大会论文集中, 2017.'
- en: '[139] Jiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen, Fang Wen, Hongdong
    Li, and Gang Hua. Neural aggregation network for video face recognition. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 4362–4371,
    2017.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Jiaolong Yang、Peiran Ren、Dongqing Zhang、Dong Chen、Fang Wen、Hongdong Li
    和 Gang Hua。用于视频面部识别的神经聚合网络。在 IEEE 计算机视觉与模式识别会议论文集，页码 4362–4371，2017 年。'
- en: '[140] Timo Hackel, Mikhail Usvyatsov, Silvano Galliani, Jan D. Wegner, and
    Konrad Schindler. Inference, learning and attention mechanisms that exploit and
    preserve sparsity in convolutional networks. arXiv:1801.10585 [cs], January 2018.
    arXiv: 1801.10585.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Timo Hackel、Mikhail Usvyatsov、Silvano Galliani、Jan D. Wegner 和 Konrad
    Schindler。推理、学习和利用稀疏性并在卷积网络中保持稀疏性的注意机制。arXiv:1801.10585 [cs]，2018 年 1 月。arXiv:
    1801.10585。'
- en: '[141] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple
    neural attentive meta-learner. arXiv:1707.03141 [cs, stat], July 2017. arXiv:
    1707.03141.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Nikhil Mishra、Mostafa Rohaninejad、Xi Chen 和 Pieter Abbeel。一种简单的神经注意力元学习器。arXiv:1707.03141
    [cs, stat]，2017 年 7 月。arXiv: 1707.03141。'
- en: '[142] Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see better: Recurrent
    attention convolutional neural network for fine-grained image recognition. In
    2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
    4476–4484, Honolulu, HI, July 2017\. IEEE.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Jianlong Fu、Heliang Zheng 和 Tao Mei。更仔细地观察以获得更好的效果：用于细粒度图像识别的递归注意卷积神经网络。在
    2017 IEEE 计算机视觉与模式识别会议（CVPR），页码 4476–4484，夏威夷檀香山，2017 年 7 月。IEEE。'
- en: '[143] Xueying Chen, Rong Zhang, and Pingkun Yan. Feature fusion encoder decoder
    network for automatic liver lesion segmentation. In 2019 IEEE 16th international
    symposium on biomedical imaging (ISBI 2019), pages 430–433\. IEEE, 2019.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Xueying Chen、Rong Zhang 和 Pingkun Yan。用于自动肝脏病变分割的特征融合编码解码网络。在 2019 IEEE
    第 16 届国际生物医学成像研讨会（ISBI 2019），页码 430–433。IEEE，2019 年。'
- en: '[144] Wanxin Tian, Zixuan Wang, Haifeng Shen, Weihong Deng, Yiping Meng, Binghui
    Chen, Xiubao Zhang, Yuan Zhao, and Xiehe Huang. Learning better features for face
    detection with feature fusion and segmentation supervision. arXiv preprint arXiv:1811.08557,
    2018.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Wanxin Tian、Zixuan Wang、Haifeng Shen、Weihong Deng、Yiping Meng、Binghui
    Chen、Xiubao Zhang、Yuan Zhao 和 Xiehe Huang。通过特征融合和分割监督来学习更好的面部检测特征。arXiv 预印本 arXiv:1811.08557，2018
    年。'
- en: '[145] Xingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao, Liping Liu, Zeyu
    Chen, and Jun Huan. Delta: Deep learning transfer using feature map with attention
    for convolutional networks. arXiv preprint arXiv:1901.09229, 2019.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Xingjian Li、Haoyi Xiong、Hanchao Wang、Yuxuan Rao、Liping Liu、Zeyu Chen
    和 Jun Huan。Delta：用于卷积网络的基于注意力的特征图深度学习迁移。arXiv 预印本 arXiv:1901.09229，2019 年。'
- en: '[146] Heliang Zheng, Jianlong Fu, Tao Mei, and Jiebo Luo. Learning multi-attention
    convolutional neural network for fine-grained image recognition. In Proceedings
    of the IEEE international conference on computer vision, pages 5209–5217, 2017.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Heliang Zheng、Jianlong Fu、Tao Mei 和 Jiebo Luo。用于细粒度图像识别的多重注意卷积神经网络。在
    IEEE 国际计算机视觉会议论文集，页码 5209–5217，2017 年。'
- en: '[147] Pau Rodríguez, Guillem Cucurull, Jordi Gonzàlez, Josep M Gonfaus, and
    Xavier Roca. A painless attention mechanism for convolutional neural networks.
    ICLR 2018, 2018.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Pau Rodríguez、Guillem Cucurull、Jordi Gonzàlez、Josep M Gonfaus 和 Xavier
    Roca。用于卷积神经网络的无痛注意机制。ICLR 2018，2018 年。'
- en: '[148] Linlin Wang, Zhu Cao, Gerard De Melo, and Zhiyuan Liu. Relation classification
    via multi-level attention cnns. In Proceedings of the 54th Annual Meeting of the
    Association for Computational Linguistics (Volume 1: Long Papers), pages 1298–1307,
    2016.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Linlin Wang、Zhu Cao、Gerard De Melo 和 Zhiyuan Liu。通过多级注意力卷积神经网络进行关系分类。在第
    54 届计算语言学协会年会（第 1 卷：长篇论文）论文集，页码 1298–1307，2016 年。'
- en: '[149] Wenpeng Yin, Hinrich Schütze, Bing Xiang, and Bowen Zhou. Abcnn: Attention-based
    convolutional neural network for modeling sentence pairs. Transactions of the
    Association for Computational Linguistics, 4:259–272, 2016.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Wenpeng Yin、Hinrich Schütze、Bing Xiang 和 Bowen Zhou。ABCNN：用于建模句子对的基于注意力的卷积神经网络。计算语言学协会学报，4:259–272，2016
    年。'
- en: '[150] Marcus Edel and Joscha Lausch. Capacity visual attention networks. In
    GCAI, pages 72–80, 2016.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Marcus Edel 和 Joscha Lausch。容量视觉注意网络。在 GCAI，页码 72–80，2016 年。'
- en: '[151] Yao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison
    Cottrell. A dual-stage attention-based recurrent neural network for time series
    prediction. arXiv preprint arXiv:1704.02971, 2017.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Yao Qin、Dongjin Song、Haifeng Chen、Wei Cheng、Guofei Jiang 和 Garrison Cottrell。用于时间序列预测的双阶段基于注意力的递归神经网络。arXiv
    预印本 arXiv:1704.02971，2017 年。'
- en: '[152] Yuxuan Liang, Songyu Ke, Junbo Zhang, Xiuwen Yi, and Yu Zheng. Geoman:
    Multi-level attention networks for geo-sensory time series prediction. In IJCAI,
    pages 3428–3434, 2018.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Yuxuan Liang, Songyu Ke, Junbo Zhang, Xiuwen Yi 和 Yu Zheng. Geoman: 多级注意力网络用于地理传感时间序列预测。在
    IJCAI，页码 3428–3434，2018年。'
- en: '[153] Wenbin Du, Yali Wang, and Yu Qiao. Rpan: An end-to-end recurrent pose-attention
    network for action recognition in videos. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 3725–3734, 2017.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Wenbin Du, Yali Wang 和 Yu Qiao. Rpan: 用于视频动作识别的端到端递归姿态注意力网络。在 IEEE 国际计算机视觉会议论文集中，页码
    3725–3734，2017年。'
- en: '[154] Kun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Michael Witbrock, and
    Vadim Sheinin. Graph2seq: Graph to sequence learning with attention-based neural
    networks. arXiv preprint arXiv:1804.00823, 2018.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Kun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Michael Witbrock 和 Vadim
    Sheinin. Graph2seq: 基于注意力的神经网络的图到序列学习。arXiv 预印本 arXiv:1804.00823，2018年。'
- en: '[155] Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition
    with visual attention. arXiv:1412.7755 [cs], December 2014. arXiv: 1412.7755.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Jimmy Ba, Volodymyr Mnih 和 Koray Kavukcuoglu. 使用视觉注意力的多对象识别。arXiv:1412.7755
    [cs]，2014年12月。arXiv: 1412.7755。'
- en: '[156] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks
    for machine reading. arXiv preprint arXiv:1601.06733, 2016.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Jianpeng Cheng, Li Dong 和 Mirella Lapata. 用于机器阅读的长短期记忆网络。arXiv 预印本 arXiv:1601.06733，2016年。'
- en: '[157] Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Michael C
    Mozer, Chris Pal, and Yoshua Bengio. Sparse attentive backtracking: Temporal creditassignment
    through reminding. arXiv preprint arXiv:1809.03702, 2018.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Michael
    C Mozer, Chris Pal 和 Yoshua Bengio. 稀疏注意力回溯: 通过提醒进行时间信用分配。arXiv 预印本 arXiv:1809.03702，2018年。'
- en: '[158] Dilruk Perera and Roger Zimmermann. Lstm networks for online cross-network
    recommendations. arXiv preprint arXiv:2008.10849, 2020.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Dilruk Perera 和 Roger Zimmermann. 用于在线跨网络推荐的 LSTM 网络。arXiv 预印本 arXiv:2008.10849，2020年。'
- en: '[159] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra,
    Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene
    decomposition and representation. arXiv preprint arXiv:1901.11390, 2019.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra,
    Irina Higgins, Matt Botvinick 和 Alexander Lerchner. Monet: 无监督场景分解与表示。arXiv 预印本
    arXiv:1901.11390，2019年。'
- en: '[160] Chongxuan Li, Jun Zhu, and Bo Zhang. Learning to generate with memory.
    In International Conference on Machine Learning, pages 1177–1186, 2016.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Chongxuan Li, Jun Zhu 和 Bo Zhang. 学习生成与记忆。在国际机器学习会议，页码 1177–1186，2016年。'
- en: '[161] Sergey Bartunov and Dmitry P Vetrov. Fast adaptation in generative models
    with generative matching networks. arXiv preprint arXiv:1612.02192, 2016.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Sergey Bartunov 和 Dmitry P Vetrov. 使用生成匹配网络在生成模型中快速适应。arXiv 预印本 arXiv:1612.02192，2016年。'
- en: '[162] Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor,
    and Daan Wierstra. One-shot generalization in deep generative models. arXiv preprint
    arXiv:1603.05106, 2016.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor 和
    Daan Wierstra. 深度生成模型中的一次性泛化。arXiv 预印本 arXiv:1603.05106，2016年。'
- en: '[163] Jörg Bornschein, Andriy Mnih, Daniel Zoran, and Danilo J Rezende. Variational
    memory addressing in generative models. arXiv preprint arXiv:1709.07116, 2017.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Jörg Bornschein, Andriy Mnih, Daniel Zoran 和 Danilo J Rezende. 生成模型中的变分记忆寻址。arXiv
    预印本 arXiv:1709.07116，2017年。'
- en: '[164] Carlos Escolano, Marta R Costa-jussà, and José AR Fonollosa. (self-attentive)
    autoencoder-based universal language representation for machine translation. arXiv
    preprint arXiv:1810.06351, 2018.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Carlos Escolano, Marta R Costa-jussà 和 José AR Fonollosa. (自注意力) 基于自编码器的通用语言表示用于机器翻译。arXiv
    预印本 arXiv:1810.06351，2018年。'
- en: '[165] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and
    Łukasz Kaiser. Universal transformers. arXiv:1807.03819 [cs, stat], July 2018.
    arXiv: 1807.03819.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit 和 Łukasz
    Kaiser. 通用变换器。arXiv:1807.03819 [cs, stat]，2018年7月。arXiv: 1807.03819。'
- en: '[166] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with
    stack-augmented recurrent nets. arXiv preprint arXiv:1503.01007, 2015.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Armand Joulin 和 Tomas Mikolov. 使用堆栈增强递归网络推断算法模式。arXiv 预印本 arXiv:1503.01007，2015年。'
- en: '[167] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu,
    and Daan Wierstra. Matching networks for one shot learning. arXiv:1606.04080 [cs,
    stat], June 2016. arXiv: 1606.04080.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu
    和 Daan Wierstra. 一次性学习的匹配网络。arXiv:1606.04080 [cs, stat]，2016年6月。arXiv: 1606.04080。'
- en: '[168] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and
    Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length
    context. arXiv:1901.02860 [cs, stat], January 2019. arXiv: 1901.02860.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, 和
    Ruslan Salakhutdinov. Transformer-xl: 超越固定长度上下文的注意力语言模型。arXiv:1901.02860 [cs,
    stat]，2019年1月。arXiv: 1901.02860。'
- en: '[169] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin.
    Adaptive attention span in transformers. arXiv preprint arXiv:1905.07799, 2019.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, 和 Armand Joulin.
    Transformer 中的自适应注意力跨度。arXiv 预印本 arXiv:1905.07799，2019年。'
- en: '[170] Alexei Baevski and Michael Auli. Adaptive input representations for neural
    language modeling. arXiv preprint arXiv:1809.10853, 2018.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Alexei Baevski 和 Michael Auli. 神经语言建模的自适应输入表示。arXiv 预印本 arXiv:1809.10853，2018年。'
- en: '[171] Michał Daniluk, Tim Rocktäschel, Johannes Welbl, and Sebastian Riedel.
    Frustratingly short attention spans in neural language modeling. arXiv preprint
    arXiv:1702.04521, 2017.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Michał Daniluk, Tim Rocktäschel, Johannes Welbl, 和 Sebastian Riedel.
    神经语言建模中的令人沮丧的短注意力跨度。arXiv 预印本 arXiv:1702.04521，2017年。'
- en: '[172] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured
    attention networks. arXiv:1702.00887 [cs], February 2017. arXiv: 1702.00887.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Yoon Kim, Carl Denton, Luong Hoang, 和 Alexander M. Rush. 结构化注意力网络。arXiv:1702.00887
    [cs]，2017年2月。arXiv: 1702.00887。'
- en: '[173] Jiangtao Feng, Lingpeng Kong, Po-Sen Huang, Chong Wang, Da Huang, Jiayuan
    Mao, Kan Qiao, and Dengyong Zhou. Neural phrase-to-phrase machine translation.
    arXiv:1811.02172 [cs, stat], November 2018. arXiv: 1811.02172.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Jiangtao Feng, Lingpeng Kong, Po-Sen Huang, Chong Wang, Da Huang, Jiayuan
    Mao, Kan Qiao, 和 Dengyong Zhou. 神经短语到短语的机器翻译。arXiv:1811.02172 [cs, stat]，2018年11月。arXiv:
    1811.02172。'
- en: '[174] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with
    relative position representations, 2018.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Peter Shaw, Jakob Uszkoreit, 和 Ashish Vaswani. 带有相对位置表示的自注意力，2018年。'
- en: '[175] Colin Raffel, Douglas Eck, Peter Liu, Ron J. Weiss, and Thang Luong.
    Online and linear-time attention by enforcing monotonic alignments, 2017.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Colin Raffel, Douglas Eck, Peter Liu, Ron J. Weiss, 和 Thang Luong. 通过强制单调对齐实现在线和线性时间注意力，2017年。'
- en: '[176] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau,
    Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations
    using rnn encoder-decoder for statistical machine translation. arXiv:1406.1078
    [cs, stat], June 2014. arXiv: 1406.1078.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau,
    Fethi Bougares, Holger Schwenk, 和 Yoshua Bengio. 使用 RNN 编码器-解码器学习短语表示以用于统计机器翻译。arXiv:1406.1078
    [cs, stat]，2014年6月。arXiv: 1406.1078。'
- en: '[177] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation
    of rare words with subword units. In Proceedings of the 54th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725,
    Berlin, Germany, August 2016\. Association for Computational Linguistics.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] Rico Sennrich, Barry Haddow, 和 Alexandra Birch. 使用子词单元进行稀有词的神经机器翻译。收录于《第54届计算语言学协会年会（第1卷：长篇论文）》，页码
    1715–1725，德国柏林，2016年8月。计算语言学协会。'
- en: '[178] Thomas Zenkel, Joern Wuebker, and John DeNero. Adding interpretable attention
    to neural translation models improves word alignment. arXiv preprint arXiv:1901.11359,
    2019.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Thomas Zenkel, Joern Wuebker, 和 John DeNero. 向神经翻译模型添加可解释的注意力以改善词对齐。arXiv
    预印本 arXiv:1901.11359，2019年。'
- en: '[179] Baosong Yang, Jian Li, Derek F Wong, Lidia S Chao, Xing Wang, and Zhaopeng
    Tu. Context-aware self-attention networks. In Proceedings of the AAAI Conference
    on Artificial Intelligence, pages 387–394, 2019.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Baosong Yang, Jian Li, Derek F Wong, Lidia S Chao, Xing Wang, 和 Zhaopeng
    Tu. 上下文感知自注意力网络。收录于《AAAI 人工智能会议论文集》，页码 387–394，2019年。'
- en: '[180] Baosong Yang, Longyue Wang, Derek Wong, Lidia S Chao, and Zhaopeng Tu.
    Convolutional self-attention networks. arXiv preprint arXiv:1904.03107, 2019.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Baosong Yang, Longyue Wang, Derek Wong, Lidia S Chao, 和 Zhaopeng Tu.
    卷积自注意力网络。arXiv 预印本 arXiv:1904.03107，2019年。'
- en: '[181] Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng
    Tu. Modeling recurrence for transformer. arXiv preprint arXiv:1904.03092, 2019.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, 和 Zhaopeng
    Tu. 针对 transformer 的递归建模。arXiv 预印本 arXiv:1904.03092，2019年。'
- en: '[182] Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov,
    Ann Clifton, and Matt Post. Sockeye: A toolkit for neural machine translation.
    arXiv preprint arXiv:1712.05690, 2017.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov,
    Ann Clifton, 和 Matt Post. Sockeye: 神经机器翻译工具包。arXiv 预印本 arXiv:1712.05690，2017年。'
- en: '[183] Xiangwen Zhang, Jinsong Su, Yue Qin, Yang Liu, Rongrong Ji, and Hongji
    Wang. Asynchronous bidirectional decoding for neural machine translation. In Proceedings
    of the AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil
    Sima’an. Graph convolutional encoders for syntax-aware neural machine translation.
    arXiv preprint arXiv:1704.04675, 2017.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent
    models with fast-forward connections for neural machine translation. Transactions
    of the Association for Computational Linguistics, 4:371–383, 2016.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,
    Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s
    neural machine translation system: Bridging the gap between human and machine
    translation. arXiv preprint arXiv:1609.08144, 2016.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. Tree-to-sequence
    attentional neural machine translation. In Proceedings of the 54th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers), pages
    823–833, 2016.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Jian Li, Baosong Yang, Zi-Yi Dou, Xing Wang, Michael R Lyu, and Zhaopeng
    Tu. Information aggregation for multi-head attention with routing-by-agreement.
    arXiv preprint arXiv:1904.03100, 2019.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way, multilingual
    neural machine translation with a shared attention mechanism. arXiv preprint arXiv:1601.01073,
    2016.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander Rush. Latent
    alignment and variational attention. In S. Bengio, H. Wallach, H. Larochelle,
    K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information
    Processing Systems 31, pages 9712–9724\. Curran Associates, Inc., 2018.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] Haijun Zhang, Jingxuan Li, Yuzhu Ji, and Heng Yue. Understanding subtitles
    by character-level sequence-to-sequence learning. IEEE Transactions on Industrial
    Informatics, 13(2):616–624, 2016.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.
    A decomposable attention model for natural language inference. arXiv:1606.01933
    [cs], June 2016. arXiv: 1606.01933.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and Chengqi
    Zhang. Disan: Directional self-attention network for rnn/cnn-free language understanding.
    In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. Bi-directional
    block self-attention for fast and memory-efficient sequence modeling. arXiv preprint
    arXiv:1804.00857, 2018.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Shuohang Wang and Jing Jiang. Learning natural language inference with
    lstm. arXiv preprint arXiv:1512.08849, 2015.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. Learning natural
    language inference using bidirectional lstm model and inner-attention. arXiv preprint
    arXiv:1605.09090, 2016.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever. Neural programmer:
    Inducing latent programs with gradient descent. arXiv:1511.04834 [cs, stat], November
    2015. arXiv: 1511.04834.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Chen Xing, Yu Wu, Wei Wu, Yalou Huang, and Ming Zhou. Hierarchical recurrent
    attention network for response generation. In Proceedings of the AAAI Conference
    on Artificial Intelligence, 2018.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Liu Yang, Qingyao Ai, Jiafeng Guo, and W. Bruce Croft. anmm: Ranking
    short answer texts with attention-based neural matching model. arXiv:1801.01641
    [cs], January 2018. arXiv: 1801.01641.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Yi Tay, Anh Tuan Luu, Siu Cheung Hui, and Jian Su. Densely connected
    attention propagation for reading comprehension. In S. Bengio, H. Wallach, H. Larochelle,
    K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information
    Processing Systems 31, pages 4906–4917\. Curran Associates, Inc., 2018.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad
    Norouzi, and Quoc V Le. Qanet: Combining local convolution with global self-attention
    for reading comprehension. arXiv preprint arXiv:1804.09541, 2018.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] Wei Wang, Ming Yan, and Chen Wu. Multi-granularity hierarchical attention
    fusion networks for reading comprehension and question answering. In Proceedings
    of the 56th Annual Meeting of the Association for Computational Linguistics, pages
    1705–1714, 2018.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Xiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying Chen, Wayne Xin Zhao,
    Dianhai Yu, and Hua Wu. Multi-turn response selection for chatbots with deep attention
    matching network. In Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), pages 1118–1127, 2018.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Cicero dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. Attentive pooling
    networks. arXiv preprint arXiv:1602.03609, 2016.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Victor Zhong, Caiming Xiong, Nitish Shirish Keskar, and Richard Socher.
    Coarse-grain fine-grain coattention network for multi-evidence question answering.
    arXiv preprint arXiv:1901.00603, 2019.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, and
    Ray Kurzweil. Generating high-quality and informative conversation responses with
    sequence-to-sequence models. arXiv preprint arXiv:1701.03185, 2017.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Chenguang Zhu, Michael Zeng, and Xuedong Huang. Sdnet: Contextualized
    attention-based deep network for conversational question answering. arXiv preprint
    arXiv:1812.03593, 2018.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. Lstm-based deep
    learning models for non-factoid answer selection. arXiv preprint arXiv:1511.04108,
    2015.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William W Cohen, and Ruslan
    Salakhutdinov. Gated-attention readers for text comprehension. arXiv preprint
    arXiv:1606.01549, 2016.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He, Zhanyi Liu, Hua Wu,
    and Jun Zhao. An end-to-end model for question answering over knowledge base with
    cross-attention combining global knowledge. In Proceedings of the 55th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    pages 221–231, 2017.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Rudolf Kadlec, Martin Schmid, Ondřej Bajgar, and Jan Kleindienst. Text
    understanding with the attention sum reader network. In Proceedings of the 54th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers), pages 908–918, 2016.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Tsendsuren Munkhdalai and Hong Yu. Neural tree indexers for text understanding.
    In Proceedings of the conference. Association for Computational Linguistics. Meeting,
    volume 1, page 11\. NIH Public Access, 2017.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Seonhoon Kim, Inho Kang, and Nojun Kwak. Semantic sentence matching with
    densely-connected recurrent and co-attentive information. In Proceedings of the
    AAAI conference on artificial intelligence, pages 6586–6593, 2019.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Lisa Bauer, Yicheng Wang, and Mohit Bansal. Commonsense for generative
    multi-hop question answering tasks. arXiv preprint arXiv:1809.06309, 2018.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] Qiu Ran, Peng Li, Weiwei Hu, and Jie Zhou. Option comparison network
    for multiple-choice reading comprehension. arXiv preprint arXiv:1903.03033, 2019.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt,
    Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend.
    In Advances in neural information processing systems, pages 1693–1701, 2015.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying
    Ma. Topic aware neural response generation. In Proceedings of the AAAI Conference
    on Artificial Intelligence, 2017.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Alessandro Sordoni, Philip Bachman, Adam Trischler, and Yoshua Bengio.
    Iterative alternating neural attention for machine reading. arXiv preprint arXiv:1606.02245,
    2016.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Bingning Wang, Kang Liu, and Jun Zhao. Inner attention based recurrent
    neural networks for answer selection. In Proceedings of the 54th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers), pages
    1288–1297, 2016.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. Attention-based
    lstm for aspect-level sentiment classification. In Proceedings of the 2016 conference
    on empirical methods in natural language processing, pages 606–615, 2016.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. Interactive attention
    networks for aspect-level sentiment classification. arXiv preprint arXiv:1709.00893,
    2017.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Kai Shuang, Xintao Ren, Qianqian Yang, Rui Li, and Jonathan Loo. Aela-dlstms:
    Attention-enabled and location-aware double lstms for aspect-level sentiment classification.
    Neurocomputing, 334:25–34, 2019.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Christos Baziotis, Nikos Pelekis, and Christos Doulkeridis. Datastories
    at semeval-2017 task 4: Deep lstm with attention for message-level and topic-based
    sentiment analysis. In Proceedings of the 11th international workshop on semantic
    evaluation (SemEval-2017), pages 747–754, 2017.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Wei Xue and Tao Li. Aspect based sentiment analysis with gated convolutional
    networks. arXiv preprint arXiv:1805.07043, 2018.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] Shi Feng, Yang Wang, Liran Liu, Daling Wang, and Ge Yu. Attention based
    hierarchical lstm network for context-aware microblog sentiment classification.
    World Wide Web, 22(1):59–81, 2019.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Youwei Song, Jiahai Wang, Tao Jiang, Zhiyue Liu, and Yanghui Rao. Attentional
    encoder network for targeted sentiment classification. arXiv preprint arXiv:1902.09314,
    2019.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Bonggun Shin, Timothy Lee, and Jinho D Choi. Lexicon integrated cnn models
    with attention for sentiment analysis. arXiv preprint arXiv:1610.06272, 2016.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Minchae Song, Hyunjung Park, and Kyung-shik Shin. Attention-based long
    short-term memory network using sentiment lexicon embedding for aspect-level sentiment
    analysis in korean. Information Processing & Management, 56(3):637–653, 2019.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] Jiachen Du, Lin Gui, Yulan He, Ruifeng Xu, and Xuan Wang. Convolution-based
    neural attention with applications to sentiment classification. IEEE Access, 7:27983–27992,
    2019.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. Recurrent attention
    network on memory for aspect sentiment analysis. In Proceedings of the 2017 conference
    on empirical methods in natural language processing, pages 452–461, 2017.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Yukun Ma, Haiyun Peng, and Erik Cambria. Targeted aspect-based sentiment
    analysis via embedding commonsense knowledge into an attentive lstm. In Proceedings
    of the AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] Jiangming Liu and Yue Zhang. Attention modeling for targeted sentiment.
    In Proceedings of the 15th Conference of the European Chapter of the Association
    for Computational Linguistics: Volume 2, Short Papers, pages 572–577, 2017.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] Huimin Chen, Maosong Sun, Cunchao Tu, Yankai Lin, and Zhiyuan Liu. Neural
    sentiment classification with user and product attention. In Proceedings of the
    2016 conference on empirical methods in natural language processing, pages 1650–1659,
    2016.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Jiangfeng Zeng, Xiao Ma, and Ke Zhou. Enhancing attention-based lstm
    with position context for aspect-level sentiment classification. IEEE Access,
    7:20462–20471, 2019.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Heng She, Bin Wu, Bai Wang, and Renjun Chi. Distant supervision for relation
    extraction with hierarchical attention and entity descriptions. In Proc. of IEEE
    IJCNN, pages 1–8\. IEEE, 2018.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] Biao Zhang, Deyi Xiong, Jinsong Su, and Min Zhang. Learning better discourse
    representation for implicit discourse relation recognition via attention networks.
    Neurocomputing, 275:1241–1249, January 2018.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen Wang, and Chengqi
    Zhang. Reinforced self-attention network: a hybrid of hard and soft attention
    for sequence modeling. arXiv:1801.10296 [cs], January 2018. arXiv: 1801.10296.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen, and Xiaodong Shi. Deep
    semantic role labeling with self-attention. In Proceedings of the AAAI Conference
    on Artificial Intelligence, 2018.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] Li Dong and Mirella Lapata. Language to logical form with neural attention.
    In Proceedings of the 54th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), pages 33–43, 2016.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Wenya Wu, Yufeng Chen, Jinan Xu, and Yujie Zhang. Attention-based convolutional
    neural networks for chinese relation extraction. In Chinese Computational Linguistics
    and Natural Language Processing Based on Naturally Annotated Big Data, pages 147–158\.
    Springer, 2018.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and
    Yoshua Bengio. Attention-based models for speech recognition. In Proceedings of
    the 28th International Conference on Neural Information Processing Systems-Volume
    1, pages 577–585, 2015.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] Rohit Prabhavalkar, Tara Sainath, Yonghui Wu, Patrick Nguyen, Zhifeng
    Chen, Chung-Cheng Chiu, and Anjuli Kannan. Minimum word error rate training for
    attention-based sequence-to-sequence models, 2018.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] Kazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann Ney. Language modeling
    with deep transformers. arXiv preprint arXiv:1905.04226, 2019.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] Christoph Lüscher, Eugen Beck, Kazuki Irie, Markus Kitza, Wilfried Michel,
    Albert Zeyer, Ralf Schlüter, and Hermann Ney. Rwth asr systems for librispeech:
    Hybrid vs attention–w/o data augmentation. arXiv preprint arXiv:1905.03072, 2019.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] Linhao Dong, Feng Wang, and Bo Xu. Self-attention aligner: A latency-control
    end-to-end model for asr using self-attention network and chunk-hopping. In ICASSP
    2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP), pages 5656–5660\. IEEE, 2019.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] Julian Salazar, Katrin Kirchhoff, and Zhiheng Huang. Self-attention networks
    for connectionist temporal classification in speech recognition. In ICASSP 2019-2019
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    pages 7115–7119\. IEEE, 2019.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] Xiaofei Wang, Ruizhi Li, Sri Harish Mallidi, Takaaki Hori, Shinji Watanabe,
    and Hynek Hermansky. Stream attention-based multi-array end-to-end speech recognition.
    In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP), pages 7105–7109\. IEEE, 2019.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] Shiyu Zhou, Linhao Dong, Shuang Xu, and Bo Xu. Syllable-based sequence-to-sequence
    speech recognition with the transformer in mandarin chinese. arXiv preprint arXiv:1804.10752,
    2018.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki
    Hayashi. Hybrid ctc/attention architecture for end-to-end speech recognition.
    IEEE Journal of Selected Topics in Signal Processing, 11(8):1240–1253, 2017.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and
    Yoshua Bengio. End-to-end attention-based large vocabulary speech recognition.
    In 2016 IEEE international conference on acoustics, speech and signal processing
    (ICASSP), pages 4945–4949\. IEEE, 2016.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. End-to-end
    continuous speech recognition using attention-based recurrent nn: First results.
    arXiv preprint arXiv:1412.1602, 2014.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] FA Rezaur rahman Chowdhury, Quan Wang, Ignacio Lopez Moreno, and Li Wan.
    Attention-based models for text-dependent speaker verification. In 2018 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5359–5363\.
    IEEE, 2018.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] Yiming Wang, Xing Fan, I-Fan Chen, Yuzong Liu, Tongfei Chen, and Björn
    Hoffmeister. End-to-end anchored speech recognition. In ICASSP 2019-2019 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    pages 7090–7094\. IEEE, 2019.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] Albert Zeyer, Kazuki Irie, Ralf Schlüter, and Hermann Ney. Improved training
    of end-to-end attention models for speech recognition. arXiv preprint arXiv:1805.03294,
    2018.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] Suyoun Kim, Takaaki Hori, and Shinji Watanabe. Joint ctc-attention based
    end-to-end speech recognition using multi-task learning. In 2017 IEEE international
    conference on acoustics, speech and signal processing (ICASSP), pages 4835–4839\.
    IEEE, 2017.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] Koji Okabe, Takafumi Koshinaka, and Koichi Shinoda. Attentive statistics
    pooling for deep speaker embedding. arXiv preprint arXiv:1803.10963, 2018.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] Takaaki Hori, Shinji Watanabe, Yu Zhang, and William Chan. Advances in
    joint ctc-attention based end-to-end speech recognition with a deep cnn encoder
    and rnn-lm. arXiv preprint arXiv:1706.02737, 2017.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] Sumit Chopra, Michael Auli, and Alexander M. Rush. Abstractive sentence
    summarization with attentive recurrent neural networks. In Proceedings of the
    2016 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, pages 93–98, San Diego, California,
    2016\. Association for Computational Linguistics.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model
    for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive
    text summarization using sequence-to-sequence rnns and beyond. arXiv preprint
    arXiv:1602.06023, 2016.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention
    model for abstractive sentence summarization. arXiv preprint arXiv:1509.00685,
    2015.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation.
    arXiv preprint arXiv:1805.04833, 2018.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] Banafsheh Rekabdar, Christos Mousas, and Bidyut Gupta. Generative adversarial
    network with policy gradient for text summarization. In 2019 IEEE 13th international
    conference on semantic computing (ICSC), pages 204–207\. IEEE, 2019.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim,
    Walter Chang, and Nazli Goharian. A discourse-aware attention model for abstractive
    summarization of long documents. arXiv preprint arXiv:1804.05685, 2018.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks.
    IEEE transactions on Signal Processing, 45(11):2673–2681, 1997.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] Miltiadis Allamanis, Hao Peng, and Charles Sutton. A convolutional attention
    network for extreme summarization of source code. In International conference
    on machine learning, pages 2091–2100, 2016.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] Jie Huang, Wengang Zhou, Qilin Zhang, Houqiang Li, and Weiping Li. Video-based
    sign language recognition without temporal segmentation. In Thirty-Second AAAI
    Conference on Artificial Intelligence, 2018.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] Seyedmahdad Mirsamadi, Emad Barsoum, and Cha Zhang. Automatic speech
    emotion recognition using recurrent neural networks with local attention. In 2017
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    pages 2227–2231\. IEEE, 2017.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] Akshi Kumar, Saurabh Raj Sangwan, Anshika Arora, Anand Nayyar, Mohamed
    Abdel-Basset, et al. Sarcasm detection using soft attention-based bidirectional
    long short-term memory model with convolution network. IEEE access, 7:23319–23328,
    2019.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] Kun-Yi Huang, Chung-Hsien Wu, and Ming-Hsiang Su. Attention-based convolutional
    neural network and long short-term memory for short-term detection of mood disorders
    based on elicited speech responses. Pattern Recognition, 88:668–678, 2019.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] Navonil Majumder, Soujanya Poria, Devamanyu Hazarika, Rada Mihalcea,
    Alexander Gelbukh, and Erik Cambria. Dialoguernn: An attentive rnn for emotion
    detection in conversations. In Proceedings of the AAAI Conference on Artificial
    Intelligence, pages 6818–6825, 2019.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] Yuanyuan Zhang, Jun Du, Zirui Wang, Jianshu Zhang, and Yanhui Tu. Attention
    based fully convolutional network for speech emotion recognition. In 2018 Asia-Pacific
    Signal and Information Processing Association Annual Summit and Conference (APSIPA
    ASC), pages 1771–1775. IEEE, 2018.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] Michael Neumann and Ngoc Thang Vu. Attentive convolutional neural network
    based speech emotion recognition: A study on the impact of input features, signal
    length, and acted speech. arXiv preprint arXiv:1706.00612, 2017.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao. Coupled
    multi-layer attentions for co-extraction of aspect and opinion terms. In Proceedings
    of the AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] Gang Liu and Jiabao Guo. Bidirectional lstm with attention mechanism
    and convolutional layer for text classification. Neurocomputing, 337:325–338,
    2019.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] Atta Norouzian, Bogdan Mazoure, Dermot Connolly, and Daniel Willett.
    Exploring attention mechanism for acoustic-based classification of speech utterances
    into system-directed and non-system-directed. In ICASSP 2019-2019 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7310–7314\.
    IEEE, 2019.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] Xinyu Li, Venkata Chebiyyam, and Katrin Kirchhoff. Multi-stream network
    with temporal attention for environmental sound classification. arXiv preprint
    arXiv:1901.08608, 2019.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] Patrick Verga, Emma Strubell, and Andrew McCallum. Simultaneously self-attending
    to all mentions for full-abstract biological relation extraction. arXiv preprint
    arXiv:1802.10569, 2018.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] Xiaoyu Guo, Hui Zhang, Haijun Yang, Lianyuan Xu, and Zhiwen Ye. A single
    attention-based combination of cnn and rnn for relation classification. IEEE Access,
    7:12467–12475, 2019.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao, and
    Bo Xu. Attention-based bidirectional long short-term memory networks for relation
    classification. In Proceedings of the 54th annual meeting of the association for
    computational linguistics (volume 2: Short papers), pages 207–212, 2016.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Neural
    relation extraction with selective attention over instances. In Proceedings of
    the 54th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 2124–2133, 2016.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D
    Manning. Position-aware attention and supervised data improve slot filling. In
    Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,
    pages 35–45, 2017.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] Qian Chen, Zhu Zhuo, and Wen Wang. Bert for joint intent classification
    and slot filling. arXiv preprint arXiv:1902.10909, 2019.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] Minsuk Choi, Cheonbok Park, Soyoung Yang, Yonggyu Kim, Jaegul Choo, and
    Sungsoo Ray Hong. Aila: Attentive interactive labeling assistant for document
    classification through attention-based deep neural networks. In Proceedings of
    the 2019 CHI Conference on Human Factors in Computing Systems, pages 1–12, 2019.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] Qiuqiang Kong, Yong Xu, Wenwu Wang, and Mark D Plumbley. Audio set classification
    with attention model: A probabilistic perspective. In 2018 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 316–320\.
    IEEE, 2018.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] Christoph Alt, Marc Hübner, and Leonhard Hennig. Improving relation extraction
    by pre-trained language representations. arXiv preprint arXiv:1906.03088, 2019.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] Yusuke Yasuda, Xin Wang, Shinji Takaki, and Junichi Yamagishi. Investigation
    of enhanced tacotron text-to-speech synthesis systems with self-attention for
    pitch accent language. In ICASSP 2019-2019 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), pages 6905–6909\. IEEE, 2019.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] Mingyang Zhang, Xin Wang, Fuming Fang, Haizhou Li, and Junichi Yamagishi.
    Joint training framework for text-to-speech and voice conversion using multi-source
    tacotron and wavenet. arXiv preprint arXiv:1903.12389, 2019.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng Kong, Chris Dyer, Graham
    Neubig, and Noah A Smith. What do recurrent neural network grammars learn about
    syntax? arXiv preprint arXiv:1611.05774, 2016.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] Matthias Sperber, Graham Neubig, Jan Niehues, and Alex Waibel. Attention-passing
    models for robust and data-efficient end-to-end speech translation. Transactions
    of the Association for Computational Linguistics, 7:313–325, 2019.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] Hideyuki Tachibana, Katsuya Uenoyama, and Shunsuke Aihara. Efficiently
    trainable text-to-speech system based on deep convolutional networks with guided
    attention. In 2018 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP), pages 4784–4788\. IEEE, 2018.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] Arthur Mensch and Mathieu Blondel. Differentiable dynamic programming
    for structured prediction and attention. arXiv preprint arXiv:1802.03676, 2018.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] Ting Zhang, Bang Liu, Di Niu, Kunfeng Lai, and Yu Xu. Multiresolution
    graph attention networks for relevance matching. In Proceedings of the 27th ACM
    International Conference on Information and Knowledge Management, pages 933–942,
    2018.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang, Xi Chen, Wei Zhang,
    and Huajun Chen. Long-tail relation extraction via knowledge graph embeddings
    and graph convolution networks. arXiv preprint arXiv:1903.01306, 2019.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping
    Hu. Attention-over-attention neural networks for reading comprehension. In Proceedings
    of the 55th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 593–602, 2017.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] Shanshan Liu, Sheng Zhang, Xin Zhang, and Hui Wang. R-trans: Rnn transformer
    network for chinese machine reading comprehension. IEEE Access, 7:27736–27745,
    2019.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] Yiming Cui, Ting Liu, Zhipeng Chen, Shijin Wang, and Guoping Hu. Consensus
    attention-based neural networks for chinese reading comprehension. arXiv preprint
    arXiv:1607.02250, 2016.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] Young-Bum Kim, Dongchan Kim, Anjishnu Kumar, and Ruhi Sarikaya. Efficient
    large-scale domain classification with personalized attention. arXiv preprint
    arXiv:1804.08065, 2018.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil
    Blunsom. Learning to transduce with unbounded memory. In Advances in neural information
    processing systems, pages 1828–1836, 2015.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] Bishan Yang and Tom Mitchell. Leveraging knowledge bases in lstms for
    improving machine reading. arXiv preprint arXiv:1902.09091, 2019.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] Bing Liu and Ian Lane. Attention-based recurrent neural network models
    for joint intent detection and slot filling. arXiv preprint arXiv:1609.01454,
    2016.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] Sina Ahmadi. Attention-based encoder-decoder networks for spelling and
    grammatical error correction. arXiv preprint arXiv:1810.00660, 2018.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] Rajarshi Das, Arvind Neelakantan, David Belanger, and Andrew McCallum.
    Chains of reasoning over entities, relations, and text using recurrent neural
    networks. arXiv preprint arXiv:1607.01426, 2016.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] Octavian-Eugen Ganea and Thomas Hofmann. Deep joint entity disambiguation
    with local neural attention. arXiv preprint arXiv:1704.04920, 2017.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang,
    Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding.
    arXiv preprint arXiv:1703.03130, 2017.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] Timo Schick and Hinrich Schütze. Attentive mimicking: Better word embeddings
    by attending to informative contexts. arXiv preprint arXiv:1904.01617, 2019.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] Yingke Zhu, Tom Ko, David Snyder, Brian Mak, and Daniel Povey. Self-attentive
    speaker embeddings for text-independent speaker verification. In Interspeech,
    pages 3573–3577, 2018.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] Timothy Dozat and Christopher D Manning. Deep biaffine attention for
    neural dependency parsing. arXiv preprint arXiv:1611.01734, 2016.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum.
    Linguistically-informed self-attention for semantic role labeling. arXiv preprint
    arXiv:1804.08199, 2018.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan
    Zhu. Commonsense knowledge aware conversation generation with graph attention.
    In IJCAI, pages 4623–4629, 2018.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] Jing-Xuan Zhang, Zhen-Hua Ling, Li-Juan Liu, Yuan Jiang, and Li-Rong
    Dai. Sequence-to-sequence acoustic modeling for voice conversion. IEEE/ACM Transactions
    on Audio, Speech, and Language Processing, 27(3):631–644, 2019.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] Bo Sun, Yunzong Zhu, Yongkang Xiao, Rong Xiao, and Yungang Wei. Automatic
    question tagging with deep neural networks. IEEE Transactions on Learning Technologies,
    12(1):29–43, 2018.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] Shikhar Sharma, Ryan Kiros, and Ruslan Salakhutdinov. Action recognition
    using visual attention. arXiv preprint arXiv:1511.04119, 2015.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zisserman. Video
    action transformer network. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pages 244–253, 2019.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] Rohit Girdhar and Deva Ramanan. Attentional pooling for action recognition.
    arXiv preprint arXiv:1711.01467, 2017.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] Sijie Song, Cuiling Lan, Junliang Xing, Wenjun Zeng, and Jiaying Liu.
    An end-to-end spatio-temporal attention model for human action recognition from
    skeleton data. In Proceedings of the AAAI conference on artificial intelligence,
    2017.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, and Alex C. Kot. Global context-aware
    attention lstm networks for 3d action recognition. In The IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), July 2017.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] Loris Bazzani, Hugo Larochelle, and Lorenzo Torresani. Recurrent mixture
    density network for spatiotemporal visual attention. arXiv preprint arXiv:1603.08199,
    2016.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] Zhenyang Li, Kirill Gavrilyuk, Efstratios Gavves, Mihir Jain, and Cees GM
    Snoek. Videolstm convolves, attends and flows for action recognition. Computer
    Vision and Image Understanding, 166:41–50, 2018.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] Chih-Yao Ma, Asim Kadav, Iain Melvin, Zsolt Kira, Ghassan AlRegib, and
    Hans Peter Graf. Attend and interact: Higher-order object interactions for video
    understanding. arXiv:1711.06330 [cs], November 2017. arXiv: 1711.06330.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] Yunbo Wang, Lu Jiang, Ming-Hsuan Yang, Li-Jia Li, Mingsheng Long, and
    Li Fei-Fei. Eidetic 3d lstm: A model for video prediction and beyond. In International
    conference on learning representations, 2018.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] Dong Li, Ting Yao, Ling-Yu Duan, Tao Mei, and Yong Rui. Unified spatio-temporal
    attention networks for action recognition in videos. IEEE Transactions on Multimedia,
    21(2):416–428, 2018.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] Pengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jianru Xue, and
    Nanning Zheng. View adaptive recurrent neural networks for high performance human
    action recognition from skeleton data. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 2117–2126, 2017.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] Wu Zheng, Lin Li, Zhaoxiang Zhang, Yan Huang, and Liang Wang. Relational
    network for skeleton-based action recognition. In 2019 IEEE International Conference
    on Multimedia and Expo (ICME), pages 826–831\. IEEE, 2019.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] Harshala Gammulle, Simon Denman, Sridha Sridharan, and Clinton Fookes.
    Two stream lstm: A deep fusion framework for human action recognition. In 2017
    IEEE Winter Conference on Applications of Computer Vision (WACV), pages 177–186\.
    IEEE, 2017.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] Zhaoxuan Fan, Xu Zhao, Tianwei Lin, and Haisheng Su. Attention-based
    multiview re-observation fusion network for skeletal action recognition. IEEE
    Transactions on Multimedia, 21(2):363–374, 2018.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] Chenyang Si, Wentao Chen, Wei Wang, Liang Wang, and Tieniu Tan. An attention
    enhanced graph convolutional lstm network for skeleton-based action recognition.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 1227–1236, 2019.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] Swathikiran Sudhakaran, Sergio Escalera, and Oswald Lanz. Lsta: Long
    short-term attention for egocentric action recognition. In Proc. of the IEEE/CVF
    CVPR, pages 9954–9963, 2019.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] Jun Liu, Gang Wang, Ling-Yu Duan, Kamila Abdiyeva, and Alex C Kot. Skeleton-based
    human action recognition with global context-aware attention lstm networks. IEEE
    Transactions on Image Processing, 27(4):1586–1599, 2017.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] Jiang Liu, Chenqiang Gao, Deyu Meng, and Alexander G Hauptmann. Decidenet:
    Counting varying density crowds through attention guided detection and density
    estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 5197–5206, 2018.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] Lingbo Liu, Hongjun Wang, Guanbin Li, Wanli Ouyang, and Liang Lin. Crowd
    counting using deep recurrent spatial-aware network. arXiv preprint arXiv:1807.00601,
    2018.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] Mohammad Hossain, Mehrdad Hosseinzadeh, Omit Chanda, and Yang Wang. Crowd
    counting using scale-aware attention networks. In 2019 IEEE Winter Conference
    on Applications of Computer Vision (WACV), pages 1280–1288\. IEEE, 2019.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[334] Youmei Zhang, Chunluan Zhou, Faliang Chang, and Alex C Kot. Multi-resolution
    attention convolutional neural network for crowd counting. Neurocomputing, 329:144–152,
    2019.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[335] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
    catastrophic forgetting with hard attention to the task. In International Conference
    on Machine Learning, pages 4548–4557\. PMLR, 2018.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] Kai Han, Jianyuan Guo, Chao Zhang, and Mingjian Zhu. Attribute-aware
    attention model for fine-grained representation learning. arXiv:1901.00392 [cs],
    January 2019. arXiv: 1901.00392.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[337] Pierre Sermanet, Andrea Frome, and Esteban Real. Attention for fine-grained
    categorization. arXiv preprint arXiv:1412.7054, 2014.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] Guoliang Kang, Liang Zheng, Yan Yan, and Yi Yang. Deep adversarial attention
    alignment for unsupervised domain adaptation: the benefit of target expectation
    maximization. arXiv:1801.10068 [cs], January 2018. arXiv: 1801.10068.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[339] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang,
    Xiaogang Wang, and Xiaoou Tang. Residual attention network for image classification.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 3156–3164, 2017.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[340] Paul Hongsuck Seo, Zhe Lin, Scott Cohen, Xiaohui Shen, and Bohyung Han.
    Hierarchical attention networks. CoRR, abs/1606.02393, 2016.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[341] Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia Xu, and Liang Lin. Multi-label
    image recognition by recurrently discovering attentional regions. In Proc. of
    the IEEE ICCV, pages 464–472, 2017.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[342] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V
    Le. Attention augmented convolutional networks. In Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pages 3286–3295, 2019.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[343] Lin Wu, Yang Wang, Xue Li, and Junbin Gao. Deep attention-based spatially
    recursive networks for fine-grained visual recognition. IEEE transactions on cybernetics,
    49(5):1791–1802, 2018.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[344] Bo Zhao, Xiao Wu, Jiashi Feng, Qiang Peng, and Shuicheng Yan. Diversified
    visual attention networks for fine-grained object classification. IEEE Transactions
    on Multimedia, 19(6):1245–1256, 2017.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[345] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Learning embedding
    adaptation for few-shot learning. arXiv preprint arXiv:1812.03664, 7, 2018.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[346] E Pesce, P Ypsilantis, S Withey, R Bakewell, V Goh, and G Montana. Learning
    to detect chest radiographs containing lung nodules using visual attention networks.
    corr. arXiv preprint arXiv:1712.00996, 2017.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[347] Bin Zhao, Xuelong Li, Xiaoqiang Lu, and Zhigang Wang. A cnn–rnn architecture
    for multi-label weather recognition. Neurocomputing, 322:47–57, 2018.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[348] Qilin Yin, Jinwei Wang, Xiangyang Luo, Jiangtao Zhai, Sunil Kr Jha, and
    Yun-Qing Shi. Quaternion convolutional neural network for color image classification
    and forensics. IEEE Access, 7:20293–20301, 2019.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[349] Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, and
    Zheng Zhang. The application of two-level attention models in deep convolutional
    neural network for fine-grained image classification. In Proc. of the IEEE CVPR,
    pages 842–850, 2015.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[350] Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, and Hironobu Fujiyoshi.
    Attention branch network: Learning of attention mechanism for visual explanation.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 10705–10714, 2019.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[351] Hailin Hu, An Xiao, Sai Zhang, Yangyang Li, Xuanling Shi, Tao Jiang,
    Linqi Zhang, Lei Zhang, and Jianyang Zeng. Deephint: understanding hiv-1 integration
    via deep learning with attention. Bioinformatics, 35(10):1660–1667, 2019.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[352] Qingji Guan, Yaping Huang, Zhun Zhong, Zhedong Zheng, Liang Zheng, and
    Yi Yang. Diagnose like a radiologist: Attention guided convolutional neural network
    for thorax disease classification. arXiv preprint arXiv:1801.09927, 2018.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[353] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui
    Shen, and Stan Sclaroff. Top-down neural attention by excitation backprop. International
    Journal of Computer Vision, 126(10):1084–1102, 2018.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[354] Wenguan Wang, Yuanlu Xu, Jianbing Shen, and Song-Chun Zhu. Attentive
    fashion grammar network for fashion landmark detection and clothing category classification.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 4271–4280, 2018.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[355] Wonsik Kim, Bhavya Goyal, Kunal Chawla, Jungmin Lee, and Keunjoo Kwon.
    Attention-based ensemble for deep metric learning. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 736–751, 2018.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[356] Mengye Ren, Renjie Liao, Ethan Fetaya, and Richard Zemel. Incremental
    few-shot learning with attention attractor networks. In Advances in Neural Information
    Processing Systems, pages 5275–5285, 2019.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[357] Yuxin Peng, Xiangteng He, and Junjie Zhao. Object-part attention model
    for fine-grained image classification. IEEE Transactions on Image Processing,
    27(3):1487–1500, 2017.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[358] Hazel Doughty, Walterio Mayol-Cuevas, and Dima Damen. The pros and cons:
    Rank-aware temporal attention for skill determination in long videos. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7862–7871,
    2019.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[359] Yuxin Peng, Xiangteng He, and Junjie Zhao. Object-part attention driven
    discriminative localization for fine-grained image classification. CoRR, abs/1704.01740,
    2017.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[360] Dimitris Kastaniotis, Ioanna Ntinou, Dimitrios Tsourounis, George Economou,
    and Spiros Fotopoulos. Attention-aware generative adversarial networks (ata-gans).
    In 2018 IEEE 13th Image, Video, and Multidimensional Signal Processing Workshop
    (IVMSP), pages 1–5\. IEEE, 2018.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[361] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang.
    Generative image inpainting with contextual attention. arXiv:1801.07892 [cs],
    January 2018. arXiv: 1801.07892.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[362] Scott Reed, Yutian Chen, Thomas Paine, Aäron van den Oord, S. M. Ali
    Eslami, Danilo Rezende, Oriol Vinyals, and Nando de Freitas. Few-shot autoregressive
    density estimation: Towards learning to learn distributions. arXiv:1710.10304
    [cs], October 2017. arXiv: 1710.10304.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[363] Xinyuan Chen, Chang Xu, Xiaokang Yang, and Dacheng Tao. Attention-gan
    for object transfiguration in wild images. In Proceedings of the European Conference
    on Computer Vision (ECCV), pages 164–180, 2018.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[364] Jianan Li, Jimei Yang, Aaron Hertzmann, Jianming Zhang, and Tingfa Xu.
    Layoutgan: Generating graphic layouts with wireframe discriminators. arXiv preprint
    arXiv:1901.06767, 2019.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[365] Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J Corso, and Yan Yan.
    Multi-channel attention selection gan with cascaded semantic guidance for cross-view
    image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pages 2417–2426, 2019.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[366] Qi Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, and Nenghai
    Yu. Online multi-object tracking using cnn-based single object tracker with spatial-temporal
    attention mechanism. In Proceedings of the IEEE International Conference on Computer
    Vision, pages 4836–4845, 2017.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[367] Zhedong Zheng, Liang Zheng, and Yi Yang. Pedestrian alignment network
    for large-scale person re-identification. IEEE Transactions on Circuits and Systems
    for Video Technology, 29(10):3037–3045, 2018.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[368] Xihui Liu, Haiyu Zhao, Maoqing Tian, Lu Sheng, Jing Shao, Shuai Yi, Junjie
    Yan, and Xiaogang Wang. Hydraplus-net: Attentive deep features for pedestrian
    analysis. In Proceedings of the IEEE international conference on computer vision,
    pages 350–359, 2017.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[369] Anfeng He, Chong Luo, Xinmei Tian, and Wenjun Zeng. A twofold siamese
    network for real-time object tracking. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 4834–4843, 2018.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[370] Xinlei Chen, Li-Jia Li, Li Fei-Fei, and Abhinav Gupta. Iterative visual
    reasoning beyond convolutions. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 7239–7248, 2018.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[371] Jianlou Si, Honggang Zhang, Chun-Guang Li, Jason Kuen, Xiangfei Kong,
    Alex C Kot, and Gang Wang. Dual attention matching network for context-aware feature
    sequence based person re-identification. In Proceedings of the IEEE CVPR, pages
    5363–5372, 2018.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[372] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang Wang. Mask-guided contrastive
    attention model for person re-identification. In Proc. of the IEEE CVPR, pages
    1179–1188, 2018.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[373] Yi Zhou and Ling Shao. Aware attentive multi-view inference for vehicle
    re-identification. In Proc. of the IEEE CVPR, pages 6489–6498, 2018.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[374] Xiaoning Zhang, Tiantian Wang, Jinqing Qi, Huchuan Lu, and Gang Wang.
    Progressive attention guided recurrent network for salient object detection. In
    IEEE CVPR), June 2018.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[375] Tao Kong, Fuchun Sun, Chuanqi Tan, Huaping Liu, and Wenbing Huang. Deep
    feature pyramid reconfiguration for object detection. In Proceedings of the European
    conference on computer vision (ECCV), pages 169–185, 2018.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[376] Guanbin Li, Yukang Gan, Hejun Wu, Nong Xiao, and Liang Lin. Cross-modal
    attentional context learning for rgb-d object detection. IEEE Transactions on
    Image Processing, 28(4):1591–1601, 2018.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[377] Shuhan Chen, Xiuli Tan, Ben Wang, and Xuelong Hu. Reverse attention for
    salient object detection. In Proceedings of the European Conference on Computer
    Vision (ECCV), pages 234–250, 2018.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[378] Hao Chen and Youfu Li. Three-stream attention-aware network for rgb-d
    salient object detection. IEEE Transactions on Image Processing, 28(6):2825–2835,
    2019.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[379] Xudong Wang, Zhaowei Cai, Dashan Gao, and Nuno Vasconcelos. Towards universal
    object detection by domain attention. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 7289–7298, 2019.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[380] Wei Li, Xiatian Zhu, and Shaogang Gong. Harmonious attention network
    for person re-identification. In The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), June 2018.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[381] Hao Liu, Jiashi Feng, Meibin Qi, Jianguo Jiang, and Shuicheng Yan. End-to-end
    comparative attention networks for person re-identification. IEEE Transactions
    on Image Processing, 26(7):3492–3506, 2017.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[382] Xingyu Liao, Lingxiao He, Zhouwang Yang, and Chi Zhang. Video-based person
    re-identification via 3d convolutional networks and non-local attention. In Asian
    Conference on Computer Vision, pages 620–634. Springer, 2018.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[383] Dapeng Chen, Hongsheng Li, Tong Xiao, Shuai Yi, and Xiaogang Wang. Video
    person re-identification with competitive snippet-similarity aggregation and co-attentive
    snippet embedding. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 1169–1178, 2018.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[384] Liming Zhao, Xi Li, Yueting Zhuang, and Jingdong Wang. Deeply-learned
    part-aligned representations for person re-identification. In Proceedings of the
    IEEE international conference on computer vision, pages 3219–3228, 2017.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[385] Zhen Zhou, Yan Huang, Wei Wang, Liang Wang, and Tieniu Tan. See the forest
    for the trees: Joint spatial and temporal recurrent neural networks for video-based
    person re-identification. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 4747–4756, 2017.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[386] Jing Xu, Rui Zhao, Feng Zhu, Huaming Wang, and Wanli Ouyang. Attention-aware
    compositional network for person re-identification. In Proceedings of the IEEE
    CVPR, pages 2119–2128, 2018.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[387] Shuang Li, Slawomir Bak, Peter Carr, and Xiaogang Wang. Diversity regularized
    spatiotemporal attention for video-based person re-identification. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 369–378,
    2018.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[388] Meng Zheng, Srikrishna Karanam, Ziyan Wu, and Richard J Radke. Re-identification
    with consistent attentive siamese networks. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 5735–5744, 2019.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[389] Deqiang Ouyang, Yonghui Zhang, and Jie Shao. Video-based person re-identification
    via spatio-temporal attentional and two-stream fusion convolutional networks.
    Pattern Recognition Letters, 117:153–160, 2019.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[390] Cheng Wang, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Mancs:
    A multi-task attentional network with curriculum sampling for person re-identification.
    In Proceedings of the European Conference on Computer Vision (ECCV), pages 365–381,
    2018.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[391] Shuangjie Xu, Yu Cheng, Kang Gu, Yang Yang, Shiyu Chang, and Pan Zhou.
    Jointly attentive spatial-temporal pooling networks for video-based person re-identification.
    In Proceedings of the IEEE international conference on computer vision, pages
    4733–4742, 2017.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[392] Ruimao Zhang, Jingyu Li, Hongbin Sun, Yuying Ge, Ping Luo, Xiaogang Wang,
    and Liang Lin. Scan: Self-and-collaborative attention network for video person
    re-identification. IEEE Transactions on Image Processing, 28(10):4870–4882, 2019.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[393] Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Hanqing Lu. Dual attention
    network for scene segmentation. arXiv:1809.02983 [cs], September 2018. arXiv:
    1809.02983.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[394] Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. Tell me
    where to look: Guided attention inference network. arXiv:1802.10171 [cs], February
    2018. arXiv: 1802.10171.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[395] Mengye Ren and Richard S. Zemel. End-to-end instance segmentation with
    recurrent attention. In The IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), July 2017.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[396] Pingping Zhang, Wei Liu, Hongyu Wang, Yinjie Lei, and Huchuan Lu. Deep
    gated attention networks for large-scale street-level scene segmentation. Pattern
    Recognition, 88:702–714, April 2019.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[397] Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and Alan L Yuille. Attention
    to scale: Scale-aware semantic image segmentation. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 3640–3649, 2016.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[398] Saumya Jetley, Nicholas A. Lord, Namhoon Lee, and Philip H. S. Torr.
    Learn to pay attention. arXiv:1804.02391 [cs], April 2018. arXiv: 1804.02391.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[399] Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task
    learning with attention. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pages 1871–1880, 2019.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[400] Yuhui Yuan and Jingdong Wang. Ocnet: Object context network for scene
    parsing. arXiv preprint arXiv:1809.00916, 2018.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[401] Hanchao Li, Pengfei Xiong, Jie An, and Lingxue Wang. Pyramid attention
    network for semantic segmentation. arXiv preprint arXiv:1805.10180, 2018.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[402] Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, and Pheng-Ann Heng. Direction-aware
    spatial context features for shadow detection. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 7454–7462, 2018.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[403] Zitao Zeng, Weihao Xie, Yunzhe Zhang, and Yao Lu. Ric-unet: An improved
    neural network based on unet for nuclei segmentation in histology images. Ieee
    Access, 7:21420–21428, 2019.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[404] Bing Shuai, Zhen Zuo, Bing Wang, and Gang Wang. Scene segmentation with
    dag-recurrent neural networks. IEEE PAMI, 40(6):1480–1493, 2017.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[405] Xinxin Hu, Kailun Yang, Lei Fei, and Kaiwei Wang. Acnet: Attention based
    network to exploit complementary features for rgbd semantic segmentation. In 2019
    IEEE International Conference on Image Processing (ICIP), pages 1440–1444\. IEEE,
    2019.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[406] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich,
    Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz,
    et al. Attention u-net: Learning where to look for the pancreas. arXiv preprint
    arXiv:1804.03999, 2018.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[407] Ruirui Li, Mingming Li, Jiacheng Li, and Yating Zhou. Connection sensitive
    attention u-net for accurate retinal vessel segmentation. arXiv preprint arXiv:1903.05558,
    2019.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[408] Shu Kong and Charless Fowlkes. Pixel-wise attentional gating for scene
    parsing. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV),
    pages 1024–1033\. IEEE, 2019.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[409] Xiaoxiao Li and Chen Change Loy. Video object segmentation with joint
    re-identification and attention-aware mask propagation. In Proceedings of the
    European Conference on Computer Vision (ECCV), pages 90–105, 2018.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[410] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua
    Lin, and Jiaya Jia. Psanet: Point-wise spatial attention network for scene parsing.
    In Proceedings of the European Conference on Computer Vision (ECCV), pages 267–283,
    2018.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[411] Jason Kuen, Zhenhua Wang, and Gang Wang. Recurrent attentional networks
    for saliency detection. In Proceedings of the IEEE Conference on computer Vision
    and Pattern Recognition, pages 3668–3677, 2016.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[412] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise
    contextual attention for saliency detection. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 3089–3098, 2018.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[413] Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, and Rita Cucchiara.
    Predicting human eye fixations via an lstm-based saliency attentive model. IEEE
    Transactions on Image Processing, 27(10):5142–5154, 2018.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[414] Xiaowei Hu, Chi-Wing Fu, Lei Zhu, Tianyu Wang, and Pheng-Ann Heng. Sac-net:
    Spatial attenuation context for salient object detection. IEEE Transactions on
    Circuits and Systems for Video Technology, 2020.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[415] Tong He, Zhi Tian, Weilin Huang, Chunhua Shen, Yu Qiao, and Changming
    Sun. An end-to-end textspotter with explicit alignment and attention. arXiv:1803.03474
    [cs], March 2018. arXiv: 1803.03474.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[416] Z. Cheng, F. Bai, Y. Xu, G. Zheng, S. Pu, and S. Zhou. Focusing attention:
    Towards accurate text recognition in natural images. In 2017 IEEE International
    Conference on Computer Vision (ICCV), pages 5086–5094, October 2017.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[417] Canjie Luo, Lianwen Jin, and Zenghui Sun. Moran: A multi-object rectified
    attention network for scene text recognition. Pattern Recognition, 90:109–118,
    2019.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[418] Hongtao Xie, Shancheng Fang, Zheng-Jun Zha, Yating Yang, Yan Li, and
    Yongdong Zhang. Convolutional attention networks for scene text recognition. ACM
    Transactions on Multimedia Computing, Communications, and Applications (TOMM),
    15(1s):1–17, 2019.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[419] Hui Li, Peng Wang, Chunhua Shen, and Guyu Zhang. Show, attend and read:
    A simple and strong baseline for irregular text recognition. In Proceedings of
    the AAAI Conference on Artificial Intelligence, pages 8610–8617, 2019.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[420] Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang Pu, and Shuigeng
    Zhou. Focusing attention: Towards accurate text recognition in natural images.
    In Proc. of the IEEE ICCV, pages 5076–5084, 2017.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[421] Miao Xin, Hong Zhang, Mingui Sun, and Ding Yuan. Recurrent temporal sparse
    autoencoder for attention-based action recognition. In 2016 International joint
    conference on neural networks (IJCNN), pages 456–463\. IEEE, 2016.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[422] Jinliang Zang, Le Wang, Ziyi Liu, Qilin Zhang, Gang Hua, and Nanning
    Zheng. Attention-based temporal weighted convolutional neural network for action
    recognition. In IFIP International Conference on Artificial Intelligence Applications
    and Innovations, pages 97–108\. Springer, 2018.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[423] Wenjie Pei, Tadas Baltrusaitis, David Mj Tax, and Louis-Philippe Morency.
    Temporal attention-gated model for robust sequence classification. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6730–6739,
    2017.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[424] Jianshu Zhang, Jun Du, and Lirong Dai. A gru-based encoder-decoder approach
    with attention for online handwritten mathematical expression recognition. In
    2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR),
    volume 1, pages 902–907\. IEEE, 2017.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[425] Zhong Ji, Yanwei Fu, Jichang Guo, Yanwei Pang, Zhongfei Mark Zhang, et al.
    Stacked semantics-guided attention model for fine-grained zero-shot learning.
    In Advances in Neural Information Processing Systems, pages 5995–6004, 2018.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[426] Jo Schlemper, Ozan Oktay, Michiel Schaap, Mattias Heinrich, Bernhard
    Kainz, Ben Glocker, and Daniel Rueckert. Attention gated networks: Learning to
    leverage salient regions in medical images. Medical image analysis, 53:197–207,
    2019.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[427] Xiandong Meng, Xuan Deng, Shuyuan Zhu, Shuaicheng Liu, Chuan Wang, Chen
    Chen, and Bing Zeng. Mganet: A robust model for quality enhancement of compressed
    video. arXiv preprint arXiv:1811.09150, 2018.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[428] Dongwon Park, Jisoo Kim, and Se Young Chun. Down-scaling with learned
    kernels in multi-scale deep neural networks for non-uniform single image deblurring.
    arXiv preprint arXiv:1903.10157, 2019.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[429] Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and Elisa Ricci. Structured
    attention guided convolutional neural fields for monocular depth estimation. In
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 3917–3925, 2018.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[430] Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task
    learning with attention. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pages 1871–1880, 2019.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[431] Masanori Suganuma, Xing Liu, and Takayuki Okatani. Attention-based adaptive
    selection of operations for image restoration in the presence of unknown combined
    distortions. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 9039–9048, 2019.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[432] Rui Qian, Robby T Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu. Attentive
    generative adversarial network for raindrop removal from a single image. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 2482–2491,
    2018.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[433] Youssef Alami Mejjati, Christian Richardt, James Tompkin, Darren Cosker,
    and Kwang In Kim. Unsupervised attention-guided image-to-image translation. In
    Advances in Neural Information Processing Systems, pages 3693–3703, 2018.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[434] Hao Tang, Dan Xu, Nicu Sebe, and Yan Yan. Attention-guided generative
    adversarial networks for unsupervised image-to-image translation. In 2019 International
    Joint Conference on Neural Networks (IJCNN), pages 1–8\. IEEE, 2019.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[435] Sheng Jin, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, Lei Zhang, and
    Xiansheng Hua. Deep saliency hashing. arXiv preprint arXiv:1807.01459, 2018.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[436] Zhan Yang, Osolo Ian Raymond, Wuqing Sun, and Jun Long. Deep attention-guided
    hashing. IEEE Access, 7:11209–11221, 2019.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[437] Huiyan Jiang, Tianyu Shi, Zhiqi Bai, and Liangliang Huang. Ahcnet: An
    application of attention mechanism and hybrid connection for liver tumor segmentation
    in ct volumes. IEEE Access, 7:24898–24909, 2019.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[438] Maximilian Ilse, Jakub M Tomczak, and Max Welling. Attention-based deep
    multiple instance learning. arXiv preprint arXiv:1802.04712, 2018.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[439] Chen-Yu Lee and Simon Osindero. Recursive recurrent nets with attention
    modeling for ocr in the wild. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 2231–2239, 2016.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[440] Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L Yuille, and Xiaogang
    Wang. Multi-context attention for human pose estimation. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 1831–1840, 2017.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[441] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu.
    Image super-resolution using very deep residual channel attention networks. In
    Proceedings of the European Conference on Computer Vision (ECCV), pages 286–301,
    2018.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[442] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention:
    Improving the performance of convolutional neural networks via attention transfer.
    arXiv:1612.03928 [cs], December 2016. arXiv: 1612.03928.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[443] Xiao-Yu Zhang, Haichao Shi, Changsheng Li, Kai Zheng, Xiaobin Zhu, and
    Lixin Duan. Learning transferable self-attentive representations for action recognition
    in untrimmed videos with weak supervision. In Proceedings of the AAAI Conference
    on Artificial Intelligence, pages 9227–9234, 2019.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[444] Adam Bielski and Tomasz Trzcinski. Pay attention to virality: understanding
    popularity of social media videos with the attention mechanism. In Proceedings
    of the IEEE CVPR Workshops, pages 2335–2337, 2018.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[445] Xiang Long, Chuang Gan, Gerard De Melo, Jiajun Wu, Xiao Liu, and Shilei
    Wen. Attention clusters: Purely attention based local feature integration for
    video classification. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 7834–7843, 2018.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[446] Yundong Zhang, Xiang Xu, and Xiaotao Liu. Robust and high performance
    face detector. arXiv preprint arXiv:1901.02350, 2019.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[447] Shengtao Xiao, Jiashi Feng, Junliang Xing, Hanjiang Lai, Shuicheng Yan,
    and Ashraf Kassim. Robust facial landmark detection via recurrent attentive-refinement
    networks. In European conference on computer vision, pages 57–72. Springer, 2016.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[448] Na Lu, Yidan Wu, Li Feng, and Jinbo Song. Deep learning for fall detection:
    Three-dimensional cnn combined with lstm on video kinematic data. IEEE journal
    of biomedical and health informatics, 23(1):314–323, 2018.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[449] Lu Zhang, Zhiyong Liu, Shifeng Zhang, Xu Yang, Hong Qiao, Kaizhu Huang,
    and Amir Hussain. Cross-modality interactive attention network for multispectral
    pedestrian detection. Information Fusion, 50:20–29, 2019.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[450] Shanshan Zhang, Jian Yang, and Bernt Schiele. Occluded pedestrian detection
    through guided attention in cnns. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 6995–7003, 2018.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[451] Yuan Yuan, Zhitong Xiong, and Qi Wang. Vssa-net: vertical spatial sequence
    attention network for traffic sign detection. IEEE transactions on image processing,
    28(7):3423–3434, 2019.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[452] Zbigniew Wojna, Alexander N Gorban, Dar-Shyang Lee, Kevin Murphy, Qian
    Yu, Yeqing Li, and Julian Ibarz. Attention-based extraction of structured information
    from street view imagery. In 2017 14th IAPR International Conference on Document
    Analysis and Recognition (ICDAR), volume 1, pages 844–850\. IEEE, 2017.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[453] Pan He, Weilin Huang, Tong He, Qile Zhu, Yu Qiao, and Xiaolin Li. Single
    shot text detector with regional attention. In Proceedings of the IEEE international
    conference on computer vision, pages 3047–3055, 2017.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[454] Ankan Kumar Bhunia, Aishik Konwer, Ayan Kumar Bhunia, Abir Bhowmick,
    Partha P. Roy, and Umapada Pal. Script identification in natural scene image and
    video frames using an attention based convolutional-lstm network. Pattern Recognition,
    85:172–184, January 2019.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[455] Siyue Xie, Haifeng Hu, and Yongbo Wu. Deep multi-path convolutional neural
    network joint with salient region attention for facial expression recognition.
    Pattern Recognition, 92:177–191, 2019.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[456] Shervin Minaee and Amirali Abdolrashidi. Deep-emotion: Facial expression
    recognition using attentional convolutional network. arXiv preprint arXiv:1902.01019,
    2019.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[457] Yong Li, Jiabei Zeng, Shiguang Shan, and Xilin Chen. Occlusion aware
    facial expression recognition using cnn with attention mechanism. IEEE Transactions
    on Image Processing, 28(5):2439–2450, 2018.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[458] Fan Yang, Lianwen Jin, Songxuan Lai, Xue Gao, and Zhaohai Li. Fully convolutional
    sequence recognition network for water meter number reading. IEEE Access, 7:11679–11687,
    2019.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[459] Dan Xu, Wanli Ouyang, Xavier Alameda-Pineda, Elisa Ricci, Xiaogang Wang,
    and Nicu Sebe. Learning deep structured multi-scale features using attention-gated
    crfs for contour prediction. In Advances in Neural Information Processing Systems,
    pages 3961–3970, 2017.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[460] De Xie, Cheng Deng, Hao Wang, Chao Li, and Dapeng Tao. Semantic adversarial
    network with multi-scale pyramid attention for video classification. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 33, pages 9030–9037,
    2019.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[461] Zhi-Xuan Tan, Arushi Goel, Thanh-Son Nguyen, and Desmond C Ong. A multimodal
    lstm for predicting listener empathic responses over time. In 2019 14th IEEE International
    Conference on Automatic Face & Gesture Recognition (FG 2019), pages 1–4\. IEEE,
    2019.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[462] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal,
    and Tamara L Berg. Mattnet: Modular attention network for referring expression
    comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 1307–1315, 2018.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[463] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald M Summers. Tienet:
    Text-image embedding network for common thorax disease classification and reporting
    in chest x-rays. In Proceedings of the IEEE conference on computer vision and
    pattern recognition, pages 9049–9058, 2018.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[464] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang,
    and Xiaodong He. Attngan: Fine-grained text to image generation with attentional
    generative adversarial networks. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 1316–1324, 2018.'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[465] Jason Poulos and Rafael Valle. Character-based handwritten text transcription
    with attention networks. arXiv preprint arXiv:1712.04046, 2017.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[466] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson,
    Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning
    and visual question answering. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 6077–6086, 2018.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[467] Xinxin Zhu, Lixiang Li, Jing Liu, Ziyi Li, Haipeng Peng, and Xinxin Niu.
    Image captioning with triple-attention and stack parallel lstm. Neurocomputing,
    319:55–65, November 2018.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[468] Shuang Ma, Jianlong Fu, Chang Wen Chen, and Tao Mei. Da-gan: Instance-level
    image translation by deep attention generative adversarial networks. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5657–5666,
    2018.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[469] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. Knowing when
    to look: Adaptive attention via a visual sentinel for image captioning. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 375–383,
    2017.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[470] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image
    captioning with semantic attention. In The IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), June 2016.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[471] Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu,
    and Tat-Seng Chua. Sca-cnn: Spatial and channel-wise attention in convolutional
    networks for image captioning. In The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), July 2017.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[472] Xinwei He, Yang Yang, Baoguang Shi, and Xiang Bai. Vd-san: Visual-densely
    semantic attention network for image caption generation. Neurocomputing, 328:48–55,
    February 2019.'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[473] Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Srivastava, Li Deng,
    Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C Platt, et al.
    From captions to visual concepts and back. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 1473–1482, 2015.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[474] Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, and William W.
    Cohen. Review networks for caption generation. arXiv:1605.07912 [cs], May 2016.
    arXiv: 1605.07912.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[475] Marco Pedersoli, Thomas Lucas, Cordelia Schmid, and Jakob Verbeek. Areas
    of attention for image captioning. arXiv:1612.01033 [cs], December 2016. arXiv:
    1612.01033.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[476] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Exploring visual relationship
    for image captioning. In Proceedings of the European conference on computer vision
    (ECCV), pages 684–699, 2018.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[477] Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang Gan, and Eric P Xing. Recurrent
    topic-transition gan for visual paragraph generation. In Proc. of the IEEE ICCV,
    pages 3362–3371, 2017.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[478] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt
    Schiele. Grounding of textual phrases in images by reconstruction. In European
    Conference on Computer Vision, pages 817–834. Springer, 2016.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[479] Lukasz Kaiser, Aidan N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar,
    Llion Jones, and Jakob Uszkoreit. One model to learn them all. arXiv preprint
    arXiv:1706.05137, 2017.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[480] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked
    cross attention for image-text matching. In Proceedings of the European Conference
    on Computer Vision (ECCV), pages 201–216, 2018.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[481] Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear attention networks
    for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pages 10971–10980, 2020.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[482] Kyunghyun Cho, Aaron Courville, and Yoshua Bengio. Describing multimedia
    content using attention-based encoder–decoder networks. IEEE Transactions on Multimedia,
    17(11):1875–1886, November 2015. arXiv: 1507.01053.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[483] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming
    Xiong. End-to-end dense video captioning with masked transformer. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8739–8748,
    2018.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[484] Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 8746–8755, 2020.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[485] Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and Wei Xu. Video paragraph
    captioning using hierarchical recurrent neural networks. arXiv:1510.07712 [cs],
    October 2015. arXiv: 1510.07712.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[486] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles.
    Dense-captioning events in videos. In Proceedings of the IEEE international conference
    on computer vision, pages 706–715, 2017.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[487] Yi Bin, Yang Yang, Fumin Shen, Ning Xie, Heng Tao Shen, and Xuelong Li.
    Describing video with attention-based bidirectional lstm. IEEE transactions on
    cybernetics, 49(7):2631–2641, 2018.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[488] Lorenzo Baraldi, Costantino Grana, and Rita Cucchiara. Hierarchical boundary-aware
    neural encoder for video captioning. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 1657–1666, 2017.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[489] Silvio Olivastri, Gurkirt Singh, and Fabio Cuzzolin. An end-to-end baseline
    for video captioning. arXiv preprint arXiv:1904.02628, 2019.'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[490] Zhong Ji, Kailin Xiong, Yanwei Pang, and Xuelong Li. Video summarization
    with attention-based encoder–decoder networks. IEEE Transactions on Circuits and
    Systems for Video Technology, 30(6):1709–1717, 2019.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[491] Jingkuan Song, Zhao Guo, Lianli Gao, Wu Liu, Dongxiang Zhang, and Heng Tao
    Shen. Hierarchical lstm with adjusted temporal attention for video captioning.
    arXiv preprint arXiv:1706.01231, 2017.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[492] Xiangpeng Li, Zhilong Zhou, Lijiang Chen, and Lianli Gao. Residual attention-based
    lstm for video captioning. World Wide Web, 22(2):621–636, 2019.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[493] Lianli Gao, Zhao Guo, Hanwang Zhang, Xing Xu, and Heng Tao Shen. Video
    captioning with attention-based lstm and semantic consistency. IEEE Transactions
    on Multimedia, 19(9):2045–2055, 2017.'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[494] Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and Yong Xu. Bidirectional
    attentive fusion with context gating for dense video captioning. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7190–7198,
    2018.'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[495] Junyeong Kim, Minuk Ma, Trung Pham, Kyungsu Kim, and Chang D Yoo. Modality
    shifting attention network for multi-modal video question answering. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10106–10115,
    2020.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[496] Ming Jiang, Shi Chen, Jinhui Yang, and Qi Zhao. Fantastic answers and
    where to find them: Immersive question-directed visual attention. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2980–2989,
    2020.'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[497] Junwei Liang, Lu Jiang, Liangliang Cao, Jia Li, and Alexander Haupmann.
    Focal visual-text attention for visual question answering. In CVPR, 2018.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[498] Drew A Hudson and Christopher D Manning. Compositional attention networks
    for machine reasoning. arXiv preprint arXiv:1803.03067, 2018.'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[499] Ahmed Osman and Wojciech Samek. Dual recurrent attention units for visual
    question answering. arXiv:1802.00209 [cs, stat], February 2018. arXiv: 1802.00209.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[500] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image
    co-attention for visual question answering. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
    I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems
    29, pages 289–297\. Curran Associates, Inc., 2016.'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[501] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear attention networks.
    In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
    editors, Advances in Neural Information Processing Systems 31, pages 1564–1574\.
    Curran Associates, Inc., 2018.'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[502] Duy-Kien Nguyen and Takayuki Okatani. Improved fusion of visual and language
    representations by dense symmetric co-attention for visual question answering.
    In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
    2018.'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[503] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Deep compositional
    question answering with neural module networks. CoRR, abs/1511.02799, 2015.'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[504] Jin-Hwa Kim, Sang-Woo Lee, Donghyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo
    Ha, and Byoung-Tak Zhang. Multimodal residual learning for visual qa. In D. D.
    Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in
    Neural Information Processing Systems 29, pages 361–369\. Curran Associates, Inc.,
    2016.'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[505] Kevin J Shih, Saurabh Singh, and Derek Hoiem. Where to look: Focus regions
    for visual question answering. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 4613–4621, 2016.'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[506] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w:
    Grounded question answering in images. arXiv:1511.03416 [cs], November 2015. arXiv:
    1511.03416.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[507] Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, and Dhruv Batra.
    Best of both worlds: Transferring knowledge from discriminative learning to a
    generative visual dialog model. arXiv preprint arXiv:1706.01554, 2017.'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[508] Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao. Beyond
    bilinear: Generalized multimodal factorized high-order pooling for visual question
    answering. IEEE Trans. Neural Netw. Learn. Syst., 29(12):5947–5959, 2018.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[509] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi.
    Don’t just assume; look and answer: Overcoming priors for visual question answering.
    In Proc. of the IEEE CVPR, pages 4971–4980, 2018.'
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[510] Yan Zhang, Jonathon Hare, and Adam Prügel-Bennett. Learning to count
    objects in natural images for visual question answering. arXiv preprint arXiv:1802.05766,
    2018.'
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[511] Zhe Gan, Yu Cheng, Ahmed El Kholy, Linjie Li, Jingjing Liu, and Jianfeng
    Gao. Multi-step reasoning via recurrent dual attention for visual dialog. arXiv
    preprint arXiv:1902.00579, 2019.'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[512] Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, and Ram
    Nevatia. Abc-cnn: An attention based convolutional neural network for visual question
    answering. arXiv preprint arXiv:1511.05960, 2015.'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[513] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided
    spatial attention for visual question answering. In European Conference on Computer
    Vision, pages 451–466. Springer, 2016.'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[514] Yundong Zhang, Juan Carlos Niebles, and Alvaro Soto. Interpretable visual
    question answering by visual grounding from attention supervision mining. In 2019
    IEEE Winter Conference on Applications of Computer Vision (WACV), pages 349–357\.
    IEEE, 2019.'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[515] Junwei Liang, Lu Jiang, Liangliang Cao, Yannis Kalantidis, Li-Jia Li,
    and Alexander G Hauptmann. Focal visual-text attention for memex question answering.
    IEEE PAMI, 41(8):1893–1908, 2019.'
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[516] Eun-Sol Kim, Woo Young Kang, Kyoung-Woon On, Yu-Jung Heo, and Byoung-Tak
    Zhang. Hypergraph attention networks for multimodal learning. In Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14581–14590,
    2020.'
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[517] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked
    attention networks for image question answering. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 21–29, 2016.'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[518] Dongfei Yu, Jianlong Fu, Tao Mei, and Yong Rui. Multi-level attention
    networks for visual question answering. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 4709–4717, 2017.'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[519] Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. Multi-modal factorized
    bilinear pooling with co-attention learning for visual question answering. In
    Proceedings of the IEEE international conference on computer vision, pages 1821–1830,
    2017.'
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[520] Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. Stamp: short-term
    attention/memory priority model for session-based recommendation. In Proc. of
    the 24th ACM SIGKDD Int. Conf. on Knowledge Discovery & Data Mining, pages 1831–1839,
    2018.'
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[521] Jingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and
    Tat-Seng Chua. Attentive collaborative filtering: Multimedia recommendation with
    item-and component-level attention. In Proceedings of the 40th International ACM
    SIGIR conference on Research and Development in Information Retrieval, pages 335–344,
    2017.'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[522] Hongyang Li, Jun Chen, Ruimin Hu, Mei Yu, Huafeng Chen, and Zengmin Xu.
    Action recognition using visual attention with reinforcement learning. In Ioannis
    Kompatsiaris, Benoit Huet, Vasileios Mezaris, Cathal Gurrin, Wen-Huang Cheng,
    and Stefanos Vrochidis, editors, MultiMedia Modeling, Lecture Notes in Computer
    Science, pages 365–376. Springer International Publishing, 2019.'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[523] Yongming Rao, Jiwen Lu, and Jie Zhou. Attention-aware deep reinforcement
    learning for video face recognition. In Proceedings of the IEEE international
    conference on computer vision, pages 3931–3940, 2017.'
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[524] Marijn Stollenga, Jonathan Masci, Faustino Gomez, and Jürgen Schmidhuber.
    Deep networks with internal selective attention through feedback connections.
    arXiv preprint arXiv:1407.3068, 2014.'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[525] Qingxing Cao, Liang Lin, Yukai Shi, Xiaodan Liang, and Guanbin Li. Attention-aware
    face hallucination via deep reinforcement learning. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 690–698, 2017.'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[526] Donghui Hu, Shengnan Zhou, Qiang Shen, Shuli Zheng, Zhongqiu Zhao, and
    Yuqi Fan. Digital image steganalysis based on visual attention and deep reinforcement
    learning. IEEE Access, 7:25924–25935, 2019.'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[527] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei. End-to-end
    learning of action detection from frame glimpses in videos. In 2016 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pages 2678–2687, Las Vegas,
    NV, USA, June 2016. IEEE.'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[528] John Boaz Lee, Ryan Rossi, and Xiangnan Kong. Graph classification using
    structural attention. In Proc. of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining, pages 1666–1674, 2018.'
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[529] Emilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory
    for deep reinforcement learning. arXiv preprint arXiv:1702.08360, 2017.'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[530] Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li,
    Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart,
    et al. Relational deep reinforcement learning. arXiv preprint arXiv:1806.01830,
    2018.'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[531] Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell,
    Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula.
    arXiv preprint arXiv:1909.07528, 2019.'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[532] Tom Schaul, Tobias Glasmachers, and Jürgen Schmidhuber. High dimensions
    and heavy tails for natural evolution strategies. In Proceedings of the 13th annual
    conference on Genetic and evolutionary computation, pages 845–852, 2011.'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[533] Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas
    Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation
    learning. In Advances in neural information processing systems, pages 1087–1098,
    2017.'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[534] Fei Xue, Xin Wang, Junqiu Wang, and Hongbin Zha. Deep visual odometry
    with adaptive memory. arXiv preprint arXiv:2008.01655, 2020.'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[535] Adrian Johnston and Gustavo Carneiro. Self-supervised monocular trained
    depth estimation using self-attention and discrete disparity volume. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4756–4765,
    2020.'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[536] Xin-Yu Kuo, Chien Liu, Kai-Chen Lin, and Chun-Yi Lee. Dynamic attention-based
    visual odometry. In Proceedings of the IEEE/CVF CVPR Workshops, pages 36–37, 2020.'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[537] Hamed Damirchi, Rooholla Khorrambakht, and Hamid D Taghirad. Exploring
    self-attention for visual odometry. arXiv preprint arXiv:2011.08634, 2020.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[538] Feng Gao, Jincheng Yu, Hao Shen, Yu Wang, and Huazhong Yang. Attentional
    separation-and-aggregation network for self-supervised depth-pose learning in
    dynamic scenes. arXiv preprint arXiv:2011.09369, 2020.'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[539] Xiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, and
    Wanqing Li. Transformer guided geometry model for flow-based unsupervised visual
    odometry. Neural Computing and Applications, pages 1–12, 2021.'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[540] Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid
    Rezatofighi, and Silvio Savarese. Sophie: An attentive gan for predicting paths
    compliant to social and physical constraints. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 1349–1358, 2019.'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[541] Anirudh Vemula, Katharina Muelling, and Jean Oh. Social attention: Modeling
    attention in human crowds. In IEEE ICRA, pages 1–7\. IEEE, 2018.'
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[542] Changan Chen, Yuejiang Liu, Sven Kreiss, and Alexandre Alahi. Crowd-robot
    interaction: Crowd-aware robot navigation with attention-based deep reinforcement
    learning. In 2019 IEEE ICRA, pages 6015–6022\. IEEE, 2019.'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[543] Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese. Scene memory
    transformer for embodied agents in long-horizon tasks. In Proceedings of the IEEE
    CVPR, pages 538–547, 2019.'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[544] Xiaoxue Zang, Ashwini Pokle, Marynel Vázquez, Kevin Chen, Juan Carlos
    Niebles, Alvaro Soto, and Silvio Savarese. Translating navigation instructions
    in natural language to a high-level plan for behavioral robot navigation. arXiv
    preprint arXiv:1810.00663, 2018.'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[545] Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks
    through representation erasure. arXiv preprint arXiv:1612.08220, 2016.'
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[546] Sofia Serrano and Noah A Smith. Is attention interpretable? arXiv preprint
    arXiv:1906.03731, 2019.'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[547] Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention
    in a transformer language model. arXiv preprint arXiv:1906.04284, 2019.'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[548] Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical
    nlp pipeline. arXiv preprint arXiv:1905.05950, 2019.'
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[549] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning.
    What does bert look at? an analysis of bert’s attention. arXiv preprint arXiv:1906.04341,
    2019.'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[550] Sarthak Jain and Byron C Wallace. Attention is not explanation. arXiv
    preprint arXiv:1902.10186, 2019.'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[551] Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui.
    Attention interpretability across nlp tasks. arXiv preprint arXiv:1909.11218,
    2019.'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[552] Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. arXiv
    preprint arXiv:1908.04626, 2019.'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[553] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing
    between capsules. arXiv preprint arXiv:1710.09829, 2017.'
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[554] Jaewoong Choi, Hyun Seo, Suii Im, and Myungjoo Kang. Attention routing
    between capsules. In Proceedings of the IEEE/CVF ICCV Workshops, pages 0–0, 2019.'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[555] Wenkai Huang and Fobao Zhou. Da-capsnet: dual attention mechanism capsule
    network. Scientific Reports, 10(1):1–13, 2020.'
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[556] Assaf Hoogi, Brian Wilcox, Yachee Gupta, and Daniel L Rubin. Self-attention
    capsule networks for image classification. arXiv preprint arXiv:1904.12483, 2019.'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[557] Yao-Hung Hubert Tsai, Nitish Srivastava, Hanlin Goh, and Ruslan Salakhutdinov.
    Capsules with inverted dot-product attention routing. arXiv preprint arXiv:2002.04764,
    2020.'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[558] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature,
    521(7553):436–444, 2015.'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[559] Hassan Khosravi and Bahareh Bina. A survey on statistical relational
    learning. In Canadian conference on artificial intelligence, pages 256–268\. Springer,
    2010.'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[560] Tarek R Besold, Artur d’Avila Garcez, Sebastian Bader, Howard Bowman,
    Pedro Domingos, Pascal Hitzler, Kai-Uwe Kühnberger, Luis C Lamb, Daniel Lowd,
    Priscila Machado Vieira Lima, et al. Neural-symbolic learning and reasoning: A
    survey and interpretation. arXiv preprint arXiv:1711.03902, 2017.'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[561] Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of
    logical rules for knowledge base reasoning. In Advances in Neural Information
    Processing Systems, pages 2319–2328, 2017.'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[562] Peifeng Wang, Jialong Han, Chenliang Li, and Rong Pan. Logic attention
    based neighborhood aggregation for inductive knowledge graph embedding. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 33, pages 7152–7159,
    2019.'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[563] L Vivek Harsha Vardhan, Guo Jia, and Stanley Kok. Probabilistic logic
    graph attention networks for reasoning. In Companion Proceedings of the Web Conference
    2020, pages 669–673, 2020.'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[564] Johan Ferret, Raphaël Marinier, Matthieu Geist, and Olivier Pietquin.
    Self-attentional credit assignment for transfer in reinforcement learning. arXiv
    preprint arXiv:1907.08027, 2019.'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[565] Cynthia Rudin. Please stop explaining black box models for high stakes
    decisions. arXiv preprint arXiv:1811.10154, 1, 2018.'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[566] Mark O Riedl. Human-centered artificial intelligence and machine learning.
    Human Behavior and Emerging Technologies, 1(1):33–36, 2019.'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[567] Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, and Xilin Chen.
    Attgan: Facial attribute editing by only changing what you want. IEEE Transactions
    on Image Processing, 28(11):5464–5478, 2019.'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[568] Biao Zhang, Deyi Xiong, and Jinsong Su. Battrae: Bidimensional attention-based
    recursive autoencoders for learning bilingual phrase embeddings. In Proceedings
    of the AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[569] Tian Tian and Zheng Felix Fang. Attention-based autoencoder topic model
    for short texts. Procedia Computer Science, 151:1134–1139, 2019.'
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
