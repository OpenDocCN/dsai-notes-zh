- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:56:11'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2103.16775] Attention, please! A survey of Neural Attention Models in Deep
    Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2103.16775](https://ar5iv.labs.arxiv.org/html/2103.16775)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Attention, please! A survey of Neural Attention Models in Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alana de Santana Correia, and Esther Luna Colombini,
  prefs: []
  type: TYPE_NORMAL
- en: Institute of Computing, University of Campinas, Av. Albert Einstein, 1251 -
    Campinas, SP - Brazil
  prefs: []
  type: TYPE_NORMAL
- en: 'e-mail: {alana.correia, esther}@ic.unicamp.br Laboratory of Robotics and Cognitive
    Systems (LaRoCS)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In humans, Attention is a core property of all perceptual and cognitive operations.
    Given our limited ability to process competing sources, attention mechanisms select,
    modulate, and focus on the information most relevant to behavior. For decades,
    concepts and functions of attention have been studied in philosophy, psychology,
    neuroscience, and computing. For the last six years, this property has been widely
    explored in deep neural networks. Currently, the state-of-the-art in Deep Learning
    is represented by neural attention models in several application domains. This
    survey provides a comprehensive overview and analysis of developments in neural
    attention models. We systematically reviewed hundreds of architectures in the
    area, identifying and discussing those in which attention has shown a significant
    impact. We also developed and made public an automated methodology to facilitate
    the development of reviews in the area. By critically analyzing 650 works, we
    describe the primary uses of attention in convolutional, recurrent networks and
    generative models, identifying common subgroups of uses and applications. Furthermore,
    we describe the impact of attention in different application domains and their
    impact on neural networks’ interpretability. Finally, we list possible trends
    and opportunities for further research, hoping that this review will provide a
    succinct overview of the main attentional models in the area and guide researchers
    in developing future approaches that will drive further improvements.
  prefs: []
  type: TYPE_NORMAL
- en: '*K*eywords Survey  $\cdot$ Attention Mechanism  $\cdot$ Neural Networks  $\cdot$
    Deep Learning  $\cdot$ Attention Models.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Attention is a behavioral and cognitive process of focusing selectively on a
    discrete aspect of information, whether subjective or objective, while ignoring
    other perceptible information [[1](#bib.bib1)], playing an essential role in human
    cognition and the survival of living beings in general. In animals of lower levels
    in the evolutionary scale, it provides perceptual resource allocation allowing
    these beings to respond correctly to the environment’s stimuli to escape predators
    and capture preys efficiently. In human beings, attention acts on practically
    all mental processes, from reactive responses to unexpected stimuli in the environment
    - guaranteeing our survival in the presence of danger - to complex mental processes,
    such as planning, reasoning, and emotions. Attention is necessary because, at
    any moment, the environment presents much more perceptual information than can
    be effectively processed, the memory contains more competing traits than can be
    remembered, and the choices, tasks, or motor responses available are much greater
    than can be dealt with [[2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: At early sensorial processing stages, data is separated between sight, hearing,
    touch, smell, and taste. At this level, Attention selects and modulates processing
    within each of the five modalities and directly impacts processing in the relevant
    cortical regions. For example, attention to visual stimuli increases discrimination
    and activates the relevant topographic areas in the retinotopic visual cortex [[3](#bib.bib3)],
    allowing observers to detect contrasting stimuli or make more precise discriminations.
    In hearing, attention allows listeners to detect weaker sounds or differences
    in extremely subtle tones but essential for recognizing emotions and feelings [[4](#bib.bib4)].
    Similar effects of attention operate on the somatosensory cortex [[5](#bib.bib5)],
    olfactory cortex [[6](#bib.bib6)], and gustatory cortex [[7](#bib.bib7)]. In addition
    to sensory perception, our cognitive control is intrinsically attentional. Our
    brain has severe cognitive limitations - the number of items that can be kept
    in working memory, the number of choices that can be selected, and the number
    of responses that can be generated at any time are limited. Hence, evolution has
    favored selective attention concepts as the brain has to prioritize.
  prefs: []
  type: TYPE_NORMAL
- en: Long before contemporary psychologists entered the discussion on Attention,
    William James [[8](#bib.bib8)] offered us a precise definition that has been,
    at least, partially corroborated more than a century later by neurophysiological
    studies. According to James, “Attention implies withdrawal from some things in
    order to deal effectively with others… Millions of items of the outward order
    are present to my senses which never properly enter into my experience. Why? Because
    they have no interest for me. My experience is what I agree to attend to. Only
    those items which I notice shape my mind — without selective interest, experience
    is an utter chaos.” Indeed, the first scientific studies of Attention have been
    reported by Herman Von Helmholtz (1821-1894) and William James (1890-1950) in
    the nineteenth century. They both conducted experiments to understand the role
    of Attention.
  prefs: []
  type: TYPE_NORMAL
- en: For the past decades, the concept of attention has permeated most aspects of
    research in perception and cognition, being considered as a property of multiple
    and different perceptual and cognitive operations [[1](#bib.bib1)]. Thus, to the
    extent that these mechanisms are specialized and decentralized, attention reflects
    this organization. These mechanisms are in wide communication, and the executive
    control processes help set priorities for the system. Selection mechanisms operate
    throughout the brain and are involved in almost every stage, from sensory processing
    to decision making and awareness. Attention has become a broad term to define
    how the brain controls its information processing, and its effects can be measured
    through conscious introspection, electrophysiology, and brain imaging. Attention
    has been studied from different perspectives for a long time.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Pre-Deep Learning Models of Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Computational attention systems based on psychophysical models, supported by
    neurobiological evidence, have existed for at least three decades [[9](#bib.bib9)].
    Treisman’s Feature Integration Theory (FIT)  [[10](#bib.bib10)], Wolfe’s Guides
    Search  [[11](#bib.bib11)], Triadic architecture  [[12](#bib.bib12)], Broadbent’s
    Model  [[13](#bib.bib13)], Norman Attentional Model  [[14](#bib.bib14)]  [[15](#bib.bib15)],
    Closed-loop Attention Model  [[16](#bib.bib16)], SeLective Attention Model  [[17](#bib.bib17)],
    among several other models, introduced the theoretical basis of computational
    attention systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, attention was mainly studied with visual experiments where a subject
    looks at a scene that changes in time [[18](#bib.bib18)]. In these models, the
    attentional system was restricted only to the selective attention component in
    visual search tasks, focusing on the extraction of multiple features through a
    sensor. Therefore, most of the attentional computational models occurred in computer
    vision to select important image regions. Koch and Ullman  [[19](#bib.bib19)]
    introduced the area’s first visual attention architecture based on FIT  [[10](#bib.bib10)].
    The idea behind it is that several features are computed in parallel, and their
    conspicuities are collected on a salience map. Winner-Take-All (WTA) determines
    the most prominent region on the map, which is finally routed to the central representation.
    From then on, only the region of interest proceeds to more specific processing.
    Neuromorphic Vision Toolkit (NVT), derived from the Koch-Ullman  [[20](#bib.bib20)]
    model, was the basis for developing research in computational visual attention
    for several years. Navalpakkam and Itti introduce a derivative of NVT which can
    deal with top-down cues  [[21](#bib.bib21)]. The idea is to learn the target’s
    feature values from a training image in which a binary mask indicates the target.
    The attention system of Hamker  [[22](#bib.bib22)]  [[23](#bib.bib23)] calculates
    various features and contrast maps and turns them into perceptual maps. With target
    information influencing processing, they combine detection units to determine
    whether a region on the perceptual map is a candidate for eye movement. VOCUS
     [[24](#bib.bib24)] introduced a way to combine bottom-up and top-down attention,
    overcoming the limitations of the time. Several other models have emerged in the
    literature, each with peculiarities according to the task. Many computational
    attention systems focus on the computation of mainly three features: intensity,
    orientation, and color. These models employed neural networks or filter models
    that use classical linear filters to compute features.'
  prefs: []
  type: TYPE_NORMAL
- en: Computational attention systems were used successfully before Deep Learning
    (DL) in object recognition  [[25](#bib.bib25)], image compression  [[26](#bib.bib26)],
    image matching  [[27](#bib.bib27)], image segmentation  [[26](#bib.bib26)], object
    tracking  [[28](#bib.bib28)], active vision  [[29](#bib.bib29)], human-robot interaction
     [[30](#bib.bib30)], object manipulation in robotics  [[31](#bib.bib31)], robotic
    navigation  [[32](#bib.bib32)], and SLAM  [[33](#bib.bib33)]. In mid-1997, Scheier
    and Egner  [[34](#bib.bib34)] presented a mobile robot that uses attention for
    navigation. Still, in the 90s, Baluja and Pomerleau  [[35](#bib.bib35)] used an
    attention system to navigate an autonomous car, which followed relevant regions
    of a projection map. Walther  [[27](#bib.bib27)] combined an attentional system
    with an object recognizer based on SIFT features and demonstrated that the attentional
    front-end enhanced the recognition results. Salah et al.  [[25](#bib.bib25)] combined
    attention with neural networks in an Observable Markov model for handwritten digit
    recognition and face recognition. Ouerhani et al.  [[26](#bib.bib26)] proposed
    the focused image compression, which determines the number of bits to be allocated
    for encoding regions of an image according to their salience. High saliency regions
    have a high quality of reconstruction concerning the rest of the image.
  prefs: []
  type: TYPE_NORMAL
- en: '1.2 Deep Learning Models of Attention: the beginning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By 2014, the DL community noticed attention as a fundamental concept for advancing
    deep neural networks. Currently, the state-of-the-art in the field uses neural
    attention models. As shown in figure [1](#S1.F1 "Figure 1 ‣ 1.2 Deep Learning
    Models of Attention: the beginning ‣ 1 Introduction ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning"), the number of published works grows
    each year significantly in the leading repositories. In neural networks, attention
    mechanisms dynamically manage the flow of information, the features, and the resources
    available, improving learning. These mechanisms filter out irrelevant stimuli
    for the task and help the network to deal with long-time dependencies simply.
    Many neural attentional models are simple, scalable, flexible, and with promising
    results in several application domains [[36](#bib.bib36)] [[37](#bib.bib37)] [[38](#bib.bib38)].
    Given the current research extent, interesting questions related to neural attention
    models arise in the literature: how these mechanisms help improve neural networks’
    performance, which classes of problems benefit from this approach, and how these
    benefits arise.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24f120d18f94a4e1ff8f1a9dbda04564.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Works published by year between 01/01/2014 to 15/02/2021\. The main
    sources collected are ArXiv, CVPR, ICCV, ICLR, IJCNN, NIPS, and AAAI. The other
    category refers mainly to the following publishing vehicles: ICML, ACL, ACM, EMNLP,
    ICRA, ICPR, ACCV, CORR, ECCV, ICASSP, ICLR, IEEE ACCESS, Neurocomputing, and several
    other magazines.'
  prefs: []
  type: TYPE_NORMAL
- en: To the best of our knowledge, most surveys available in the literature do not
    address all of these questions or are more specific to some domain. Wang et al. [[39](#bib.bib39)]
    propose a review on recurrent networks and applications in computer vision, Hu [[40](#bib.bib40)],
    and Galassi et al. [[41](#bib.bib41)] offer surveys on attention in natural language
    processing (NLP). Lee et al. [[42](#bib.bib42)] present a review on attention
    in graph neural networks, and Chaudhari et al. [[43](#bib.bib43)] presented a
    more general, yet short, review.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Contributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To assess the breadth of attention applications in deep neural networks, we
    present a systemic review of the field in this survey. Throughout our review,
    we critically analyzed 650 papers while addressing quantitatively 6,567.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the main contributions of our work, we highlight:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A replicable research methodology. We provide, in the Appendix, the detailed
    process conducted to collect our data and we make available the scripts to collect
    the papers and create the graphs we use;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An in-depth overview of the field. We critically analyzed 650 papers and extracted
    different metrics from 6,567, employing various visualization techniques to highlight
    overall trends in the area;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We describe the main attentional mechanisms;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We present the main neural architectures that employ attention mechanisms, describing
    how they have contributed to the NN field;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We introduce how attentional modules or interfaces have been used in classic
    DL architectures extending the Neural Network Zoo diagrams;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we present a broad description of application domains, trends, and
    research opportunities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This survey is structured as follows. In Section [2](#S2 "2 Overview ‣ Attention,
    please! A survey of Neural Attention Models in Deep Learning") we present the
    field overview reporting the main events from 2014 to the present. Section [3](#S3
    "3 Attention Mechanisms ‣ Attention, please! A survey of Neural Attention Models
    in Deep Learning") contains a description of attention main mechanisms. In Section [4](#S4
    "4 Attention-based Classic Deep Learning Architectures ‣ Attention, please! A
    survey of Neural Attention Models in Deep Learning") we analyze how attentional
    modules are used in classic DL architectures. Section [5](#S5 "5 Applications
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning") explains
    the main classes of problems and applications of attention. Finally, in Section [6](#S6
    "6 Trends and Opportunities ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning") we discuss limitations, open challenges, current trends,
    and future directions in the area, concluding our work in section [7](#S7 "7 Conclusions
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning") with
    directions for further improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Historically, research in computational attention systems has existed since
    the 1980s. Only in mid-2014, the Neural Attentional Networks (NANs) emerged in
    Natural Language Processing (NLP), where attention provided significant advances,
    bringing promising results through scalable and straightforward networks. Attention
    allowed us to move towards the complex tasks of conversational machine comprehension,
    sentiment analysis, machine translation, question-answering, and transfer learning,
    previously challenging. Subsequently, NANs appeared in other fields equally important
    for artificial intelligence, such as computer vision, reinforcement learning,
    and robotics. There are currently numerous attentional architectures, but few
    of them have a significantly higher impact, as shown in Figure [2](#S2.F2 "Figure
    2 ‣ 2 Overview ‣ Attention, please! A survey of Neural Attention Models in Deep
    Learning"). In this image, we depict the most relevant group of works organized
    according to citation levels and innovations where RNNSearch [[44](#bib.bib44)],
    Transformer [[37](#bib.bib37)], Memory Networks [[38](#bib.bib38)], “show, attend
    and tell” [[45](#bib.bib45)], and RAM [[46](#bib.bib46)] stand out as key developments.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f446c62d2808514ee63dab2c00b96b4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Main Neural Attention Networks (NAN). Each circle corresponds to
    an architecture. The radius of the circles is defined based on the impact of the
    NAN on the field. The impact was defined by the citation number and the architecture
    innovation level. The greater the radius of the circle, the more significant the
    impact of architecture, and vice versa. Architectures labels are color-coded as
    follows: orange - natural language processing, red - computer vision, dark brown
    - computer vision and natural language processing, dark yellow - reinforcement
    learning and computer vision, light yellow - reinforcement learning and natural
    language processing, blue - imitation learning and robotics, and purple - others.'
  prefs: []
  type: TYPE_NORMAL
- en: The bottleneck problem in the classic encoder-decoder framework worked as the
    initial motivation for attention research in Deep Learning. In this framework,
    the encoder encodes a source sentence into a fixed-length vector from which a
    decoder generates the translation. The main issue is that a neural network needs
    to compress all the necessary information from a source sentence into a fixed-length
    vector. Cho et al.  [[47](#bib.bib47)] showed that the performance of the classic
    encoder-decoder deteriorates rapidly as the size of the input sentence increases.
    To minimize this bottleneck, Bahdanau et al. [[44](#bib.bib44)] proposed RNNSearch,
    an extension to the encoder-decoder model that learns to align and translate together.
    RNNSearch generates a translated word at each time-step, looking for a set of
    positions in the source sentence with the most relevant words. The model predicts
    a target word based on the context vectors associated with those source positions
    and all previously generated target words. The main advantage is that RNNSearch
    does not encode an entire input sentence into a single fixed-length vector. Instead,
    it encodes the input sentence into a sequence of vectors, choosing a subset of
    these vectors adaptively while generating the translation. The attention mechanism
    allows extra information to be propagated through the network, eliminating the
    fixed-size context vector’s information bottleneck. This approach demonstrated
    that the attentive model outperforms classic encoder-decoder frameworks for long
    sentences for the first time.
  prefs: []
  type: TYPE_NORMAL
- en: RNNSearch was instrumental in introducing the first attention mechanism, soft
    attention (Section [3](#S3 "3 Attention Mechanisms ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning")). This mechanism has the main characteristic
    of smoothly selecting the network’s most relevant elements. Based on RNNSearch,
    there have been numerous attempts to augment neural networks with new properties.
    Two research directions stand out as particularly interesting - attentional interfaces
    and end-to-end attention. Attentional interfaces treat attention as a module or
    set of elective modules, easily plugged into classic Deep Learning neural networks,
    just like RNNSearch. So far, this is the most explored research direction in the
    area, mainly for simplicity, general use, and the good results of generalization
    that the attentional interfaces bring. End-to-end attention is a younger research
    direction, where the attention block covers the entire neural network. High and
    low-level attentional layers act recursively or cascaded at all network abstraction
    levels to produce the desired output in these models. End-to-end attention models
    introduce a new class of neural networks in Deep Learning. End-to-end attention
    research makes sense since no isolated attention center exists in the human brain,
    and its mechanisms are used in different cognitive processes.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Attentional interfaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RNNSearch is the basis for research on attentional interfaces. The attentional
    module of this architecture is widely used in several other applications. In voice
    recognition [[48](#bib.bib48)], allowing one RNN to process the audio while another
    examines it focusing on the relevant parts as it generates a description. In-text
    analysis [[49](#bib.bib49)], it allows a model to look at the words as it generates
    an analysis tree. In conversational modeling [[50](#bib.bib50)], it allows the
    model to focus on the last parts of the conversation as it generates its response.
    There are also important extensions to deal with other information bottlenecks
    in addition to the classic encoder-decoder problem. BiDAF [[51](#bib.bib51)] proposes
    a multi-stage hierarchical process to question-answering. It uses the bidirectional
    attention flow to build a multi-stage hierarchical network with context paragraph
    representations at different granularity levels. The attention layer does not
    summarize the context paragraph in a fixed-length vector. Instead, attention is
    calculated for each step, and the vector assisted at each step, along with representations
    of previous layers, can flow to the subsequent modeling layer. This reduces the
    loss of information caused by the early summary. At each stage of time, attention
    is only a function of the query and the paragraph of the context in the current
    stage and does not depend directly on the previous stage’s attention. The hypothesis
    is that this simplification leads to a work division between the attention layer
    and the modeling layer, forcing the attention layer to focus on learning attention
    between the query and the context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yang et al. [[52](#bib.bib52)] proposed the Hierarchical Attention Network
    (HAN) to capture two essential insights about document structure. Documents have
    a hierarchical structure: words form sentences, sentences form a document. Humans,
    likewise, construct a document representation by first building representations
    of sentences and then aggregating them into a document representation. Different
    words and sentences in a document are differentially informative. Moreover, the
    importance of words and sentences is highly context-dependent, i.e., the same
    word or sentence may have different importance in different contexts. To include
    sensitivity to this fact, HAN consists of two levels of attention mechanisms -
    one at the word level and one at the sentence level - that let the model pay more
    or less attention to individual words and sentences when constructing the document’s
    representation. Xiong et al. [[53](#bib.bib53)] created a coattentive encoder
    that captures the interactions between the question and the document with a dynamic
    pointing decoder that alternates between estimating the start and end of the answer
    span. To learn approximate solutions to computationally intractable problems,
    Ptr-Net [[54](#bib.bib54)] modifies the RNNSearch’s attentional mechanism to represent
    variable-length dictionaries. It uses the attention mechanism as a pointer.'
  prefs: []
  type: TYPE_NORMAL
- en: See et. al. [[55](#bib.bib55)] used a hybrid between classic sequence-to-sequence
    attentional models and a Ptr-Net [[54](#bib.bib54)] to abstractive text summarization.
    The hybrid pointer-generator [[55](#bib.bib55)] copies words from the source text
    via pointing, which aids accurate reproduction of information while retaining
    the ability to produce novel words through the generator. Finally, it uses a mechanism
    to keep track of what has been summarized, which discourages repetition. FusionNet [[56](#bib.bib56)]
    presents a novel concept of "history-of-word" to characterize attention information
    from the lowest word-embedding level up to the highest semantic-level representation.
    This concept considers that data input is gradually transformed into a more abstract
    representation, forming each word’s history in human mental flow. FusionNet employs
    a fully-aware multi-level attention mechanism and an attention score-function
    that takes advantage of the history-of-word. Rocktäschel et al. [[57](#bib.bib57)]
    introduce two-away attention for recognizing textual entailment (RTE). The mechanism
    allows the model to attend over past output vectors, solving the LSTM’s cell state
    bottleneck. The LSTM with attention does not need to capture the premise’s whole
    semantics in the LSTM cell state. Instead, attention generates output vectors
    while reading the premise and accumulating a representation in the cell state
    that informs the second LSTM which of the premises’ output vectors to attend to
    determine the RTE class. Luong, et al. [[58](#bib.bib58)], proposed global and
    local attention in machine translation. Global attention is similar to soft attention,
    while local is an improvement to make hard attention differentiable - the model
    first provides for a single position aligned to the current target word, and a
    window centered around the position is used to calculate a vector of context.
  prefs: []
  type: TYPE_NORMAL
- en: Attentional interfaces have also emerged in architectures for computer vision
    tasks. Initially, they are based on human saccadic movements and robustness to
    change. The human visual attention mechanism can explore local differences in
    an image while highlighting the relevant parts. One person focuses attention on
    parts of the image simultaneously, glimpsing to quickly scan the entire image
    to find the main areas during the recognition process. In this process, the different
    regions’ internal relationship guides the eyes’ movement to find the next area
    to focus. Ignoring the irrelevant parts makes it easier to learn in the presence
    of disorder. Another advantage of glimpse and visual attention is its robustness.
    Our eyes can see an object in a real-world scene but ignore irrelevant parts.
    Convolutional neural networks (CNNs) are extremely different. CNNs are rigid,
    and the number of parameters grows linearly with the size of the image. Also,
    for the network to capture long-distance dependencies between pixels, the architecture
    needs to have many layers, compromising the model’s convergence. Besides, the
    network treats all pixels in the same way. This process does not resemble the
    human visual system that contains visual attention mechanisms and a glimpse structure
    that provides unmatched performance in object recognition.
  prefs: []
  type: TYPE_NORMAL
- en: RAM [[46](#bib.bib46)] and STN are pioneering architectures with attentional
    interfaces based on human visual attention. RAM [[46](#bib.bib46)] can extract
    information from an image or video by adaptively selecting a sequence of regions,
    glimpses, only processing the selected areas at high resolution. The model is
    a Recurrent Neural Network that processes different parts of the images (or video
    frames) at each instant of time t, building a dynamic internal representation
    of the scene via Reinforcement Learning training. The main model advantages are
    the reduced number of parameters and the architecture’s independence to the input
    image size, which does not occur in convolutional neural networks. This approach
    is generic. It can use static images, videos, or a perceptual module of an agent
    that interacts with the environment. STN (Spatial Transformer Network) [[59](#bib.bib59)]
    is a module robust to spatial transformation changes. In STN, if the input is
    transformed, the model must generate the correct classification label, even if
    it is distorted in unusual ways. STN works as an attentional module attachable
    – with few modifications – to any neural network to actively spatially transform
    feature maps. STN learns transformation during the training process. Unlike pooling
    layers, where receptive fields are fixed and local, a Spatial Transformer is a
    dynamic mechanism that can spatially transform an image, or feature map, producing
    the appropriate transformation for each input sample. The transformation is performed
    across the map and may include changes in scale, cut, rotations, and non-rigid
    body deformations. This approach allows the network to select the most relevant
    image regions (attention) and transform them into a desired canonical position
    by simplifying recognition in the following layers.
  prefs: []
  type: TYPE_NORMAL
- en: Following the RAM approach, the Deep Recurrent Attentive Writer (DRAW) [[36](#bib.bib36)]
    represents a change to a more natural way of constructing the image in which parts
    of a scene are created independently of the others. This process is how human
    beings draw a scene by recreating a visual scene sequentially, refining all parts
    of the drawing for several iterations, and reevaluating their work after each
    modification. Although natural to humans, most approaches to automatic image generation
    aim to generate complete scenes at once. This means that all pixels are conditioned
    in a single latent distribution, making it challenging to scale large image approaches.
    DRAW belongs to the family of variational autoencoders. It has an encoder that
    compresses the images presented during training and a decoder that reconstructs
    the images. Unlike other generative models, DRAW iteratively constructs the scenes
    by accumulating modifications emitted by the decoder, each observed by the encoder.
    DRAW uses RAM attention mechanisms to attend to parts of the scene while ignoring
    others selectively. This mechanism’s main challenge is to learn where to look,
    which is usually addressed by reinforcement learning techniques. However, at DRAW,
    the attention mechanism is differentiable, making it possible to use backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Multimodality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first attention interfaces’ use in DL were limited to NLP and computer vision
    domains to solve isolated tasks. Currently, attentional interfaces are studied
    in multimodal learning. Sensory multimodality in neural networks is a historical
    problem widely discussed by the scientific community [[60](#bib.bib60)] [[61](#bib.bib61)].
    Multimodal data improves the robustness of perception through complementarity
    and redundancy. The human brain continually deals with multimodal data and integrates
    it into a coherent representation of the world. However, employing different sensors
    present a series of challenges computationally, such as incomplete or spurious
    data, different properties (i.e. dimensionality or range of values), and the need
    for data alignment association. The integration of multiple sensors depends on
    a reasoning structure over the data to build a common representation, which does
    not exist in classical neural networks. Attentional interfaces adapted for multimodal
    perception are an efficient alternative for reasoning about misaligned data from
    different sensory sources.
  prefs: []
  type: TYPE_NORMAL
- en: The first widespread use of attention for multimodality occurs with the attentional
    interface between a convolutional neural network and an LSTM in image captioning [[45](#bib.bib45)].
    In this model, a CNN processes the image, extracting high-level features, whereas
    the LSTM consumes the features to produce descriptive words, one by one. The attention
    mechanism guides the LSTM to relevant image information for each word’s generation,
    equivalent to the human visual attention mechanism. The visualization of attention
    weights in multimodal tasks improved the understanding of how architecture works.
    This approach derived from countless other works with attentional interfaces that
    deal with video-text data [[62](#bib.bib62)] [[63](#bib.bib63)] [[64](#bib.bib64)],
    image-text data [[65](#bib.bib65)] [[66](#bib.bib66)], monocular/RGB-D images [[67](#bib.bib67)] [[68](#bib.bib68)] [[69](#bib.bib69)],
    RADAR [[68](#bib.bib68)], remote sensing data [[70](#bib.bib70)] [[71](#bib.bib71)] [[72](#bib.bib72)] [[73](#bib.bib73)],
    audio-video [[74](#bib.bib74)] [[75](#bib.bib75)], and diverse sensors [[76](#bib.bib76)] [[77](#bib.bib77)] [[78](#bib.bib78)],
    as shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Multimodality ‣ 2 Overview ‣ Attention,
    please! A survey of Neural Attention Models in Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. [[79](#bib.bib79)] used an adaptive attention mechanism to learn
    to emphasize different visual and textual sources for dialogue systems for fashion
    retail. An adaptive attention scheme automatically decided the evidence source
    for tracking dialogue states based on visual and textual context. Dual Attention
    Networks [[80](#bib.bib80)] presented attention mechanisms to capture the fine-grained
    interplay between images and textual information. The mechanism allows visual
    and textual attention to guide each other during collaborative inference. HATT [[63](#bib.bib63)]
    presented a new attention-based hierarchical fusion to explore the complementary
    features of multimodal features progressively, fusing temporal, motion, audio,
    and semantic label features for video representation. The model consists of three
    attention layers. First, the low-level attention layer deals with temporal, motion,
    and audio features inside each modality and across modalities. Second, high-level
    attention selectively focuses on semantic label features. Finally, the sequential
    attention layer incorporates hidden information generated by encoded low-level
    attention and high-level attention. Hori et. al. [[74](#bib.bib74)] extended simple
    attention multimodal fusion. Unlike the simple multimodal fusion method, the feature-level
    attention weights can change according to the decoder state and the context vectors,
    enabling the decoder network to pay attention to a different set of features or
    modalities when predicting each subsequent word in the description. Memory Fusion
    Network [[76](#bib.bib76)] presented the Delta-memory Attention module for multi-view
    sequential learning. First, an LSTM system, one for each of the modalities, encodes
    the modality-specific dynamics and interactions. Delta-memory attention discovers
    both cross-modality and temporal interactions in different memory dimensions of
    LSTMs. Finally, Multi-view Gated Memory (unifying memory) stores the cross-modality
    interactions over time.
  prefs: []
  type: TYPE_NORMAL
- en: Huang et al. [[81](#bib.bib81)] investigated the problem of matching image-text
    by exploiting the bi-directional attention with fine-granularity correlations
    between visual regions and textual words. Bi-directional attention connects the
    word to regions and objects to words for learning mage-text matching. Li et. al. [[82](#bib.bib82)]
    introduced Long Short-Term Memory with Pointing (LSTM-P) inspired by humans pointing
    behavior [[83](#bib.bib83)], and Pointer Networks [[54](#bib.bib54)]. The pointing
    mechanism encapsulates dynamic contextual information (current input word and
    LSTM cell output) to deal with the image captioning scenario’s novel objects.
    Liu et. al. [[84](#bib.bib84)] proposed a cross-modal attention-guided erasing
    approach for referring expressions. Previous attention models focus on only the
    most dominant features of both modalities and neglect textual-visual correspondences
    between images and referring expressions. To tackle this issue, cross-modal attention
    discards the most dominant information from either textual or visual domains to
    generate difficult training samples and drive the model to discover complementary
    textual-visual correspondences. Abolghasemi et al. [[85](#bib.bib85)] demonstrated
    an approach for augmenting a deep visuomotor policy trained through demonstrations
    with Task Focused Visual Attention (TFA). Attention receives as input a manipulation
    task specified in natural language text, an image with the environment, and returns
    as output the area with an object that the robot needs to manipulate. TFA allows
    the policy to be significantly more robust from the baseline policy, i.e., no
    visual attention. Pu et al. [[66](#bib.bib66)] adaptively select features from
    the multiple CNN layers for video captioning. Previous models often use the output
    from a specific layer of a CNN as video features. However, this attention model
    adaptively and sequentially focuses on different layers of CNN features.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2f08ad1d0e77dd8154ae11e9ed5d3d71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A diagram showing sensory modalities of neural attention models.
    Radial segments correspond to attention architectures, and each track corresponds
    to a modality. Modalities are: (A) audio, (B) biomedical signals, (I) image, (O)
    other sensors, (L) LiDAR, (R) remote sensing data, (T) text, and (V) video. The
    following coloring convention is used for the individual segments: white (the
    modality is not implemented), light yellow (CNN), light orange (RNN), orange (Self-attentive
    networks), red (Memory networks), dark red (framework), and brown (GNN). This
    diagram emphasizes multimodal architectures so that only the most representative
    single modality (i.e., text or image) architectures are shown. Most multimodal
    architectures use the image/text or video/text modalities.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Attention-augmented memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attentional interfaces also allow the neural network iteration with other cognitive
    elements (i.e., memories, working memory). Memory control and logic flow are essential
    for learning. However, they are elements that do not exist in classical architectures.
    The memory of classic RNNs, encoded by hidden states and weights, is usually minimal
    and is not sufficient to remember facts from the past accurately. Most Deep Learning
    models do not have a simple way to read and write data to an external memory component.
    The Neural Turing Machine (NTM) [[86](#bib.bib86)] and Memory Networks (MemNN) [[38](#bib.bib38)]
    - a new class of neural networks - introduced the possibility for a neural network
    dealing with addressable memory. NTM is a differentiable approach that can be
    trained with gradient descent algorithms, producing a practical learning program
    mechanism. NTM memory is a short-term storage space for information with its rules-based
    manipulation. Computationally, these rules are simple programs, where data are
    those programs’ arguments. Therefore, an NTM resembles a working memory designed
    to solve tasks that require rules, where variables are quickly linked to memory
    slots. NTMs use an attentive process to read and write elements to memory selectively.
    This attentional mechanism makes the network learn to use working memory instead
    of implementing a fixed set of symbolic data rules.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Networks [[38](#bib.bib38)] are a relatively new framework of models
    designed to alleviate the problem of learning long-term dependencies in sequential
    data by providing an explicit memory representation for each token in the sequence.
    Instead of forgetting the past, Memory Networks explicitly consider the input
    history, with a dedicated vector representation for each history element, effectively
    removing the chance to forget. The limit on memory size becomes a hyper-parameter
    to tune, rather than an intrinsic limitation of the model itself. This model was
    used in question-answering tasks where the long-term memory effectively acts as
    a (dynamic) knowledge base, and the output is a textual response. Large-scale
    question-answer tests were performed, and the reasoning power of memory networks
    that answer questions that require an in-depth analysis of verb intent was demonstrated.
    Mainly due to the success of MemNN, networks with external memory are a growing
    research direction in DL, with several branches under development as shown in
    figure [4](#S2.F4 "Figure 4 ‣ 2.3 Attention-augmented memory ‣ 2 Overview ‣ Attention,
    please! A survey of Neural Attention Models in Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bbd23bfe7b9f7e2d17ee5010481a8343.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Memory-based neural networks (MemNN). Architectures labels are color-coded
    as follows: orange - natural language processing, red - computer vision, purple
    - others. The end-to-End Memory networks is the first end-to-end differentiable
    version of MemNN. GMN [[87](#bib.bib87)] and MemGNN [[87](#bib.bib87)] are the
    first graph networks with memory. DMN [[88](#bib.bib88)], MemGNN [[87](#bib.bib87)],
    Episodic graph memory networks [[89](#bib.bib89)], Episodic CAMN [[90](#bib.bib90)],
    are the first instances of the episodic memory framework.'
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end Memory Networks [[91](#bib.bib91)] is the first version of MemNN
    applicable to realistic, trainable end-to-end scenarios, which requires low supervision
    during training. Aug Oh. et al. [[92](#bib.bib92)] extends Memory Networks to
    suit the task of semi-supervised segmentation of video objects. Frames with object
    masks are placed in memory, and a frame to be segmented acts as a query. The memory
    is updated with the new masks provided and faces challenges such as changes, occlusions,
    and accumulations of errors without online learning. The algorithm acts as an
    attentional space-time system calculating when and where to meet each query pixel
    to decide whether the pixel belongs to a foreground object or not. Kumar et al. [[93](#bib.bib93)]
    propose the first network with episodic memory - a type of memory extremely relevant
    to humans - to iterate over representations emitted by the input module updating
    its internal state through an attentional interface. In [[89](#bib.bib89)], an
    episodic memory with a key-value retrieval mechanism chooses which parts of the
    input to focus on thorough attention. The module then produces a summary representation
    of the memory, taking into account the query and the stored memory. Finally, the
    latest research has invested in Graph Memory Networks (GMN), which are memories
    in GNNs [[94](#bib.bib94)], to better handle unstructured data using key-value
    structured memories [[95](#bib.bib95)] [[87](#bib.bib87)] [[87](#bib.bib87)] [[96](#bib.bib96)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 End-to-end attention models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In mid-2017, research aiming at end-to-end attention models appeared in the
    area. The Neural Transformer (NT) [[37](#bib.bib37)] and Graph Attention Networks [[97](#bib.bib97)]
    - purely attentional architectures - demonstrated to the scientific community
    that attention is a key element for the future development in Deep Learning. The
    Transformer’s goal is to use self-attention (Section [3](#S3 "3 Attention Mechanisms
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")) to
    minimize traditional recurrent neural networks’ difficulties. The Neural Transformer
    is the first neural architecture that uses only attentional modules and fully-connected
    neural networks to process sequential data successfully. It dispenses recurrences
    and convolutions, capturing the relationship between the sequence elements regardless
    of their distance. Attention allows the Transformer to be simple, parallelizable,
    and low training cost [[37](#bib.bib37)]. Graph Attention Networks (GATs) are
    an end-to-end attention version of GNNs [[94](#bib.bib94)]. They have stacks of
    attentional layers that help the model focus on the unstructured data’s most relevant
    parts to make decisions. The main purpose of attention is to avoid noisy parts
    of the graph by improving the signal-to-noise ratio (SNR) while also reducing
    the structure’s complexity. Furthermore, they provide a more interpretable structure
    for solving the problem. For example, when analyzing the Attention of a model
    under different components in a graph, it is possible to identify the main factors
    contributing to achieving a particular response condition.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2774271a31f8db4247b4a9e80d88da4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Transformer-based neural networks. Architectures labels are color-coded
    as follows: orange - natural language processing, red - computer vision, purple
    - others.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a growing interest in NT and GATs, and some extensions have been proposed [[98](#bib.bib98)] [[99](#bib.bib99)] [[100](#bib.bib100)] [[101](#bib.bib101)],
    with numerous Transformer-based architectures as shown figure [5](#S2.F5 "Figure
    5 ‣ 2.4 End-to-end attention models ‣ 2 Overview ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning"). These architectures and all that
    use self-attention belong to a new category of neural networks, called Self-Attentive
    Neural Networks. They aim to explore self-attention in various tasks and improve
    the following drawbacks: 1) a Large number of parameters and training iterations
    to converge; 2) High memory cost per layer and quadratic growth of memory according
    to sequence length; 3) Auto-regressive model; 4) Low parallelization in the decoder
    layers. Specifically, Weighted Transformer [[102](#bib.bib102)] proposes modifications
    in the attention layers achieving a 40 % faster convergence. The multi-head attention
    modules are replaced by modules called branched attention that the model learns
    to match during the training process. The Star-transformer [[103](#bib.bib103)]
    proposes a lightweight alternative to reduce the model’s complexity with a star-shaped
    topology. To reduce the cost of memory, Music Transformer [[104](#bib.bib104)],
    and Sparse Transformer [[105](#bib.bib105)] introduces relative self-attention
    and factored self-attention, respectively. Lee et al. [[106](#bib.bib106)] also
    features an attention mechanism that reduces self-attention from quadratic to
    linear, allowing scaling for high inputs and data sets.'
  prefs: []
  type: TYPE_NORMAL
- en: Some approaches adapt the Transformer to new applications and areas. In natural
    language processing, several new architectures have emerged, mainly in multimodal
    learning. Doubly Attentive Transformer [[107](#bib.bib107)] proposes a multimodal
    machine-translation method, incorporating visual information. It modifies the
    attentional decoder, allowing textual features from a pre-trained CNN encoder
    and visual features. The Multi-source Transformer [[108](#bib.bib108)] explores
    four different strategies for combining input into the multi-head attention decoder
    layer for multimodal translation. Style Transformer [[109](#bib.bib109)], Hierarchical
    Transformer [[110](#bib.bib110)], HighWay Recurrent Transformer [[111](#bib.bib111)],
    Lattice-Based Transformer [[112](#bib.bib112)], Transformer TTS Network [[113](#bib.bib113)],
    Phrase-Based Attention [[114](#bib.bib114)] are some important architectures in
    style transfer, document summarization and machine translation. Transfer Learning
    in NLP is one of Transformer’s major contribution areas. BERT [[115](#bib.bib115)],
    GPT-2 [[116](#bib.bib116)], and GPT-3 [[117](#bib.bib117)] based NT architecture
    to solve the problem of Transfer Learning in NLP because current techniques restrict
    the power of pre-trained representations. In computer vision, the generation of
    images is one of the Transformer’s great news. Image Transformer [[118](#bib.bib118)],
    SAGAN [[119](#bib.bib119)], and Image GPT [[120](#bib.bib120)] uses self-attention
    mechanism to attend the local neighborhoods. The size of the images that the model
    can process in practice significantly increases, despite maintaining significantly
    larger receptive fields per layer than the typical convolutional neural networks.
    Recently, at the beginning of 2021, OpenAi introduced the scientific community
    to DALL·E [[121](#bib.bib121)], the Newest language model based on Transformer
    and GPT-3, capable of generating images from texts extending the knowledge of
    GPT-3 for viewing with only 12 billions of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Attention today
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, hybrid models that employ the main key developments in attention’s
    use in Deep Learning (Figure [6](#S2.F6 "Figure 6 ‣ 2.5 Attention today ‣ 2 Overview
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")) have
    aroused the scientific community’s interest. Mainly, hybrid models based on Transformer,
    GATs, and Memory Networks have emerged for multimodal learning and several other
    application domains. Hyperbolic Attention Networks (HAN) [[122](#bib.bib122)],
    Hyperbolic Graph Attention Networks (GHN) [[123](#bib.bib123)], Temporal Graph
    Networks (TGN) [[124](#bib.bib124)] and Memory-based Graph Networks (MGN) [[87](#bib.bib87)]
    are some of the most promising developments. Hyperbolic networks are a new class
    of architecture that combine the benefits of self-attention, memory, graphs, and
    hyperbolic geometry in activating neural networks to reason with high capacity
    over embeddings produced by deep neural networks. Since 2019 these networks have
    stood out as a new research branch because they represent state-of-the-art generalization
    on neural machine translation, learning on graphs, and visual question answering
    tasks while keeping the neural representations compact. Since 2019, GATs have
    also received much attention due to their ability to learn complex relationships
    or interactions in a wide spectrum of problems ranging from biology, particle
    physics, social networks to recommendation systems. To improve the representation
    of nodes and expand the capacity of GATs to deal with data of a dynamic nature
    (i.e. evolving features or connectivity over time), architectures that combine
    memory modules and the temporal dimension, like MGNs and TGNs, were proposed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7a0b658889059f94cf0ded2ad38364c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Key developments in Attention in DL Timeline. RNNSearch presented
    the first attention mechanism. Neural Turing machine and Memory networks introduced
    memory and dynamic flow control. RAM and DRAW learned to combine multi-glimpse,
    visual attention, and sequential processing. Spatial Transformer introduced a
    module to increase the robustness of CNNs to variations in spatial transformations.
    Show, attend and tell created attention for multimodality. The Pointer network
    used attention as a pointer. BiDAF, HAN, and DCN presented attentional techniques
    to align data with different hierarchical levels. ACT introduced the computation
    time topic. Transformer [[37](#bib.bib37)] was the first self-attentive neural
    network with an end-to-end attention approach. GATs introduced attention in GNNs.
    BERT [[115](#bib.bib115)], GPT-2 [[116](#bib.bib116)], GPT-3 [[117](#bib.bib117)],
    and DALL·E [[121](#bib.bib121)] are the state-of-the-art in language models and
    text-to-image generation. Finally, BRIMs [[125](#bib.bib125)] learned to combine
    bottom-up and top-down signals.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of 2020, two research branches still little explored in the literature
    were strengthened: 1) explicit combination of bottom-up and top-down stimuli in
    bidirectional recurrent neural networks and 2) adaptive computation time. Classic
    recurrent neural networks perform recurring iteration within a particular level
    of representation instead of using a top-down iteration, in which higher levels
    act at lower levels. However, Mittal et al. [[125](#bib.bib125)] revisited the
    bidirectional recurrent layers with attentional mechanisms to explicitly route
    the flow of bottom-up and top-down information, promoting selection iteration
    between the two levels of stimuli. The approach separates the hidden state into
    several modules so that upward iterations between bottom-up and top-down signals
    can be appropriately focused. The layer structure has concurrent modules so that
    each hierarchical layer can send information both in the bottom-up and top-down
    directions.'
  prefs: []
  type: TYPE_NORMAL
- en: The adaptive computation time is an interesting little-explored topic in the
    literature that began to expand only in 2020 despite initial studies emerging
    in 2017\. ACT applies to different neural networks (e.g. RNNs, CNNs, LSTMs, Transformers).
    The general idea is that complex data might require more computation to produce
    a final result, while some unimportant or straightforward data might require less.
    The attention mechanism dynamically decides how long to process network training
    data. The seminal approach by Graves et al. [[126](#bib.bib126)] made minor modifications
    to an RNN, allowing the network to perform a variable number of state transitions
    and a variable number of outputs at each stage of the input. The resulting output
    is a weighted sum of the intermediate outputs, i.e., soft attention. A halting
    unit decides when the network should stop or continue. To limit computation time,
    attention adds a time penalty to the cost function by preventing the network from
    processing data for unnecessary amounts of time. This approach has recently been
    updated and expanded to other architectures. Spatially Adaptive Computation Time
    (SACT) [[127](#bib.bib127)] adapts ACT to adjust the per-position amount of computation
    to each spatial position of the block in convolutional layers, learning to focus
    computing on the regions of interest and to stop when the features maps are "good
    enough". Finally, Differentiable Adaptive Computation Time (DACT) [[128](#bib.bib128)]
    introduced the first differentiable end-to-end approach to computation time on
    recurring networks.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Attention Mechanisms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep attention mechanisms can be categorized into soft attention (global attention),
    hard attention (local attention), and self-attention (intra-attention).
  prefs: []
  type: TYPE_NORMAL
- en: Soft Attention. Soft attention assigns a weight of 0 to 1 for each input element.
    It decides how much attention should be focused on each element, considering the
    interdependence between the input of the deep neural network’s mechanism and target.
    It uses softmax functions in the attention layers to calculate weights so that
    the entire attentional model is deterministic and differentiable. Soft attention
    can act in the spatial and temporal context. The spatial context operates mainly
    to extract the features or the weighting of the most relevant features. For the
    temporal context, it works by adjusting the weights of all samples in sliding
    time windows, as samples at different times have different contributions. Despite
    being deterministic and differentiable, soft mechanisms have a high computational
    cost for large inputs. Figure [7](#S3.F7 "Figure 7 ‣ 3 Attention Mechanisms ‣
    Attention, please! A survey of Neural Attention Models in Deep Learning") shows
    an intuitive example of a soft attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/394d9990a688a2c8d9b9f80ba53095c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An intuitive example of Soft Attention. Visual QA architecture outputs
    an answer given an image and a textual question as input. It uses a soft attention
    mechanism that weighted visual features for the task for further processing. The
    premise is that the norm of the visual features correlates with their relevance.
    Besides, those feature vectors with high magnitudes correspond to image regions
    that contain relevant semantic content.'
  prefs: []
  type: TYPE_NORMAL
- en: Hard Attention. Hard attention determines whether a part of the mechanism’s
    input should be considered or not, reflecting the interdependence between the
    input of the mechanism and the target of the deep neural network. The weight assigned
    to an input part is either 0 or 1\. Hence, as input elements are either seen,
    the objective is non-differentiable. The process involves making a sequence of
    selections on which part to attend. In the temporal context, for example, the
    model attends to a part of the input to obtain information, decidinng where to
    attend in the next step based on the known information. A neural network can make
    a selection based on this information. However, as there is no ground truth to
    indicate the correct selection policy, the hard-attention type mechanisms are
    represented by stochastic processes. As the model is not differentiable, reinforcement
    learning techniques are necessary to train models with hard attention. Inference
    time and computational costs are reduced compared to soft mechanisms once the
    entire input is not being stored or processed. Figure [8](#S3.F8 "Figure 8 ‣ 3
    Attention Mechanisms ‣ Attention, please! A survey of Neural Attention Models
    in Deep Learning") shows an intuitive example of a hard attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1d9cf79e6dddc51cb1ec6ba55ec31a69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: An intuitive example of Hard Attention. Given an image and a textual
    question as input, the Visual QA architecture outputs an answer. It uses a hard
    attention mechanism that selects only the important visual features for the task
    for further processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Self-Attention. Self-attention quantifies the interdependence between the input
    elements of the mechanism. This mechanism allows the inputs to interact with each
    other "self" and determine what they should pay more attention to. The self-attention
    layer’s main advantages compared to soft and hard mechanisms are parallel computing
    ability for a long input. This mechanism layer checks the attention with all the
    same input elements using simple and easily parallelizable matrix calculations.
    Figure [9](#S3.F9 "Figure 9 ‣ 3 Attention Mechanisms ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning") shows an intuitive example of a
    self-attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9a706b342a86b8e26b5448286807187f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Self-Attention examples. a) Self-attention in sentences b) Self-attention
    in images. The first image shows five representative query locations with color-coded
    dots with the corresponding color-coded arrows summarizing the most-attended regions.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Attention-based Classic Deep Learning Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section introduces details about attentional interfaces in classic DL architectures.
    Specifically, we present the uses of attention in convolutional, recurrent networks
    and generative models.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Attention-based Convolutional Neural Networks (CNNs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Attention emerges in CNNs to filter information and allocate resources to the
    neural network efficiently. There are numerous ways to use attention on CNNs,
    which makes it very difficult to summarize how this occurs and the impacts of
    each use. We divided the uses of attention into six distinct groups (Figure [10](#S4.F10
    "Figure 10 ‣ 4.1 Attention-based Convolutional Neural Networks (CNNs) ‣ 4 Attention-based
    Classic Deep Learning Architectures ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")): 1) DCN attention pool – attention replaces the classic
    CNN pooling mechanism; 2) DCN attention input – the attentional modules are filter
    masks for the input data. This mask assigns low weights to regions irrelevant
    to neural network processing and high weights to relevant areas; 3) DCN attention
    layer – attention is between the convolutional layers; 4) DCN attention prediction
    – attentional mechanisms assist the model directly in the prediction process;
    5) DCN residual attention – extracts information from the feature maps and presents
    a residual input connection to the next layer; 6) DCN attention out – attention
    captures important stimuli of feature maps for other architectures, or other instances
    of the same architecture. To maintain consistency with the Deep Neural Network’s
    area, we extend The Neural Network Zoo schematics ¹¹1https://www.asimovinstitute.org/neural-network-zoo/
    to accommodate attention elements.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a5ba90de2a125a34761113a3b6405fc2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Attention-based convolutional neural networks. DCN attention pool
    group uses an attention pool, instead of regular pooling, as a strategy to determine
    the importance of each individual in a given feature map window. The premise is
    that only a few of these windows are significant and must be selected concerning
    a particular objective. DCN attention input group uses structures similar to the
    human’s visual attention. DCN attention layer group collects important stimuli
    of high (semantic level) and low level (saliency) for subsequent layers of architecture.
    DCN attention prediction group uses attention in the final stages of prediction,
    sometimes as an ensemble element. DCN residual attention group uses attention
    as a residual module between any convolutional layers to mitigate the vanishing
    problem, capturing only the relevant stimuli from each feature map. DCN attention
    out-group can represent the category of recurrent attention processes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DCN attention input mainly uses attention to filter input data - a structure
    similar to the multi-glimpse mechanism and visual attention of human beings. Multi-glimpse
    refers to the ability to quickly scan the entire image and find the main areas
    relevant to the recognition process, while visual attention focuses on a critical
    area by extracting key features to understand the scene. When a person focuses
    on one part of the image, the different regions’ internal relationship is captured,
    guiding eye movement to find the next relevant area—ignoring the irrelevant parts
    easy learning in the presence of disorder. For this reason, human vision has an
    incomparable performance in object recognition. The main contribution of attention
    at the CNNs’ input is robustness. If our eyes see an object in a real-world scene,
    parts far from the object are ignored. Therefore, the distant background of the
    fixed object does not interfere in recognition. However, CNNs treat all parts
    of the image equally. The irrelevant regions confuse the classification and make
    it sensitive to visual disturbances, including background, changes in camera views,
    and lighting conditions. Attention in CNNs’ input contributes to increasing robustness
    in several ways: 1) It makes architectures more scalable, in which the number
    of parameters does not vary linearly with the size of the input image; 2) Eliminates
    distractors; 3) Minimizes the effects of changing camera lighting, scale, and
    views. 4) It allows the extension of models for more complex tasks, i.e., fine-grained
    classification or segmentation. 5) Simplifies CNN encoding. 6) Facilitates learning
    by including relevant priorities for architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhao et al. [[129](#bib.bib129)] used visual attention-based image processing
    to generate the focused image. Then, the focused image is input into CNN to be
    classified. According to the classification, the information entropy guides reinforcement
    learning agents to achieve a better image classification policy. Wang et al. [[130](#bib.bib130)]
    used attention to create representations rich in motion information for action
    recognition. The attention extracts saliency maps using both motion and appearance
    information to calculate the objectness scores. For a video, attention process
    frame by frame to generate a saliency-aware map for each frame. The classic pipeline
    uses only CNN sequence features as input for LSTMs, failing to capture adjacent
    frames’ motion information. The saliency-aware maps capture only regions with
    relevant movements making CNN encoding simple and representative for the task.
    Liu et al. [[131](#bib.bib131)] used attention as input of a CNN to provide important
    priors in counting crowded tasks. An attention map generator first provides two
    types of priors for the system: candidate crowd regions and crowd regions’ congestion
    degree. The priors guide subsequent CNNs to pay more attention to those regions
    with crowds and improving their capacity to be resistant to noise. Specifically,
    the congestion degree prior provides fine-grained density estimation for a system.'
  prefs: []
  type: TYPE_NORMAL
- en: In classic CNNs, the size of the receptive fields is relatively small. Most
    of them extract features locally with convolutional operations, which fail to
    capture long-range dependencies between pixels throughout the image. However,
    larger receptive fields allow for better use of training inputs, and much more
    context information is available at the expense of instability or even convergence
    in training. Also, traditional CNNs treat channel features equally. This naive
    treatment lacks the flexibility to deal with low and high-frequency information.
    Some frequencies may contain more relevant information for a task than others,
    but equal treatment by the network makes it difficult to converge the models.
    To mitigate such problems, most literature approaches use attention between convolutional
    layers (i.e., DCN attention layer and DCN residual attention), as shown in figure [11](#S4.F11
    "Figure 11 ‣ 4.1 Attention-based Convolutional Neural Networks (CNNs) ‣ 4 Attention-based
    Classic Deep Learning Architectures ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning"). Between layers, attention acts mainly for feature recalibration,
    capturing long-term dependencies, internalizing, and correctly using past experiences.
  prefs: []
  type: TYPE_NORMAL
- en: The pioneering approach to adopting attention between convolutional layers is
    the Squeeze-and-Excitation Networks [[132](#bib.bib132)] created in 2016 and winner
    of the ILSVRC in 2017\. It is also the first architecture to model channel interdependencies
    to recalibrate filter responses in two steps, squeeze and excitation, i.e., SE
    blocks. To explore local dependencies, the squeeze module encodes spatial information
    into a channel descriptor. The output is a collection of local descriptors with
    expressive characteristics for the entire image. To make use of the information
    aggregated by the squeeze operation, excitation captures channel-wise dependencies
    by learning a non-linear and non-mutually exclusive relationship between channels,
    ensuring that multiple channels can be emphasized. In this sense, SE blocks intrinsically
    introduce attentional dynamics to boost feature discrimination between convolutional
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ef7fd56020fc0f6e213fbc74693a4153.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Attention-based convolutional neural networks main architectures
    by task and attention use.'
  prefs: []
  type: TYPE_NORMAL
- en: The inter-channel and intra-channel attention to capturing long-term dependencies
    and simultaneously taking advantage of high and low-level stimuli are widely explored
    in the literature. Zhang. et al. [[133](#bib.bib133)] proposed residual local
    and non-local attention blocks consisting of trunk and mask branches. Their attention
    mechanism helps to learn local and non-local information from the hierarchical
    features, further preserving low-level features while maintaining a representational
    quality of high-level features. The Cbam [[134](#bib.bib134)] infers attentional
    maps in two separate dimensions, channel and spatial, for adaptive feature refinement.
    The double attention block in [[135](#bib.bib135)] aggregates and propagates global
    informational features considering the entire spatio-temporal context of images
    and videos, allowing subsequent convolution layers to access resources from across
    space efficiently. In the first stage, attention gathers features from all space
    into a compact set employing groupings. In the second stage, it selects and adaptively
    distributes the resources for each architectural location. Following similar exploration
    proposals, several attentional modules can be easily plugged into classic CNNs [[136](#bib.bib136)] [[137](#bib.bib137)] [[138](#bib.bib138)] [[139](#bib.bib139)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Hackel et al. [[140](#bib.bib140)] explored attention to preserving sparsity
    in convolutional operations. Convolutions with kernels greater than $1\times 1$
    generate fill-in, reducing feature maps’ sparse nature. Generally, the change
    in data sparsity has little influence on the network output, but memory consumption
    and execution time considerably increase when it occurs in many layers. To guarantee
    low memory consumption, attention acts as a $k-selection$ filter, which has two
    different versions of selection: 1) it acts on the output of the convolution,
    preferring the largest $k$ positive responses similar to a rectified linear unit;
    2) it chooses the $k$ highest absolute values, expressing a preference for responses
    of great magnitude. The parameter $k$ controls the level of sparse data and, consequently,
    computational resources during training and inference. Results point out that
    training with attentional control of data sparsity can reduce in more than $200\%$
    the forward pass runtime in one layer.'
  prefs: []
  type: TYPE_NORMAL
- en: To previous aggregate information and dynamically point to past experiences,
    SNAIL [[141](#bib.bib141)] - a pioneering class of meta-learner based attention
    architectures - has proposed combining temporal convolutions with soft attention.
    This approach demonstrates that attention acts as a complement to the disadvantages
    of convolution. Attention allows precise access in an infinitely large context,
    while convolutions provide high-bandwidth access at the expense of a finite context.
    By merging convolutional layers with attentional layers, SNAIL can have unrestricted
    access to the number of previous experiences effectively, as well as the model
    can learn a more efficient representation of features. As additional benefits,
    SNAIL architectures become simpler to train than classic RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: The DCN attention out-group uses attention to share relevant feature maps with
    other architectures or even with instances of the current architecture. Usually,
    the main objective is to facilitate the fusion of features, multimodality, and
    external knowledge. In some cases, attention regularly works by turning classic
    CNNs into recurrent convolutional neural networks - a new trend in Deep Learning
    to deal with challenging images’ problems. RA-CNN [[142](#bib.bib142)] is a pioneering
    framework for recurrent convolutional networks. In their framework, attention
    proceeds along two dimensions, i.e., discriminative feature learning and sophisticated
    part localization. Given an input image, a classic CNN extracts feature maps,
    and the attention proposal network maps convolutional features to a feature vector
    that could be matched with the category entries. Then, attention estimates the
    focus region for the next CNN instance, i.e., the next finer scale. Once the focus
    region is located, the system cuts and enlarges the region to a finer scale with
    higher resolution to extract more refined features. Thus, each CNN in the stack
    generates a prediction so that the stack’s deepest layers generate more accurate
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: For merging features, Cheng. et. al. [[143](#bib.bib143)] presented Feature-fusion
    Encoder-Decoder Network (FED-net) to image segmentation. Their model uses attention
    to fuse features of different levels of an encoder. At each encoder level, the
    attention module merges features from its current level with features from later
    levels. After the merger, the decoder performs convolutional upsampling with the
    information from each attention level, which contributes by modulating the most
    relevant stimuli for segmentation. Tian et al. [[144](#bib.bib144)] used feature
    pyramid-based attention to combine meaningful semantic features with semantically
    weak but visually strong features in a face detection task. Their goal is to learn
    more discriminative hierarchical features with enriched semantics and details
    at all levels to detect hard-to-detect faces, like tiny or partially occluded
    faces. Their attention mechanism can fuse different feature maps from top to bottom
    recursively by combining transposed convolutions and element-wise multiplication
    maximizing mutual information between the lower and upper-level representations.
  prefs: []
  type: TYPE_NORMAL
- en: Delta [[145](#bib.bib145)] framework presented an efficient strategy for transfer
    learning. Their attention system acts as a behavior regulator between the source
    model and the target model. The attention identifies the source model’s completely
    transferable channels, preserving their responses and identifying the non-transferable
    channels to dynamically modulate their signals, increasing the target model’s
    generalization capacity. Specifically, the attentional system characterizes the
    distance between the source/target model through the feature maps’ outputs and
    incorporates that distance to regularize the loss function. Optimization normally
    affects the weights of the neural network and assigns generalization capacity
    to the target model. Regularization modulated by attention on high and low semantic
    stimuli manages to take important steps in the semantic problem to plug in external
    knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: The DCN attention prediction group uses attention directly in the prediction
    process. Various attentional systems capture features from different convolutional
    layers as input and generate a prediction as an output. Voting between different
    predictors generates the final prediction. Reusing activations of CNNs feature
    maps to find the most informative parts of the image at different depths makes
    prediction tasks more discriminative. Each attentional system learns to relate
    stimuli and part-based fine-grained features, which, although correlated, are
    not explored together in classical approaches. Zheng et al. [[146](#bib.bib146)]
    proposed a multi-attention mechanism to group channels, creating part classification
    sub-networks. The mechanism takes as input feature maps from convolutional layers
    and generates multiple single clusters spatially-correlated subtle patterns as
    a compact representation. The sub-network classifies an image by each individual
    part. The attention mechanism proposed in [[147](#bib.bib147)] uses a similar
    approach. However, instead of grouping features into clusters, the attentional
    system has the most relevant feature map regions selected by the attention heads.
    The output heads generate a hypothesis given the attended information, and the
    confidence gates generate a confidence score for each attention head.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the DCN attention pool group replaces classic pooling strategies with
    attention-based pooling. The objective is to create a non-linear encoding to select
    only stimuli relevant to the task, given that classical strategies select only
    the most contrasting stimuli. To modulate the resulting stimuli, attentional pooling
    layers generally capture different relationships between feature maps or between
    different layers. For example, Wang et al. [[148](#bib.bib148)] created an attentional
    mechanism that captures pertinent relationships between convoluted context windows
    and the relation class embedding through a correlation matrix learned during training.
    The correlation matrix modulates the convolved windows, and finally, the mechanism
    selects only the most salient stimuli. A similar approach is also followed in [[149](#bib.bib149)]
    for modeling sentence pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Attention-based Recurrent Neural Networks (RNNs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Attention in RNNs is mainly responsible for capturing long-distance dependencies.
    Currently, there are not many ways to use attention on RNNs. RNNSearch’s mechanism
    for encoder-decoder frameworks inspires most approaches [[44](#bib.bib44)]. We
    divided the uses of attention into three distinct groups (Figure [12](#S4.F12
    "Figure 12 ‣ 4.2 Attention-based Recurrent Neural Networks (RNNs) ‣ 4 Attention-based
    Classic Deep Learning Architectures ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")): 1) Recurrent attention input – the first stage of
    attention to select elementary input stimulus, i.e., elementary features, 2) recurrent
    memory attention – the first stage of attention to historical weight components,
    3) Recurrent hidden attention – the second stage of attention to select categorical
    information to the decode stage.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a07dc2c9f018b878efa80b1427d51d9c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Attention-based recurrent neural networks. The architecture is a
    classic recurrent network or a Bi-RNN when hidden layers are recurrent cells.
    When the hidden layer is a recurrent memory, the architecture is an LSTM or Bi-LSTM.
    Finally, the architecture is a GRU when the hidden layer is a gated memory cell.
    The recurrent attention input group uses attention to filter input data. Recurrent
    hidden attention groups automatically select relevant encoder hidden states across
    all time steps. Usually, this group implements attention in encoder-decoder frameworks.
    The recurrent memory attention group implements attention within the memory cell.
    There are not many architectures in this category as far as we know, but the main
    uses are related to filtering the input data and the weighting of different historical
    components for predicting the current time step.'
  prefs: []
  type: TYPE_NORMAL
- en: The recurrent attention input group main uses are item-wise hard, local-wise
    hard, item-wise soft, and local-wise soft selection. Item-wise hard selects discretely
    relevant input data for further processing, whereas location-wise hard discretely
    focuses only on the most relevant features for the task. Item-wise soft assigns
    a continuous weight to each input data given a sequence of items as input, and
    location-wise soft assigns a continuous weight between input features. Location-wise
    soft estimates high weights for features more correlated with the global context
    of the task. Hard selection for input elements are applied more frequently in
    computer vision approaches [[46](#bib.bib46)] [[150](#bib.bib150)]. On the other
    hand, soft mechanisms are often applied in other fields, mainly in Natural Language
    Processing. The soft selection normally weighs relevant parts of the series or
    input features, and the attention layer is a feed-forward network differentiable
    and with a low computational cost. Soft approaches are interesting to filter noise
    from time series and to dynamically learn the correlation between input features
    and output [[151](#bib.bib151)] [[152](#bib.bib152)] [[153](#bib.bib153)]. Besides,
    this approach is useful for addressing graph-to-sequence learning problems that
    learn a mapping between graph-structured inputs to sequence outputs, which current
    Seq2Seq and Tree2Seq may be inadequate to handle [[154](#bib.bib154)].
  prefs: []
  type: TYPE_NORMAL
- en: Hard mechanisms take inspiration from how humans perform visual sequence recognition
    tasks, such as reading by continually moving the fovea to the next relevant object
    or character, recognizing the individual entity, and adding the knowledge to our
    internal representation. A deep recurrent neural network, at each step, processes
    a multi-resolution crop of the input image, called a glimpse. The network uses
    information from the glimpse to update its internal representation and outputs
    the next glimpse location. The Glimpse network captures salient information about
    the input image at a specific position and region size. The internal state is
    formed by the hidden units $h_{t}$ of the recurrent neural network, which is updated
    over time by the core network. At each step, the location network estimates the
    next focus localization, and action networks depend on the task (e.g., for the
    classification task, the action network’s outputs are a prediction for the class
    label.). Hard attention is not entirely differentiable and therefore uses reinforcement
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAM [[46](#bib.bib46)] was the first architecture to use a recurrent network
    implementing hard selection for image classification tasks. While this model has
    learned successful strategies in various image data sets, it only uses several
    static glimpse sizes. CRAM [[150](#bib.bib150)] uses an additional sub-network
    to dynamically change the glimpse size, with the assumption to increase the performance,
    and in Jimmy et al. [[155](#bib.bib155)] explore modifications in RAM for real-world
    image tasks and multiple objects classification. CRAM is a similar RAM model except
    for two key differences: Firstly, a dynamically updated attention mechanism restrains
    the input region observed by the glimpse network and the next output region prediction
    from the emission network – a network that incorporates the location and capacity
    information as well as past information. In a more straightforward way, the sub-network
    decides at each time-step what the focus region’s capacity should be. Secondly,
    the capacity sub-network outputs are successively added to the emission network’s
    input that will ultimately generate the information for the next focus region—allowing
    the emission network to combine the information from the location and the capacity
    networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Nearly all important works in the field belong to the recurrent hidden attention
    group, as shown in Figure [13](#S4.F13 "Figure 13 ‣ 4.2 Attention-based Recurrent
    Neural Networks (RNNs) ‣ 4 Attention-based Classic Deep Learning Architectures
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning"). In
    this category, the attention mechanism selects elements that are in the RNN’s
    hidden layers for inter-alignment, contextual embedding, multiple-input processing,
    memory management, and capturing long-term dependencies, a typical problem with
    recurrent neural networks. Inter-alignment involves the encoder-decoder framework,
    and the attention module between these two networks is the most common approach.
    This mechanism builds a context vector dynamically from all previous decoder hidden
    states and the current encoder hidden state. Attention in inter-alignment helps
    minimize the bottleneck problem, with RNNSearch [[44](#bib.bib44)] for machine
    translation tasks as its first representative. Further, several other architectures
    implemented the same approach in other tasks [[156](#bib.bib156)] [[52](#bib.bib52)] [[51](#bib.bib51)].
    For example, Zichao Yang et al. [[52](#bib.bib52)] extended the soft selection
    to the hierarchical attention structure, which allows the calculation of soft
    attention at the word level and the sentence level in the GRU networks encoder
    for document classification.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b8405bed190fb524f4cabed0992081e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Attention-based recurrent neural networks main architectures by
    task and attention use.'
  prefs: []
  type: TYPE_NORMAL
- en: To create contextual embeddings and to manipulate multimodal inputs, co-attention
    is highly effective for text matching applications. Co-attention enables the learning
    of pairwise attention, i.e., learning to attend based on computing word-level
    affinity scores between two documents. Such a mechanism is designed for architectures
    comprised of queries and context, such as questions and answers and emotions analysis.
    Co-attention models can be fine-grained or coarse-grained. Fine-grained models
    consider each element of input concerning each element of the other input. Coarse-grained
    models calculate attention for each input, using an embedding of the other input
    as a query. Although efficient, co-attention suffers from information loss from
    the target and the context due to the anticipated summary. Attention flow emerges
    as an alternative to summary problems. Unlike co-attention, attention flow links
    and merges context and query information at each stage of time, allowing embeddings
    from previous layers to flow to subsequent modeling layers. The attention flow
    layer is not used to summarize the query and the context in vectors of unique
    features, reducing information loss. Attention is calculated in two directions,
    from the context to the query and from the query to the context. The output is
    the query-aware representations of context words. Attention flow allows a hierarchical
    process of multiple stages to represent the context at different granularity levels
    without an anticipated summary.
  prefs: []
  type: TYPE_NORMAL
- en: Hard attention mechanisms do not often occur on recurrent hidden attention networks.
    However, Nan Rosemary et al. [[157](#bib.bib157)] demonstrate that hard selection
    to retrieve past hidden states based on the current state mimics an effect similar
    to the brain’s ability. Humans use a very sparse subset of past experiences and
    can access them directly and establish relevance with the present, unlike classic
    RNNs and self-attentive networks. Hard attention is an efficient mechanism for
    RNNs to recover sparse memories. It determines which memories will be selected
    on the forward pass, and therefore which will receive gradient updates. At time
    $t$, RNN receives a vector of hidden states $h^{t-1}$, a vector of cell states
    $c^{t-1}$, and an input $x^{t}$, and computes new cell states $c^{t}$ and a provisional
    hidden state vector $\widetilde{h}^{t}$ that also serves as a provisional output.
    First, the provisional hidden state vector $\widetilde{h}^{t}$ is concatenated
    to each memory vector $m_{i}$ in the memory $M$. MLP maps each vector to an attention
    weight $a^{t}_{i}$, representing memory relevance $i$ in current moment $t$. With
    attention weights $a^{t}_{i}$ sparse attention computes a hard decision. The attention
    mechanism is differentiable but implements a hard selection to forget memories
    with no prominence over others. This is quite different from typical approaches
    as the mechanism does not allow the gradient to flow directly to a previous step
    in the training process. Instead, it propagates to some local timesteps as a type
    of local credit given to a memory.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, recurrent memory attention groups implement attention within the memory
    cell. As far as our research goes, there are not many architectures in this category.
    Pengfei et al. [[69](#bib.bib69)] proposed an approach that modulates the input
    adaptively within the memory cell by assigning different levels of importance
    to each element/dimension of the input shown in Figure [14](#S4.F14 "Figure 14
    ‣ 4.2 Attention-based Recurrent Neural Networks (RNNs) ‣ 4 Attention-based Classic
    Deep Learning Architectures ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")a. Dilruk et al. [[158](#bib.bib158)] proposed mechanisms
    of attention within memory cell to improve the past encoding history in the cell’s
    state vector since all parts of the data history are not equally relevant to the
    current prediction. As shown in Figure [14](#S4.F14 "Figure 14 ‣ 4.2 Attention-based
    Recurrent Neural Networks (RNNs) ‣ 4 Attention-based Classic Deep Learning Architectures
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")b,
    the mechanism uses additional gates to update LSTM’s current cell.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1ac967d064461b56e74d8557fb5383e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: recurrent memory attention approaches. a) Illustration of Element-wise-Attention
    Gate in GRU. Specifically, the input modulation is adaptable to the content and
    is performed in fine granularity, element-wise rather than input-wise. b) Gate
    attention in LSTM. The model calculates attention scores to weigh the relevance
    of different parts of history. There are two additional gates for updating the
    current cell using the previous state $h_{A}^{t-1}$. The first, input attention
    gate layer, analyzes the current input $i^{\triangle t}$ and $h_{A}^{t-1}$ to
    determine which values to be updated in current cell $C^{t}$. The second, the
    modulation attention gate, analyzes the current input $i^{\triangle t}$ and $h_{A}^{t-1}$.
    Then computes the set of candidate values that must be added when updating the
    current cell state $C^{t}$. The attention mechanisms in the memory cell help to
    more easily capture long-term dependencies and the problem of data scarcity.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Attention-based Generative Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Attention emerges in generative models essentially to augmented memory. Currently,
    there are not many ways to use attention on generative models. Since GANs are
    not a neural network architecture but a framework, we do not discuss the use of
    attention in GANs but autoencoders. We divided the uses of attention into three
    distinct groups (Figure [15](#S4.F15 "Figure 15 ‣ 4.3 Attention-based Generative
    Models ‣ 4 Attention-based Classic Deep Learning Architectures ‣ Attention, please!
    A survey of Neural Attention Models in Deep Learning")): 1) Autoencoder input
    attention – attention provides spatial masks corresponding to all the parts for
    a given input, while a component autoencoder (e.g., AE, VAE, SAE) independently
    models each of the parts indicated by the masks. 2) Autoencoder memory attention
    – attention module acts as a layer between the encoder-decoder to augmented memory.,
    3) Autoencoder attention encoder-decoder – a fully attentive architecture acts
    on the encoder, decoder, or both.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6107590c04baa9edd76c3e4bbb2cd674.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Attention-based generative models. The Autoencoder input attention
    group uses attention to facilitate the decomposition of scenes in abstract building
    blocks. The input components extracted by the masks share significant properties
    and help to imagine new scenarios. This approach is very efficient for the network
    to learn to decompose challenging scenarios between semantically significant components.
    Autoencoder memory attention group handles a memory buffer that allows read/writes
    operations and is persistent over time. Such models generally handle input and
    output to the memory buffer using write/read operations guided by the attention
    system. The use of attention and memory history in autoencoders helps to increase
    the generalizability of architecture. Autoencoder attention encoder-decoder using
    a model (usually self-attentive model) to increase the ability to generalize.'
  prefs: []
  type: TYPE_NORMAL
- en: MONet [[159](#bib.bib159)] is one of the few architectures to implement attention
    at the VAE input. A VAE is a neural network with an encoder parameterized by $\phi$
    and a decoder parameterized by $\theta$. The encoder parameterizes a distribution
    over the component latent $z_{k}$, conditioned on both the input data x and an
    attention mask $m_{k}$. The mask indicates which regions of the input the VAE
    should focus on representing via its latent posterior distribution, $q\phi(z_{k}|x,m_{k})$.
    During training, the VAE’s decoder likelihood term in the loss $p_{\theta}(x|z_{k})$
    is weighted according to the mask, such that it is unconstrained outside of the
    masked regions. In [[160](#bib.bib160)], the authors use soft attention with learned
    memory contents to augment models to have more parameters in the autoencoder.
    In [[161](#bib.bib161)], Generative Matching Networks use attention to access
    the exemplar memory, with the address weights computed based on a learned similarity
    function between an observation at the address and a function of the latent state
    of the generative model. In [[162](#bib.bib162)], external memory and attention
    work as a way of implementing one-shot generalization by treating the exemplars
    conditioned on as memory entries accessed through a soft attention mechanism at
    each step of the incremental generative process similar to DRAW [[36](#bib.bib36)].
    Although most approaches use soft attention to address the memory, in [[163](#bib.bib163)]
    the authors use a stochastic, hard attention approach, which allows using variational
    inference about it in a context of few-shot learning.
  prefs: []
  type: TYPE_NORMAL
- en: In [[164](#bib.bib164)], self-attentive networks increase the autoencoder ability
    to generalize. The advantage of using this model instead of other alternatives,
    such as recurrent or convolutional encoders, is that this model is based only
    on self-attention and traditional attention over the whole representation created
    by the encoder. This approach allows us to easily employ the different components
    of the networks (encoder and decoder) as modules that, during inference, can be
    used with other parts of the network without the need for previous step information.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a few years, neural attention networks have been used in numerous domains
    due to versatility, interpretability, and significance of results. These networks
    have been explored mainly in computer vision, natural language processing, and
    multi-modal tasks, as shown in figure [16](#S5.F16 "Figure 16 ‣ 5 Applications
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning"). In
    some applications, these models transformed the area entirely (i.e., question-answering,
    machine translation, document representations/embeddings, graph embeddings), mainly
    due to significant performance impacts on the task in question. In others, they
    helped learn better representations and deal with temporal dependencies over long
    distances. This section explores a list of application domains and subareas, mainly
    discussing each domain’s main models and how it benefits from attention. We also
    present the most representative instances within each area and list them with
    reference approaches in a wide range of applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fc7f7d487fb4bc02d3567dbbc473cf9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Diagram showing the main existing applications of neural attention
    networks. The main areas are Natural language processing (NLP), Computer Vision
    (CV), multimodal tasks (mainly with images and texts - CV/NLP), reinforcement
    learning (RL), robotics, recommendation systems, and others (e.i., graph embeddings,
    interpretability.).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Natural Language Processing (NLP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the NLP domain, attention plays a vital role in many sub-areas, as shown
    in figure [16](#S5.F16 "Figure 16 ‣ 5 Applications ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning"). There are several state-of-the-art
    approaches, mainly in language modeling, machine translation, natural language
    inference, question answering, sentiment analysis, semantic analysis, speech recognition,
    and text summarization. Table [1](#S5.T1 "Table 1 ‣ 5.1 Natural Language Processing
    (NLP) ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention Models
    in Deep Learning") groups works developed in each of these areas. Several applications
    have been facing an increasing expansion, with few representative works, such
    as emotion recognition, speech classification, sequence prediction, semantic matching,
    and grammatical correction, as shown in Table [2](#S5.T2 "Table 2 ‣ 5.1 Natural
    Language Processing (NLP) ‣ 5 Applications ‣ Attention, please! A survey of Neural
    Attention Models in Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | References |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Language Modeling |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[165](#bib.bib165)] [[166](#bib.bib166)] [[157](#bib.bib157)] [[156](#bib.bib156)] [[167](#bib.bib167)] [[168](#bib.bib168)] [[169](#bib.bib169)] [[170](#bib.bib170)] [[171](#bib.bib171)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Machine Translation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[172](#bib.bib172)] [[37](#bib.bib37)] [[122](#bib.bib122)] [[173](#bib.bib173)] [[165](#bib.bib165)] [[44](#bib.bib44)] [[58](#bib.bib58)] [[174](#bib.bib174)] [[175](#bib.bib175)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[176](#bib.bib176)] [[177](#bib.bib177)] [[178](#bib.bib178)] [[179](#bib.bib179)] [[180](#bib.bib180)] [[181](#bib.bib181)] [[182](#bib.bib182)] [[183](#bib.bib183)] [[184](#bib.bib184)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[185](#bib.bib185)] [[186](#bib.bib186)] [[187](#bib.bib187)] [[188](#bib.bib188)] [[189](#bib.bib189)] [[190](#bib.bib190)] [[191](#bib.bib191)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Natural Language Inference |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[172](#bib.bib172)] [[192](#bib.bib192)] [[193](#bib.bib193)] [[156](#bib.bib156)] [[194](#bib.bib194)] [[103](#bib.bib103)] [[195](#bib.bib195)] [[196](#bib.bib196)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question Answering |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[172](#bib.bib172)] [[197](#bib.bib197)] [[198](#bib.bib198)] [[165](#bib.bib165)] [[38](#bib.bib38)] [[93](#bib.bib93)] [[50](#bib.bib50)] [[199](#bib.bib199)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[200](#bib.bib200)] [[51](#bib.bib51)] [[201](#bib.bib201)] [[202](#bib.bib202)] [[203](#bib.bib203)] [[204](#bib.bib204)] [[205](#bib.bib205)] [[206](#bib.bib206)] [[207](#bib.bib207)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[208](#bib.bib208)] [[209](#bib.bib209)] [[210](#bib.bib210)] [[211](#bib.bib211)] [[91](#bib.bib91)] [[212](#bib.bib212)] [[213](#bib.bib213)] [[214](#bib.bib214)] [[215](#bib.bib215)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[216](#bib.bib216)] [[56](#bib.bib56)] [[217](#bib.bib217)] [[218](#bib.bib218)] [[219](#bib.bib219)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sentiment Analysis |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[220](#bib.bib220)] [[193](#bib.bib193)] [[156](#bib.bib156)] [[221](#bib.bib221)] [[222](#bib.bib222)] [[223](#bib.bib223)] [[224](#bib.bib224)] [[225](#bib.bib225)] [[226](#bib.bib226)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[227](#bib.bib227)] [[228](#bib.bib228)] [[229](#bib.bib229)] [[230](#bib.bib230)] [[231](#bib.bib231)] [[232](#bib.bib232)] [[233](#bib.bib233)] [[234](#bib.bib234)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Semantic Analysis |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[57](#bib.bib57)] [[235](#bib.bib235)] [[236](#bib.bib236)] [[237](#bib.bib237)] [[238](#bib.bib238)] [[239](#bib.bib239)] [[240](#bib.bib240)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Speech Recognition |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[241](#bib.bib241)] [[175](#bib.bib175)] [[242](#bib.bib242)] [[243](#bib.bib243)] [[244](#bib.bib244)] [[245](#bib.bib245)] [[246](#bib.bib246)] [[247](#bib.bib247)] [[248](#bib.bib248)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[249](#bib.bib249)] [[250](#bib.bib250)] [[251](#bib.bib251)] [[252](#bib.bib252)] [[253](#bib.bib253)] [[254](#bib.bib254)] [[255](#bib.bib255)] [[256](#bib.bib256)] [[257](#bib.bib257)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Text Summarization |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[258](#bib.bib258)] [[259](#bib.bib259)] [[260](#bib.bib260)] [[55](#bib.bib55)] [[175](#bib.bib175)] [[261](#bib.bib261)] [[262](#bib.bib262)] [[263](#bib.bib263)] [[264](#bib.bib264)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary state-of-the-art approaches in several natural language processing
    sub-areas.'
  prefs: []
  type: TYPE_NORMAL
- en: For machine translation (MT), question answering (QA), and automatic speech
    recognition (ASR), attention works mainly in the alignment input and output sequences
    capturing long-range dependencies. For example, in ASR tasks, attention aligns
    acoustic frames extracting information from anchor words to recognize the main
    speaker while ignoring background noise and interfering speech. Hence, only information
    on the desired speech is used for the decoder as it provides a straightforward
    way to align each output symbol with different input frames with selective noise
    decoding. In MT, automatic alignment translates long sentences more efficiently.
    It is a powerful tool for multilingual machine translation (NMT), efficiently
    capturing subjects, verbs, and nouns in sentences of different languages that
    differ significantly in their syntactic structure and semantics.
  prefs: []
  type: TYPE_NORMAL
- en: In QA, alignment usually occurs between a query and the content, looking for
    key terms to answer the question. The classic QA approaches do not support very
    long sequences and fail to correctly model the meaning of context-dependent words.
    Different words can have different meanings, which increases the difficulty of
    extracting the essential semantic logical flow of each sentence in different paragraphs
    of context. These models are unable to address uncertain situations that require
    additional information to answer a particular question. In contrast, attention
    networks allow rich dialogues through addressing mechanisms for explicit memories
    or alignment structures in the query-context and context-query directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention also contributes to summarize or classify texts/documents. It mainly
    helps build more effective embeddings that generally consider contextual, semantic,
    and hierarchical information between words, phrases, and paragraphs. Specifically,
    in summarization tasks, attention minimizes critical problems involving: 1) modeling
    of keywords; 2) summary of abstract sentences; 3) capture of the sentence’s hierarchical
    structure; 4) repetitions of inconsistent phrases; and 5) generation of short
    sentences preserving their meaning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5feda58974fd04590605d70a82fe7441.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Illustration of RNNSearch [[44](#bib.bib44)] for machine translation,
    and End-to-End Memory Networks [[91](#bib.bib91)] for question answering. a) The
    RNNSearch architecture. The attention guided by the decoder’s previous state dynamically
    searches for important source words for the next time step. b) The End-to-End
    Memory Networks. The architecture consists of external memory and several stacked
    attention modules. To generate a response, the model makes several hops in memory
    using only attentional layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/900a646ad01ef15cdd245da44bd1287a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Illustration of Neural Transformer [[37](#bib.bib37)] for machine
    translation. The architecture consists of several stacked attentional encoders
    and decoders. The encoder process is massively parallel and eliminates recurrences,
    and the decoder generates the translated words sequentially. Each encoder uses
    multiple heads of the self-attention mechanism followed by fusion and normalization
    layers. Similarly, to generate the output, each decoder has multiple heads of
    self-attention and masked-self attention to mask words not yet generated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [17](#S5.F17 "Figure 17 ‣ 5.1 Natural Language Processing (NLP) ‣ 5
    Applications ‣ Attention, please! A survey of Neural Attention Models in Deep
    Learning") illustrates two models working in NLP tasks: RNNSearch [[44](#bib.bib44)],
    in machine translation, and End-to-End Memory Networks [[91](#bib.bib91)] in question
    answering. In RNNSearch, the attention guided by the decoder’s previous state
    dynamically searches for important source words for the next time step. It consists
    of an encoder followed by a decoder. The encoder is a bidirectional RNN (BiRNN) [[265](#bib.bib265)]
    that consists of forward and backward RNN’s. The forward RNN reads the input sequence
    in order and calculates the forward hidden state sequence. The backward RNN reads
    the sequence in the reverse order, resulting in the backward hidden states sequence.
    The decoder has an RNN and an attention system that calculates a probability distribution
    for all possible output symbols from a context vector.'
  prefs: []
  type: TYPE_NORMAL
- en: In End-to-End Memory Networks, attention looks for the memory elements most
    related to query $q$ using an alignment function that dispenses the RNNs’ complex
    structure. It consists of a memory and a stack of identical attentional systems.
    Each layer $i$ takes as input set $\left\{x_{1},...,x_{N}\right\}$ to store in
    the memory. The input set is converted in memory vectors $\left\{m_{1},...,m_{N}\right\}$
    and $\left\{h_{1},...,h_{N}\right\}$, in the simplest case using the embedding
    matrix $A^{i}\in\mathbb{R}^{d\times V}$ to generate each $m_{i}\in\mathbb{R}^{d}$,
    and the matrix $C^{i}\in\mathbb{R}^{d\times V}$ to generate each $h_{i}\in\mathbb{R}^{d}$.
    In the first, layer the query $q$ is also embedded, via embedding matrix $B^{1}$
    to obtain an internal state $u^{1}$. From the second layer, the internal state
    $u^{i+1}$ is the sum of the $i$ layer output and the internal state $u^{i}$. Finally,
    the last layer generates $\hat{a}$.
  prefs: []
  type: TYPE_NORMAL
- en: The Neural Transformer [[37](#bib.bib37)], illustrated in figure [18](#S5.F18
    "Figure 18 ‣ 5.1 Natural Language Processing (NLP) ‣ 5 Applications ‣ Attention,
    please! A survey of Neural Attention Models in Deep Learning"), is the basis model
    for state-of-the-art results in NLP. The architecture consists of an arbitrary
    amount of stacked encoders and decoders. Each encoder has linear layers, an attention
    system, feed-forward neural networks, and normalization layers. The attention
    system has several parallel heads. Each head has $N$ attentional subsystems that
    perform the same task but have different contextual inputs. The encoder receives
    a word embedding matrix $I=\left\{x_{1},...,x_{N}\right\}$, $I$ $\in\mathbb{R}^{N\times
    d_{emb}}$ as input. As the architecture does not use recurrences, the input tokens’
    position information is not explicit, but it is necessary. To represent the spatial
    position information, the Transformer adds a positional encoding to each embedding
    vector. Positional encoding is fixed and uses sinusoidal functions.
  prefs: []
  type: TYPE_NORMAL
- en: The input $I$ goes through linear layers and generates, for each word, a query
    vector ($q_{i}$), a key vector ($k_{i}$), and a value vector ($v_{i}$).The attentional
    system receives all $Q$, $K$, and $V$ arrays as input and uses several parallel
    attention heads. The motivation for using a multi-head structure is to explore
    multiple subspaces since each head gets a different projection of the data. Each
    head learns a different aspect of attention to the input, calculating different
    attentional distributions. Having multiple heads on the Transformer is similar
    to having multiple feature extraction filters on CNNs. The head outputs an attentional
    mask that relates all queries to a certain key. In a simplified way, the operation
    performed by a head is a matrix multiplication between a matrix of queries and
    keys.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the data is added to the residual output from the previous layer and
    normalized, representing the encoder output. This data is input to the next encoder.
    The last encoder’s data are transformed into the attention matrices $K_{encdec}$
    and $V_{encdec}$. They are input to all decoder layers. This data help the decoder
    to focus on the appropriate locations in the input sequence. The decoder has two
    layers of attention, Feed-Foward layers and normalization layers. The attentional
    layers are the masked multi-head attention and the decoder multi-head attention.
  prefs: []
  type: TYPE_NORMAL
- en: The masked multi-head attention is very similar to the encoder multi-head attention,
    with the difference that the attention matrices $Q$, $K$, and $V$ are created
    only with the previous data words, masking future positions with $-\infty$ values
    before the softmax step. The decoder multi-head attention is equal to the encoder
    multi-head attention, except it creates the Q matrix from the data of the previous
    layer and uses the $K_{encdec}$ and $V_{encdec}$ matrices of the encoder output.
    The $K_{encdec}$ and $V_{encdec}$ matrices are the memory structure of the network,
    storing context information of the input sequence, and given the previous words
    in the output decoder, the relevant information is selected in memory for the
    prediction of the next word. Finally, a linear layer followed by a softmax function
    projects the decoder vector by the last decoder into a probability vector in which
    each position defines the probability of the output word being a given vocabulary
    word. At each time step $t$, the position with the highest probability value is
    chosen, and the word associated with it is the output.
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | References |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Code Summarization | [[266](#bib.bib266)] |'
  prefs: []
  type: TYPE_TB
- en: '| Language Recognition | [[267](#bib.bib267)] |'
  prefs: []
  type: TYPE_TB
- en: '| Emotion Recognition |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[268](#bib.bib268)] [[269](#bib.bib269)] [[270](#bib.bib270)] [[271](#bib.bib271)] [[272](#bib.bib272)] [[273](#bib.bib273)] [[274](#bib.bib274)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Text Classification | [[275](#bib.bib275)] [[103](#bib.bib103)] |'
  prefs: []
  type: TYPE_TB
- en: '| Speech Classification | [[276](#bib.bib276)] [[277](#bib.bib277)] |'
  prefs: []
  type: TYPE_TB
- en: '| Relation Classification |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[278](#bib.bib278)] [[279](#bib.bib279)] [[148](#bib.bib148)] [[280](#bib.bib280)] [[281](#bib.bib281)] [[282](#bib.bib282)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Intent Classification | [[283](#bib.bib283)] |'
  prefs: []
  type: TYPE_TB
- en: '| Document Classification | [[284](#bib.bib284)] [[52](#bib.bib52)] |'
  prefs: []
  type: TYPE_TB
- en: '| Audio Classification | [[285](#bib.bib285)] |'
  prefs: []
  type: TYPE_TB
- en: '| Transfer Learning | [[286](#bib.bib286)] [[287](#bib.bib287)] |'
  prefs: []
  type: TYPE_TB
- en: '| Text-to-Speech | [[288](#bib.bib288)] [[289](#bib.bib289)] [[113](#bib.bib113)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Syntax Analysis | [[290](#bib.bib290)] |'
  prefs: []
  type: TYPE_TB
- en: '| Speech Translation | [[291](#bib.bib291)] |'
  prefs: []
  type: TYPE_TB
- en: '| Speech Transcription | [[48](#bib.bib48)] |'
  prefs: []
  type: TYPE_TB
- en: '| Speech Production | [[292](#bib.bib292)] |'
  prefs: []
  type: TYPE_TB
- en: '| Sequence Prediction | [[293](#bib.bib293)] |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic Matching | [[294](#bib.bib294)] |'
  prefs: []
  type: TYPE_TB
- en: '| Relation Extraction | [[295](#bib.bib295)] |'
  prefs: []
  type: TYPE_TB
- en: '| Reading Comprehension | [[194](#bib.bib194)] [[202](#bib.bib202)] [[296](#bib.bib296)] [[297](#bib.bib297)] [[298](#bib.bib298)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Natural Language Understanding | [[299](#bib.bib299)] |'
  prefs: []
  type: TYPE_TB
- en: '| Natural Language Transduction | [[300](#bib.bib300)] |'
  prefs: []
  type: TYPE_TB
- en: '| Natural Language Generation | [[154](#bib.bib154)] |'
  prefs: []
  type: TYPE_TB
- en: '| Machine Reading | [[301](#bib.bib301)] |'
  prefs: []
  type: TYPE_TB
- en: '| Intent Detection | [[302](#bib.bib302)] |'
  prefs: []
  type: TYPE_TB
- en: '| Grammatical Correction | [[303](#bib.bib303)] |'
  prefs: []
  type: TYPE_TB
- en: '| Entity Resolution | [[304](#bib.bib304)] [[305](#bib.bib305)] |'
  prefs: []
  type: TYPE_TB
- en: '| Entity Description | [[138](#bib.bib138)] |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding | [[306](#bib.bib306)] [[307](#bib.bib307)] [[308](#bib.bib308)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dependency Parsing | [[309](#bib.bib309)] [[310](#bib.bib310)] |'
  prefs: []
  type: TYPE_TB
- en: '| Conversation Model | [[311](#bib.bib311)] [[312](#bib.bib312)] |'
  prefs: []
  type: TYPE_TB
- en: '| Automatic Question Tagging | [[313](#bib.bib313)] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Summary others applications of attention in natural language processing.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Computer Vision (CV)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Visual attention has become popular in many CV tasks. Action recognition, counting
    crowds, image classification, image generation, object detection, person recognition,
    segmentation, saliency detection, text recognition, and tracking targets are the
    most explored sub-areas, as shown in Table [3](#S5.T3 "Table 3 ‣ 5.2 Computer
    Vision (CV) ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning"). Applications in other sub-areas still have few representative
    works, such as clustering, compression, deblurring, depth estimation, image restoration,
    among others, as shown in Table [4](#S5.T4 "Table 4 ‣ 5.2 Computer Vision (CV)
    ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention Models in Deep
    Learning").
  prefs: []
  type: TYPE_NORMAL
- en: Visual attention in image classification tasks was first addressed by Graves
    et al. [[46](#bib.bib46)]. In this domain, there are sequential approaches inspired
    by human saccadic movements [[46](#bib.bib46)] and feedforward-augmented structures
    CNNs (Section [4](#S4 "4 Attention-based Classic Deep Learning Architectures ‣
    Attention, please! A survey of Neural Attention Models in Deep Learning")). The
    general goal is usually to amplify fine-grained recognition, improve classification
    in the presence of occlusions, sudden variations in points of view, lighting,
    and rotation. Some approaches aim to learn to look at the most relevant parts
    of the input image, while others try to discern between discriminating regions
    through feature recalibration and ensemble predictors via attention. To fine-grained
    recognition, important advances have been achieved through recurrent convolutional
    networks in the classification of bird subspecies [[142](#bib.bib142)] and architectures
    trained via RL to classify vehicle subtypes [[129](#bib.bib129)].
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | References |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Action Recognition |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[314](#bib.bib314)] [[315](#bib.bib315)] [[316](#bib.bib316)] [[317](#bib.bib317)] [[69](#bib.bib69)] [[135](#bib.bib135)] [[318](#bib.bib318)] [[319](#bib.bib319)] [[153](#bib.bib153)] [[320](#bib.bib320)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[321](#bib.bib321)] [[322](#bib.bib322)] [[323](#bib.bib323)] [[324](#bib.bib324)] [[325](#bib.bib325)] [[326](#bib.bib326)] [[130](#bib.bib130)] [[327](#bib.bib327)] [[328](#bib.bib328)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[329](#bib.bib329)] [[330](#bib.bib330)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Counting Crowds |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[331](#bib.bib331)] [[332](#bib.bib332)] [[333](#bib.bib333)] [[334](#bib.bib334)] [[131](#bib.bib131)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Image Classification |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[142](#bib.bib142)] [[335](#bib.bib335)] [[336](#bib.bib336)] [[337](#bib.bib337)] [[140](#bib.bib140)] [[338](#bib.bib338)] [[46](#bib.bib46)] [[135](#bib.bib135)] [[157](#bib.bib157)] [[147](#bib.bib147)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[339](#bib.bib339)] [[59](#bib.bib59)] [[340](#bib.bib340)] [[167](#bib.bib167)] [[132](#bib.bib132)] [[341](#bib.bib341)] [[342](#bib.bib342)] [[343](#bib.bib343)] [[344](#bib.bib344)] [[344](#bib.bib344)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[345](#bib.bib345)] [[346](#bib.bib346)] [[73](#bib.bib73)] [[347](#bib.bib347)] [[348](#bib.bib348)] [[349](#bib.bib349)] [[350](#bib.bib350)] [[134](#bib.bib134)] [[351](#bib.bib351)] [[352](#bib.bib352)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[71](#bib.bib71)] [[353](#bib.bib353)] [[354](#bib.bib354)] [[355](#bib.bib355)] [[356](#bib.bib356)] [[357](#bib.bib357)] [[72](#bib.bib72)] [[358](#bib.bib358)] [[359](#bib.bib359)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Image Generation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[36](#bib.bib36)] [[119](#bib.bib119)] [[360](#bib.bib360)] [[361](#bib.bib361)] [[163](#bib.bib163)] [[362](#bib.bib362)] [[118](#bib.bib118)] [[363](#bib.bib363)] [[105](#bib.bib105)] [[364](#bib.bib364)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[365](#bib.bib365)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Object Recognition |'
  prefs: []
  type: TYPE_TB
- en: '&#124;  [[366](#bib.bib366)] [[146](#bib.bib146)] [[367](#bib.bib367)] [[368](#bib.bib368)] [[369](#bib.bib369)] [[370](#bib.bib370)] [[371](#bib.bib371)] [[372](#bib.bib372)] [[373](#bib.bib373)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Object Detection |'
  prefs: []
  type: TYPE_TB
- en: '&#124;  [[374](#bib.bib374)] [[375](#bib.bib375)] [[376](#bib.bib376)] [[377](#bib.bib377)] [[378](#bib.bib378)] [[379](#bib.bib379)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Person Recognition |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[380](#bib.bib380)] [[381](#bib.bib381)] [[382](#bib.bib382)] [[383](#bib.bib383)] [[384](#bib.bib384)] [[385](#bib.bib385)] [[386](#bib.bib386)] [[387](#bib.bib387)] [[388](#bib.bib388)] [[389](#bib.bib389)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[390](#bib.bib390)] [[391](#bib.bib391)] [[392](#bib.bib392)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[393](#bib.bib393)] [[394](#bib.bib394)] [[395](#bib.bib395)] [[396](#bib.bib396)] [[397](#bib.bib397)] [[398](#bib.bib398)] [[399](#bib.bib399)] [[400](#bib.bib400)] [[401](#bib.bib401)] [[402](#bib.bib402)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[403](#bib.bib403)] [[404](#bib.bib404)] [[405](#bib.bib405)] [[406](#bib.bib406)] [[143](#bib.bib143)] [[407](#bib.bib407)] [[408](#bib.bib408)] [[409](#bib.bib409)] [[410](#bib.bib410)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Saliency Detection | [[411](#bib.bib411)] [[412](#bib.bib412)] [[412](#bib.bib412)] [[413](#bib.bib413)] [[414](#bib.bib414)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Text Recognition | [[415](#bib.bib415)] [[416](#bib.bib416)] [[417](#bib.bib417)] [[418](#bib.bib418)] [[419](#bib.bib419)] [[420](#bib.bib420)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Tracking Targets |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[393](#bib.bib393)] [[394](#bib.bib394)] [[395](#bib.bib395)] [[396](#bib.bib396)] [[397](#bib.bib397)] [[398](#bib.bib398)] [[399](#bib.bib399)] [[400](#bib.bib400)] [[401](#bib.bib401)] [[402](#bib.bib402)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[403](#bib.bib403)] [[404](#bib.bib404)] [[405](#bib.bib405)] [[406](#bib.bib406)] [[143](#bib.bib143)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Summary state-of-art approaches in computer vision sub-areas.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual attention also provides significant benefits for action recognition
    tasks by capturing spatio-temporal relationships. The biggest challenge’s classical
    approaches are capturing discriminative features of movement in the sequences
    of images or videos. The attention allows the network to focus the processing
    only on the relevant joints or on the movement features easily. Generally, the
    main approaches use the following strategies: 1) saliency maps: spatiotemporal
    attention models learn where to look in video directly human fixation data. These
    models express the probability of saliency for each pixel. Deep 3D CNNs extract
    features only high saliency regions to represent spatial and short time relations
    at clip level, and LSTMs expand the temporal domain from few frames to seconds [[319](#bib.bib319)];
    2) self-attention: modeling context-dependencies. The person being classified
    is the Query (Q), and the clip around the person is the memory, represented by
    keys (K) and values (V) vectors. The network process the query and memory to generate
    an updated query vector. Intuitively self-attention adds context to other people
    and objects in the clip to assist in subsequent classification [[315](#bib.bib315)];
    3) recurrent attention mechanisms: captures relevant positions of joints or movement
    features and, through a recurring structure, refines the attentional focus at
    each time step [[67](#bib.bib67)] [[153](#bib.bib153)]; and 4) temporal attention:
    captures relevant spatial-temporal locations [[320](#bib.bib320)] [[320](#bib.bib320)] [[317](#bib.bib317)] [[421](#bib.bib421)] [[422](#bib.bib422)] [[423](#bib.bib423)].'
  prefs: []
  type: TYPE_NORMAL
- en: Liu et al. [[67](#bib.bib67)] model is a recurrent attention approach to capturing
    the person’s relevant positions. This model presented a pioneering approach using
    two layers of LSTMs and the context memory cell that recurrently interact with
    each other, as shown in figure [19](#S5.F19 "Figure 19 ‣ 5.2 Computer Vision (CV)
    ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention Models in Deep
    Learning")a. First, a layer of LSTMs generates an encoding of a skeleton sequence,
    initializing the context memory cell. The memory representation is input to the
    second layer of LSTMs and helps the network selectively focus on each frame’s
    informational articulations. Finally, attentional representation feeds back the
    context memory cell to refine the focus’s orientation again by paying attention
    more reliably. Similarly, Du et al. [[153](#bib.bib153)] proposed RPAN - a recurrent
    attention approach between sequentially modeling by LSTMs and convolutional features
    extractors. First, CNNs extract features from the current frame, and the attentional
    mechanism guided by the LSTM’s previous hidden state estimates a series of features
    related to human articulations related to the semantics of movements of interest.
    Then, these highly discriminative features feed LSTM time sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b74eec800b750624e915345f217f1f1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Illustration of Global Context-Aware Attention [[67](#bib.bib67)]
    for action recognition, and DRAW [[36](#bib.bib36)] for image generation. a) The
    Context-Aware Attention Network. The first LSTM layer encodes the skeleton sequence
    and generates an initial global context memory. The second layer performs attention
    over the inputs with global context memory assistance and generates a refined
    representation. The refine representation is then used back to the global context.
    Multiple attention iterations are carried out to refine the global context progressively.
    Finally, memory information is used for classification. b) The DRAW architecture.
    At each time step $t$, the input is read by attention and passed to the encoder
    RNN. The encoder’s output is used to compute the approximate posterior over the
    latent variables. On the right, an illustration shows the iterative process of
    generating some images. Each row shows successive stages in the generation of
    a single digit. The network guided by attention draws and refine regions successively.
    The red rectangle delimits the area attended to by the network at each time-step.'
  prefs: []
  type: TYPE_NORMAL
- en: In image generation, there were also notable benefits. DRAW [[36](#bib.bib36)]
    introduced visual attention with an innovative approach - image patches are generated
    sequentially and gradually refined, in which to generate the entire image in a
    single pass (figure [19](#S5.F19 "Figure 19 ‣ 5.2 Computer Vision (CV) ‣ 5 Applications
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")b).
    Subsequently, attentional mechanisms emerged in generative adversarial networks
    (GANs) to minimize the challenges in modeling images with structural constraints.
    Naturally, GANs efficiently synthesize elements differentiated by texture (i.e.,
    oceans, sky, natural landscapes) but suffer to generate geometric patterns (i.e.,
    faces, animals, people, fine details). The central problem is the convolutions
    that fail to model dependencies between distant regions. Besides, the statistical
    and computational efficiency of the model suffers from the stacking of many layers.
    The attentional mechanisms, especially self-attention, offered a computationally
    inexpensive alternative to model long-range dependencies easily. Self-attention
    as a complement to convolution contributes significantly to the advancement of
    the area with approaches capable of generating fine details [[119](#bib.bib119)],
    high-resolution images, and with intricate geometric patterns [[120](#bib.bib120)].
  prefs: []
  type: TYPE_NORMAL
- en: In expression recognition, attention optimizes the entire segmentation process
    by scanning input as a whole sequence, choosing the most relevant region to describe
    a segmented symbol or implicit space operator [[424](#bib.bib424)]. In information
    retriever, attention helps obtain appropriate semantic resources using individual
    class semantic resources to progressively orient visual aids to generate an attention
    map to ponder the importance of different local regions. [[425](#bib.bib425)].
  prefs: []
  type: TYPE_NORMAL
- en: In medical image analysis, attention helps implicitly learn to suppress irrelevant
    areas in an input image while highlighting useful resources for a specific task.
    This allows us to eliminate the need to use explicit external tissue/organ localization
    modules using convolutional neural networks (CNNs). Besides, it allows generating
    both images and maps of attention in unsupervised learning useful for data annotation.
    For this, there are the ATA-GANS [[360](#bib.bib360)] and attention gate [[426](#bib.bib426)]
    modules that work with unsupervised and supervised learning, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | References |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Clustering |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[106](#bib.bib106)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Compression | [[427](#bib.bib427)] |'
  prefs: []
  type: TYPE_TB
- en: '| Deblurring | [[428](#bib.bib428)] |'
  prefs: []
  type: TYPE_TB
- en: '| Depth Estimation | [[429](#bib.bib429)] [[430](#bib.bib430)] |'
  prefs: []
  type: TYPE_TB
- en: '| Image Restoration | [[133](#bib.bib133)] [[431](#bib.bib431)] [[432](#bib.bib432)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Image-to-Image Translation | [[433](#bib.bib433)] [[434](#bib.bib434)] |'
  prefs: []
  type: TYPE_TB
- en: '| Information Retriever | [[425](#bib.bib425)] [[435](#bib.bib435)] [[436](#bib.bib436)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Image Analysis | [[360](#bib.bib360)] [[426](#bib.bib426)] [[437](#bib.bib437)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multiple Instance Learning | [[438](#bib.bib438)] |'
  prefs: []
  type: TYPE_TB
- en: '| Ocr | [[439](#bib.bib439)] |'
  prefs: []
  type: TYPE_TB
- en: '| One-Shot Learning | [[162](#bib.bib162)] |'
  prefs: []
  type: TYPE_TB
- en: '| Pose Estimation | [[440](#bib.bib440)] |'
  prefs: []
  type: TYPE_TB
- en: '| Super-Resolution | [[441](#bib.bib441)] |'
  prefs: []
  type: TYPE_TB
- en: '| Transfer Learning | [[442](#bib.bib442)] [[443](#bib.bib443)] [[145](#bib.bib145)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Video Classification | [[444](#bib.bib444)] [[445](#bib.bib445)] |'
  prefs: []
  type: TYPE_TB
- en: '| Facial Detection | [[144](#bib.bib144)] [[446](#bib.bib446)] [[447](#bib.bib447)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fall Detection | [[448](#bib.bib448)] |'
  prefs: []
  type: TYPE_TB
- en: '| Person Detection | [[449](#bib.bib449)] [[450](#bib.bib450)] |'
  prefs: []
  type: TYPE_TB
- en: '| Sign Detection | [[451](#bib.bib451)] |'
  prefs: []
  type: TYPE_TB
- en: '| Text Detection | [[415](#bib.bib415)] [[452](#bib.bib452)] [[453](#bib.bib453)] [[454](#bib.bib454)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Face Recognition | [[139](#bib.bib139)] |'
  prefs: []
  type: TYPE_TB
- en: '| Facial Expression Recognition | [[455](#bib.bib455)] [[456](#bib.bib456)] [[457](#bib.bib457)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sequence Recognition | [[458](#bib.bib458)] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Summary others applications of attention in computer vision.'
  prefs: []
  type: TYPE_NORMAL
- en: For person recognition, attention has become essential in in-person re-identification
    (re-id) [[336](#bib.bib336)] [[388](#bib.bib388)] [[380](#bib.bib380)]. Re-id
    aims to search for people seen from a surveillance camera implanted in different
    locations. In classical approaches, the bounding boxes of detected people were
    not optimized for re-identification suffering from misalignment problems, background
    disorder, occlusion, and absent body parts. Misalignment is one of the biggest
    challenges, as people are often captured in various poses, and the system needs
    to compare different images. In this sense, neural attention models started to
    lead the developments mainly with multiple attentional mechanisms of alignment
    between different bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: There are still less popular applications, but for which attention plays an
    essential role. Self-attention models iterations between the input set for clustering
    tasks [[106](#bib.bib106)]. Attention refines and merges multi-scale feature maps
    in-depth estimation and edge detection [[459](#bib.bib459)] [[429](#bib.bib429)].
    In video classification, attention helps capture global and local resources generating
    a comprehensive representation [[460](#bib.bib460)]. It also measures each time
    interval’s relevance in a sequence [[423](#bib.bib423)], promoting a more intuitive
    interpretation of the impact of content on the video’s popularity, providing the
    regions that contribute the most to the prediction [[444](#bib.bib444)]. In face
    detection, attention dynamically selects the main reference points of the face [[447](#bib.bib447)].
    It improves deblurring in each convolutional layer in deblurring, preserving fine
    details [[428](#bib.bib428)]. Finally, in emotion recognition, it captures complex
    relationships between audio and video data by obtaining regions where both signals
    relate to emotion [[75](#bib.bib75)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Multimodal Tasks (CV/NLP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Attention has been used extensively in multimodal learning, mainly for mapping
    complex relationships between different sensory modalities. In this domain, the
    importance of attention is quite intuitive, given that communication and human
    sensory processing are completely multimodal. The first approaches emerged from
    2015 inspired by an attentive encoder-decoder framework entitled “Show, attend
    and tell: Neural image caption generation with visual attention” by Xu et al. [[45](#bib.bib45)].
    In this framework, depicted in figure [20](#S5.F20 "Figure 20 ‣ 5.3 Multimodal
    Tasks (CV/NLP) ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")a at each time step $t$, attention generates a vector
    with a dynamic context of visual features based on the words previously generated
    - a principle very similar to that presented in RNNSearch [[44](#bib.bib44)].
    Later, more elaborate methods using visual and textual sources were developed
    mainly in image captioning, video captioning, and visual question answering, as
    shown in the Table [5](#S5.T5 "Table 5 ‣ 5.3 Multimodal Tasks (CV/NLP) ‣ 5 Applications
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | References |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Emotion Recognition | [[461](#bib.bib461)] [[76](#bib.bib76)] [[77](#bib.bib77)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Expression Comprehension | [[462](#bib.bib462)] |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification | [[463](#bib.bib463)] |'
  prefs: []
  type: TYPE_TB
- en: '| Text-to-Image Generation | [[464](#bib.bib464)] |'
  prefs: []
  type: TYPE_TB
- en: '| Image-to-Text Generation | [[465](#bib.bib465)] |'
  prefs: []
  type: TYPE_TB
- en: '| Image Captioning |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[466](#bib.bib466)] [[467](#bib.bib467)] [[45](#bib.bib45)] [[468](#bib.bib468)] [[469](#bib.bib469)] [[470](#bib.bib470)] [[471](#bib.bib471)] [[472](#bib.bib472)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[81](#bib.bib81)] [[473](#bib.bib473)] [[474](#bib.bib474)] [[475](#bib.bib475)] [[476](#bib.bib476)] [[82](#bib.bib82)] [[477](#bib.bib477)] [[70](#bib.bib70)] [[478](#bib.bib478)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[479](#bib.bib479)] [[480](#bib.bib480)] [[481](#bib.bib481)] [[466](#bib.bib466)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Video Captioning |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[482](#bib.bib482)] [[63](#bib.bib63)] [[483](#bib.bib483)] [[484](#bib.bib484)] [[66](#bib.bib66)] [[62](#bib.bib62)] [[485](#bib.bib485)] [[74](#bib.bib74)] [[486](#bib.bib486)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[321](#bib.bib321)] [[487](#bib.bib487)] [[483](#bib.bib483)] [[488](#bib.bib488)] [[489](#bib.bib489)] [[490](#bib.bib490)] [[491](#bib.bib491)] [[492](#bib.bib492)] [[493](#bib.bib493)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[494](#bib.bib494)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Visual Question Answering |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[495](#bib.bib495)] [[496](#bib.bib496)] [[497](#bib.bib497)] [[498](#bib.bib498)] [[122](#bib.bib122)] [[499](#bib.bib499)] [[500](#bib.bib500)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[80](#bib.bib80)] [[190](#bib.bib190)] [[501](#bib.bib501)] [[502](#bib.bib502)] [[503](#bib.bib503)] [[504](#bib.bib504)] [[505](#bib.bib505)] [[88](#bib.bib88)] [[506](#bib.bib506)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[507](#bib.bib507)] [[508](#bib.bib508)] [[509](#bib.bib509)] [[510](#bib.bib510)] [[511](#bib.bib511)] [[512](#bib.bib512)] [[513](#bib.bib513)] [[514](#bib.bib514)] [[79](#bib.bib79)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[515](#bib.bib515)] [[516](#bib.bib516)] [[517](#bib.bib517)] [[518](#bib.bib518)] [[466](#bib.bib466)] [[519](#bib.bib519)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Summary of state-of-art approaches in multimodal tasks (CV/NLP).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For image captioning, Yan et al. [[474](#bib.bib474)] extended the seminal
    framework by Xu et al. [[45](#bib.bib45)] with review attention, a sequence of
    modules that capture global information in various stages of reviewing hidden
    states and generate more compact, abstract, and global context vectors. Zhu et
    al. [[467](#bib.bib467)] presented a triple attention model which enhances object
    information at the text generation stage. Two attention mechanisms capture semantic
    visual information in input, and a mechanism in the prediction stage integrates
    word and image information better. Lu et al. [[469](#bib.bib469)] presented an
    adaptive attention encoder-decoder framework that decides when to trust visual
    signals and when to trust only the language model. Specifically, their mechanism
    has two complementary elements: the visual sentinel vector decides when to look
    at the image, and the sentinel gate decides how much new information the decoder
    wants from the image. Recently, Pan et al. [[481](#bib.bib481)] created attentional
    mechanisms based on bilinear pooling capable of capturing high order interactions
    between multi-modal features, unlike the classic mechanisms that capture only
    first-order feature interactions.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in visual question-answering tasks, methods seek to align salient
    textual features with visual features via feedforward or recurrent soft attention
    methods [[466](#bib.bib466)] [[499](#bib.bib499)] [[500](#bib.bib500)] [[517](#bib.bib517)].
    More recent approaches aim to generate complex inter-modal representations. In
    this line, Kim et al. [[516](#bib.bib516)] proposed Hypergraph Attention Networks
    (HANs), a solution to minimize the disparity between different levels of abstraction
    from different sensory sources. So far, HAN is the first approach to define a
    common semantic space with symbolic graphs of each modality and extract an inter-modal
    representation based on co-attention maps in the constructed semantic space, as
    shown in figure [20](#S5.F20 "Figure 20 ‣ 5.3 Multimodal Tasks (CV/NLP) ‣ 5 Applications
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")b.
    Liang et al. [[497](#bib.bib497)] used attention to capture hierarchical relationships
    between sequences of image-text pairs not directly related. The objective is to
    answer questions and justify what results in the system were based on answers.
  prefs: []
  type: TYPE_NORMAL
- en: For video captioning most approaches generally align textual features and spatio-temporal
    representations of visual features via simple soft attention mechanisms [[482](#bib.bib482)] [[485](#bib.bib485)] [[62](#bib.bib62)] [[74](#bib.bib74)].
    For example, Pu et al. [[66](#bib.bib66)] design soft attention to adaptively
    emphasize different CNN layers while also imposing attention within local spatiotemporal
    regions of the feature maps at particular layers. These mechanisms define the
    importance of regions and layers to produce a word based on word-history information.
    Recently, self-attention mechanisms have also been used to capture more complex
    and explicit relationships between different modalities. Zhu et al. [[484](#bib.bib484)]
    introduced ActBERT, a transformer-based approach trained via self-supervised learning
    to encode complex relations between global actions and local, regional objects
    and linguistic descriptions. Zhou et al. [[483](#bib.bib483)] proposed a multimodal
    transformer via supervised learning, which employs a masking network to restrict
    its attention to the proposed event over the encoding feature.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a2b0a4b5b4b418123e986ed3986d842.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: Illustration of classic attentive encoder-decoder framework by Xu
    et al. [[45](#bib.bib45)] and Hypergraph Attention Network [[516](#bib.bib516)]
    for question answering tasks. a) The show, attend and tell framework. At each
    time step $t$, attention takes as input visual feature maps and previous hidden
    state of the decoder and produces a dynamic context vector with essential visual
    features to predict the next word. b) The Hypergraph Attention Network. For a
    given pair of images and questions, two symbolic graphs $G_{i}$, and $G_{q}$ are
    constructed. After, two hypergraphs $HG_{i}$, and $HG_{q}$ with random-walk hyperedge
    are constructed and combined via co-attention map $A$. Finally, the final representation
    $Z_{s}$ is used to predict an answer for the given question.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other applications also benefit from the attention. In the emotion recognition
    domain, the main approaches use memory fusion structures inspired by the human
    brain’s communication understanding mechanisms. Biologically, different regions
    process and understand different modalities connected via neural links to integrate
    multimodal information over time. Similarly, in existing approaches, an attentional
    component models view-specific dynamics within each modality via recurrent neural
    networks, and a second component simultaneously finds multiple cross-view dynamics
    in each recurrence timestep by storing them in hybrid memories. Memory updates
    occur based on all the sequential data seen. Finally, to generate the output,
    the predictor integrates the two levels of information: view-specific and multiple
    cross-view memory information [[76](#bib.bib76)] [[77](#bib.bib77)].'
  prefs: []
  type: TYPE_NORMAL
- en: There are still few multimodal methods for classification. Whang et al. [[463](#bib.bib463)]
    presented a pioneering framework for classifying and describing image regions
    simultaneously from textual and visual sources. Their framework detects, classifies,
    and generates explanatory reports regarding abnormalities observed in chest X-ray
    images through multi-level attentional modules end-to-end in LSTMs and CNNs. In
    LSTMs, attention combines all hidden states and generates a dynamic context vector,
    then a spatial mechanism guided by a textual mechanism highlights the regions
    of the image with more meaningful information. Intuitively, the salient features
    of the image are extracted based on high-relevance textual regions.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Recommender Systems (RS)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attention has also been used in recommender systems for behavioral modeling
    of users. Capturing user interests is a challenging problem for neural networks,
    as some iterations are transient, some clicks are unintentional, and interests
    can change quickly in the same session. Classical approaches (i.e., Markov Chains
    and RNNs) have limited performance predicting the user’s next actions, present
    different performances in sparse and dense datasets, and long-term memory problems.
    In this sense, attention has been used mainly to assign weights to a user’s interacted
    items capturing long and short-term interests more effectively than traditional
    ones. Self-attention and memory approaches have been explored to improve the area’s
    development. STAMP [[520](#bib.bib520)] model, based on attention and memory,
    manages users’ general interests in long-term memories and current interests in
    short-term memories resulting in behavioral representations that are more coherent.
    The Collaborative Filtering [[521](#bib.bib521)] framework, and SASRec [[338](#bib.bib338)]
    explored self-attention in capturing long-term semantics for finding the most
    relevant items in user’s history.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Reinforcement Learning (RL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attention has been gradually introduced in reinforcement learning to deal with
    unstructured environments in which rewards and actions depend on past states and
    where it is challenging guaranteeing the Markov property. Specifically, the goals
    are to increase the agent’s generalizability and minimize long-term memory problems.
    Currently, the main attentional reinforcement learning approaches are computer
    vision, graph reasoning, natural language processing, and virtual navigation,
    as shown in Table [6](#S5.T6 "Table 6 ‣ 5.5 Reinforcement Learning (RL) ‣ 5 Applications
    ‣ Attention, please! A survey of Neural Attention Models in Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | References |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Computer Vision |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[155](#bib.bib155)] [[522](#bib.bib522)] [[523](#bib.bib523)] [[524](#bib.bib524)] [[525](#bib.bib525)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[129](#bib.bib129)] [[526](#bib.bib526)] [[150](#bib.bib150)] [[527](#bib.bib527)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Graph reasoning | [[528](#bib.bib528)] |'
  prefs: []
  type: TYPE_TB
- en: '| Natural Language Processing | [[78](#bib.bib78)] |'
  prefs: []
  type: TYPE_TB
- en: '| Navigation | [[141](#bib.bib141)] [[529](#bib.bib529)] [[530](#bib.bib530)] [[78](#bib.bib78)] [[531](#bib.bib531)]
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Summary of main state-of-art approaches in attentional reinforcement
    learning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: To increase the ability to generalize in partially observable environments,
    some approaches use attention in the policy network. Mishra et al. [[141](#bib.bib141)]
    used attention to easily capture long-term temporal dependencies in convolutions
    in an agent’s visual navigation task in random mazes. At each time step $t$, the
    model receives as input the current observation $o_{t}$ and previous sequences
    of observations, rewards, and actions so that attention allows the policy to maintain
    a long memory of past episodes. Other approaches implement attention directly
    to the representation of the state. State representation is a classic and critical
    problem in RL, given that state space is one of the major bottlenecks for speed,
    efficiency, and generalization of training techniques. In this sense, the importance
    of attention on this topic is quite intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are still few approaches exploring the representation of states.
    The neural map [[529](#bib.bib529)] maintains an internal memory in the agent
    controlled via attention mechanisms. While the agent navigates the environment,
    an attentional mechanism alters the internal memory, dynamically constructing
    a history summary. At the same time, another generates a representation $o_{t}$,
    based on the contextual information of the memory and the state’s current observation.
    Then, the policy network receives $o_{t}$ as input and generates the distribution
    of shares. Some more recent approaches affect the representation of the current
    $o_{t}$ observation of the state via self attention in iterative reasoning between
    entities in the scene [[530](#bib.bib530)] [[531](#bib.bib531)], or between the
    current observation $o_{t}$ and memory units [[78](#bib.bib78)] to guide model-free
    policies.
  prefs: []
  type: TYPE_NORMAL
- en: The most discussed topic is the use of the policy network to guide the attentional
    focus of the agent’s glimpses sensors on the environment so that the representation
    of the state refers to only a small portion of the entire operating environment.
    This approach emerged initially by Graves et al. [[46](#bib.bib46)] using policy
    gradient methods (i.e., REINFORCE algorithm) in the hybrid training of recurrent
    networks in image classification tasks. Their model consists of a glimpse sensor
    that captures only a portion of the input image, a core network that maintains
    a summary of the history of patches seen by the agent, an action network that
    estimates the class of the image seen, and a location network trained via RL which
    estimates the focus of the glimpse on the next time step, as shown in figure [21](#S5.F21
    "Figure 21 ‣ 5.5 Reinforcement Learning (RL) ‣ 5 Applications ‣ Attention, please!
    A survey of Neural Attention Models in Deep Learning")a. This structure considers
    the network as the agent, the image as the environment, and the reward is the
    number of correct network ratings in an episode. Stollenga et al. [[524](#bib.bib524)]
    proposed a similar approach, however directly focused on CNNs, as shown in figure [21](#S5.F21
    "Figure 21 ‣ 5.5 Reinforcement Learning (RL) ‣ 5 Applications ‣ Attention, please!
    A survey of Neural Attention Models in Deep Learning")b. The structure allows
    each layer to influence all the others through attentional bottom-up and top-down
    connections that modulate convolutional filters’ activity. After supervised training,
    the attentional connections’ weights implement a control policy via RL and SNES [[532](#bib.bib532)].
    The policy learns to suppress or enhance features at various levels by improving
    the classification of difficult cases not captured by the initial supervised training.
    Subsequently, variants similar to these approaches appeared in multiple image
    classification [[155](#bib.bib155)] [[150](#bib.bib150)] [[129](#bib.bib129)],
    action recognition [[527](#bib.bib527)], and face hallucination [[525](#bib.bib525)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/00343537d2d383782b79c11e8627b808.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Illustration of RAM [[46](#bib.bib46)] and dasNet [[524](#bib.bib524)]
    for image classification tasks. a) The RAM framework. At each time step $t$, the
    glimpse networks extracts a retina-like representation based on input image and
    an focus location $l_{t-1}$. The core network takes the glimpse representation
    as input and combining it with the internal representation at the previous time
    step and produces the new internal state of the model $h_{t}$. The location network
    and action network use the $h_{t}$ to produce the next location $l_{t}$ to attend
    and the classification. b) The dasNet network. Each image is classified after
    $T$ passes through CNNs. After each forward propagation, the averages of feature
    maps are combined into an observation vector $o_{t}$ that is used by a deterministic
    policy to choose an action $a_{t}$ that changes the focus of all the feature maps
    for the next pass of the same image. Finally, after pass $T$ times, the output
    is used to classify the image.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Robotics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In robotics, there are still few applications with neural attentional models.
    A small portion of current work is focused on control, visual odometry, navigation,
    and human-robot interaction, as shown in Table [7](#S5.T7 "Table 7 ‣ 5.6 Robotics
    ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention Models in Deep
    Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | References |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Control | [[533](#bib.bib533)] [[85](#bib.bib85)] |'
  prefs: []
  type: TYPE_TB
- en: '| Visual odometry | [[534](#bib.bib534)] [[535](#bib.bib535)] [[536](#bib.bib536)] [[537](#bib.bib537)] [[538](#bib.bib538)] [[539](#bib.bib539)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Navigation | [[540](#bib.bib540)] [[541](#bib.bib541)] [[542](#bib.bib542)] [[543](#bib.bib543)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Human-robot interaction | [[544](#bib.bib544)] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Summary state-of-art main approaches in robotics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigation and visual odometry are the most explored domains, although still
    with very few published works. For classic DL approaches, navigating tasks in
    real or complex environments are still very challenging. These approaches have
    limited performance in dynamic and unstructured environments and over long horizon
    tasks. In real environments, the robot must deal with dynamic and unexpected changes
    in humans and other obstacles around it. Also, decision-making depends on the
    information received in the past and the ability to infer the future state of
    the environment. Some seminal approaches in the literature have demonstrated the
    potential of attention to minimizing these problems without compromising the techniques’
    computational cost. Sadeghian et al. [[540](#bib.bib540)] proposed Sophie: an
    interpretable framework based on GANs for robotic agents in environments with
    human crowds. Their framework via attention extracts two levels of information:
    a physical extractor learns spatial and physical constraints generating a $C$
    context vector that focuses on viable paths for each agent. In contrast, a social
    extractor learns the interactions between agents and their influence on each agent’s
    future path. Finally, LSTMs based on GAN generate realistic samples capturing
    the nature of future paths, and the attentional mechanisms allow the framework
    to predict physically and socially feasible paths for agents, achieving cutting-edge
    performances on several different trajectories.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Vemula et al. [[541](#bib.bib541)] proposed a trajectory prediction model that
    captures each person’s relative importance when navigating in the crowd, regardless
    of their proximity via spatio-temporal graphs. Chen et al. [[542](#bib.bib542)]
    proposed the crowd-aware robot navigation with attention-based deep reinforcement
    learning. Specifically, a self-attention mechanism models interactions between
    human-robot and human-human pairs, improving the robot’s inference capacity of
    future environment states. It also captures how human-human interactions can indirectly
    affect decision-making, as shown in figure [22](#S5.F22 "Figure 22 ‣ 5.6 Robotics
    ‣ 5 Applications ‣ Attention, please! A survey of Neural Attention Models in Deep
    Learning") in a). Fang et. al. [[543](#bib.bib543)] proposed the novel memory-based
    policy (i.e., scene memory transformer - SMT) for embodied agents in long-horizon
    tasks. The SMT policy consists of two modules: 1) scene memory which stores all
    past observations in an embedded form, and 2) an attention-based policy network
    that uses the updated scene memory to compute a distribution over actions. The
    SMT model is based on an encoder-decoder Transformer and showed strong performance
    as the agent moves in a large environment, and the number of observations grows
    rapidly.'
  prefs: []
  type: TYPE_NORMAL
- en: In visual odometry (VO), the classic learning-based methods consider the VO
    task a problem of pure tracking through the recovery of camera poses from fragments
    of the image, leading to the accumulation of errors. Such approaches often disregard
    crucial global information to alleviate accumulated errors. However, it is challenging
    to preserve this information in end-to-end systems effectively. Attention represents
    an alternative that is still little explored in this area to alleviate such disadvantages.
    Xue et al. [[534](#bib.bib534)] proposed an adaptive memory approach to avoid
    the network’s catastrophic forgetfulness. Their framework consists mainly of a
    memory, a remembering, and a refining module, as shown in figure  [22](#S5.F22
    "Figure 22 ‣ 5.6 Robotics ‣ 5 Applications ‣ Attention, please! A survey of Neural
    Attention Models in Deep Learning")b). First, it remembers to select the main
    hidden states based on camera movement while preserving selected hidden states
    in the memory slot to build a global map. The memory stores the global information
    of the entire sequence, allowing refinements on previous results. Finally, the
    refining module estimates each view’s absolute pose, allowing previously refined
    outputs to pass through recurrent units, thus improving the next estimate.
  prefs: []
  type: TYPE_NORMAL
- en: Another common problem in VO classical approaches is selecting the features
    to derive ego-motion between consecutive frames. In scenes, there are dynamic
    objects and non-textured surfaces that generate inconsistencies in the estimation
    of movement. Recently, self-attention mechanisms have been successfully employed
    in dynamic reweighting of features, and in the semantic selection of image regions
    to extract more refined egomotion [[536](#bib.bib536)] [[537](#bib.bib537)] [[538](#bib.bib538)].
    Additionally, self-attentive neural networks have been used to replace traditional
    recurrent networks that consume training time and are inaccurate in the temporal
    integration of long sequences [[539](#bib.bib539)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c3efb3033ac966491d220948288be9df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: Illustration of deep visual odometry with Adaptive Memory [[534](#bib.bib534)]
    and Crowd-Robot Interaction [[542](#bib.bib542)] for navigation. a) The deep visual
    odometry with adaptive memory framework. This framework introduces two important
    components called remembering and refining, as opposed to classic frameworks that
    treat VO as just a tracking task. Remembering preserves long-time information
    by adopting an adaptive context selection, and refining improves previous outputs
    using spatial-temporal feature reorganization mechanism. b) Crowd-Robot Interaction
    network. The network consists of three modules: interaction, pooling, and planning.
    Interaction extracts robot-human interactions in crowds. Pooling aggregates all
    interaction information, and planning estimates the state’s value based on robots
    and humans for navigation in crowds.'
  prefs: []
  type: TYPE_NORMAL
- en: In human-robot interaction, Zang et al. [[544](#bib.bib544)] proposed a framework
    that interprets navigation instructions in natural language and finds a mapping
    of commands in an executable navigation plan. The attentional mechanisms correlate
    navigation instructions very efficiently with the commands to be executed by the
    robot in only one trainable end-to-end model, unlike the classic approaches that
    use decoupled training and external interference during the system’s operation.
    In control, existing applications mainly use manipulator robots in visual-motor
    tasks. Duan et al. [[533](#bib.bib533)] used attention to improve the model’s
    generalization capacity in imitation learning approaches with a complex manipulator
    arm. The objective is to build a one-shot learning system capable of successfully
    performing instances of tasks not seen in the training. Thus, it employs soft
    attention mechanisms to process a long sequence of (states, actions) demonstration
    pairs. Finally, Abolghasemi et al. [[85](#bib.bib85)] proposed a deep visual engine
    policy through task-focused visual attention to make the policy more robust and
    to prevent the robot from releasing manipulated objects even under physical attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Interpretability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A long-standing criticism of neural network models is their lack of interpretability [[545](#bib.bib545)].
    Academia and industry have a great interest in the development of interpretable
    models mainly for the following aspects: 1) critical decisions: when critical
    decisions need to be made (e.i., medical analysis, stock market, autonomous cars),
    it is essential to provide explanations to increase the confidence of the specialist
    human results; 2) failure analysis: an interpretable model can retrospectively
    inspect where bad decisions were made and understand how to improve the system;
    3) verification: there is no evidence of the models’ robustness and convergence
    even with small errors in the test set. It is difficult to explain the influence
    of spurious correlations on performance and why the models are sometimes excellent
    in some test cases and flawed in others; and 4) model improvements: interpretability
    can guide improvements in the model’s structure if the results are not acceptable.'
  prefs: []
  type: TYPE_NORMAL
- en: Attention as an interpretability tool is still an open discussion. For some
    researchers, it allows to inspect the models’ internal dynamics – the hypothesis
    is that the attentional weights’ magnitude is correlated with the data’s relevance
    for predicting the output. Li et al. [[545](#bib.bib545)] proposed a general methodology
    to analyze the effect of erasing particular representations of neural networks’
    input. When analyzing the effects of erasure, they found that attentional focuses
    are essential to understand networks’ internal functioning. In [[546](#bib.bib546)]
    the results showed that higher attentional weights generally contribute with more
    significant impact to the model’s decision, but multiple weights generally do
    not fully identify the most relevant representations for the final decision. In
    this investigation, the researchers concluded that attention is an ideal tool
    to identify which elements are responsible for the output but do not yet fully
    explain the model’s decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some studies have also shown that attention encodes linguistic notions relevant
    to understanding NLP models [[547](#bib.bib547)] [[548](#bib.bib548)] [[549](#bib.bib549)].
    However, Jain et al. [[550](#bib.bib550)] showed that although attention improves
    NLP results, its ability to provide transparency or significant explanations for
    the model’s predictions is questionable. Specifically, the researchers investigated
    the relationship between attentional weights and model results by answering the
    following questions: (i) to what extent do weights of attention correlate with
    metrics of the importance of features, specifically those resulting from gradient?
    Moreover, (ii) do different attentional maps produce different predictions? The
    results showed that the correlation between intuitive metrics about the features’
    importance (e.i., gradient approaches, erasure of features) and attentional weights
    is low in recurrent encoders. Besides, the selection of features other than the
    attentional distribution did not significantly impact the output as attentional
    weights exchanged at random also induced minimal output changes. The researchers
    also concluded that such results depend significantly on the type of architecture,
    given that feedforward encoders obtained more coherent relationships between attentional
    weights and output than other models.'
  prefs: []
  type: TYPE_NORMAL
- en: Vashishth et al. [[551](#bib.bib551)] systematically investigated explanations
    for the researchers’ distinct views through experiments on NLP tasks with single
    sequence models, pair sequence models, and self-attentive neural networks. The
    experiments showed that attentional weights in single sequences tasks work like
    gates and do not reflect the reasoning behind the model’s prediction, justifying
    the observations made by Jain et al. [[550](#bib.bib550)]. However, for pair sequence
    tasks, attentional weights were essential to explaining the model’s reasoning.
    Manual tests have also shown that attentional weights are highly similar to the
    manual assessment of human observers’ attention. Recently, Wiegreffe et al. [[552](#bib.bib552)]
    also investigated these issues in depth through an extensive protocol of experiments.
    The authors observed that attention as an explanation depends on the definition
    of explainability considered. If the focus is on plausible explainability, the
    authors concluded that attention could help interpret model insights. However,
    if the focus is a faithful and accurate interpretation of the link that the model
    establishes between inputs and outputs, results are not always positive. These
    authors confirmed that good alternatives distributions could be found in LSTMs
    and classification tasks, as hypothesized by Jain et al. [[550](#bib.bib550)].
    However, in some experiments, adversarial training’s alternative distributions
    had poor performances concerning attention’s traditional mechanisms. These results
    indicate that the attention mechanisms trained mainly in RNNs learn something
    significant about the relationship between tokens and prediction, which cannot
    be easily hacked. In the end, they showed that attention efficiency as an explanation
    depends on the data set and the model’s properties.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Trends and Opportunities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Attention has been one of the most influential ideas in the Deep Learning community
    in recent years, with several profound advances, mainly in computer vision and
    natural language processing. However, there is much space to grow, and many contributions
    are still to appear. In this section, we highlight some gaps and opportunities
    in this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 End-To-End Attention models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Over the past eight years, most of the papers published in the literature have
    involved attentional mechanisms. Models that are state of the art in DL use attention.
    Specifically, we note that end-to-end attention networks, such as Transformers [[37](#bib.bib37)]
    and Graph Attention Networks [[97](#bib.bib97)], have been expanding significantly
    and have been used successfully in tasks across multiple domains (Section [2](#S2
    "2 Overview ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")).
    In particular, Transformer has introduced a new form of computing in which the
    neural network’s core is fully attentional. Transformer-based language models
    like BERT [[286](#bib.bib286)], GPT2 [[116](#bib.bib116)], and GPT3 [[117](#bib.bib117)]
    are the most advanced language models in NLP. Image GPT [[120](#bib.bib120)] has
    recently revolutionized the results of unsupervised learning in imaging. It is
    already a trend to propose Transfomer based models with sparse attentional mechanisms
    to reduce the Transformer’s complexity from quadratic to linear and use attentional
    mechanisms to deal with multimodality in GATs. However, Transformer is still an
    autoregressive architecture in the decoder and does not use other cognitive mechanisms
    such as memory. As research in attention and DL is still at early stages, there
    is still plenty of space in the literature for new attentional mechanisms, and
    we believe that end-to-end attention architectures might be very influential in
    Deep Learning’s future models.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Learning Multimodality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attention has played a crucial role in the growth of learning from multimodal
    data. Multimodality is extremely important for learning complex tasks. Human beings
    use different sensory signals all the time to interpret situations and decide
    which action to take. For example, while recognizing emotions, humans use visual
    data, gestures, and voice tones to analyze feelings. Attention allowed models
    to learn the synergistic relationship between the different sensory data, even
    if they are not synchronized, allowing the development of increasingly complex
    applications mainly in emotion recognition, [[75](#bib.bib75)], feelings [[461](#bib.bib461)],
    and language-based image generation [[121](#bib.bib121)]. We note that multimodal
    applications are continually growing in recent years. However, most research efforts
    are still focused on relating a pair of sensory data, mostly visual and textual
    data. Architectures that can scale easily to handle more than one pair of sensors
    are not yet widely explored. Multimodal learning exploring voice data, RGBD images,
    images from monocular cameras, data from various sensors, such as accelerometers,
    gyroscopes, GPS, RADAR, biomedical sensors, are still scarce in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Cognitive Elements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attention proposed a new way of thinking about the architecture of neural networks.
    For many years, the scientific community neglected using other cognitive elements
    in neural network architectures, such as memory and logic flow control. Attention
    has made possible including in neural networks other elements that are widely
    important in human cognition. Memory Networks [[38](#bib.bib38)], and Neural Turing
    Machine [[86](#bib.bib86)] are essential approaches in which attention makes updates
    and recoveries in external memory. However, research on this topic is at an early
    stage. The Neural Turing Machine has not yet been explored in several application
    domains, being used only in simple datasets for algorithmic tasks, with a slow
    and unstable convergence. We believe that there is plenty of room to explore the
    advantages of NTM in a wide range of problems and develop more stable and efficient
    models. Still, Memory Networks [[38](#bib.bib38)] presents some developments (Section [2](#S2
    "2 Overview ‣ Attention, please! A survey of Neural Attention Models in Deep Learning")),
    but few studies explore the use of attention to managing complex and hierarchical
    structures of memory. Attention to managing different memory types simultaneously
    (i.e., working memory, declarative, non-declarative, semantic, and long and short
    term) is still absent in the literature. To the best of our knowledge, the most
    significant advances have been made in Dynamic Memory Networks [[93](#bib.bib93)]
    with the use of episodic memory. Another open challenge is how to use attention
    to plug external knowledge into memory and make training faster. Finally, undoubtedly
    one of the biggest challenges still lies in including other human cognition elements
    such as imagination, reasoning, creativity, and consciousness working in harmony
    with attentional structures.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Computer Vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recurrent Attention Models (RAM) [[46](#bib.bib46)] introduced a new form of
    image computing using glimpses and hard attention. The architecture is simple,
    scalable, and flexible. Spatial Transformer (STN) [[59](#bib.bib59)] presented
    a simple module for learning image transformations that can be easily plugged
    into different architectures. We note that RAM has a high potential for many tasks
    in which convolutional neural networks have difficulties, such as large, high-resolution
    images. However, currently, RAM has been explored with simple datasets. We believe
    that it is interesting to validate RAM in complex classification and regression
    tasks. Another proposal is to add new modules to the architecture, such as memory,
    multimodal glimpses, and scaling. It is interesting to explore STN in conjunction
    with RAM in classification tasks or use STN to predict transformations between
    sets of images. RAM aligned with STN can help address robostusnees to spatial
    transformation, learn the system dynamics in Visual Odometry tasks, enhance multiple-instance
    learning, addressing multiple view-points.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Capsule Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Capsule networks (CapsNets), a new class of deep neural network architectures
    proposed recently by Hinton et al. [[553](#bib.bib553)], have shown excellent
    performance in many fields, particularly in image recognition and natural language
    processing. However, few studies in the literature implement attention in capsule
    networks. AR CapsNet [[554](#bib.bib554)] implements a dynamic routing algorithm
    where routing between capsules is made through an attention module. The attention
    routing is a fast forward-pass while keeping spatial information. DA-CapsNet [[555](#bib.bib555)]
    proposes a dual attention mechanism, the first layer is added after the convolution
    layer, and the second layer is added after the primary caps. SACN [[556](#bib.bib556)]
    is the first model that incorporates the self-attention mechanism as an integral
    layer. Recently, Tsai. et al. [[557](#bib.bib557)] introduced a new attentional
    routing mechanism in which a daughter capsule is routed to a parent capsule-based
    between the father’s state and the daughter’s vote. We particularly believe that
    attention is essential to improve the relational and hierarchical nature that
    CapsNets propose. The development of works aiming at the dynamic attentional routing
    of the capsules and incorporating attentional capsules of self-attention, soft
    and hard attention can bring significant results to current models.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Neural-Symbolic Learning and Reasoning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to LeCun [[558](#bib.bib558)] one of the great challenges of artificial
    intelligence is to combine the robustness of connectionist systems (i.e., neural
    networks) with symbolic representation to perform complex reasoning tasks. While
    symbolic representation is highly recursive and declarative, neural networks encode
    knowledge implicitly by adjusting weights. For many decades exploring the fusion
    between connectionist and symbolic systems has been overlooked by the scientific
    community. Only over the past decade, research with hybrid approaches using the
    two families of AI methodologies has grown again. Approaches such as statistical
    relational learning (SRL) [[559](#bib.bib559)] and neural-symbolic learning [[560](#bib.bib560)]
    were proposed. Recently, attention mechanisms have been integrated into some neural-symbolic
    models, the development of which is still at an early stage. Memory Networks [[38](#bib.bib38)]
    (Section [2](#S2 "2 Overview ‣ Attention, please! A survey of Neural Attention
    Models in Deep Learning")) and Neural Turing Machine [[86](#bib.bib86)] (Section [2](#S2
    "2 Overview ‣ Attention, please! A survey of Neural Attention Models in Deep Learning"))
    were the first initiatives to include reasoning in deep connectionist models.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of neural logic programming, attention has been exploited to
    reason about knowledge graphs or memory structures to combine the learning of
    parameters and structures of logical rules. Neural Logic Programming [[561](#bib.bib561)]
    uses attention on a neural controller that learns to select a subset of operations
    and memory content to execute first-order rules. Logic Attention Networks [[562](#bib.bib562)]
    facilitates inductive KG embedding and uses attention to aggregate information
    coming from graph neighbors with rules and attention weights. A pGAT [[563](#bib.bib563)]
    uses attention to knowledge base completion, which involves the prediction of
    missing relations between entities in a knowledge graph. While producing remarkable
    advances, recent approaches to reasoning with deep networks do not adequately
    address the task of symbolic reasoning. Current efforts are only about using attention
    to ensure efficient memory management. We believe that attention can be better
    explored to understand which pieces of knowledge are relevant to formulate a hypothesis
    to provide a correct answer, which are rarely present in current neural systems
    of reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 6.7 Incremental Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Incremental learning is one of the challenges for the DL community in the coming
    years. Machine learning classifiers are trained to recognize a fixed set of classes.
    However, it is desirable to have the flexibility to learn additional classes with
    limited data without re-training in the complete training set. Attention can significantly
    contribute to advances in the area and has been little explored. Ren et al. [[356](#bib.bib356)]
    were the first to introduce seminal work in the area. They use Attention Attractor
    Networks to regularize the learning of new classes. In each episode, a set of
    new weights is trained to recognize new classes until they converge. Attention
    Attractor Networks helps recognize new classes while remembering the classes beforehand
    without revising the original training set.
  prefs: []
  type: TYPE_NORMAL
- en: 6.8 Credit Assignment Problem (CAP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Reinforcement Learning (RL), an action that leads to a higher final cumulative
    reward should have more value. Therefore, more "credit" should be assigned to
    it than an action that leads to a lower final reward. However, measuring the individual
    contribution of actions to future rewards is not simple and has been studied by
    the RL community for years. There are at least three variations of the CAP problem
    that have been explored. The temporal CAP refers to identifying which actions
    were useful or useless in obtaining the final feedback. The structural CAP seeks
    to find the set of sensory situations in which a given sequence of actions will
    produce the same result. Transfer CAP refers to learning how to generalize a sequence
    of actions in tasks. Few works in the literature explore attention to the CAP
    problem. We believe that attention will be fundamental to advance credit assignment
    research. Recently, Ferret et al. [[564](#bib.bib564)] started the first research
    in the area by proposing a seminal work with attention to learn how to assign
    credit through a separate supervised problem and transfer credit assignment capabilities
    to new environments.
  prefs: []
  type: TYPE_NORMAL
- en: 6.9 Attention and Interpretability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are investigations to verify attention as an interpretability tool. Some
    recent studies suggest that attention can be considered reliable for this purpose.
    However, other researchers criticize the use of attention weights as an analytical
    tool. Jain and Wallace [[550](#bib.bib550)] proved that attention is not consistent
    with other explainability metrics and that it is easy to create distributions
    similar to those of the trained model but to produce a different result. Their
    conclusion is that changing attention weights does not significantly affect the
    model’s prediction, contrary to research by Rudin [[565](#bib.bib565)] and Riedl [[566](#bib.bib566)]
    (Section [5.7](#S5.SS7 "5.7 Interpretability ‣ 5 Applications ‣ Attention, please!
    A survey of Neural Attention Models in Deep Learning")). On the other hand, some
    studies have found how attention in neural models captures various notions of
    syntax and co-reference [[547](#bib.bib547)] [[549](#bib.bib549)] [[548](#bib.bib548)].
    Amid such confusion, Vashishth et al. [[551](#bib.bib551)] investigated attention
    more systematically. They attempted to justify the two types of observation (that
    is, when attention is interpretable and not), employing various experiments on
    various NLP tasks. The conclusion was that attention weights are interpretable
    and are correlated with metrics of the importance of features. However, this is
    only valid for cases where weights are essential for predicting models and cannot
    simply be reduced to a gating unit. Despite the existing studies, there are numerous
    research opportunities to develop systematic methodologies to analyze attention
    as an interpretability tool. The current conclusions are based on experiments
    with few architectures in a specific set of applications in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: 6.10 Unsupervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the last decade, unsupervised learning has also been recognized as one of
    the most critical challenges of machine learning since, in fact, human learning
    is mainly unsupervised [[558](#bib.bib558)]. Some works have recently successfully
    explored attention within purely unsupervised models. In GANs, attention has been
    used to improve the global perception of a model (i.e., the model learns which
    part of the image gives more attention to the others). SAGAN [[119](#bib.bib119)]
    was one of the pioneering efforts to incorporate self-attention in Convolutional
    Gans to improve the quality of the images generated. Image Transformer is an end-to-end
    attention network created to generate high-resolution images that significantly
    surpassed state-of-the-art in ImageNet in 2018\. AttGan [[567](#bib.bib567)] uses
    attention to easily take advantage of multimodality to improve the generation
    of images. Combining a region of the image with a corresponding part of the word-context
    vector helps to generate new features with more details in each stage.
  prefs: []
  type: TYPE_NORMAL
- en: Attention has still been little explored to make generative models simpler,
    scalable, and more stable. Perhaps the only approach in the literature to explore
    such aspects more deeply is DRAW [[36](#bib.bib36)], which presents a sequential
    and straightforward way to generate images, being possible to refine image patches
    while more information is captured sequentially. However, the architecture was
    tested only in simple datasets, leaving open spaces for new developments. There
    is not much exploration of attention using autoencoders. Using VAEs, Bornschein
    et al. [[163](#bib.bib163)] increased the generative models with external memory
    and used an attentional system to address and retrieve the corresponding memory
    content.
  prefs: []
  type: TYPE_NORMAL
- en: In Natural Language Processing, attention is explored in unsupervised models
    mainly to extract aspects of sentiment analysis. It is also used within autoencoders
    to generate semantic representations of phrases [[568](#bib.bib568)][[569](#bib.bib569)].
    However, most studies still use supervised learning attention, and few approaches
    still focus on computer vision and NLP. Therefore, we believe that there is still
    a great path for research and exploration of attention in the unsupervised context,
    particularly we note that the construction of purely bottom-up attentional systems
    is not explored in the literature and especially in the context of unsupervised
    learning, these systems can great value, accompanied by inhibition and return
    mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 6.11 New Tasks and Robotics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although attention has been used in several domains, there are still potential
    applications that can benefit from it. The prediction of time series, medical
    applications, and robotics applications are little-explored areas of the literature.
    Predicting time series becomes challenging as the size of the series increases.
    Attentional neural networks can contribute significantly to improving results.
    Specifically, we believe that exploring RAM [[46](#bib.bib46)] with multiple glimpses
    looking at different parts of the series or different frequency ranges can introduce
    a new way of computing time series. In medical applications, there are still few
    works that explore biomedical signals in attentional architectures. There are
    opportunities to apply attention to all applications, ranging from segmentation
    and image classification, support for disease diagnosis to support treatments
    such as Parkinson’s, Alzheimer’s, and other chronic diseases.
  prefs: []
  type: TYPE_NORMAL
- en: For robotics, there are countless opportunities. For years the robotics community
    has been striving for robots to perform tasks in a safe manner and with behaviors
    closer to humans. However, DL techniques need to cope well with multimodality,
    active learning, incremental learning, identify unknowns, uncertainty estimation,
    object and scene semantics, reasoning, awareness, and planning for this task.
    Architectures like RAM [[46](#bib.bib46)], DRAW [[36](#bib.bib36)] and Transformer [[37](#bib.bib37)]
    can contribute a lot by being applied to visual odometry, SLAM and mapping tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we presented a systematic review of the literature on attention
    in Deep Learning to overview the area from its main approaches, historical landmarks,
    uses of attention, applications, and research opportunities. In total, we critically
    analyzed more than 600 relevant papers published from 2014 to the present. To
    the best of our knowledge, this is the broadest survey in the literature, given
    that most of the existing reviews cover only particular domains with a slightly
    smaller number of reviewed works. Throughout the paper, we have identified and
    discussed the relationship between attention mechanisms in established deep neural
    network models, emphasizing CNNs, RNNs, and generative models. We discussed how
    attention led to performance gains, improvements in computational efficiency,
    and a better understanding of networks’ knowledge. We present an exhaustive list
    of application domains discussing the main benefits of attention, highlighting
    each domain’s most representative instances. We also showed recent discussions
    about attention on the explanation and interpretability of models, a branch of
    research that is widely discussed today. Finally, we present what we consider
    trends and opportunities for new developments around attentive models. We hope
    that this survey will help the audience understand the different existing research
    directions and provide significant scientific community background in generating
    future research.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that our survey results from an extensive and exhaustive
    process of searching, filtering, and critical analysis of papers published between
    01/01/2014 until 15/02/2021 in the central publication repositories for machine
    learning and related areas. In total, we collected more than 20,000 papers. After
    successive automatic and manual filtering, we selected approximately 650 papers
    for critical analysis and more than 6,000 for quantitative analyses, which correspond
    mainly to identifying the main application domains, places of publication, and
    main architectures. For automatic filtering, we use keywords from the area and
    set up different combinations of filters to eliminate noise from psychology and
    classic computational visual attention techniques (i.e., saliency maps). In manual
    filtering, we separate the papers by year and define the originality and number
    of citations of the work as the main selection criteria. In the appendix, we provide
    our complete methodology and links to our search codes to facilitate improving
    future revisions on any topic in the area.
  prefs: []
  type: TYPE_NORMAL
- en: We are currently complementing this survey with a theoretical analysis of the
    main neural attention models. This complementary survey will help to address an
    urgent need for an attentional framework supported by taxonomies based on theoretical
    aspects of attention, which predate the era of Deep Learning. The few existing
    taxonomies in the area do not yet use theoretical concepts and are challenging
    to extend to various architectures and application domains. Taxonomies inspired
    by classical concepts are essential to understand how attention has acted in deep
    neural networks and whether the roles played corroborate with theoretical foundations
    studied for more than 40 years in psychology and neuroscience. This study is already
    in the final stages of development by our team and will hopefully help researchers
    develop new attentional structures with functions still little explored in the
    literature. We hope to make it available to the scientific community as soon as
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This survey employs a systematic review (SR) approach aiming to collect, critically
    evaluate, and synthesize the results of multiple primary studies concerning Attention
    in Deep Learning. The selection and evaluation of the works should be meticulous
    and easily reproducible. Also, SR should be objective, systematic, transparent,
    and replicable. Although recent, the use of attention in Deep Learning is extensive.
    Therefore, we systematically reviewed the literature, collecting works from a
    variety of sources. SR consists of the following steps: defining the scientific
    questions, identifying the databases, establishing the criteria for selecting
    papers, searching the databases, performing a critical analysis to choose the
    most relevant works, and preparing a critical summary of the most relevant papers,
    as shown Figure [23](#Sx1.F23 "Figure 23 ‣ Appendix ‣ Attention, please! A survey
    of Neural Attention Models in Deep Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d6e8e24a057497c03af4fcbca0040ff1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: Steps of the systematic review used in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This survey covers the following aspects: 1) The uses of attention in Deep
    Learning; 2) Attention mechanisms; 3) Uses of attention; 4) Attention applications;
    5) Attention and interpretability; 6) Trends and challenges. These aspects provide
    the main topics regarding attention in Deep Learning, which can help understand
    the field’s fundamentals. The second step identifies the main databases in the
    machine learning area, such as arXiv, DeepMind, Google AI, OpenAI, Facebook AI
    research, Microsoft research, Amazon research, Google Scholar, IEEE Xplore, DBLP,
    ACM, NIPS, ICML, ICLR, AAAI, CVPR, ICCV, CoRR, IJCNN, Neurocomputing, and Google
    general search (including blogs, distill, and Quora). Our searching period comprises
    01/01/2014 to 06/30/2019 (first stage) and 07/01/2019 to 02/15/2021 (second stage),
    and the search was performed via a Phyton script ²²2https://github.com/larocs/attention_dl.
    The papers’ title, abstract, year, DOI, and source publication were downloaded
    and stored in a JSON file. The most appropriate set of keywords to perform the
    searches was defined by partially searching the field with expert knowledge from
    our research group. The final set of keywords wore: attention, attentional, and
    attentive.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, these keywords are also relevant in psychology and visual attention.
    Hence, we performed a second selection to eliminate these papers and remmove duplicate
    papers unrelated to the DL field. After removing duplicates, 18,257 different
    papers remained. In the next selection step, we performed a sequential combination
    of three types of filters: 1) Filter I: Selecting the works with general terms
    of attention (i.e., attention, attentive, attentional, saliency, top-down, bottom-up,
    memory, focus, and mechanism); 2) Filter II: Selecting the works with terms related
    to DL (i.e. deep learning, neural network, ann, dnn deep neural, encoder, decoder,
    recurrent neural network, recurrent network, rnn, long short term memory, long
    short-term memory, lstm, gated recurrent unit, gru, autoencoder, ae, variational
    autoencoder, vae, denoising ae, dae, sparse ae, sae, markov chain, mc, hopfield
    network, boltzmann machine, em, restricted boltzmann machine, rbm, deep belief
    network, dbn, deep convolutional network, dcn, deconvolution network, dn, deep
    convolutional inverse graphics network, dcign, generative adversarial network,
    gan, liquid state machine, lsm, extreme learnng, machine, elm, echo state network,
    esn, deep residual network, drn, konohen network, kn, turing machine, ntm, convolutional
    network, cnn, and capsule network); 3) Filter III: Selecting the works with specific
    words of attention in Deep Learning (i.e., attention network, soft attention,
    hard attention, self-attention, self attention deep attention, hierarchical attention,
    transformer, local attention, global attention, coattention, co-attention, flow
    attention, attention-over-attention, way attention, intra-attention, self-attentive,
    and self attentive).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6ec84a129e9597cb685d2cc5f9ecdddb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: The filter strategies for selecting the relevant works. Search I
    corresponds to articles collected between 01/01/2014 to 06/30/2019 (first stage),
    and Search II corresponds to papers collected between 07/01/2019 to 02/15/2021
    (second stage).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision tree with the second selection is shown in Figure [24](#Sx1.F24
    "Figure 24 ‣ Appendix ‣ Attention, please! A survey of Neural Attention Models
    in Deep Learning"). The third filtering selects works with at least one specific
    term of attention in deep learning. In the next filtering, we remove papers without
    abstract, the collection of filters verify if there is at least one specific term
    of Deep Learning and remove the works with the following keywords: visual attention,
    saliency, and eye-tracking. For the papers with abstract, the selection is more
    complex, requiring three cascade conditions: 1) First condition: Selecting the
    works that have more than five filter terms from filter II; 2) Second condition:
    selecting the works that have between three and five terms from filter II and
    where there is at least one of the following: attention model, attention mechanism,
    attention, or attentive; 3) Third condition: Selecting the works with one or two
    terms from filter II; without the terms: salience, visual attention, attentive,
    and attentional mechanism. A total of 6,338 works remained for manual selection.
    We manually excluded the papers without a title, abstract, or introduction related
    to the DL field. After manual selection, 3,567 works were stored in Zotero. Given
    the number of papers, we grouped them by year and chose those above a threshold
    (average citations in the group). Only works above average were read and classified
    as relevant or not for critical analysis. To find the number of citations, we
    automated the process with a Python script. 650 papers were considered relevant
    for this survey’s critical analysis, and 6,567 were used to perform quantitative
    analyzes.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Esther Luna Colombini, A da Silva Simoes, and CHC Ribeiro. An attentional
    model for intelligent robotics agents. PhD thesis, Instituto Tecnológico de Aeronáutica,
    São José dos Campos, Brazil, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Marvin M Chun, Julie D Golomb, and Nicholas B Turk-Browne. A taxonomy of
    external and internal attention. Annual review of psychology, 62:73–101, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Roger BH Tootell, Nouchine Hadjikhani, E Kevin Hall, Sean Marrett, Wim
    Vanduffel, J Thomas Vaughan, and Anders M Dale. The retinotopy of visual spatial
    attention. Neuron, 21(6):1409–1422, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Marty G Woldorff, Christopher C Gallen, Scott A Hampson, Steven A Hillyard,
    Christo Pantev, David Sobel, and Floyd E Bloom. Modulation of early sensory processing
    in human auditory cortex during auditory selective attention. Proceedings of the
    National Academy of Sciences, 90(18):8722–8726, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Heidi Johansen-Berg and Donna M Lloyd. The physiology and psychology of
    selective attention to touch. Front Biosci, 5:D894–D904, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Christina Zelano, Moustafa Bensafi, Jess Porter, Joel Mainland, Brad Johnson,
    Elizabeth Bremner, Christina Telles, Rehan Khan, and Noam Sobel. Attentional modulation
    in human primary olfactory cortex. Nature neuroscience, 8(1):114–120, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Maria G Veldhuizen, Genevieve Bender, R Todd Constable, and Dana M Small.
    Trying to detect taste in a tasteless solution: modulation of early gustatory
    cortex by attention to taste. Chemical Senses, 32(6):569–581, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] William James. The Principles of Psychology. Dover Publications, 1890.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Simone Frintrop, Erich Rome, and Henrik Christensen. Computational visual
    attention systems and their cognitive foundations: A survey. ACM Transactions
    on Applied Perception, 7, 01 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Anne M Treisman and Garry Gelade. A feature-integration theory of attention.
    Cognitive psychology, 12(1):97–136, 1980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Jeremy M Wolfe, Kyle R Cave, and Susan L Franzel. Guided search: an alternative
    to the feature integration model for visual search. Journal of Experimental Psychology:
    Human perception and performance, 15(3):419, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Ronald A Rensink. The dynamic representation of scenes. Visual cognition,
    7(1-3):17–42, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Donald Eric Broadbent. Perception and communication. Elsevier, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Donald A Norman. Toward a theory of memory and attention. Psychological
    review, 75(6):522, 1968.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Daniel Kahneman. Attention and effort, volume 1063. Citeseer, 1973.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Frank Van der Velde, Marc de Kamps, et al. Clam: Closed-loop attention
    model for visual search. Neurocomputing, 58:607–612, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] R Hans Phaf, AHC Van der Heijden, and Patrick TW Hudson. Slam: A connectionist
    model for attention in visual selection tasks. Cognitive psychology, 22(3):273–341,
    1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Simone Frintrop, Erich Rome, and Henrik I Christensen. Computational visual
    attention systems and their cognitive foundations: A survey. ACM Transactions
    on Applied Perception (TAP), 7(1):1–39, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Christof Koch and Shimon Ullman. Shifts in selective visual attention:
    towards the underlying neural circuitry. In Matters of intelligence, pages 115–141\.
    Springer, 1987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based
    visual attention for rapid scene analysis. IEEE PAMI, 20(11):1254–1259, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Vidhya Navalpakkam and Laurent Itti. An integrated model of top-down and
    bottom-up attention for optimizing detection speed. In 2006 IEEE CVPR), volume 2,
    pages 2049–2056\. IEEE, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Fred H Hamker. The emergence of attention by population-based inference
    and its role in distributed processing and cognitive control of vision. Computer
    Vision and Image Understanding, 100(1-2):64–106, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Fred H Hamker. Modeling feature-based attention as an active top-down
    inference process. BioSystems, 86(1-3):91–99, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Simone Frintrop. VOCUS: A visual attention system for object detection
    and goal-directed search, volume 3899. Springer, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Albert Ali Salah, Ethem Alpaydin, and Lale Akarun. A selective attention-based
    method for visual pattern recognition with application to handwritten digit recognition
    and face recognition. IEEE PAMI, 24(3):420–425, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Nabil Ouerhani. Visual attention: from bio-inspired modeling to real-time
    implementation. PhD thesis, Université de Neuchâtel, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Dirk Walther. Interactions of visual attention and object recognition:
    computational modeling, algorithms, and psychophysics. PhD thesis, California
    Institute of Technology, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Dirk Walther, Duane R Edgington, and Christof Koch. Detection and tracking
    of objects in underwater video. In Proc. of the IEEE CVPR, volume 1, pages I–I.
    IEEE, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] James J Clark and Nicola J Ferrier. Modal control of an attentive vision
    system. In IEEE ICCV, pages 514–523\. IEEE, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Cynthia Breazeal and Brian Scassellati. A context-dependent attention
    system for a social robot. rn, 255:3, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] A Rotenstein, Alexander Andreopoulos, Ehzan Fazl, David Jacob, Matt Robinson,
    Ksenia Shubina, Yuliang Zhu, and J Tsotsos. Towards the dream of intelligent,
    visually-guided wheelchairs. In Proc. 2nd Int’l Conf. on Technology and Aging,
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] James J Clark and Nicola J Ferrier. Attentive visual servoing. In Active
    vision. Citeseer, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Simone Frintrop and Patric Jensfelt. Attentional landmarks and active
    gaze control for visual slam. IEEE Transactions on Robotics, 24(5):1054–1065,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Christian Scheier and Steffen Egner. Visual attention in a mobile robot.
    In ISIE’97 Proceeding of the IEEE International Symposium on Industrial Electronics,
    volume 1, pages SS48–SS52\. IEEE, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Shumeet Baluja and Dean A Pomerleau. Expectation-based selective attention
    for visual monitoring and control of a robot vehicle. Robotics and autonomous
    systems, 22(3-4):329–344, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and
    Daan Wierstra. Draw: A recurrent neural network for image generation. arXiv preprint
    arXiv:1502.04623, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    arXiv:1706.03762 [cs], June 2017. arXiv: 1706.03762.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv
    preprint arXiv:1410.3916, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Feng Wang and David MJ Tax. Survey on the attention based rnn model and
    its applications in computer vision. arXiv preprint arXiv:1601.06823, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Dichao Hu. An introductory survey on attention mechanisms in nlp problems.
    In Proceedings of SAI Intelligent Systems Conference, pages 432–448\. Springer,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Andrea Galassi, Marco Lippi, and Paolo Torroni. Attention in natural language
    processing. IEEE Transactions on Neural Networks and Learning Systems, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] John Boaz Lee, Ryan A. Rossi, Sungchul Kim, Nesreen K. Ahmed, and Eunyee
    Koh. Attention models in graphs: A survey. arXiv:1807.07984 [cs], July 2018. arXiv:
    1807.07984.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Sneha Chaudhari, Gungor Polatkan, Rohan Ramanath, and Varun Mithal. An
    attentive survey of attention models. arXiv preprint arXiv:1904.02874, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation
    by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
    Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image
    caption generation with visual attention. In International Conference on Machine
    Learning, pages 2048–2057, June 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. Recurrent
    models of visual attention. arXiv preprint arXiv:1406.6247, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio.
    On the properties of neural machine translation: Encoder-decoder approaches. arXiv
    preprint arXiv:1409.1259, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals. Listen, attend
    and spell. arXiv preprint arXiv:1508.01211, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever,
    and Geoffrey Hinton. Grammar as a foreign language. In Advances in neural information
    processing systems, pages 2773–2781, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Oriol Vinyals and Quoc Le. A neural conversational model. arXiv preprint
    arXiv:1506.05869, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi.
    Bidirectional attention flow for machine comprehension. arXiv:1611.01603 [cs],
    November 2016. arXiv: 1611.01603.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard
    Hovy. Hierarchical attention networks for document classification. In Proceedings
    of the 2016 conference of the North American chapter of the association for computational
    linguistics: human language technologies, pages 1480–1489, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic coattention networks
    for question answering. arXiv preprint arXiv:1611.01604, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks.
    In Advances in neural information processing systems, pages 2692–2700, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Abigail See, Peter J Liu, and Christopher D Manning. Get to the point:
    Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and Weizhu Chen. Fusionnet:
    Fusing via fully-aware attention with application to machine comprehension. arXiv
    preprint arXiv:1711.07341, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiskỳ,
    and Phil Blunsom. Reasoning about entailment with neural attention. arXiv preprint
    arXiv:1509.06664, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches
    to attention-based neural machine translation. In Proceedings of the 2015 Conference
    on Empirical Methods in Natural Language Processing, pages 1412–1421, Lisbon,
    Portugal, 2015\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and koray kavukcuoglu.
    Spatial transformer networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,
    and R. Garnett, editors, Advances in Neural Information Processing Systems 28,
    pages 2017–2025\. Curran Associates, Inc., 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Dhanesh Ramachandram and Graham W Taylor. Deep multimodal learning: A
    survey on recent advances and trends. IEEE Signal Processing Magazine, 34(6):96–108,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Jing Gao, Peng Li, Zhikui Chen, and Jianing Zhang. A survey on deep learning
    for multimodal data fusion. Neural Computation, 32(5):829–864, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal,
    Hugo Larochelle, and Aaron Courville. Describing videos by exploiting temporal
    structure. In Proceedings of the IEEE international conference on computer vision,
    pages 4507–4515, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Chunlei Wu, Yiwei Wei, Xiaoliang Chu, Sun Weichen, Fei Su, and Leiquan
    Wang. Hierarchical attention-based multimodal fusion for video captioning. Neurocomputing,
    315:362–370, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Rasool Fakoor, Abdel-rahman Mohamed, Margaret Mitchell, Sing Bing Kang,
    and Pushmeet Kohli. Memory-augmented attention modelling for videos. arXiv preprint
    arXiv:1611.02261, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Jiang Tian, Cong Li, Zhongchao Shi, and Feiyu Xu. A diagnostic report
    generator from ct volumes on liver tumor with semi-supervised attention mechanism.
    In International Conference on Medical Image Computing and Computer-Assisted Intervention,
    pages 702–710\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Yunchen Pu, Martin Renqiang Min, Zhe Gan, and Lawrence Carin. Adaptive
    feature abstraction for translating video to text. In Thirty-Second AAAI Conference
    on Artificial Intelligence, April 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, and Alex C Kot. Global context-aware
    attention lstm networks for 3d action recognition. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 1647–1656, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Liang Zhang, Guangming Zhu, Lin Mei, Peiyi Shen, Syed Afaq Ali Shah, and
    Mohammed Bennamoun. Attention in convolutional lstm for gesture recognition. In
    Advances in Neural Information Processing Systems, pages 1953–1962, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Pengfei Zhang, Jianru Xue, Cuiling Lan, Wenjun Zeng, Zhanning Gao, and
    Nanning Zheng. Adding attentiveness to the neurons in recurrent neural networks.
    In Proceedings of the European Conference on Computer Vision (ECCV), pages 135–151,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Xiangrong Zhang, Xin Wang, Xu Tang, Huiyu Zhou, and Chen Li. Description
    generation for remote sensing images using attribute attention mechanism. Remote
    Sensing, 11(6):612, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Bei Fang, Ying Li, Haokui Zhang, and Jonathan Cheung-Wai Chan. Hyperspectral
    images classification based on dense convolutional networks with spectral-wise
    attention mechanism. Remote Sensing, 11(2):159, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Qi Wang, Shaoteng Liu, Jocelyn Chanussot, and Xuelong Li. Scene classification
    with recurrent attention of vhr remote sensing images. IEEE Transactions on Geoscience
    and Remote Sensing, 57(2):1155–1167, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Xiaoguang Mei, Erting Pan, Yong Ma, Xiaobing Dai, Jun Huang, Fan Fan,
    Qinglei Du, Hong Zheng, and Jiayi Ma. Spectral-spatial attention networks for
    hyperspectral image classification. Remote Sensing, 11(8):963, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Kazuhiro Sumi, John R. Hershey,
    and Tim K. Marks. Attention-based multimodal fusion for video description. arXiv:1701.03126
    [cs], January 2017. arXiv: 1701.03126.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Yuanyuan Zhang, Zi-Rui Wang, and Jun Du. Deep fusion: An attention guided
    factorized bilinear pooling for audio-video emotion recognition. In 2019 International
    Joint Conference on Neural Networks (IJCNN), pages 1–8\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria,
    and Louis-Philippe Morency. Memory fusion network for multi-view sequential learning.
    In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria,
    and Louis-Philippe Morency. Multi-attention recurrent network for human communication
    comprehension. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski,
    Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap.
    Relational recurrent neural networks. arXiv preprint arXiv:1806.01822, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Zheng Zhang, Lizi Liao, Minlie Huang, Xiaoyan Zhu, and Tat-Seng Chua.
    Neural multimodal belief tracker with adaptive attention for dialogue systems.
    In The World Wide Web Conference, pages 2401–2412, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual attention networks
    for multimodal reasoning and matching. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 299–307, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Feiran Huang, Xiaoming Zhang, Zhonghua Zhao, and Zhoujun Li. Bi-directional
    spatial-semantic attention networks for image-text matching. IEEE Transactions
    on Image Processing, 28(4):2008–2020, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei. Pointing
    novel objects in image captioning. In Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pages 12497–12506, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Danielle Matthews, Tanya Behne, Elena Lieven, and Michael Tomasello. Origins
    of the human pointing gesture: a training study. Developmental science, 15(6):817–829,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, and Hongsheng Li. Improving
    referring expression grounding with cross-modal attention-guided erasing. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1950–1959,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Pooya Abolghasemi, Amir Mazaheri, Mubarak Shah, and Ladislau Boloni. Pay
    attention!-robustifying a deep visuomotor policy through task-focused visual attention.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 4254–4262, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv:1410.5401
    [cs], October 2014. arXiv: 1410.5401.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Amir Hosein Khasahmadi, Kaveh Hassani, Parsa Moradi, Leo Lee, and Quaid
    Morris. Memory-based graph networks. arXiv preprint arXiv:2002.09518, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks
    for visual and textual question answering. arXiv:1603.01417 [cs], March 2016.
    arXiv: 1603.01417.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Xinkai Lu, Wenguan Wang, Martin Danelljan, Tianfei Zhou, Jianbing Shen,
    and Luc Van Gool. Video object segmentation with episodic graph memory networks.
    arXiv preprint arXiv:2007.07020, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Abrar H Abdulnabi, Bing Shuai, Stefan Winkler, and Gang Wang. Episodic
    camn: Contextual attention-based memory networks with iterative feedback for scene
    labeling. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 5561–5570, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory
    networks. In Advances in neural information processing systems, pages 2440–2448,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object
    segmentation using space-time memory networks. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 9226–9235, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury,
    Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything:
    Dynamic memory networks for natural language processing. In International conference
    on machine learning, pages 1378–1387\. PMLR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
    S Yu Philip. A comprehensive survey on graph neural networks. IEEE Transactions
    on Neural Networks and Learning Systems, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine
    Bordes, and Jason Weston. Key-value memory networks for directly reading documents.
    arXiv preprint arXiv:1606.03126, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Seungwhan Moon, Pararth Shah, Anuj Kumar, and Rajen Subba. Memory graph
    networks for explainable memory-grounded question answering. In Proceedings of
    the 23rd Conference on Computational Natural Language Learning (CoNLL), pages
    728–736, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
    Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S
    Yu. Heterogeneous graph attention network. In The World Wide Web Conference, pages
    2022–2032, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and Jie Shan. Graph
    attention convolution for point cloud semantic segmentation. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pages 10296–10305,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alexander A Alemi.
    Watch your step: Learning node embeddings via graph attention. In Advances in
    Neural Information Processing Systems, pages 9180–9190, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Linjie Li, Zhe Gan, Yu Cheng, and Jingjing Liu. Relation-aware graph
    attention network for visual question answering. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 10313–10322, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Karim Ahmed, Nitish Shirish Keskar, and Richard Socher. Weighted transformer
    network for machine translation. arXiv preprint arXiv:1711.02132, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and
    Zheng Zhang. Star-transformer. arXiv preprint arXiv:1902.09113, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer,
    Ian Simon, Curtis Hawthorne, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu,
    and Douglas Eck. Music transformer. arXiv preprint arXiv:1809.04281, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating
    long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R Kosiorek, Seungjin Choi, and
    Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant
    neural networks. arXiv preprint arXiv:1810.00825, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Hasan Sait Arslan, Mark Fishel, and Gholamreza Anbarjafari. Doubly attentive
    transformer machine translation. arXiv preprint arXiv:1807.11605, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Jindřich Libovickỳ, Jindřich Helcl, and David Mareček. Input combination
    strategies for multi-source transformer decoder. arXiv preprint arXiv:1811.04716,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Ning Dai, Jianze Liang, Xipeng Qiu, and Xuanjing Huang. Style transformer:
    Unpaired text style transfer without disentangled latent representation. arXiv
    preprint arXiv:1905.05621, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Yang Liu and Mirella Lapata. Hierarchical transformers for multi-document
    summarization. arXiv preprint arXiv:1905.13164, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Ting-Rui Chiang, Chao-Wei Huang, Shang-Yu Su, and Yun-Nung Chen. Learning
    multi-level information for dialogue response selection by highway recurrent transformer.
    arXiv preprint arXiv:1903.08953, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Fengshun Xiao, Jiangtong Li, Hai Zhao, Rui Wang, and Kehai Chen. Lattice-based
    transformer encoder for neural machine translation. arXiv preprint arXiv:1906.01282,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural
    speech synthesis with transformer network. In Proceedings of the AAAI Conference
    on Artificial Intelligence, pages 6706–6713, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Phi Xuan Nguyen and Shafiq Joty. Phrase-based attentions. arXiv preprint
    arXiv:1810.03444, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805
    [cs], October 2018. arXiv: 1810.04805.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and
    Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog,
    1(8):9, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,
    Alexander Ku, and Dustin Tran. Image transformer. In International Conference
    on Machine Learning, pages 4055–4064\. PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention
    generative adversarial networks. In International conference on machine learning,
    pages 7354–7363\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal,
    David Luan, and Ilya Sutskever. Generative pretraining from pixels. In Proceedings
    of the 37th International Conference on Machine Learning, volume 1, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, and Scott Gray. Dall·e: Creating
    images from text, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan
    Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam
    Santoro, et al. Hyperbolic attention networks. arXiv preprint arXiv:1805.09786,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Yiding Zhang, Xiao Wang, Xunqiang Jiang, Chuan Shi, and Yanfang Ye. Hyperbolic
    graph attention network. arXiv preprint arXiv:1912.03046, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
    Monti, and Michael Bronstein. Temporal graph networks for deep learning on dynamic
    graphs. arXiv preprint arXiv:2006.10637, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Sarthak Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan,
    Guillaume Lajoie, Michael Mozer, and Yoshua Bengio. Learning to combine top-down
    and bottom-up signals in recurrent neural networks with attention over modules.
    In International Conference on Machine Learning, pages 6972–6986\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Alex Graves. Adaptive computation time for recurrent neural networks.
    arXiv:1603.08983 [cs], March 2016. arXiv: 1603.08983.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang,
    Dmitry Vetrov, and Ruslan Salakhutdinov. Spatially adaptive computation time for
    residual networks. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 1039–1048, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Cristobal Eyzaguirre and Alvaro Soto. Differentiable adaptive computation
    time for visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pages 12817–12825, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Dongbin Zhao, Yaran Chen, and Le Lv. Deep reinforcement learning with
    visual attention for vehicle classification. IEEE Transactions on Cognitive and
    Developmental Systems, 9(4):356–367, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Xuanhan Wang, Lianli Gao, Jingkuan Song, and Hengtao Shen. Beyond frame-level
    cnn: saliency-aware 3-d cnn with lstm for video action recognition. IEEE Signal
    Processing Letters, 24(4):510–514, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Ning Liu, Yongchao Long, Changqing Zou, Qun Niu, Li Pan, and Hefeng Wu.
    Adcrowdnet: An attention-injective deformable convolutional network for crowd
    understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pages 3225–3234, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. Squeeze-and-excitation
    networks. arXiv:1709.01507 [cs], September 2017. arXiv: 1709.01507.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and Yun Fu. Residual non-local
    attention networks for image restoration. arXiv preprint arXiv:1903.10082, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional
    block attention module. In Proceedings of the European conference on computer
    vision (ECCV), pages 3–19, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and Jiashi
    Feng. A^2-nets: Double attention networks. In S. Bengio, H. Wallach, H. Larochelle,
    K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information
    Processing Systems 31, pages 352–361\. Curran Associates, Inc., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, and Hironobu Fujiyoshi.
    Attention branch network: Learning of attention mechanism for visual explanation.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 10705–10714, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Zhizhong Han, Xiyang Wang, Chi-Man Vong, Yu-Shen Liu, Matthias Zwicker,
    and CL Chen. 3dviewgraph: Learning global features for 3d shapes from a graph
    of unordered views with attention. arXiv preprint arXiv:1905.07503, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao. Distant supervision for
    relation extraction with sentence-level attention and entity descriptions. In
    Proceedings of the AAAI Conference on Artificial Intelligence, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Jiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen, Fang Wen, Hongdong
    Li, and Gang Hua. Neural aggregation network for video face recognition. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 4362–4371,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Timo Hackel, Mikhail Usvyatsov, Silvano Galliani, Jan D. Wegner, and
    Konrad Schindler. Inference, learning and attention mechanisms that exploit and
    preserve sparsity in convolutional networks. arXiv:1801.10585 [cs], January 2018.
    arXiv: 1801.10585.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple
    neural attentive meta-learner. arXiv:1707.03141 [cs, stat], July 2017. arXiv:
    1707.03141.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see better: Recurrent
    attention convolutional neural network for fine-grained image recognition. In
    2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
    4476–4484, Honolulu, HI, July 2017\. IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Xueying Chen, Rong Zhang, and Pingkun Yan. Feature fusion encoder decoder
    network for automatic liver lesion segmentation. In 2019 IEEE 16th international
    symposium on biomedical imaging (ISBI 2019), pages 430–433\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Wanxin Tian, Zixuan Wang, Haifeng Shen, Weihong Deng, Yiping Meng, Binghui
    Chen, Xiubao Zhang, Yuan Zhao, and Xiehe Huang. Learning better features for face
    detection with feature fusion and segmentation supervision. arXiv preprint arXiv:1811.08557,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Xingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao, Liping Liu, Zeyu
    Chen, and Jun Huan. Delta: Deep learning transfer using feature map with attention
    for convolutional networks. arXiv preprint arXiv:1901.09229, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Heliang Zheng, Jianlong Fu, Tao Mei, and Jiebo Luo. Learning multi-attention
    convolutional neural network for fine-grained image recognition. In Proceedings
    of the IEEE international conference on computer vision, pages 5209–5217, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Pau Rodríguez, Guillem Cucurull, Jordi Gonzàlez, Josep M Gonfaus, and
    Xavier Roca. A painless attention mechanism for convolutional neural networks.
    ICLR 2018, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Linlin Wang, Zhu Cao, Gerard De Melo, and Zhiyuan Liu. Relation classification
    via multi-level attention cnns. In Proceedings of the 54th Annual Meeting of the
    Association for Computational Linguistics (Volume 1: Long Papers), pages 1298–1307,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Wenpeng Yin, Hinrich Schütze, Bing Xiang, and Bowen Zhou. Abcnn: Attention-based
    convolutional neural network for modeling sentence pairs. Transactions of the
    Association for Computational Linguistics, 4:259–272, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Marcus Edel and Joscha Lausch. Capacity visual attention networks. In
    GCAI, pages 72–80, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Yao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison
    Cottrell. A dual-stage attention-based recurrent neural network for time series
    prediction. arXiv preprint arXiv:1704.02971, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Yuxuan Liang, Songyu Ke, Junbo Zhang, Xiuwen Yi, and Yu Zheng. Geoman:
    Multi-level attention networks for geo-sensory time series prediction. In IJCAI,
    pages 3428–3434, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Wenbin Du, Yali Wang, and Yu Qiao. Rpan: An end-to-end recurrent pose-attention
    network for action recognition in videos. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 3725–3734, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Kun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Michael Witbrock, and
    Vadim Sheinin. Graph2seq: Graph to sequence learning with attention-based neural
    networks. arXiv preprint arXiv:1804.00823, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition
    with visual attention. arXiv:1412.7755 [cs], December 2014. arXiv: 1412.7755.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks
    for machine reading. arXiv preprint arXiv:1601.06733, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Michael C
    Mozer, Chris Pal, and Yoshua Bengio. Sparse attentive backtracking: Temporal creditassignment
    through reminding. arXiv preprint arXiv:1809.03702, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Dilruk Perera and Roger Zimmermann. Lstm networks for online cross-network
    recommendations. arXiv preprint arXiv:2008.10849, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra,
    Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene
    decomposition and representation. arXiv preprint arXiv:1901.11390, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Chongxuan Li, Jun Zhu, and Bo Zhang. Learning to generate with memory.
    In International Conference on Machine Learning, pages 1177–1186, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Sergey Bartunov and Dmitry P Vetrov. Fast adaptation in generative models
    with generative matching networks. arXiv preprint arXiv:1612.02192, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor,
    and Daan Wierstra. One-shot generalization in deep generative models. arXiv preprint
    arXiv:1603.05106, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Jörg Bornschein, Andriy Mnih, Daniel Zoran, and Danilo J Rezende. Variational
    memory addressing in generative models. arXiv preprint arXiv:1709.07116, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Carlos Escolano, Marta R Costa-jussà, and José AR Fonollosa. (self-attentive)
    autoencoder-based universal language representation for machine translation. arXiv
    preprint arXiv:1810.06351, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and
    Łukasz Kaiser. Universal transformers. arXiv:1807.03819 [cs, stat], July 2018.
    arXiv: 1807.03819.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with
    stack-augmented recurrent nets. arXiv preprint arXiv:1503.01007, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu,
    and Daan Wierstra. Matching networks for one shot learning. arXiv:1606.04080 [cs,
    stat], June 2016. arXiv: 1606.04080.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and
    Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length
    context. arXiv:1901.02860 [cs, stat], January 2019. arXiv: 1901.02860.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin.
    Adaptive attention span in transformers. arXiv preprint arXiv:1905.07799, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Alexei Baevski and Michael Auli. Adaptive input representations for neural
    language modeling. arXiv preprint arXiv:1809.10853, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Michał Daniluk, Tim Rocktäschel, Johannes Welbl, and Sebastian Riedel.
    Frustratingly short attention spans in neural language modeling. arXiv preprint
    arXiv:1702.04521, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured
    attention networks. arXiv:1702.00887 [cs], February 2017. arXiv: 1702.00887.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Jiangtao Feng, Lingpeng Kong, Po-Sen Huang, Chong Wang, Da Huang, Jiayuan
    Mao, Kan Qiao, and Dengyong Zhou. Neural phrase-to-phrase machine translation.
    arXiv:1811.02172 [cs, stat], November 2018. arXiv: 1811.02172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with
    relative position representations, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Colin Raffel, Douglas Eck, Peter Liu, Ron J. Weiss, and Thang Luong.
    Online and linear-time attention by enforcing monotonic alignments, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau,
    Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations
    using rnn encoder-decoder for statistical machine translation. arXiv:1406.1078
    [cs, stat], June 2014. arXiv: 1406.1078.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation
    of rare words with subword units. In Proceedings of the 54th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725,
    Berlin, Germany, August 2016\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Thomas Zenkel, Joern Wuebker, and John DeNero. Adding interpretable attention
    to neural translation models improves word alignment. arXiv preprint arXiv:1901.11359,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Baosong Yang, Jian Li, Derek F Wong, Lidia S Chao, Xing Wang, and Zhaopeng
    Tu. Context-aware self-attention networks. In Proceedings of the AAAI Conference
    on Artificial Intelligence, pages 387–394, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Baosong Yang, Longyue Wang, Derek Wong, Lidia S Chao, and Zhaopeng Tu.
    Convolutional self-attention networks. arXiv preprint arXiv:1904.03107, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng
    Tu. Modeling recurrence for transformer. arXiv preprint arXiv:1904.03092, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov,
    Ann Clifton, and Matt Post. Sockeye: A toolkit for neural machine translation.
    arXiv preprint arXiv:1712.05690, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Xiangwen Zhang, Jinsong Su, Yue Qin, Yang Liu, Rongrong Ji, and Hongji
    Wang. Asynchronous bidirectional decoding for neural machine translation. In Proceedings
    of the AAAI Conference on Artificial Intelligence, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil
    Sima’an. Graph convolutional encoders for syntax-aware neural machine translation.
    arXiv preprint arXiv:1704.04675, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent
    models with fast-forward connections for neural machine translation. Transactions
    of the Association for Computational Linguistics, 4:371–383, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,
    Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s
    neural machine translation system: Bridging the gap between human and machine
    translation. arXiv preprint arXiv:1609.08144, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. Tree-to-sequence
    attentional neural machine translation. In Proceedings of the 54th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers), pages
    823–833, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Jian Li, Baosong Yang, Zi-Yi Dou, Xing Wang, Michael R Lyu, and Zhaopeng
    Tu. Information aggregation for multi-head attention with routing-by-agreement.
    arXiv preprint arXiv:1904.03100, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way, multilingual
    neural machine translation with a shared attention mechanism. arXiv preprint arXiv:1601.01073,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander Rush. Latent
    alignment and variational attention. In S. Bengio, H. Wallach, H. Larochelle,
    K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information
    Processing Systems 31, pages 9712–9724\. Curran Associates, Inc., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] Haijun Zhang, Jingxuan Li, Yuzhu Ji, and Heng Yue. Understanding subtitles
    by character-level sequence-to-sequence learning. IEEE Transactions on Industrial
    Informatics, 13(2):616–624, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.
    A decomposable attention model for natural language inference. arXiv:1606.01933
    [cs], June 2016. arXiv: 1606.01933.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and Chengqi
    Zhang. Disan: Directional self-attention network for rnn/cnn-free language understanding.
    In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. Bi-directional
    block self-attention for fast and memory-efficient sequence modeling. arXiv preprint
    arXiv:1804.00857, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Shuohang Wang and Jing Jiang. Learning natural language inference with
    lstm. arXiv preprint arXiv:1512.08849, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. Learning natural
    language inference using bidirectional lstm model and inner-attention. arXiv preprint
    arXiv:1605.09090, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever. Neural programmer:
    Inducing latent programs with gradient descent. arXiv:1511.04834 [cs, stat], November
    2015. arXiv: 1511.04834.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Chen Xing, Yu Wu, Wei Wu, Yalou Huang, and Ming Zhou. Hierarchical recurrent
    attention network for response generation. In Proceedings of the AAAI Conference
    on Artificial Intelligence, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Liu Yang, Qingyao Ai, Jiafeng Guo, and W. Bruce Croft. anmm: Ranking
    short answer texts with attention-based neural matching model. arXiv:1801.01641
    [cs], January 2018. arXiv: 1801.01641.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Yi Tay, Anh Tuan Luu, Siu Cheung Hui, and Jian Su. Densely connected
    attention propagation for reading comprehension. In S. Bengio, H. Wallach, H. Larochelle,
    K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information
    Processing Systems 31, pages 4906–4917\. Curran Associates, Inc., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad
    Norouzi, and Quoc V Le. Qanet: Combining local convolution with global self-attention
    for reading comprehension. arXiv preprint arXiv:1804.09541, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] Wei Wang, Ming Yan, and Chen Wu. Multi-granularity hierarchical attention
    fusion networks for reading comprehension and question answering. In Proceedings
    of the 56th Annual Meeting of the Association for Computational Linguistics, pages
    1705–1714, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Xiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying Chen, Wayne Xin Zhao,
    Dianhai Yu, and Hua Wu. Multi-turn response selection for chatbots with deep attention
    matching network. In Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), pages 1118–1127, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Cicero dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. Attentive pooling
    networks. arXiv preprint arXiv:1602.03609, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Victor Zhong, Caiming Xiong, Nitish Shirish Keskar, and Richard Socher.
    Coarse-grain fine-grain coattention network for multi-evidence question answering.
    arXiv preprint arXiv:1901.00603, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, and
    Ray Kurzweil. Generating high-quality and informative conversation responses with
    sequence-to-sequence models. arXiv preprint arXiv:1701.03185, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Chenguang Zhu, Michael Zeng, and Xuedong Huang. Sdnet: Contextualized
    attention-based deep network for conversational question answering. arXiv preprint
    arXiv:1812.03593, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. Lstm-based deep
    learning models for non-factoid answer selection. arXiv preprint arXiv:1511.04108,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William W Cohen, and Ruslan
    Salakhutdinov. Gated-attention readers for text comprehension. arXiv preprint
    arXiv:1606.01549, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He, Zhanyi Liu, Hua Wu,
    and Jun Zhao. An end-to-end model for question answering over knowledge base with
    cross-attention combining global knowledge. In Proceedings of the 55th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    pages 221–231, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Rudolf Kadlec, Martin Schmid, Ondřej Bajgar, and Jan Kleindienst. Text
    understanding with the attention sum reader network. In Proceedings of the 54th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers), pages 908–918, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Tsendsuren Munkhdalai and Hong Yu. Neural tree indexers for text understanding.
    In Proceedings of the conference. Association for Computational Linguistics. Meeting,
    volume 1, page 11\. NIH Public Access, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Seonhoon Kim, Inho Kang, and Nojun Kwak. Semantic sentence matching with
    densely-connected recurrent and co-attentive information. In Proceedings of the
    AAAI conference on artificial intelligence, pages 6586–6593, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Lisa Bauer, Yicheng Wang, and Mohit Bansal. Commonsense for generative
    multi-hop question answering tasks. arXiv preprint arXiv:1809.06309, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] Qiu Ran, Peng Li, Weiwei Hu, and Jie Zhou. Option comparison network
    for multiple-choice reading comprehension. arXiv preprint arXiv:1903.03033, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt,
    Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend.
    In Advances in neural information processing systems, pages 1693–1701, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying
    Ma. Topic aware neural response generation. In Proceedings of the AAAI Conference
    on Artificial Intelligence, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Alessandro Sordoni, Philip Bachman, Adam Trischler, and Yoshua Bengio.
    Iterative alternating neural attention for machine reading. arXiv preprint arXiv:1606.02245,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Bingning Wang, Kang Liu, and Jun Zhao. Inner attention based recurrent
    neural networks for answer selection. In Proceedings of the 54th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers), pages
    1288–1297, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. Attention-based
    lstm for aspect-level sentiment classification. In Proceedings of the 2016 conference
    on empirical methods in natural language processing, pages 606–615, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. Interactive attention
    networks for aspect-level sentiment classification. arXiv preprint arXiv:1709.00893,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Kai Shuang, Xintao Ren, Qianqian Yang, Rui Li, and Jonathan Loo. Aela-dlstms:
    Attention-enabled and location-aware double lstms for aspect-level sentiment classification.
    Neurocomputing, 334:25–34, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Christos Baziotis, Nikos Pelekis, and Christos Doulkeridis. Datastories
    at semeval-2017 task 4: Deep lstm with attention for message-level and topic-based
    sentiment analysis. In Proceedings of the 11th international workshop on semantic
    evaluation (SemEval-2017), pages 747–754, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Wei Xue and Tao Li. Aspect based sentiment analysis with gated convolutional
    networks. arXiv preprint arXiv:1805.07043, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] Shi Feng, Yang Wang, Liran Liu, Daling Wang, and Ge Yu. Attention based
    hierarchical lstm network for context-aware microblog sentiment classification.
    World Wide Web, 22(1):59–81, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Youwei Song, Jiahai Wang, Tao Jiang, Zhiyue Liu, and Yanghui Rao. Attentional
    encoder network for targeted sentiment classification. arXiv preprint arXiv:1902.09314,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Bonggun Shin, Timothy Lee, and Jinho D Choi. Lexicon integrated cnn models
    with attention for sentiment analysis. arXiv preprint arXiv:1610.06272, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Minchae Song, Hyunjung Park, and Kyung-shik Shin. Attention-based long
    short-term memory network using sentiment lexicon embedding for aspect-level sentiment
    analysis in korean. Information Processing & Management, 56(3):637–653, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] Jiachen Du, Lin Gui, Yulan He, Ruifeng Xu, and Xuan Wang. Convolution-based
    neural attention with applications to sentiment classification. IEEE Access, 7:27983–27992,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. Recurrent attention
    network on memory for aspect sentiment analysis. In Proceedings of the 2017 conference
    on empirical methods in natural language processing, pages 452–461, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Yukun Ma, Haiyun Peng, and Erik Cambria. Targeted aspect-based sentiment
    analysis via embedding commonsense knowledge into an attentive lstm. In Proceedings
    of the AAAI Conference on Artificial Intelligence, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] Jiangming Liu and Yue Zhang. Attention modeling for targeted sentiment.
    In Proceedings of the 15th Conference of the European Chapter of the Association
    for Computational Linguistics: Volume 2, Short Papers, pages 572–577, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] Huimin Chen, Maosong Sun, Cunchao Tu, Yankai Lin, and Zhiyuan Liu. Neural
    sentiment classification with user and product attention. In Proceedings of the
    2016 conference on empirical methods in natural language processing, pages 1650–1659,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Jiangfeng Zeng, Xiao Ma, and Ke Zhou. Enhancing attention-based lstm
    with position context for aspect-level sentiment classification. IEEE Access,
    7:20462–20471, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Heng She, Bin Wu, Bai Wang, and Renjun Chi. Distant supervision for relation
    extraction with hierarchical attention and entity descriptions. In Proc. of IEEE
    IJCNN, pages 1–8\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] Biao Zhang, Deyi Xiong, Jinsong Su, and Min Zhang. Learning better discourse
    representation for implicit discourse relation recognition via attention networks.
    Neurocomputing, 275:1241–1249, January 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen Wang, and Chengqi
    Zhang. Reinforced self-attention network: a hybrid of hard and soft attention
    for sequence modeling. arXiv:1801.10296 [cs], January 2018. arXiv: 1801.10296.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen, and Xiaodong Shi. Deep
    semantic role labeling with self-attention. In Proceedings of the AAAI Conference
    on Artificial Intelligence, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] Li Dong and Mirella Lapata. Language to logical form with neural attention.
    In Proceedings of the 54th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), pages 33–43, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Wenya Wu, Yufeng Chen, Jinan Xu, and Yujie Zhang. Attention-based convolutional
    neural networks for chinese relation extraction. In Chinese Computational Linguistics
    and Natural Language Processing Based on Naturally Annotated Big Data, pages 147–158\.
    Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and
    Yoshua Bengio. Attention-based models for speech recognition. In Proceedings of
    the 28th International Conference on Neural Information Processing Systems-Volume
    1, pages 577–585, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] Rohit Prabhavalkar, Tara Sainath, Yonghui Wu, Patrick Nguyen, Zhifeng
    Chen, Chung-Cheng Chiu, and Anjuli Kannan. Minimum word error rate training for
    attention-based sequence-to-sequence models, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] Kazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann Ney. Language modeling
    with deep transformers. arXiv preprint arXiv:1905.04226, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] Christoph Lüscher, Eugen Beck, Kazuki Irie, Markus Kitza, Wilfried Michel,
    Albert Zeyer, Ralf Schlüter, and Hermann Ney. Rwth asr systems for librispeech:
    Hybrid vs attention–w/o data augmentation. arXiv preprint arXiv:1905.03072, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] Linhao Dong, Feng Wang, and Bo Xu. Self-attention aligner: A latency-control
    end-to-end model for asr using self-attention network and chunk-hopping. In ICASSP
    2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP), pages 5656–5660\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] Julian Salazar, Katrin Kirchhoff, and Zhiheng Huang. Self-attention networks
    for connectionist temporal classification in speech recognition. In ICASSP 2019-2019
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    pages 7115–7119\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] Xiaofei Wang, Ruizhi Li, Sri Harish Mallidi, Takaaki Hori, Shinji Watanabe,
    and Hynek Hermansky. Stream attention-based multi-array end-to-end speech recognition.
    In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP), pages 7105–7109\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] Shiyu Zhou, Linhao Dong, Shuang Xu, and Bo Xu. Syllable-based sequence-to-sequence
    speech recognition with the transformer in mandarin chinese. arXiv preprint arXiv:1804.10752,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki
    Hayashi. Hybrid ctc/attention architecture for end-to-end speech recognition.
    IEEE Journal of Selected Topics in Signal Processing, 11(8):1240–1253, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and
    Yoshua Bengio. End-to-end attention-based large vocabulary speech recognition.
    In 2016 IEEE international conference on acoustics, speech and signal processing
    (ICASSP), pages 4945–4949\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. End-to-end
    continuous speech recognition using attention-based recurrent nn: First results.
    arXiv preprint arXiv:1412.1602, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] FA Rezaur rahman Chowdhury, Quan Wang, Ignacio Lopez Moreno, and Li Wan.
    Attention-based models for text-dependent speaker verification. In 2018 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5359–5363\.
    IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] Yiming Wang, Xing Fan, I-Fan Chen, Yuzong Liu, Tongfei Chen, and Björn
    Hoffmeister. End-to-end anchored speech recognition. In ICASSP 2019-2019 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    pages 7090–7094\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] Albert Zeyer, Kazuki Irie, Ralf Schlüter, and Hermann Ney. Improved training
    of end-to-end attention models for speech recognition. arXiv preprint arXiv:1805.03294,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] Suyoun Kim, Takaaki Hori, and Shinji Watanabe. Joint ctc-attention based
    end-to-end speech recognition using multi-task learning. In 2017 IEEE international
    conference on acoustics, speech and signal processing (ICASSP), pages 4835–4839\.
    IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] Koji Okabe, Takafumi Koshinaka, and Koichi Shinoda. Attentive statistics
    pooling for deep speaker embedding. arXiv preprint arXiv:1803.10963, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] Takaaki Hori, Shinji Watanabe, Yu Zhang, and William Chan. Advances in
    joint ctc-attention based end-to-end speech recognition with a deep cnn encoder
    and rnn-lm. arXiv preprint arXiv:1706.02737, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] Sumit Chopra, Michael Auli, and Alexander M. Rush. Abstractive sentence
    summarization with attentive recurrent neural networks. In Proceedings of the
    2016 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, pages 93–98, San Diego, California,
    2016\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model
    for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive
    text summarization using sequence-to-sequence rnns and beyond. arXiv preprint
    arXiv:1602.06023, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention
    model for abstractive sentence summarization. arXiv preprint arXiv:1509.00685,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation.
    arXiv preprint arXiv:1805.04833, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] Banafsheh Rekabdar, Christos Mousas, and Bidyut Gupta. Generative adversarial
    network with policy gradient for text summarization. In 2019 IEEE 13th international
    conference on semantic computing (ICSC), pages 204–207\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim,
    Walter Chang, and Nazli Goharian. A discourse-aware attention model for abstractive
    summarization of long documents. arXiv preprint arXiv:1804.05685, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks.
    IEEE transactions on Signal Processing, 45(11):2673–2681, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] Miltiadis Allamanis, Hao Peng, and Charles Sutton. A convolutional attention
    network for extreme summarization of source code. In International conference
    on machine learning, pages 2091–2100, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] Jie Huang, Wengang Zhou, Qilin Zhang, Houqiang Li, and Weiping Li. Video-based
    sign language recognition without temporal segmentation. In Thirty-Second AAAI
    Conference on Artificial Intelligence, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] Seyedmahdad Mirsamadi, Emad Barsoum, and Cha Zhang. Automatic speech
    emotion recognition using recurrent neural networks with local attention. In 2017
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    pages 2227–2231\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] Akshi Kumar, Saurabh Raj Sangwan, Anshika Arora, Anand Nayyar, Mohamed
    Abdel-Basset, et al. Sarcasm detection using soft attention-based bidirectional
    long short-term memory model with convolution network. IEEE access, 7:23319–23328,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] Kun-Yi Huang, Chung-Hsien Wu, and Ming-Hsiang Su. Attention-based convolutional
    neural network and long short-term memory for short-term detection of mood disorders
    based on elicited speech responses. Pattern Recognition, 88:668–678, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] Navonil Majumder, Soujanya Poria, Devamanyu Hazarika, Rada Mihalcea,
    Alexander Gelbukh, and Erik Cambria. Dialoguernn: An attentive rnn for emotion
    detection in conversations. In Proceedings of the AAAI Conference on Artificial
    Intelligence, pages 6818–6825, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] Yuanyuan Zhang, Jun Du, Zirui Wang, Jianshu Zhang, and Yanhui Tu. Attention
    based fully convolutional network for speech emotion recognition. In 2018 Asia-Pacific
    Signal and Information Processing Association Annual Summit and Conference (APSIPA
    ASC), pages 1771–1775. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] Michael Neumann and Ngoc Thang Vu. Attentive convolutional neural network
    based speech emotion recognition: A study on the impact of input features, signal
    length, and acted speech. arXiv preprint arXiv:1706.00612, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao. Coupled
    multi-layer attentions for co-extraction of aspect and opinion terms. In Proceedings
    of the AAAI Conference on Artificial Intelligence, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] Gang Liu and Jiabao Guo. Bidirectional lstm with attention mechanism
    and convolutional layer for text classification. Neurocomputing, 337:325–338,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] Atta Norouzian, Bogdan Mazoure, Dermot Connolly, and Daniel Willett.
    Exploring attention mechanism for acoustic-based classification of speech utterances
    into system-directed and non-system-directed. In ICASSP 2019-2019 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7310–7314\.
    IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] Xinyu Li, Venkata Chebiyyam, and Katrin Kirchhoff. Multi-stream network
    with temporal attention for environmental sound classification. arXiv preprint
    arXiv:1901.08608, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] Patrick Verga, Emma Strubell, and Andrew McCallum. Simultaneously self-attending
    to all mentions for full-abstract biological relation extraction. arXiv preprint
    arXiv:1802.10569, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] Xiaoyu Guo, Hui Zhang, Haijun Yang, Lianyuan Xu, and Zhiwen Ye. A single
    attention-based combination of cnn and rnn for relation classification. IEEE Access,
    7:12467–12475, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao, and
    Bo Xu. Attention-based bidirectional long short-term memory networks for relation
    classification. In Proceedings of the 54th annual meeting of the association for
    computational linguistics (volume 2: Short papers), pages 207–212, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Neural
    relation extraction with selective attention over instances. In Proceedings of
    the 54th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 2124–2133, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D
    Manning. Position-aware attention and supervised data improve slot filling. In
    Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,
    pages 35–45, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] Qian Chen, Zhu Zhuo, and Wen Wang. Bert for joint intent classification
    and slot filling. arXiv preprint arXiv:1902.10909, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] Minsuk Choi, Cheonbok Park, Soyoung Yang, Yonggyu Kim, Jaegul Choo, and
    Sungsoo Ray Hong. Aila: Attentive interactive labeling assistant for document
    classification through attention-based deep neural networks. In Proceedings of
    the 2019 CHI Conference on Human Factors in Computing Systems, pages 1–12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] Qiuqiang Kong, Yong Xu, Wenwu Wang, and Mark D Plumbley. Audio set classification
    with attention model: A probabilistic perspective. In 2018 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 316–320\.
    IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] Christoph Alt, Marc Hübner, and Leonhard Hennig. Improving relation extraction
    by pre-trained language representations. arXiv preprint arXiv:1906.03088, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] Yusuke Yasuda, Xin Wang, Shinji Takaki, and Junichi Yamagishi. Investigation
    of enhanced tacotron text-to-speech synthesis systems with self-attention for
    pitch accent language. In ICASSP 2019-2019 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), pages 6905–6909\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] Mingyang Zhang, Xin Wang, Fuming Fang, Haizhou Li, and Junichi Yamagishi.
    Joint training framework for text-to-speech and voice conversion using multi-source
    tacotron and wavenet. arXiv preprint arXiv:1903.12389, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng Kong, Chris Dyer, Graham
    Neubig, and Noah A Smith. What do recurrent neural network grammars learn about
    syntax? arXiv preprint arXiv:1611.05774, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] Matthias Sperber, Graham Neubig, Jan Niehues, and Alex Waibel. Attention-passing
    models for robust and data-efficient end-to-end speech translation. Transactions
    of the Association for Computational Linguistics, 7:313–325, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] Hideyuki Tachibana, Katsuya Uenoyama, and Shunsuke Aihara. Efficiently
    trainable text-to-speech system based on deep convolutional networks with guided
    attention. In 2018 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP), pages 4784–4788\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] Arthur Mensch and Mathieu Blondel. Differentiable dynamic programming
    for structured prediction and attention. arXiv preprint arXiv:1802.03676, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] Ting Zhang, Bang Liu, Di Niu, Kunfeng Lai, and Yu Xu. Multiresolution
    graph attention networks for relevance matching. In Proceedings of the 27th ACM
    International Conference on Information and Knowledge Management, pages 933–942,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang, Xi Chen, Wei Zhang,
    and Huajun Chen. Long-tail relation extraction via knowledge graph embeddings
    and graph convolution networks. arXiv preprint arXiv:1903.01306, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping
    Hu. Attention-over-attention neural networks for reading comprehension. In Proceedings
    of the 55th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 593–602, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] Shanshan Liu, Sheng Zhang, Xin Zhang, and Hui Wang. R-trans: Rnn transformer
    network for chinese machine reading comprehension. IEEE Access, 7:27736–27745,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] Yiming Cui, Ting Liu, Zhipeng Chen, Shijin Wang, and Guoping Hu. Consensus
    attention-based neural networks for chinese reading comprehension. arXiv preprint
    arXiv:1607.02250, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] Young-Bum Kim, Dongchan Kim, Anjishnu Kumar, and Ruhi Sarikaya. Efficient
    large-scale domain classification with personalized attention. arXiv preprint
    arXiv:1804.08065, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil
    Blunsom. Learning to transduce with unbounded memory. In Advances in neural information
    processing systems, pages 1828–1836, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] Bishan Yang and Tom Mitchell. Leveraging knowledge bases in lstms for
    improving machine reading. arXiv preprint arXiv:1902.09091, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] Bing Liu and Ian Lane. Attention-based recurrent neural network models
    for joint intent detection and slot filling. arXiv preprint arXiv:1609.01454,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] Sina Ahmadi. Attention-based encoder-decoder networks for spelling and
    grammatical error correction. arXiv preprint arXiv:1810.00660, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] Rajarshi Das, Arvind Neelakantan, David Belanger, and Andrew McCallum.
    Chains of reasoning over entities, relations, and text using recurrent neural
    networks. arXiv preprint arXiv:1607.01426, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] Octavian-Eugen Ganea and Thomas Hofmann. Deep joint entity disambiguation
    with local neural attention. arXiv preprint arXiv:1704.04920, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang,
    Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding.
    arXiv preprint arXiv:1703.03130, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] Timo Schick and Hinrich Schütze. Attentive mimicking: Better word embeddings
    by attending to informative contexts. arXiv preprint arXiv:1904.01617, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] Yingke Zhu, Tom Ko, David Snyder, Brian Mak, and Daniel Povey. Self-attentive
    speaker embeddings for text-independent speaker verification. In Interspeech,
    pages 3573–3577, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] Timothy Dozat and Christopher D Manning. Deep biaffine attention for
    neural dependency parsing. arXiv preprint arXiv:1611.01734, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum.
    Linguistically-informed self-attention for semantic role labeling. arXiv preprint
    arXiv:1804.08199, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan
    Zhu. Commonsense knowledge aware conversation generation with graph attention.
    In IJCAI, pages 4623–4629, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] Jing-Xuan Zhang, Zhen-Hua Ling, Li-Juan Liu, Yuan Jiang, and Li-Rong
    Dai. Sequence-to-sequence acoustic modeling for voice conversion. IEEE/ACM Transactions
    on Audio, Speech, and Language Processing, 27(3):631–644, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] Bo Sun, Yunzong Zhu, Yongkang Xiao, Rong Xiao, and Yungang Wei. Automatic
    question tagging with deep neural networks. IEEE Transactions on Learning Technologies,
    12(1):29–43, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] Shikhar Sharma, Ryan Kiros, and Ruslan Salakhutdinov. Action recognition
    using visual attention. arXiv preprint arXiv:1511.04119, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zisserman. Video
    action transformer network. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pages 244–253, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] Rohit Girdhar and Deva Ramanan. Attentional pooling for action recognition.
    arXiv preprint arXiv:1711.01467, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] Sijie Song, Cuiling Lan, Junliang Xing, Wenjun Zeng, and Jiaying Liu.
    An end-to-end spatio-temporal attention model for human action recognition from
    skeleton data. In Proceedings of the AAAI conference on artificial intelligence,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, and Alex C. Kot. Global context-aware
    attention lstm networks for 3d action recognition. In The IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), July 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] Loris Bazzani, Hugo Larochelle, and Lorenzo Torresani. Recurrent mixture
    density network for spatiotemporal visual attention. arXiv preprint arXiv:1603.08199,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] Zhenyang Li, Kirill Gavrilyuk, Efstratios Gavves, Mihir Jain, and Cees GM
    Snoek. Videolstm convolves, attends and flows for action recognition. Computer
    Vision and Image Understanding, 166:41–50, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] Chih-Yao Ma, Asim Kadav, Iain Melvin, Zsolt Kira, Ghassan AlRegib, and
    Hans Peter Graf. Attend and interact: Higher-order object interactions for video
    understanding. arXiv:1711.06330 [cs], November 2017. arXiv: 1711.06330.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] Yunbo Wang, Lu Jiang, Ming-Hsuan Yang, Li-Jia Li, Mingsheng Long, and
    Li Fei-Fei. Eidetic 3d lstm: A model for video prediction and beyond. In International
    conference on learning representations, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] Dong Li, Ting Yao, Ling-Yu Duan, Tao Mei, and Yong Rui. Unified spatio-temporal
    attention networks for action recognition in videos. IEEE Transactions on Multimedia,
    21(2):416–428, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] Pengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jianru Xue, and
    Nanning Zheng. View adaptive recurrent neural networks for high performance human
    action recognition from skeleton data. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 2117–2126, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] Wu Zheng, Lin Li, Zhaoxiang Zhang, Yan Huang, and Liang Wang. Relational
    network for skeleton-based action recognition. In 2019 IEEE International Conference
    on Multimedia and Expo (ICME), pages 826–831\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] Harshala Gammulle, Simon Denman, Sridha Sridharan, and Clinton Fookes.
    Two stream lstm: A deep fusion framework for human action recognition. In 2017
    IEEE Winter Conference on Applications of Computer Vision (WACV), pages 177–186\.
    IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] Zhaoxuan Fan, Xu Zhao, Tianwei Lin, and Haisheng Su. Attention-based
    multiview re-observation fusion network for skeletal action recognition. IEEE
    Transactions on Multimedia, 21(2):363–374, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] Chenyang Si, Wentao Chen, Wei Wang, Liang Wang, and Tieniu Tan. An attention
    enhanced graph convolutional lstm network for skeleton-based action recognition.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 1227–1236, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] Swathikiran Sudhakaran, Sergio Escalera, and Oswald Lanz. Lsta: Long
    short-term attention for egocentric action recognition. In Proc. of the IEEE/CVF
    CVPR, pages 9954–9963, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] Jun Liu, Gang Wang, Ling-Yu Duan, Kamila Abdiyeva, and Alex C Kot. Skeleton-based
    human action recognition with global context-aware attention lstm networks. IEEE
    Transactions on Image Processing, 27(4):1586–1599, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] Jiang Liu, Chenqiang Gao, Deyu Meng, and Alexander G Hauptmann. Decidenet:
    Counting varying density crowds through attention guided detection and density
    estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 5197–5206, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] Lingbo Liu, Hongjun Wang, Guanbin Li, Wanli Ouyang, and Liang Lin. Crowd
    counting using deep recurrent spatial-aware network. arXiv preprint arXiv:1807.00601,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] Mohammad Hossain, Mehrdad Hosseinzadeh, Omit Chanda, and Yang Wang. Crowd
    counting using scale-aware attention networks. In 2019 IEEE Winter Conference
    on Applications of Computer Vision (WACV), pages 1280–1288\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[334] Youmei Zhang, Chunluan Zhou, Faliang Chang, and Alex C Kot. Multi-resolution
    attention convolutional neural network for crowd counting. Neurocomputing, 329:144–152,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[335] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
    catastrophic forgetting with hard attention to the task. In International Conference
    on Machine Learning, pages 4548–4557\. PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] Kai Han, Jianyuan Guo, Chao Zhang, and Mingjian Zhu. Attribute-aware
    attention model for fine-grained representation learning. arXiv:1901.00392 [cs],
    January 2019. arXiv: 1901.00392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[337] Pierre Sermanet, Andrea Frome, and Esteban Real. Attention for fine-grained
    categorization. arXiv preprint arXiv:1412.7054, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] Guoliang Kang, Liang Zheng, Yan Yan, and Yi Yang. Deep adversarial attention
    alignment for unsupervised domain adaptation: the benefit of target expectation
    maximization. arXiv:1801.10068 [cs], January 2018. arXiv: 1801.10068.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[339] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang,
    Xiaogang Wang, and Xiaoou Tang. Residual attention network for image classification.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 3156–3164, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[340] Paul Hongsuck Seo, Zhe Lin, Scott Cohen, Xiaohui Shen, and Bohyung Han.
    Hierarchical attention networks. CoRR, abs/1606.02393, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[341] Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia Xu, and Liang Lin. Multi-label
    image recognition by recurrently discovering attentional regions. In Proc. of
    the IEEE ICCV, pages 464–472, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[342] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V
    Le. Attention augmented convolutional networks. In Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pages 3286–3295, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[343] Lin Wu, Yang Wang, Xue Li, and Junbin Gao. Deep attention-based spatially
    recursive networks for fine-grained visual recognition. IEEE transactions on cybernetics,
    49(5):1791–1802, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[344] Bo Zhao, Xiao Wu, Jiashi Feng, Qiang Peng, and Shuicheng Yan. Diversified
    visual attention networks for fine-grained object classification. IEEE Transactions
    on Multimedia, 19(6):1245–1256, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[345] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Learning embedding
    adaptation for few-shot learning. arXiv preprint arXiv:1812.03664, 7, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[346] E Pesce, P Ypsilantis, S Withey, R Bakewell, V Goh, and G Montana. Learning
    to detect chest radiographs containing lung nodules using visual attention networks.
    corr. arXiv preprint arXiv:1712.00996, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[347] Bin Zhao, Xuelong Li, Xiaoqiang Lu, and Zhigang Wang. A cnn–rnn architecture
    for multi-label weather recognition. Neurocomputing, 322:47–57, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[348] Qilin Yin, Jinwei Wang, Xiangyang Luo, Jiangtao Zhai, Sunil Kr Jha, and
    Yun-Qing Shi. Quaternion convolutional neural network for color image classification
    and forensics. IEEE Access, 7:20293–20301, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[349] Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, and
    Zheng Zhang. The application of two-level attention models in deep convolutional
    neural network for fine-grained image classification. In Proc. of the IEEE CVPR,
    pages 842–850, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[350] Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, and Hironobu Fujiyoshi.
    Attention branch network: Learning of attention mechanism for visual explanation.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 10705–10714, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[351] Hailin Hu, An Xiao, Sai Zhang, Yangyang Li, Xuanling Shi, Tao Jiang,
    Linqi Zhang, Lei Zhang, and Jianyang Zeng. Deephint: understanding hiv-1 integration
    via deep learning with attention. Bioinformatics, 35(10):1660–1667, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[352] Qingji Guan, Yaping Huang, Zhun Zhong, Zhedong Zheng, Liang Zheng, and
    Yi Yang. Diagnose like a radiologist: Attention guided convolutional neural network
    for thorax disease classification. arXiv preprint arXiv:1801.09927, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[353] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui
    Shen, and Stan Sclaroff. Top-down neural attention by excitation backprop. International
    Journal of Computer Vision, 126(10):1084–1102, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[354] Wenguan Wang, Yuanlu Xu, Jianbing Shen, and Song-Chun Zhu. Attentive
    fashion grammar network for fashion landmark detection and clothing category classification.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 4271–4280, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[355] Wonsik Kim, Bhavya Goyal, Kunal Chawla, Jungmin Lee, and Keunjoo Kwon.
    Attention-based ensemble for deep metric learning. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 736–751, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[356] Mengye Ren, Renjie Liao, Ethan Fetaya, and Richard Zemel. Incremental
    few-shot learning with attention attractor networks. In Advances in Neural Information
    Processing Systems, pages 5275–5285, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[357] Yuxin Peng, Xiangteng He, and Junjie Zhao. Object-part attention model
    for fine-grained image classification. IEEE Transactions on Image Processing,
    27(3):1487–1500, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[358] Hazel Doughty, Walterio Mayol-Cuevas, and Dima Damen. The pros and cons:
    Rank-aware temporal attention for skill determination in long videos. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7862–7871,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[359] Yuxin Peng, Xiangteng He, and Junjie Zhao. Object-part attention driven
    discriminative localization for fine-grained image classification. CoRR, abs/1704.01740,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[360] Dimitris Kastaniotis, Ioanna Ntinou, Dimitrios Tsourounis, George Economou,
    and Spiros Fotopoulos. Attention-aware generative adversarial networks (ata-gans).
    In 2018 IEEE 13th Image, Video, and Multidimensional Signal Processing Workshop
    (IVMSP), pages 1–5\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[361] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang.
    Generative image inpainting with contextual attention. arXiv:1801.07892 [cs],
    January 2018. arXiv: 1801.07892.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[362] Scott Reed, Yutian Chen, Thomas Paine, Aäron van den Oord, S. M. Ali
    Eslami, Danilo Rezende, Oriol Vinyals, and Nando de Freitas. Few-shot autoregressive
    density estimation: Towards learning to learn distributions. arXiv:1710.10304
    [cs], October 2017. arXiv: 1710.10304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[363] Xinyuan Chen, Chang Xu, Xiaokang Yang, and Dacheng Tao. Attention-gan
    for object transfiguration in wild images. In Proceedings of the European Conference
    on Computer Vision (ECCV), pages 164–180, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[364] Jianan Li, Jimei Yang, Aaron Hertzmann, Jianming Zhang, and Tingfa Xu.
    Layoutgan: Generating graphic layouts with wireframe discriminators. arXiv preprint
    arXiv:1901.06767, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[365] Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J Corso, and Yan Yan.
    Multi-channel attention selection gan with cascaded semantic guidance for cross-view
    image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pages 2417–2426, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[366] Qi Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, and Nenghai
    Yu. Online multi-object tracking using cnn-based single object tracker with spatial-temporal
    attention mechanism. In Proceedings of the IEEE International Conference on Computer
    Vision, pages 4836–4845, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[367] Zhedong Zheng, Liang Zheng, and Yi Yang. Pedestrian alignment network
    for large-scale person re-identification. IEEE Transactions on Circuits and Systems
    for Video Technology, 29(10):3037–3045, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[368] Xihui Liu, Haiyu Zhao, Maoqing Tian, Lu Sheng, Jing Shao, Shuai Yi, Junjie
    Yan, and Xiaogang Wang. Hydraplus-net: Attentive deep features for pedestrian
    analysis. In Proceedings of the IEEE international conference on computer vision,
    pages 350–359, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[369] Anfeng He, Chong Luo, Xinmei Tian, and Wenjun Zeng. A twofold siamese
    network for real-time object tracking. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 4834–4843, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[370] Xinlei Chen, Li-Jia Li, Li Fei-Fei, and Abhinav Gupta. Iterative visual
    reasoning beyond convolutions. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 7239–7248, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[371] Jianlou Si, Honggang Zhang, Chun-Guang Li, Jason Kuen, Xiangfei Kong,
    Alex C Kot, and Gang Wang. Dual attention matching network for context-aware feature
    sequence based person re-identification. In Proceedings of the IEEE CVPR, pages
    5363–5372, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[372] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang Wang. Mask-guided contrastive
    attention model for person re-identification. In Proc. of the IEEE CVPR, pages
    1179–1188, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[373] Yi Zhou and Ling Shao. Aware attentive multi-view inference for vehicle
    re-identification. In Proc. of the IEEE CVPR, pages 6489–6498, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[374] Xiaoning Zhang, Tiantian Wang, Jinqing Qi, Huchuan Lu, and Gang Wang.
    Progressive attention guided recurrent network for salient object detection. In
    IEEE CVPR), June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[375] Tao Kong, Fuchun Sun, Chuanqi Tan, Huaping Liu, and Wenbing Huang. Deep
    feature pyramid reconfiguration for object detection. In Proceedings of the European
    conference on computer vision (ECCV), pages 169–185, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[376] Guanbin Li, Yukang Gan, Hejun Wu, Nong Xiao, and Liang Lin. Cross-modal
    attentional context learning for rgb-d object detection. IEEE Transactions on
    Image Processing, 28(4):1591–1601, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[377] Shuhan Chen, Xiuli Tan, Ben Wang, and Xuelong Hu. Reverse attention for
    salient object detection. In Proceedings of the European Conference on Computer
    Vision (ECCV), pages 234–250, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[378] Hao Chen and Youfu Li. Three-stream attention-aware network for rgb-d
    salient object detection. IEEE Transactions on Image Processing, 28(6):2825–2835,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[379] Xudong Wang, Zhaowei Cai, Dashan Gao, and Nuno Vasconcelos. Towards universal
    object detection by domain attention. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 7289–7298, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[380] Wei Li, Xiatian Zhu, and Shaogang Gong. Harmonious attention network
    for person re-identification. In The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[381] Hao Liu, Jiashi Feng, Meibin Qi, Jianguo Jiang, and Shuicheng Yan. End-to-end
    comparative attention networks for person re-identification. IEEE Transactions
    on Image Processing, 26(7):3492–3506, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[382] Xingyu Liao, Lingxiao He, Zhouwang Yang, and Chi Zhang. Video-based person
    re-identification via 3d convolutional networks and non-local attention. In Asian
    Conference on Computer Vision, pages 620–634. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[383] Dapeng Chen, Hongsheng Li, Tong Xiao, Shuai Yi, and Xiaogang Wang. Video
    person re-identification with competitive snippet-similarity aggregation and co-attentive
    snippet embedding. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 1169–1178, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[384] Liming Zhao, Xi Li, Yueting Zhuang, and Jingdong Wang. Deeply-learned
    part-aligned representations for person re-identification. In Proceedings of the
    IEEE international conference on computer vision, pages 3219–3228, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[385] Zhen Zhou, Yan Huang, Wei Wang, Liang Wang, and Tieniu Tan. See the forest
    for the trees: Joint spatial and temporal recurrent neural networks for video-based
    person re-identification. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 4747–4756, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[386] Jing Xu, Rui Zhao, Feng Zhu, Huaming Wang, and Wanli Ouyang. Attention-aware
    compositional network for person re-identification. In Proceedings of the IEEE
    CVPR, pages 2119–2128, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[387] Shuang Li, Slawomir Bak, Peter Carr, and Xiaogang Wang. Diversity regularized
    spatiotemporal attention for video-based person re-identification. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 369–378,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[388] Meng Zheng, Srikrishna Karanam, Ziyan Wu, and Richard J Radke. Re-identification
    with consistent attentive siamese networks. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 5735–5744, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[389] Deqiang Ouyang, Yonghui Zhang, and Jie Shao. Video-based person re-identification
    via spatio-temporal attentional and two-stream fusion convolutional networks.
    Pattern Recognition Letters, 117:153–160, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[390] Cheng Wang, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Mancs:
    A multi-task attentional network with curriculum sampling for person re-identification.
    In Proceedings of the European Conference on Computer Vision (ECCV), pages 365–381,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[391] Shuangjie Xu, Yu Cheng, Kang Gu, Yang Yang, Shiyu Chang, and Pan Zhou.
    Jointly attentive spatial-temporal pooling networks for video-based person re-identification.
    In Proceedings of the IEEE international conference on computer vision, pages
    4733–4742, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[392] Ruimao Zhang, Jingyu Li, Hongbin Sun, Yuying Ge, Ping Luo, Xiaogang Wang,
    and Liang Lin. Scan: Self-and-collaborative attention network for video person
    re-identification. IEEE Transactions on Image Processing, 28(10):4870–4882, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[393] Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Hanqing Lu. Dual attention
    network for scene segmentation. arXiv:1809.02983 [cs], September 2018. arXiv:
    1809.02983.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[394] Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. Tell me
    where to look: Guided attention inference network. arXiv:1802.10171 [cs], February
    2018. arXiv: 1802.10171.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[395] Mengye Ren and Richard S. Zemel. End-to-end instance segmentation with
    recurrent attention. In The IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), July 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[396] Pingping Zhang, Wei Liu, Hongyu Wang, Yinjie Lei, and Huchuan Lu. Deep
    gated attention networks for large-scale street-level scene segmentation. Pattern
    Recognition, 88:702–714, April 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[397] Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and Alan L Yuille. Attention
    to scale: Scale-aware semantic image segmentation. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 3640–3649, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[398] Saumya Jetley, Nicholas A. Lord, Namhoon Lee, and Philip H. S. Torr.
    Learn to pay attention. arXiv:1804.02391 [cs], April 2018. arXiv: 1804.02391.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[399] Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task
    learning with attention. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pages 1871–1880, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[400] Yuhui Yuan and Jingdong Wang. Ocnet: Object context network for scene
    parsing. arXiv preprint arXiv:1809.00916, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[401] Hanchao Li, Pengfei Xiong, Jie An, and Lingxue Wang. Pyramid attention
    network for semantic segmentation. arXiv preprint arXiv:1805.10180, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[402] Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, and Pheng-Ann Heng. Direction-aware
    spatial context features for shadow detection. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 7454–7462, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[403] Zitao Zeng, Weihao Xie, Yunzhe Zhang, and Yao Lu. Ric-unet: An improved
    neural network based on unet for nuclei segmentation in histology images. Ieee
    Access, 7:21420–21428, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[404] Bing Shuai, Zhen Zuo, Bing Wang, and Gang Wang. Scene segmentation with
    dag-recurrent neural networks. IEEE PAMI, 40(6):1480–1493, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[405] Xinxin Hu, Kailun Yang, Lei Fei, and Kaiwei Wang. Acnet: Attention based
    network to exploit complementary features for rgbd semantic segmentation. In 2019
    IEEE International Conference on Image Processing (ICIP), pages 1440–1444\. IEEE,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[406] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich,
    Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz,
    et al. Attention u-net: Learning where to look for the pancreas. arXiv preprint
    arXiv:1804.03999, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[407] Ruirui Li, Mingming Li, Jiacheng Li, and Yating Zhou. Connection sensitive
    attention u-net for accurate retinal vessel segmentation. arXiv preprint arXiv:1903.05558,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[408] Shu Kong and Charless Fowlkes. Pixel-wise attentional gating for scene
    parsing. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV),
    pages 1024–1033\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[409] Xiaoxiao Li and Chen Change Loy. Video object segmentation with joint
    re-identification and attention-aware mask propagation. In Proceedings of the
    European Conference on Computer Vision (ECCV), pages 90–105, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[410] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua
    Lin, and Jiaya Jia. Psanet: Point-wise spatial attention network for scene parsing.
    In Proceedings of the European Conference on Computer Vision (ECCV), pages 267–283,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[411] Jason Kuen, Zhenhua Wang, and Gang Wang. Recurrent attentional networks
    for saliency detection. In Proceedings of the IEEE Conference on computer Vision
    and Pattern Recognition, pages 3668–3677, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[412] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise
    contextual attention for saliency detection. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 3089–3098, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[413] Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, and Rita Cucchiara.
    Predicting human eye fixations via an lstm-based saliency attentive model. IEEE
    Transactions on Image Processing, 27(10):5142–5154, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[414] Xiaowei Hu, Chi-Wing Fu, Lei Zhu, Tianyu Wang, and Pheng-Ann Heng. Sac-net:
    Spatial attenuation context for salient object detection. IEEE Transactions on
    Circuits and Systems for Video Technology, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[415] Tong He, Zhi Tian, Weilin Huang, Chunhua Shen, Yu Qiao, and Changming
    Sun. An end-to-end textspotter with explicit alignment and attention. arXiv:1803.03474
    [cs], March 2018. arXiv: 1803.03474.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[416] Z. Cheng, F. Bai, Y. Xu, G. Zheng, S. Pu, and S. Zhou. Focusing attention:
    Towards accurate text recognition in natural images. In 2017 IEEE International
    Conference on Computer Vision (ICCV), pages 5086–5094, October 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[417] Canjie Luo, Lianwen Jin, and Zenghui Sun. Moran: A multi-object rectified
    attention network for scene text recognition. Pattern Recognition, 90:109–118,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[418] Hongtao Xie, Shancheng Fang, Zheng-Jun Zha, Yating Yang, Yan Li, and
    Yongdong Zhang. Convolutional attention networks for scene text recognition. ACM
    Transactions on Multimedia Computing, Communications, and Applications (TOMM),
    15(1s):1–17, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[419] Hui Li, Peng Wang, Chunhua Shen, and Guyu Zhang. Show, attend and read:
    A simple and strong baseline for irregular text recognition. In Proceedings of
    the AAAI Conference on Artificial Intelligence, pages 8610–8617, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[420] Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang Pu, and Shuigeng
    Zhou. Focusing attention: Towards accurate text recognition in natural images.
    In Proc. of the IEEE ICCV, pages 5076–5084, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[421] Miao Xin, Hong Zhang, Mingui Sun, and Ding Yuan. Recurrent temporal sparse
    autoencoder for attention-based action recognition. In 2016 International joint
    conference on neural networks (IJCNN), pages 456–463\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[422] Jinliang Zang, Le Wang, Ziyi Liu, Qilin Zhang, Gang Hua, and Nanning
    Zheng. Attention-based temporal weighted convolutional neural network for action
    recognition. In IFIP International Conference on Artificial Intelligence Applications
    and Innovations, pages 97–108\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[423] Wenjie Pei, Tadas Baltrusaitis, David Mj Tax, and Louis-Philippe Morency.
    Temporal attention-gated model for robust sequence classification. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6730–6739,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[424] Jianshu Zhang, Jun Du, and Lirong Dai. A gru-based encoder-decoder approach
    with attention for online handwritten mathematical expression recognition. In
    2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR),
    volume 1, pages 902–907\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[425] Zhong Ji, Yanwei Fu, Jichang Guo, Yanwei Pang, Zhongfei Mark Zhang, et al.
    Stacked semantics-guided attention model for fine-grained zero-shot learning.
    In Advances in Neural Information Processing Systems, pages 5995–6004, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[426] Jo Schlemper, Ozan Oktay, Michiel Schaap, Mattias Heinrich, Bernhard
    Kainz, Ben Glocker, and Daniel Rueckert. Attention gated networks: Learning to
    leverage salient regions in medical images. Medical image analysis, 53:197–207,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[427] Xiandong Meng, Xuan Deng, Shuyuan Zhu, Shuaicheng Liu, Chuan Wang, Chen
    Chen, and Bing Zeng. Mganet: A robust model for quality enhancement of compressed
    video. arXiv preprint arXiv:1811.09150, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[428] Dongwon Park, Jisoo Kim, and Se Young Chun. Down-scaling with learned
    kernels in multi-scale deep neural networks for non-uniform single image deblurring.
    arXiv preprint arXiv:1903.10157, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[429] Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and Elisa Ricci. Structured
    attention guided convolutional neural fields for monocular depth estimation. In
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 3917–3925, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[430] Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task
    learning with attention. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pages 1871–1880, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[431] Masanori Suganuma, Xing Liu, and Takayuki Okatani. Attention-based adaptive
    selection of operations for image restoration in the presence of unknown combined
    distortions. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 9039–9048, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[432] Rui Qian, Robby T Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu. Attentive
    generative adversarial network for raindrop removal from a single image. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 2482–2491,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[433] Youssef Alami Mejjati, Christian Richardt, James Tompkin, Darren Cosker,
    and Kwang In Kim. Unsupervised attention-guided image-to-image translation. In
    Advances in Neural Information Processing Systems, pages 3693–3703, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[434] Hao Tang, Dan Xu, Nicu Sebe, and Yan Yan. Attention-guided generative
    adversarial networks for unsupervised image-to-image translation. In 2019 International
    Joint Conference on Neural Networks (IJCNN), pages 1–8\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[435] Sheng Jin, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, Lei Zhang, and
    Xiansheng Hua. Deep saliency hashing. arXiv preprint arXiv:1807.01459, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[436] Zhan Yang, Osolo Ian Raymond, Wuqing Sun, and Jun Long. Deep attention-guided
    hashing. IEEE Access, 7:11209–11221, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[437] Huiyan Jiang, Tianyu Shi, Zhiqi Bai, and Liangliang Huang. Ahcnet: An
    application of attention mechanism and hybrid connection for liver tumor segmentation
    in ct volumes. IEEE Access, 7:24898–24909, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[438] Maximilian Ilse, Jakub M Tomczak, and Max Welling. Attention-based deep
    multiple instance learning. arXiv preprint arXiv:1802.04712, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[439] Chen-Yu Lee and Simon Osindero. Recursive recurrent nets with attention
    modeling for ocr in the wild. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 2231–2239, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[440] Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L Yuille, and Xiaogang
    Wang. Multi-context attention for human pose estimation. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 1831–1840, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[441] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu.
    Image super-resolution using very deep residual channel attention networks. In
    Proceedings of the European Conference on Computer Vision (ECCV), pages 286–301,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[442] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention:
    Improving the performance of convolutional neural networks via attention transfer.
    arXiv:1612.03928 [cs], December 2016. arXiv: 1612.03928.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[443] Xiao-Yu Zhang, Haichao Shi, Changsheng Li, Kai Zheng, Xiaobin Zhu, and
    Lixin Duan. Learning transferable self-attentive representations for action recognition
    in untrimmed videos with weak supervision. In Proceedings of the AAAI Conference
    on Artificial Intelligence, pages 9227–9234, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[444] Adam Bielski and Tomasz Trzcinski. Pay attention to virality: understanding
    popularity of social media videos with the attention mechanism. In Proceedings
    of the IEEE CVPR Workshops, pages 2335–2337, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[445] Xiang Long, Chuang Gan, Gerard De Melo, Jiajun Wu, Xiao Liu, and Shilei
    Wen. Attention clusters: Purely attention based local feature integration for
    video classification. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 7834–7843, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[446] Yundong Zhang, Xiang Xu, and Xiaotao Liu. Robust and high performance
    face detector. arXiv preprint arXiv:1901.02350, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[447] Shengtao Xiao, Jiashi Feng, Junliang Xing, Hanjiang Lai, Shuicheng Yan,
    and Ashraf Kassim. Robust facial landmark detection via recurrent attentive-refinement
    networks. In European conference on computer vision, pages 57–72. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[448] Na Lu, Yidan Wu, Li Feng, and Jinbo Song. Deep learning for fall detection:
    Three-dimensional cnn combined with lstm on video kinematic data. IEEE journal
    of biomedical and health informatics, 23(1):314–323, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[449] Lu Zhang, Zhiyong Liu, Shifeng Zhang, Xu Yang, Hong Qiao, Kaizhu Huang,
    and Amir Hussain. Cross-modality interactive attention network for multispectral
    pedestrian detection. Information Fusion, 50:20–29, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[450] Shanshan Zhang, Jian Yang, and Bernt Schiele. Occluded pedestrian detection
    through guided attention in cnns. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 6995–7003, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[451] Yuan Yuan, Zhitong Xiong, and Qi Wang. Vssa-net: vertical spatial sequence
    attention network for traffic sign detection. IEEE transactions on image processing,
    28(7):3423–3434, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[452] Zbigniew Wojna, Alexander N Gorban, Dar-Shyang Lee, Kevin Murphy, Qian
    Yu, Yeqing Li, and Julian Ibarz. Attention-based extraction of structured information
    from street view imagery. In 2017 14th IAPR International Conference on Document
    Analysis and Recognition (ICDAR), volume 1, pages 844–850\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[453] Pan He, Weilin Huang, Tong He, Qile Zhu, Yu Qiao, and Xiaolin Li. Single
    shot text detector with regional attention. In Proceedings of the IEEE international
    conference on computer vision, pages 3047–3055, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[454] Ankan Kumar Bhunia, Aishik Konwer, Ayan Kumar Bhunia, Abir Bhowmick,
    Partha P. Roy, and Umapada Pal. Script identification in natural scene image and
    video frames using an attention based convolutional-lstm network. Pattern Recognition,
    85:172–184, January 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[455] Siyue Xie, Haifeng Hu, and Yongbo Wu. Deep multi-path convolutional neural
    network joint with salient region attention for facial expression recognition.
    Pattern Recognition, 92:177–191, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[456] Shervin Minaee and Amirali Abdolrashidi. Deep-emotion: Facial expression
    recognition using attentional convolutional network. arXiv preprint arXiv:1902.01019,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[457] Yong Li, Jiabei Zeng, Shiguang Shan, and Xilin Chen. Occlusion aware
    facial expression recognition using cnn with attention mechanism. IEEE Transactions
    on Image Processing, 28(5):2439–2450, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[458] Fan Yang, Lianwen Jin, Songxuan Lai, Xue Gao, and Zhaohai Li. Fully convolutional
    sequence recognition network for water meter number reading. IEEE Access, 7:11679–11687,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[459] Dan Xu, Wanli Ouyang, Xavier Alameda-Pineda, Elisa Ricci, Xiaogang Wang,
    and Nicu Sebe. Learning deep structured multi-scale features using attention-gated
    crfs for contour prediction. In Advances in Neural Information Processing Systems,
    pages 3961–3970, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[460] De Xie, Cheng Deng, Hao Wang, Chao Li, and Dapeng Tao. Semantic adversarial
    network with multi-scale pyramid attention for video classification. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 33, pages 9030–9037,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[461] Zhi-Xuan Tan, Arushi Goel, Thanh-Son Nguyen, and Desmond C Ong. A multimodal
    lstm for predicting listener empathic responses over time. In 2019 14th IEEE International
    Conference on Automatic Face & Gesture Recognition (FG 2019), pages 1–4\. IEEE,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[462] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal,
    and Tamara L Berg. Mattnet: Modular attention network for referring expression
    comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 1307–1315, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[463] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald M Summers. Tienet:
    Text-image embedding network for common thorax disease classification and reporting
    in chest x-rays. In Proceedings of the IEEE conference on computer vision and
    pattern recognition, pages 9049–9058, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[464] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang,
    and Xiaodong He. Attngan: Fine-grained text to image generation with attentional
    generative adversarial networks. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 1316–1324, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[465] Jason Poulos and Rafael Valle. Character-based handwritten text transcription
    with attention networks. arXiv preprint arXiv:1712.04046, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[466] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson,
    Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning
    and visual question answering. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 6077–6086, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[467] Xinxin Zhu, Lixiang Li, Jing Liu, Ziyi Li, Haipeng Peng, and Xinxin Niu.
    Image captioning with triple-attention and stack parallel lstm. Neurocomputing,
    319:55–65, November 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[468] Shuang Ma, Jianlong Fu, Chang Wen Chen, and Tao Mei. Da-gan: Instance-level
    image translation by deep attention generative adversarial networks. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5657–5666,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[469] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. Knowing when
    to look: Adaptive attention via a visual sentinel for image captioning. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 375–383,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[470] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image
    captioning with semantic attention. In The IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), June 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[471] Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu,
    and Tat-Seng Chua. Sca-cnn: Spatial and channel-wise attention in convolutional
    networks for image captioning. In The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), July 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[472] Xinwei He, Yang Yang, Baoguang Shi, and Xiang Bai. Vd-san: Visual-densely
    semantic attention network for image caption generation. Neurocomputing, 328:48–55,
    February 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[473] Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Srivastava, Li Deng,
    Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C Platt, et al.
    From captions to visual concepts and back. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 1473–1482, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[474] Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, and William W.
    Cohen. Review networks for caption generation. arXiv:1605.07912 [cs], May 2016.
    arXiv: 1605.07912.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[475] Marco Pedersoli, Thomas Lucas, Cordelia Schmid, and Jakob Verbeek. Areas
    of attention for image captioning. arXiv:1612.01033 [cs], December 2016. arXiv:
    1612.01033.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[476] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Exploring visual relationship
    for image captioning. In Proceedings of the European conference on computer vision
    (ECCV), pages 684–699, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[477] Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang Gan, and Eric P Xing. Recurrent
    topic-transition gan for visual paragraph generation. In Proc. of the IEEE ICCV,
    pages 3362–3371, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[478] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt
    Schiele. Grounding of textual phrases in images by reconstruction. In European
    Conference on Computer Vision, pages 817–834. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[479] Lukasz Kaiser, Aidan N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar,
    Llion Jones, and Jakob Uszkoreit. One model to learn them all. arXiv preprint
    arXiv:1706.05137, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[480] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked
    cross attention for image-text matching. In Proceedings of the European Conference
    on Computer Vision (ECCV), pages 201–216, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[481] Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear attention networks
    for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pages 10971–10980, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[482] Kyunghyun Cho, Aaron Courville, and Yoshua Bengio. Describing multimedia
    content using attention-based encoder–decoder networks. IEEE Transactions on Multimedia,
    17(11):1875–1886, November 2015. arXiv: 1507.01053.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[483] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming
    Xiong. End-to-end dense video captioning with masked transformer. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8739–8748,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[484] Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 8746–8755, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[485] Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and Wei Xu. Video paragraph
    captioning using hierarchical recurrent neural networks. arXiv:1510.07712 [cs],
    October 2015. arXiv: 1510.07712.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[486] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles.
    Dense-captioning events in videos. In Proceedings of the IEEE international conference
    on computer vision, pages 706–715, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[487] Yi Bin, Yang Yang, Fumin Shen, Ning Xie, Heng Tao Shen, and Xuelong Li.
    Describing video with attention-based bidirectional lstm. IEEE transactions on
    cybernetics, 49(7):2631–2641, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[488] Lorenzo Baraldi, Costantino Grana, and Rita Cucchiara. Hierarchical boundary-aware
    neural encoder for video captioning. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 1657–1666, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[489] Silvio Olivastri, Gurkirt Singh, and Fabio Cuzzolin. An end-to-end baseline
    for video captioning. arXiv preprint arXiv:1904.02628, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[490] Zhong Ji, Kailin Xiong, Yanwei Pang, and Xuelong Li. Video summarization
    with attention-based encoder–decoder networks. IEEE Transactions on Circuits and
    Systems for Video Technology, 30(6):1709–1717, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[491] Jingkuan Song, Zhao Guo, Lianli Gao, Wu Liu, Dongxiang Zhang, and Heng Tao
    Shen. Hierarchical lstm with adjusted temporal attention for video captioning.
    arXiv preprint arXiv:1706.01231, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[492] Xiangpeng Li, Zhilong Zhou, Lijiang Chen, and Lianli Gao. Residual attention-based
    lstm for video captioning. World Wide Web, 22(2):621–636, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[493] Lianli Gao, Zhao Guo, Hanwang Zhang, Xing Xu, and Heng Tao Shen. Video
    captioning with attention-based lstm and semantic consistency. IEEE Transactions
    on Multimedia, 19(9):2045–2055, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[494] Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and Yong Xu. Bidirectional
    attentive fusion with context gating for dense video captioning. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7190–7198,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[495] Junyeong Kim, Minuk Ma, Trung Pham, Kyungsu Kim, and Chang D Yoo. Modality
    shifting attention network for multi-modal video question answering. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10106–10115,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[496] Ming Jiang, Shi Chen, Jinhui Yang, and Qi Zhao. Fantastic answers and
    where to find them: Immersive question-directed visual attention. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2980–2989,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[497] Junwei Liang, Lu Jiang, Liangliang Cao, Jia Li, and Alexander Haupmann.
    Focal visual-text attention for visual question answering. In CVPR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[498] Drew A Hudson and Christopher D Manning. Compositional attention networks
    for machine reasoning. arXiv preprint arXiv:1803.03067, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[499] Ahmed Osman and Wojciech Samek. Dual recurrent attention units for visual
    question answering. arXiv:1802.00209 [cs, stat], February 2018. arXiv: 1802.00209.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[500] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image
    co-attention for visual question answering. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
    I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems
    29, pages 289–297\. Curran Associates, Inc., 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[501] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear attention networks.
    In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
    editors, Advances in Neural Information Processing Systems 31, pages 1564–1574\.
    Curran Associates, Inc., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[502] Duy-Kien Nguyen and Takayuki Okatani. Improved fusion of visual and language
    representations by dense symmetric co-attention for visual question answering.
    In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[503] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Deep compositional
    question answering with neural module networks. CoRR, abs/1511.02799, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[504] Jin-Hwa Kim, Sang-Woo Lee, Donghyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo
    Ha, and Byoung-Tak Zhang. Multimodal residual learning for visual qa. In D. D.
    Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in
    Neural Information Processing Systems 29, pages 361–369\. Curran Associates, Inc.,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[505] Kevin J Shih, Saurabh Singh, and Derek Hoiem. Where to look: Focus regions
    for visual question answering. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 4613–4621, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[506] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w:
    Grounded question answering in images. arXiv:1511.03416 [cs], November 2015. arXiv:
    1511.03416.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[507] Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, and Dhruv Batra.
    Best of both worlds: Transferring knowledge from discriminative learning to a
    generative visual dialog model. arXiv preprint arXiv:1706.01554, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[508] Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao. Beyond
    bilinear: Generalized multimodal factorized high-order pooling for visual question
    answering. IEEE Trans. Neural Netw. Learn. Syst., 29(12):5947–5959, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[509] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi.
    Don’t just assume; look and answer: Overcoming priors for visual question answering.
    In Proc. of the IEEE CVPR, pages 4971–4980, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[510] Yan Zhang, Jonathon Hare, and Adam Prügel-Bennett. Learning to count
    objects in natural images for visual question answering. arXiv preprint arXiv:1802.05766,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[511] Zhe Gan, Yu Cheng, Ahmed El Kholy, Linjie Li, Jingjing Liu, and Jianfeng
    Gao. Multi-step reasoning via recurrent dual attention for visual dialog. arXiv
    preprint arXiv:1902.00579, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[512] Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, and Ram
    Nevatia. Abc-cnn: An attention based convolutional neural network for visual question
    answering. arXiv preprint arXiv:1511.05960, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[513] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided
    spatial attention for visual question answering. In European Conference on Computer
    Vision, pages 451–466. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[514] Yundong Zhang, Juan Carlos Niebles, and Alvaro Soto. Interpretable visual
    question answering by visual grounding from attention supervision mining. In 2019
    IEEE Winter Conference on Applications of Computer Vision (WACV), pages 349–357\.
    IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[515] Junwei Liang, Lu Jiang, Liangliang Cao, Yannis Kalantidis, Li-Jia Li,
    and Alexander G Hauptmann. Focal visual-text attention for memex question answering.
    IEEE PAMI, 41(8):1893–1908, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[516] Eun-Sol Kim, Woo Young Kang, Kyoung-Woon On, Yu-Jung Heo, and Byoung-Tak
    Zhang. Hypergraph attention networks for multimodal learning. In Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14581–14590,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[517] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked
    attention networks for image question answering. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 21–29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[518] Dongfei Yu, Jianlong Fu, Tao Mei, and Yong Rui. Multi-level attention
    networks for visual question answering. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 4709–4717, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[519] Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. Multi-modal factorized
    bilinear pooling with co-attention learning for visual question answering. In
    Proceedings of the IEEE international conference on computer vision, pages 1821–1830,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[520] Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. Stamp: short-term
    attention/memory priority model for session-based recommendation. In Proc. of
    the 24th ACM SIGKDD Int. Conf. on Knowledge Discovery & Data Mining, pages 1831–1839,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[521] Jingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and
    Tat-Seng Chua. Attentive collaborative filtering: Multimedia recommendation with
    item-and component-level attention. In Proceedings of the 40th International ACM
    SIGIR conference on Research and Development in Information Retrieval, pages 335–344,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[522] Hongyang Li, Jun Chen, Ruimin Hu, Mei Yu, Huafeng Chen, and Zengmin Xu.
    Action recognition using visual attention with reinforcement learning. In Ioannis
    Kompatsiaris, Benoit Huet, Vasileios Mezaris, Cathal Gurrin, Wen-Huang Cheng,
    and Stefanos Vrochidis, editors, MultiMedia Modeling, Lecture Notes in Computer
    Science, pages 365–376. Springer International Publishing, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[523] Yongming Rao, Jiwen Lu, and Jie Zhou. Attention-aware deep reinforcement
    learning for video face recognition. In Proceedings of the IEEE international
    conference on computer vision, pages 3931–3940, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[524] Marijn Stollenga, Jonathan Masci, Faustino Gomez, and Jürgen Schmidhuber.
    Deep networks with internal selective attention through feedback connections.
    arXiv preprint arXiv:1407.3068, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[525] Qingxing Cao, Liang Lin, Yukai Shi, Xiaodan Liang, and Guanbin Li. Attention-aware
    face hallucination via deep reinforcement learning. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 690–698, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[526] Donghui Hu, Shengnan Zhou, Qiang Shen, Shuli Zheng, Zhongqiu Zhao, and
    Yuqi Fan. Digital image steganalysis based on visual attention and deep reinforcement
    learning. IEEE Access, 7:25924–25935, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[527] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei. End-to-end
    learning of action detection from frame glimpses in videos. In 2016 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pages 2678–2687, Las Vegas,
    NV, USA, June 2016. IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[528] John Boaz Lee, Ryan Rossi, and Xiangnan Kong. Graph classification using
    structural attention. In Proc. of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining, pages 1666–1674, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[529] Emilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory
    for deep reinforcement learning. arXiv preprint arXiv:1702.08360, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[530] Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li,
    Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart,
    et al. Relational deep reinforcement learning. arXiv preprint arXiv:1806.01830,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[531] Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell,
    Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula.
    arXiv preprint arXiv:1909.07528, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[532] Tom Schaul, Tobias Glasmachers, and Jürgen Schmidhuber. High dimensions
    and heavy tails for natural evolution strategies. In Proceedings of the 13th annual
    conference on Genetic and evolutionary computation, pages 845–852, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[533] Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas
    Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation
    learning. In Advances in neural information processing systems, pages 1087–1098,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[534] Fei Xue, Xin Wang, Junqiu Wang, and Hongbin Zha. Deep visual odometry
    with adaptive memory. arXiv preprint arXiv:2008.01655, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[535] Adrian Johnston and Gustavo Carneiro. Self-supervised monocular trained
    depth estimation using self-attention and discrete disparity volume. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4756–4765,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[536] Xin-Yu Kuo, Chien Liu, Kai-Chen Lin, and Chun-Yi Lee. Dynamic attention-based
    visual odometry. In Proceedings of the IEEE/CVF CVPR Workshops, pages 36–37, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[537] Hamed Damirchi, Rooholla Khorrambakht, and Hamid D Taghirad. Exploring
    self-attention for visual odometry. arXiv preprint arXiv:2011.08634, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[538] Feng Gao, Jincheng Yu, Hao Shen, Yu Wang, and Huazhong Yang. Attentional
    separation-and-aggregation network for self-supervised depth-pose learning in
    dynamic scenes. arXiv preprint arXiv:2011.09369, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[539] Xiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, and
    Wanqing Li. Transformer guided geometry model for flow-based unsupervised visual
    odometry. Neural Computing and Applications, pages 1–12, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[540] Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid
    Rezatofighi, and Silvio Savarese. Sophie: An attentive gan for predicting paths
    compliant to social and physical constraints. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 1349–1358, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[541] Anirudh Vemula, Katharina Muelling, and Jean Oh. Social attention: Modeling
    attention in human crowds. In IEEE ICRA, pages 1–7\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[542] Changan Chen, Yuejiang Liu, Sven Kreiss, and Alexandre Alahi. Crowd-robot
    interaction: Crowd-aware robot navigation with attention-based deep reinforcement
    learning. In 2019 IEEE ICRA, pages 6015–6022\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[543] Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese. Scene memory
    transformer for embodied agents in long-horizon tasks. In Proceedings of the IEEE
    CVPR, pages 538–547, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[544] Xiaoxue Zang, Ashwini Pokle, Marynel Vázquez, Kevin Chen, Juan Carlos
    Niebles, Alvaro Soto, and Silvio Savarese. Translating navigation instructions
    in natural language to a high-level plan for behavioral robot navigation. arXiv
    preprint arXiv:1810.00663, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[545] Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks
    through representation erasure. arXiv preprint arXiv:1612.08220, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[546] Sofia Serrano and Noah A Smith. Is attention interpretable? arXiv preprint
    arXiv:1906.03731, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[547] Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention
    in a transformer language model. arXiv preprint arXiv:1906.04284, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[548] Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical
    nlp pipeline. arXiv preprint arXiv:1905.05950, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[549] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning.
    What does bert look at? an analysis of bert’s attention. arXiv preprint arXiv:1906.04341,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[550] Sarthak Jain and Byron C Wallace. Attention is not explanation. arXiv
    preprint arXiv:1902.10186, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[551] Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui.
    Attention interpretability across nlp tasks. arXiv preprint arXiv:1909.11218,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[552] Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. arXiv
    preprint arXiv:1908.04626, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[553] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing
    between capsules. arXiv preprint arXiv:1710.09829, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[554] Jaewoong Choi, Hyun Seo, Suii Im, and Myungjoo Kang. Attention routing
    between capsules. In Proceedings of the IEEE/CVF ICCV Workshops, pages 0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[555] Wenkai Huang and Fobao Zhou. Da-capsnet: dual attention mechanism capsule
    network. Scientific Reports, 10(1):1–13, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[556] Assaf Hoogi, Brian Wilcox, Yachee Gupta, and Daniel L Rubin. Self-attention
    capsule networks for image classification. arXiv preprint arXiv:1904.12483, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[557] Yao-Hung Hubert Tsai, Nitish Srivastava, Hanlin Goh, and Ruslan Salakhutdinov.
    Capsules with inverted dot-product attention routing. arXiv preprint arXiv:2002.04764,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[558] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature,
    521(7553):436–444, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[559] Hassan Khosravi and Bahareh Bina. A survey on statistical relational
    learning. In Canadian conference on artificial intelligence, pages 256–268\. Springer,
    2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[560] Tarek R Besold, Artur d’Avila Garcez, Sebastian Bader, Howard Bowman,
    Pedro Domingos, Pascal Hitzler, Kai-Uwe Kühnberger, Luis C Lamb, Daniel Lowd,
    Priscila Machado Vieira Lima, et al. Neural-symbolic learning and reasoning: A
    survey and interpretation. arXiv preprint arXiv:1711.03902, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[561] Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of
    logical rules for knowledge base reasoning. In Advances in Neural Information
    Processing Systems, pages 2319–2328, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[562] Peifeng Wang, Jialong Han, Chenliang Li, and Rong Pan. Logic attention
    based neighborhood aggregation for inductive knowledge graph embedding. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 33, pages 7152–7159,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[563] L Vivek Harsha Vardhan, Guo Jia, and Stanley Kok. Probabilistic logic
    graph attention networks for reasoning. In Companion Proceedings of the Web Conference
    2020, pages 669–673, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[564] Johan Ferret, Raphaël Marinier, Matthieu Geist, and Olivier Pietquin.
    Self-attentional credit assignment for transfer in reinforcement learning. arXiv
    preprint arXiv:1907.08027, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[565] Cynthia Rudin. Please stop explaining black box models for high stakes
    decisions. arXiv preprint arXiv:1811.10154, 1, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[566] Mark O Riedl. Human-centered artificial intelligence and machine learning.
    Human Behavior and Emerging Technologies, 1(1):33–36, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[567] Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, and Xilin Chen.
    Attgan: Facial attribute editing by only changing what you want. IEEE Transactions
    on Image Processing, 28(11):5464–5478, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[568] Biao Zhang, Deyi Xiong, and Jinsong Su. Battrae: Bidimensional attention-based
    recursive autoencoders for learning bilingual phrase embeddings. In Proceedings
    of the AAAI Conference on Artificial Intelligence, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[569] Tian Tian and Zheng Felix Fang. Attention-based autoencoder topic model
    for short texts. Procedia Computer Science, 151:1134–1139, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
