- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:31:39'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2406.08115] Resource Allocation and Workload Scheduling for Large-Scale Distributed
    Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.08115](https://ar5iv.labs.arxiv.org/html/2406.08115)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '10081'
  prefs: []
  type: TYPE_NORMAL
- en: 'Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep
    Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feng Liang [fliang@smbu.edu.cn](mailto:fliang@smbu.edu.cn) [0000-0002-8542-9871](https://orcid.org/0000-0002-8542-9871
    "ORCID identifier") Artificial Intelligence Research Institute, Shenzhen MSU-BIT
    UniversityChina518107 Guangdong-Hong Kong-Macao Joint Laboratory for Emotional
    Intelligence and Pervasive Computing, Shenzhen MSU-BIT UniversityChina518107 , 
    Zhen Zhang [zhangzhen19@lzu.edu.cn](mailto:zhangzhen19@lzu.edu.cn) [0009-0007-9955-0916](https://orcid.org/0009-0007-9955-0916
    "ORCID identifier") ,  Haifeng Lu [luhf18@lzu.edu.cn](mailto:luhf18@lzu.edu.cn)
    [0000-0003-0155-8447](https://orcid.org/0000-0003-0155-8447 "ORCID identifier")
    Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science
    and Engineering, Lanzhou UniversityChina730000 Guangdong-Hong Kong-Macao Joint
    Laboratory for Emotional Intelligence and Pervasive Computing, Shenzhen MSU-BIT
    UniversityChina518107 ,  Chengming Li [licm@smbu.edu.cn](mailto:licm@smbu.edu.cn)
    [0000-0002-8542-9871](https://orcid.org/0000-0002-8542-9871 "ORCID identifier")
    Artificial Intelligence Research Institute, Shenzhen MSU-BIT UniversityChina518107
    Guangdong-Hong Kong-Macao Joint Laboratory for Emotional Intelligence and Pervasive
    Computing, Shenzhen MSU-BIT UniversityChina518107 ,  Victor C. M. Leung [vleung@ieee.org](mailto:vleung@ieee.org)
    [0000-0003-3529-2640](https://orcid.org/0000-0003-3529-2640 "ORCID identifier")
    Artificial Intelligence Research Institute, Shenzhen MSU-BIT UniversityChina518107
    Department of Electrical and Computer Engineering, The University of British ColumbiaCanadaV6T
    1Z4 ,  Yanyi Guo [guoyy@smbu.edu.cn](mailto:guoyy@smbu.edu.cn) [0009-0000-7682-6667](https://orcid.org/0009-0000-7682-6667
    "ORCID identifier") Frontier Cross Disciplinary Research Institute, Shenzhen MSU-BIT
    UniversityChina School of Mechanical and Electrical Engineering, Beijing Institute
    of TechnologyChina  and  Xiping Hu [huxp@smbu.edu.cn](mailto:huxp@smbu.edu.cn)
    [0000-0002-4952-699X](https://orcid.org/0000-0002-4952-699X "ORCID identifier")
    Artificial Intelligence Research Institute, Shenzhen MSU-BIT UniversityChina518107
    Guangdong-Hong Kong-Macao Joint Laboratory for Emotional Intelligence and Pervasive
    Computing, Shenzhen MSU-BIT UniversityChina518107 School of Medical Technology,
    Beijing Institute of TechnologyChina10081
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With rapidly increasing distributed deep learning workloads in large-scale data
    centers, efficient distributed deep learning framework strategies for resource
    allocation and workload scheduling have become the key to high-performance deep
    learning. The large-scale environment with large volumes of datasets, models,
    and computational and communication resources raises various unique challenges
    for resource allocation and workload scheduling in distributed deep learning,
    such as scheduling complexity, resource and workload heterogeneity, and fault
    tolerance. To uncover these challenges and corresponding solutions, this survey
    reviews the literature, mainly from 2019 to 2024, on efficient resource allocation
    and workload scheduling strategies for large-scale distributed DL. We explore
    these strategies by focusing on various resource types, scheduling granularity
    levels, and performance goals during distributed training and inference processes.
    We highlight critical challenges for each topic and discuss key insights of existing
    technologies. To illustrate practical large-scale resource allocation and workload
    scheduling in real distributed deep learning scenarios, we use a case study of
    training large language models. This survey aims to encourage computer science,
    artificial intelligence, and communications researchers to understand recent advances
    and explore future research directions for efficient framework strategies for
    large-scale distributed deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed deep learning, Resource allocation, GPU sharing, Task scheduling,
    Large model, Pipeline parallelism^†^†ccs: Computer systems organization Distributed
    architectures^†^†ccs: Computing methodologies Machine learning^†^†ccs: Computing
    methodologies Artificial intelligence^†^†ccs: Networks Cloud computing'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the rapid increase in the sizes of datasets and deep learning (DL) models,
    distributed DL (Liang et al., [2024](#bib.bib65); Yu et al., [2023](#bib.bib137))
    has become the state-of-the-art practice for various artificial intelligence technologies,
    federated learning (Liu et al., [2022b](#bib.bib68)) and smart Internet of Things (Al-Garadi
    et al., [2020](#bib.bib6)). In contrast to traditional single-node DL that works
    on a single computing node or even a single GPU, distributed DL can leverage multiple
    GPUs and computing nodes to handle massive training and inference workloads and
    improve learning throughput. Notably, in the era of extremely large models with
    tens of billions of parameters, distributed DL enables efficient large-model training (et al.,
    [2023](#bib.bib27)) across hundreds of computing nodes with thousands of GPUs
    in the data center.
  prefs: []
  type: TYPE_NORMAL
- en: However, distributed DL faces numerous critical challenges related to efficient
    framework strategies for resource allocation and workload scheduling in large-scale
    environments. Firstly, with a large number of computational and communication
    devices in the data center for distributed DL, managing and allocating resources
    efficiently for distributed DL workloads to utilize resources fully becomes challenging.
    This challenge is amplified in heterogeneous resource environments, where GPUs
    have various computational capacities and networks have various communication
    capacities and topologies. Secondly, distributed DL workloads exhibits more complicated
    characteristics than those of single-node DL. On the one hand, various parallelism
    modes of distributed DL workloads give rise to new communication patterns involving
    significant communication overhead for data transfer and model synchronization.
    On the other hand, the combination of many computational and communication tasks
    in distributed DL workloads complicates the execution dependency paradigm, which
    allows for significant optimization. Thirdly, the exponential increase in large
    model sizes raises concerns about the cost of computational and communication
    resources and efficiency of distributed training in a large scale. Tackling these
    challenges is urgent and requires researchers in the fields of computer sciences,
    artificial intelligence, and communications to understand critical problems in
    this domain systematically.
  prefs: []
  type: TYPE_NORMAL
- en: Several existing surveys (Zhou et al., [2019](#bib.bib151); Mayer and Jacobsen,
    [2020](#bib.bib78); Shi et al., [2020](#bib.bib101); Ouyang et al., [2021](#bib.bib85);
    Chen et al., [2021](#bib.bib17); Yu et al., [2023](#bib.bib137); Cao et al., [2023](#bib.bib15);
    Tang et al., [2023a](#bib.bib110); Ye et al., [2024a](#bib.bib134)) have touched
    on some topics of efficient resource allocation and workload scheduling strategies
    for distributed DL. For example, Ye et al. have introduced scheduling distributed
    training and inference workloads on GPUs at the job level. However, these surveys
    lack a systematic exploration of distributed DL framework strategies for scheduling
    computational and communication resources and workloads at various granularity
    levels in large-scale environments. Researchers in the fields of computer science,
    artificial intelligence, and communications need a comprehensive understanding
    of representative and critical challenges for framework strategies in large-scale
    distributed DL environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'To fill the gap in existing surveys on distributed DL framework strategies,
    this survey systematically investigates critical challenges and efficient distributed
    DL strategies for resource allocation and workload scheduling. We review the literature
    over the period mainly between the year 2019 and 2024. The discussion covers various
    resource types, scheduling granularity levels, and performance goals. For resource
    allocation strategies, we discuss GPU sharing technologies applying different
    approaches and network bandwidth-sharing technologies working at different granularity
    levels. For workload scheduling strategies, we categorize the technologies based
    on various performance goals and scheduling granularities. Both sets of strategies
    organized primarily based on the application stage: distributed training and inference.
    Focusing on efficient strategies for large-scale distributed DL, we highlight
    key challenges for each topic and provide insights about crucial research outputs.
    To illustrate how to apply these efficient framework strategies practically in
    real life, we conduct a case study on large-model distributed training, a rapidly
    trending and probably long-lasting research topic and one of the best application
    scenarios that require efficient distributed DL framework strategies. We also
    provide outlooks for future research directions.'
  prefs: []
  type: TYPE_NORMAL
- en: The major contribution of this survey is summarized as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We thoroughly and comprehensively survey up-to-date resource allocation and
    workload scheduling framework strategies for large-scale distributed DL.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We highlight critical challenges for each topic of these framework strategies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use a case study on large-model training to illustrate how to apply efficient
    framework strategies in practice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Table 1\. A Comparison of Related Surveys
  prefs: []
  type: TYPE_NORMAL
- en: '|   |  | Resource Allocation | Workload Scheduling |'
  prefs: []
  type: TYPE_TB
- en: '| Ref. | Year | GPU Allocation | Network Bandwidth Allocation | Job Scheduling
    | Pipeline Scheduling | Network Flow Scheduling |'
  prefs: []
  type: TYPE_TB
- en: '|    (Mayer and Jacobsen, [2020](#bib.bib78)) | 2020 | ✓ |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| (Shi et al., [2020](#bib.bib101)) | 2020 |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| (Ouyang et al., [2021](#bib.bib85)) | 2021 |  |  |  | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| (Yu et al., [2023](#bib.bib137)) | 2023 |  |  |  | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| (Cao et al., [2023](#bib.bib15)) | 2023 |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| (Tang et al., [2023a](#bib.bib110)) | 2023 |  |  |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (Ye et al., [2024a](#bib.bib134)) | 2024 | ✓ |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| (Liang et al., [2024](#bib.bib65)) | 2024 |  | ✓ |  |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|   Ours | - | ✓ | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 1.1\. Related Surveys
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Existing surveys on distributed DL lack systematic coverage over strategies
    for resource allocation of various resource types and workload scheduling at various
    levels. Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣ Resource Allocation and
    Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey") compares
    our survey with other related surveys on the covered topics in the domain of distributed
    DL framework strategies, including resource allocation based on the GPU and network
    bandwidth and workload scheduling based on the job, pipeline, and network flow.'
  prefs: []
  type: TYPE_NORMAL
- en: Some surveys focus on scheduling distributed jobs on GPU data centers. Both
    Mayer and Jacobsen (Mayer and Jacobsen, [2020](#bib.bib78)) and Ye et al. (Ye
    et al., [2024a](#bib.bib134)) have conducted surveys on job-level GPU allocation
    and workload scheduling for distributed training and inference in the data center
    environment. However, these surveys concern only about job-level strategies focusing
    on the overall performance of the entire data center but not finer-grained strategies
    focusing on individual job performance. They also only investigate technologies
    related to the single resource type, the GPU, but not the network bandwidth, which
    is a significant performance factor for distributed DL with communication as the
    bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the GPU-centric surveys, some works focus on communications technologies
    and network bandwidth-allocation strategies for distributed DL. Both Shi et al. (Shi
    et al., [2020](#bib.bib101)) and Cao et al. (Cao et al., [2023](#bib.bib15)) have
    reviewed the literature on bandwidth allocation strategies for federating learning
    over wireless networks. Liang et al. (Liang et al., [2024](#bib.bib65)) have not
    only investigated bandwidth-allocation strategies on general networks but also
    studied network-flow-scheduling strategies on different network layers. However,
    covering communications-only technologies does not reveal the whole picture of
    efficient scheduling in distributed DL, which requires the joint optimization
    of computation and communication.
  prefs: []
  type: TYPE_NORMAL
- en: For finer-grained workload scheduling in distributed DL, some works (Ouyang
    et al., [2021](#bib.bib85); Yu et al., [2023](#bib.bib137); Tang et al., [2023a](#bib.bib110))
    explore pipeline-level scheduling strategies for overlapping computation and communication
    workloads to improve throughput. However, they do not highlight primary challenges
    related to this topic and lack the investigation into job-level resource allocation
    and workload scheduling strategies, which can orchestrate with these pipeline-level
    strategies as a synthesis framework solution for distributed DL in data centers.
  prefs: []
  type: TYPE_NORMAL
- en: Our survey fills the gap in existing surveys. We instigate resource allocation
    strategies for both computational and communication resources, primarily the GPU
    and network bandwidth, to match the resource requirements of distributed DL workloads.
    We also explore workload scheduling strategies at the job, pipeline, and network
    flow levels to improve both overall data center throughput and individual job
    efficiency. The systematic literature study involving various resource types and
    scheduling granularity levels makes this article a comprehensive survey of up-to-date
    technologies in the distributed DL framework domain.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/302454633bb89b4b7bd583de75569faf.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. The organization of the survey
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Survey Organization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Fig. [1](#S1.F1 "Figure 1 ‣ 1.1\. Related Surveys ‣ 1\. Introduction ‣ Resource
    Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning:
    A Survey") outlines the organization of the remaining sections in this survey.
    Section [2](#S2 "2\. Fundamentals of DL and Distributed DL ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey")
    provides fundamental knowledge about distributed DL and the resource-management
    and workload scheduling framework. Sections [3](#S3 "3\. Resource Allocation ‣
    Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning:
    A Survey") and [4](#S4 "4\. Workload Scheduling ‣ Resource Allocation and Workload
    Scheduling for Large-Scale Distributed Deep Learning: A Survey") introduce various
    framework strategies for resource allocation and workload scheduling, respectively.
    These framework strategies are categorized primarily based on their application
    scenarios, including distributed training and inference, and secondarily based
    on resource types, approaches, or performance goals. We discuss the insights at
    the end of each section. We use a case study of distributed large-model training
    to show how to apply these framework strategies practically in real-life data
    centers in Section [5](#S5 "5\. Distributed Training of LLMs: A Case Study ‣ Resource
    Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning:
    A Survey"). In Section [6](#S6 "6\. Conclusion and Outlook ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey"),
    we conclude this survey and present outlooks of future research directions.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. A List of Frequently Used Abbreviations
  prefs: []
  type: TYPE_NORMAL
- en: '|    Abbreviation | Description |'
  prefs: []
  type: TYPE_TB
- en: '|   DL | Deep Learning |'
  prefs: []
  type: TYPE_TB
- en: '| DRL | Deep Reinforcement Learning |'
  prefs: []
  type: TYPE_TB
- en: '| DNN | Deep Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| GPU | Graphics Processing Unit |'
  prefs: []
  type: TYPE_TB
- en: '| LLM | Large Language Model |'
  prefs: []
  type: TYPE_TB
- en: '| MPS | Multiple Process Sharing |'
  prefs: []
  type: TYPE_TB
- en: '| NLP | Natural Language Processing |'
  prefs: []
  type: TYPE_TB
- en: '| PS | Parameter Server |'
  prefs: []
  type: TYPE_TB
- en: '| SLO | Service-Level Objective |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |'
  prefs: []
  type: TYPE_TB
- en: 2\. Fundamentals of DL and Distributed DL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we introduce the fundamental knowledge of DL, distributed
    DL, and the resource allocation and workload scheduling framework for distributed
    DL. Table [2](#S1.T2 "Table 2 ‣ 1.2\. Survey Organization ‣ 1\. Introduction ‣
    Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning:
    A Survey") lists frequently used abbreviations used in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DL is a subfield of machine learning that utilizes deep artificial neural networks,
    also known as deep neural networks (DNN), to extract complex patterns from training
    data in a hierarchical manner. The trained DNN is capable to recognize/predict
    patterns in unseen data. DL has been used in various fields, including NLP (Bahdanau
    et al., [2015](#bib.bib10)), computer vision (Dosovitskiy et al., [2020](#bib.bib24)),
    and biomedical engineering (Thieme et al., [2023](#bib.bib112)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d091d6bbd7853048f68ba8f4b3dc628c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Fully Connected DNN
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/310eef3acdc5226a5be1ea92d3e41042.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) CNN
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3f583d867a1bc125d41a38e9e624ab5f.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) RNN
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2\. Common artificial neural network models for DL
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1\. DL models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A DNN consists of multiple hidden layers. Each layer is comprised of neurons,
    which are typically activated by non-linear functions. Based on the connections
    between neurons within and between layers, there can be various types of DNN models.
    In this survey, when referring to models or DL models, we mean DNNs unless the
    context otherwise specifies. Fig. [2](#S2.F2 "Figure 2 ‣ 2.1\. Deep Learning ‣
    2\. Fundamentals of DL and Distributed DL ‣ Resource Allocation and Workload Scheduling
    for Large-Scale Distributed Deep Learning: A Survey") illustrates three basic
    DNN models: the fully connected DNN, convolutional neural network (CNN), and recurrent
    neural network (RNN).'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Fully connected DNN: The fully connected DNN, also known as the feedforward
    neural network, constitutes a dense network with an input layer, a number of hidden
    layers, and an output layer, as depicted in Fig. [2a](#S2.F2.sf1 "In Figure 2
    ‣ 2.1\. Deep Learning ‣ 2\. Fundamentals of DL and Distributed DL ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey").
    Neurons in a preceding layer connect to all neurons in the subsequent layer, and
    each connection has a learnable weight parameter indicating the strength of the
    connection. This architecture enables the fully connected DNN to capture complex
    relationships within data, finding extensive application in tasks such as classification (Sagduyu
    et al., [2023](#bib.bib94)), regression (Saikia et al., [2022](#bib.bib95)), and
    feature representation embedding (Ferrand et al., [2020](#bib.bib30)).'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ CNN: CNN stands as a prevalent model designed for feature extraction
    and classification, primarily tailored for image and video data. As depicted in
    Fig. [2b](#S2.F2.sf2 "In Figure 2 ‣ 2.1\. Deep Learning ‣ 2\. Fundamentals of
    DL and Distributed DL ‣ Resource Allocation and Workload Scheduling for Large-Scale
    Distributed Deep Learning: A Survey"), CNN comprises a stack of convolutional
    layers and pooling layers for context feature extraction. Unlike the fully connected
    layer, which assigns a weight parameter to each neuron connection, the convolutional
    layer substantially reduces the number of weight parameters by utilizing a number
    of kernels, each containing shared weights for feature extraction. The feature
    extraction process of the convolutional layer’s feature extraction process is
    empowered by the convolution operation, wherein kernels traverse the receptive
    fields of an image, extracting new features through weighted summations followed
    by a non-linear activation function. The pooling layer downsamples the data in
    the convolutional layer to reduce feature dimensions and alleviate overfitting
    issues. CNN has found widespread applications in various computer vision tasks,
    including image classification (Amerini et al., [2019](#bib.bib7)), semantic segmentation (Zhang
    et al., [2022](#bib.bib143)), and object detection (Bi et al., [2022](#bib.bib13)).'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ RNN: RNN, a DL model that deals with sequential data like time-series
    data, natural language, and speech audio, is illustrated in Fig. [2c](#S2.F2.sf3
    "In Figure 2 ‣ 2.1\. Deep Learning ‣ 2\. Fundamentals of DL and Distributed DL
    ‣ Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep
    Learning: A Survey"). The general architecture of RNN includes hidden units that
    capture and propagate temporal context from the input sequence to subsequent hidden
    unites. It updates continuously and utilizes the temporal context based on the
    current input and previous temporal context to make predictions. To address the
    challenge of capturing long-range temporal dependencies, two common variants of
    RNNs, known as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), have
    been developed, providing a trade-off between modeling such dependencies and reducing
    computation complexity effectively. Common applications of RNN include tasks such
    as time series forecasting (Hua et al., [2019](#bib.bib45)), NLP (Feng et al.,
    [2020](#bib.bib28)), and automated planning (Say, [2021](#bib.bib96)).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2\. Training and inference
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The training of a DL model is the process of optimizing its parameters to minimize
    the prediction error on a training dataset, as determined by a specified loss
    function, or objective function. Loss functions can be either convex or non-convex,
    leading to convex or non-convex optimization problems. Training can be decomposed
    into two key processes: feedforward and backpropagation. In the feedforward process,
    training data are passed into the model’s input layer, and the output prediction
    is computed by forwarding data through the network using the current model parameters.
    In the backpropagation process, the prediction error and gradients are calculated
    with respect to the loss function, and trainable parameters are updated iteratively
    in a backward manner, optimizing the model for the minimum loss. Common optimizers
    for backpropagation updating include mini-batch Stochastic Gradient Descent (SGD) (Li
    et al., [2014](#bib.bib59)), SGD with momentum (Sutskever et al., [2013](#bib.bib106)),
    Adagrad (Duchi et al., [2011](#bib.bib26)), and Adam (Kingma and Ba, [2015](#bib.bib56)).
    The training process usually operates on batches of training data iteratively
    over multiple epochs until the model converges. A model is said to have converged
    when the training error settles to within a predefined error range, and additional
    training will not further decrease the error. After completing the training process,
    the weight parameters in the DL model are learned and fixed. Following the training
    process, there is typically a validation process for validating the performance
    of the trained model, providing information for fine-tuning hyperparameters and
    retraining the model for better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: The inference process passes forward unseen data through the trained DL model
    to make predictions. Depending on the specific requirements of an application,
    the resulting prediction can be extracted either either from the output layer
    or from the predicted latent representation in an intermediate hidden layer. For
    example, in the context of network traffic analysis, an end-to-end DNN model may
    be trained to classify traffic types directly (Lotfollahi et al., [2020](#bib.bib75)),
    Alternatively, an encoder-decoder model trained on traffic data can utilize the
    latent representation generated by the encoder for subsequent tasks such as attack
    detection (Vu et al., [2020](#bib.bib117)).
  prefs: []
  type: TYPE_NORMAL
- en: Computing tasks related to a specific portion of the DL model for specific epochs
    during the training or inference process are generally referred to as DL tasks
    in this survey, when it is not necessary to distinguish training tasks and inference
    tasks in the context.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Distributed DL Parallelism Modes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Distributed DL partitions data and models into multiple processing units (typically
    GPUs) for parallel execution to leverage the computational capacity of many computing
    nodes in a cluster. As illustrated in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2\. Distributed
    DL Parallelism Modes ‣ 2\. Fundamentals of DL and Distributed DL ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey"),
    distributed DL has three basic parallelism modes: data parallelism, model parallelism,
    and pipeline parallelism.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec460a3b9ca782ade0e5a1091d8be231.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Data Parallelism
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f15f2f7a8a6be2e0a0ac238194239e20.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Model Parallelism
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e2365c3f9ce66b00ac4682b176b1452c.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Pipeline Parallelism
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3\. Parallelism modes of distributed DL
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1\. Data parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As illustrated in Fig. [3a](#S2.F3.sf1 "In Figure 3 ‣ 2.2\. Distributed DL
    Parallelism Modes ‣ 2\. Fundamentals of DL and Distributed DL ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey"),
    data-parallel training partitions the entire training dataset into several splits
    and distributes them across many GPUs for parallel training (Li et al., [2020c](#bib.bib61)).
    Each GPU has a replicate of a whole model with an identical structure and trains
    it on a specific dataset partition. Throughout the distributed training process,
    these local models share their knowledge to update a global model using a specific
    model synchronization mechanism, usually via a parameter server (PS).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2\. Model parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As illustrated in Fig. [3b](#S2.F3.sf2 "In Figure 3 ‣ 2.2\. Distributed DL
    Parallelism Modes ‣ 2\. Fundamentals of DL and Distributed DL ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey"),
    model-parallel training divides an entire DL model into submodels and distributes
    them onto many GPUs within a cluster when the model exceeds the capacity of a
    single GPU or computing node (Gomez et al., [2022](#bib.bib36)). This parallelism
    mode concerns the model division strategy focusing on workload balance and the
    submodel placement strategy focusing on communication overhead between submodels.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3\. Pipeline parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As illustrated in Fig. [3c](#S2.F3.sf3 "In Figure 3 ‣ 2.2\. Distributed DL
    Parallelism Modes ‣ 2\. Fundamentals of DL and Distributed DL ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey"),
    pipeline-parallel training enhances DL parallelism by ordering different stages
    of distributed training in a pipeline and preventing computational and communication
    resources from idling (Li and Hoefler, [2021](#bib.bib60); Liu et al., [2022c](#bib.bib73);
    Oh et al., [2022](#bib.bib84)). Pipeline-parallel training can be considered a
    special case of model-parallel training by decomposing the training of submodels
    layer by layer into subtasks and overlapping their computation of different stages
    across different GPUs. Computational and communication tasks can also overlap
    in the pipeline. This mode is typically applicable in the domains of LLM training (Narayanan
    et al., [2021](#bib.bib82)), edge computing (Yao et al., [2022](#bib.bib133)),
    and the Internet of Things (Baccour et al., [2022](#bib.bib9)), where devices
    have heterogeneous computational and communication capabilities to handle various
    distributed DL subtasks. In practice, pipeline parallelism can work with other
    parallelism modes to tackle complex distributed DL workloads with large model
    structures (Tarnawski et al., [2021](#bib.bib111); Ryabinin et al., [2023](#bib.bib93)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/26ec23f00301d8fcbfcc736ce4790460.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. An overview illustration of resource allocation and workload scheduling
    mechanisms in distributed DL
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Resource Allocation and Workload Scheduling for Distributed DL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Resource allocation and workload scheduling strategies for high-performance
    distributed DL are typically integrated in cluster-level and distributed-DL-level
    frameworks. Fig. [4](#S2.F4 "Figure 4 ‣ 2.2.3\. Pipeline parallelism ‣ 2.2\. Distributed
    DL Parallelism Modes ‣ 2\. Fundamentals of DL and Distributed DL ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey")
    illustrates the procedure of resource allocation and workload scheduling mechanisms
    for large-scale distributed DL within a cluster. This procedure comprises six
    major steps. (1) For a queue of distributed DL jobs, the task scheduler conducts
    job profiling based on various workload characters, such as resource-utilization
    status and working progress. (2) The resource manager allocates GPU and network
    resources distributed within the cluster for jobs based on their characteristic
    profiling. The resources can be represented in a physical or virtual manner, and
    virtual resources can be encapsulated in virtual machines or containers. (3) The
    job-level scheduler determines job-execution priorities based on resource constraints
    and job performance estimation. (4) The pipeline-level scheduler divides the job
    into subtasks and locates them onto available resources for the pipeline execution
    of the subtasks, aiming to increase task parallelism and overlap computational
    and communication workloads. (5) The network-flow-level scheduler optimizes the
    network flows or coflows of numerous subtasks by considering the relation and
    dependency of network flows. (6) Scheduled jobs, pipelines, and network flows
    run efficiently on the allocated GPU and network resources within the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Studies on Resource Allocation Strategies for Large-Scale Distributed
    DL
  prefs: []
  type: TYPE_NORMAL
- en: '|    Category | Ref. | Year | Highlight |'
  prefs: []
  type: TYPE_TB
- en: '|    Training ([3.1](#S3.SS1 "3.1\. Resource Allocation for Distributed Training
    ‣ 3\. Resource Allocation ‣ Resource Allocation and Workload Scheduling for Large-Scale
    Distributed Deep Learning: A Survey")) | GPU Sharing. Approach: (1) Workload Profiling
    [[C1]](#S3.SS1.SSS1.tab1 "3.1.1\. GPU sharing: ‣ 3.1\. Resource Allocation for
    Distributed Training ‣ 3\. Resource Allocation ‣ Resource Allocation and Workload
    Scheduling for Large-Scale Distributed Deep Learning: A Survey") ([3.1.1](#S3.SS1.SSS1
    "3.1.1\. GPU sharing: ‣ 3.1\. Resource Allocation for Distributed Training ‣ 3\.
    Resource Allocation ‣ Resource Allocation and Workload Scheduling for Large-Scale
    Distributed Deep Learning: A Survey")) | Gandiva (Xiao et al., [2018](#bib.bib128))
    | 2018 | Using the profiles of the DL workload to improve efficiency of training
    DL models and latency in a GPU cluster. |'
  prefs: []
  type: TYPE_TB
- en: '| AntMan (Xiao et al., [2020](#bib.bib129)) | 2020 | Introducing co-designing
    the cluster scheduler and dynamic scaling mechanisms. |'
  prefs: []
  type: TYPE_TB
- en: '|  | FGD (Weng et al., [2023](#bib.bib126)) | 2023 | Monitoring the individual
    evaluation functions of DL jobs at runtime to make placement decisions and resource
    allocations elastically. |'
  prefs: []
  type: TYPE_TB
- en: '|  | TGS (Wu et al., [2023](#bib.bib127)) | 2023 | Designing adaptive rate-control
    and transparent unified-memory mechanisms . |'
  prefs: []
  type: TYPE_TB
- en: '|  | Orion (Strati et al., [2024](#bib.bib103)) | 2024 | Co-scheduling GPU
    kernels based on the computation and memory profiles of DNN workloads. |'
  prefs: []
  type: TYPE_TB
- en: '| (2) Context Switching [[C2]](#S3.SS1.SSS1.tab2 "3.1.1\. GPU sharing: ‣ 3.1\.
    Resource Allocation for Distributed Training ‣ 3\. Resource Allocation ‣ Resource
    Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning:
    A Survey") ([3.1.1](#S3.SS1.SSS1 "3.1.1\. GPU sharing: ‣ 3.1\. Resource Allocation
    for Distributed Training ‣ 3\. Resource Allocation ‣ Resource Allocation and Workload
    Scheduling for Large-Scale Distributed Deep Learning: A Survey")) | Salus (Yu
    and Chowdhury, [2020](#bib.bib139)) | 2019 | Achieving fine-grained GPU sharing
    among multiple DL applications. |'
  prefs: []
  type: TYPE_TB
- en: '| PipeSwitch (Bai et al., [2020](#bib.bib11)) | 2020 | Exploiting the profiles
    of DL applications to achieve millisecond-scale context switching. |'
  prefs: []
  type: TYPE_TB
- en: '|  | DistMind (Jin et al., [2024](#bib.bib54)) | 2024 | Exposing the abstractions
    of a GPU pool and a memory pool and designing high-performance three-stage pipelining.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | G-Safe (Pavlidakis et al., [2024](#bib.bib87)) | 2024 | Offering transparent
    memory protection for context switching. |'
  prefs: []
  type: TYPE_TB
- en: '| (3) Performance Estimating [[C3]](#S3.SS1.SSS1.tab3 "3.1.1\. GPU sharing:
    ‣ 3.1\. Resource Allocation for Distributed Training ‣ 3\. Resource Allocation
    ‣ Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep
    Learning: A Survey") ([3.1.1](#S3.SS1.SSS1 "3.1.1\. GPU sharing: ‣ 3.1\. Resource
    Allocation for Distributed Training ‣ 3\. Resource Allocation ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey"))
    | Optimus (Peng et al., [2018](#bib.bib88)) | 2018 | Estimating a DL task’s remaining
    execution time and designing a marginal gain-based allocation algorithm. |'
  prefs: []
  type: TYPE_TB
- en: '| Harmony (Bao et al., [2019](#bib.bib12)) | 2019 | Placing training jobs in
    a manner that minimizes interference and maximizes performance. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Horus (Yeung et al., [2021](#bib.bib136)) | 2021 | Proposing a data-driven
    approach to predict the GPU utilization of heterogeneous DL tasks. |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPARS (Wang et al., [2024](#bib.bib122)) | 2024 | Leveraging spatiotemporal
    correlations among jobs to allocate suitable GPU types for newly submitted jobs.
    |'
  prefs: []
  type: TYPE_TB
- en: '| (4) Elastic Scaling [[C4]](#S3.SS1.SSS1.tab4 "3.1.1\. GPU sharing: ‣ 3.1\.
    Resource Allocation for Distributed Training ‣ 3\. Resource Allocation ‣ Resource
    Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning:
    A Survey") ([3.1.1](#S3.SS1.SSS1 "3.1.1\. GPU sharing: ‣ 3.1\. Resource Allocation
    for Distributed Training ‣ 3\. Resource Allocation ‣ Resource Allocation and Workload
    Scheduling for Large-Scale Distributed Deep Learning: A Survey")) | Pollux (Qiao
    et al., [2021](#bib.bib89)) | 2021 | Combining system throughput with statistical
    efficiency and introducing a formulation of goodput. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Zico (Lim et al., [2021](#bib.bib67)) | 2021 | Monitoring the memory-usage
    pattern of individual DL jobs by tracking computational progress of training jobs.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | AFS (Hwang et al., [2021](#bib.bib48)) | 2021 | Handling future jobs requires
    proactive preparation based on current share calculations. |'
  prefs: []
  type: TYPE_TB
- en: '|  | EasyScale (Li et al., [2023a](#bib.bib58)) | 2023 | Utilizing a thread
    abstraction called EasyScaleThread to preserve consistent training accuracy with
    scalable GPUs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | FlowCon (Mao et al., [2023](#bib.bib77)) | 2023 | Minimizing the growth
    of GPU fragmentation through packing tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| (5) For Hyperparameter Tuning [[C5]](#S3.SS1.SSS1.tab5 "3.1.1\. GPU sharing:
    ‣ 3.1\. Resource Allocation for Distributed Training ‣ 3\. Resource Allocation
    ‣ Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep
    Learning: A Survey") ([3.1.1](#S3.SS1.SSS1 "3.1.1\. GPU sharing: ‣ 3.1\. Resource
    Allocation for Distributed Training ‣ 3\. Resource Allocation ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey"))
    | Fluid (Yu et al., [2021a](#bib.bib140)) | 2021 | Utilizing a water-filling approach
    to accelerate the hyperparameter optimization process. |'
  prefs: []
  type: TYPE_TB
- en: '| Titan (Gao et al., [2022](#bib.bib32)) | 2022 | Merging several fine-tuning
    workloads into one to improve resource utilization. |'
  prefs: []
  type: TYPE_TB
- en: '|  | DISC (Liu et al., [2022d](#bib.bib70)) | 2022 | Leveraging adaptive scaling
    to adjust the size of GPU time slices and formalizing the dynamic allocation of
    GPU time slices into an optimization problem. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hydro (Hu et al., [2023](#bib.bib44)) | 2023 | Expanding resources for
    hyperparameter tuning workloads by interleaving them with pipeline-enabled large-model
    training tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| Network Bandwidth Sharing. Granularity: (1) Job; (2) Gradient Block; (3)
    Coflow [[C6]](#S3.SS1.SSS1.tab6 "3.1.1\. GPU sharing: ‣ 3.1\. Resource Allocation
    for Distributed Training ‣ 3\. Resource Allocation ‣ Resource Allocation and Workload
    Scheduling for Large-Scale Distributed Deep Learning: A Survey") ([3.1.2](#S3.SS1.SSS2
    "3.1.2\. Network bandwidth sharing: ‣ 3.1\. Resource Allocation for Distributed
    Training ‣ 3\. Resource Allocation ‣ Resource Allocation and Workload Scheduling
    for Large-Scale Distributed Deep Learning: A Survey")) | Liquid (Gu et al., [2022](#bib.bib39))
    | 2021 | (1) Proposing intelligent cluster network-efficient scheduling methods
    in both immediate and batch modes. |'
  prefs: []
  type: TYPE_TB
- en: '| Prophet (Zhang et al., [2021](#bib.bib146)) | 2021 | (2) Employing the monitored
    network bandwidth and the profiled gradient time interval to predict the number
    of gradients into gradient blocks. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Parrot (Li et al., [2020a](#bib.bib62)) | 2020 | (3) Using a linear program
    (LP) solution to derive a weighted bandwidth scaling strategy to minimize the
    time cost in the communication stage. |'
  prefs: []
  type: TYPE_TB
- en: '|    Inference ([3.2](#S3.SS2 "3.2\. Resource Allocation for Distributed Inference
    ‣ 3\. Resource Allocation ‣ Resource Allocation and Workload Scheduling for Large-Scale
    Distributed Deep Learning: A Survey")) | Spatial Sharing [[C7]](#S3.SS1.SSS2.tab1
    "3.1.2\. Network bandwidth sharing: ‣ 3.1\. Resource Allocation for Distributed
    Training ‣ 3\. Resource Allocation ‣ Resource Allocation and Workload Scheduling
    for Large-Scale Distributed Deep Learning: A Survey") | GSLICE (Dhakal et al.,
    [2020](#bib.bib23)) | 2020 | Developing self-learning and adaptive GPU-resource
    allocation and batching schemes. |'
  prefs: []
  type: TYPE_TB
- en: '| iGniter (Xu et al., [2022](#bib.bib130)) | 2022 | Leveraging inference performance
    model to calculate the appropriate batch size and lower bound of allocated GPU
    resources. |'
  prefs: []
  type: TYPE_TB
- en: '|  | SLO-aware (Cho et al., [2022](#bib.bib20)) | 2022 | Distributing inference
    requests to the deployed functions based on the autoscaling decision. |'
  prefs: []
  type: TYPE_TB
- en: '|  | AlpaServe (Li et al., [2023c](#bib.bib64)) | 2023 | Finding a partitioning
    strategy that minimizes the stage imbalance for inter-operator parallelism. |'
  prefs: []
  type: TYPE_TB
- en: '| Temporal Sharing [[C7]](#S3.SS1.SSS2.tab1 "3.1.2\. Network bandwidth sharing:
    ‣ 3.1\. Resource Allocation for Distributed Training ‣ 3\. Resource Allocation
    ‣ Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep
    Learning: A Survey") | Nexus (Shen et al., [2019](#bib.bib97)) | 2019 | Applying
    a heuristic approach to select the requests. |'
  prefs: []
  type: TYPE_TB
- en: '| INFaaS (Romero et al., [2021](#bib.bib92)) | 2021 | Identifying the colocation
    interference caused by the shared hardware resources. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Cocktail (Gunasekaran et al., [2022](#bib.bib40)) | 2022 | Building a
    distributed-weighted auto-scaling policy that utilizes the importance sampling
    technique. |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid Sharing [[C7]](#S3.SS1.SSS2.tab1 "3.1.2\. Network bandwidth sharing:
    ‣ 3.1\. Resource Allocation for Distributed Training ‣ 3\. Resource Allocation
    ‣ Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep
    Learning: A Survey") | Gpulet (Choi et al., [2022](#bib.bib21)) | 2022 | Allowing
    heterogeneous ML models to be mapped to multiple gpulets in the most cost-effective
    way. |'
  prefs: []
  type: TYPE_TB
- en: '| FaST-GShare (Gu et al., [2023](#bib.bib38)) | 2023 | Introducing the FaST-Manager
    to limit and isolate spatio-temporal resources. |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 3\. Resource Allocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we introduce resource allocation strategies for both distributed
    training and inference, which have different workload characteristics and performance
    requirements. Table [3](#S2.T3 "Table 3 ‣ 2.3\. Resource Allocation and Workload
    Scheduling for Distributed DL ‣ 2\. Fundamentals of DL and Distributed DL ‣ Resource
    Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning:
    A Survey") summarizes these strategies with related challenges [Cx], highlighted
    in box texts in the coming sections. Resource allocation strategies for distributed
    training are classified into two categories: GPU sharing and network bandwidth
    sharing. GPU sharing strategies are further classified into five approaches based
    on techniques they applied, including workload profiling, context switching, performance
    estimating, elastic scaling, and special considerations for hyperparameter tuning
    workloads. Network bandwidth sharing strategies are further classified based on
    the targeting granularity of resource, including the job, gradient block task,
    and coflow. Resource allocation strategies for distributed inference are classified
    into three categories based on sharing patterns of GPUs, including spatial, temporal,
    and hybrid sharing.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Resource Allocation for Distributed Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training process of distributed DL requires an intensive consumption of
    computational power and memory of GPUs and network communication bandwidth across
    GPUs. Therefore, GPU and network bandwidth sharing is the focus of the discussion
    on resource allocation for distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: '3.1.1\. GPU sharing:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although GPUs have found extensive applications in distributed DL, a prevalent
    issue of underutilization is observed in production clusters. The recorded GPU
    utilization typically ranges from 25% to below 50% (Narayanan et al., [2020](#bib.bib81);
    Hu et al., [2021](#bib.bib43); Weng et al., [2022](#bib.bib125); Li et al., [2023b](#bib.bib57);
    Weng et al., [2023](#bib.bib126); Cheng et al., [2023](#bib.bib19)). This concern
    is particularly noteworthy in large-scale distributed computing environments.
    To address this issue, various distributed technologies have been developed to
    enable DL tasks to run efficiently on numerous devices (Ye et al., [2024b](#bib.bib135)).
  prefs: []
  type: TYPE_NORMAL
- en: GPU sharing strategies typically leverage partial resource allocation through
    virtualization to mitigate the challenge of low GPU utilization in large-scale
    distributed DL. NVIDIA, acknowledged as the leading GPU provider, introduces Multiple
    Process Sharing (MPS) (Nvi, [2024b](#bib.bib4)) that offers an operating-system-level
    virtualization solution. Nevertheless, its implementation requires application-specific
    expertise to define resource limits for ensuring performance isolation. Moreover,
    MPS lacks compatibility with various DL frameworks. To address the performance
    isolation issue with MPS, another NVIDIA technology, Multi-Instance GPU (MIG) (Nvi,
    [2024a](#bib.bib3)), enables the partitioning of a GPU into multiple discrete
    instances, each with dedicated resources. However, as MIG cannot dynamically adjust
    the partitions for GPU sharing to fit the GPU requirement of online workloads,
    it must initially allocate peak GPU resources for online workloads initially and
    retain them during the entire execution life cycle, leading to a significant waste
    of GPU resources. To address the problems with efficient performance isolation
    for online workloads in MPS and MIG, Muxflow (Zhao et al., [2023a](#bib.bib148))
    proposes a two-level protection mechanism to guarantee GPU isolation for online
    workloads. The workload level protection resides between the CUDA (cud, [2024a](#bib.bib2))
    drive layer and CUDA runtime layer and controls the offline workloads to protect
    online workloads. The GPU level protection monitors the GPU device status to enable
    dynamic adjustment of the GPU memory quota for offline workloads. ByteDance has
    successfully deployed Muxflow in clusters with more than 20,000 GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C1]: Utilizing representative distributed DL workloads and profiling
    general characteristics so that the profiling result accurately reflects the workload
    characteristics of the working environment for the GPU-allocation strategy. |'
  prefs: []
  type: TYPE_TB
- en: '$\bullet$ Workload profiling: Some solutions leverage the profiling of complex
    distributed DL workloads of production large-scale clusters or clouds to instruct
    the GPU allocation strategies, tackling Challenge [[C1]](#S3.SS1.SSS1.tab1 "3.1.1\.
    GPU sharing: ‣ 3.1\. Resource Allocation for Distributed Training ‣ 3\. Resource
    Allocation ‣ Resource Allocation and Workload Scheduling for Large-Scale Distributed
    Deep Learning: A Survey"). Gandiva (Xiao et al., [2018](#bib.bib128)) leverages
    the profiles of distributed DL tasks and addresses the issue of GPU underutilization
    in three key ways. Initially, Gandiva allows incoming jobs to time-share GPUs
    with existing jobs when overloaded. Then, it permits time-sliced jobs to migrate
    to other GPUs. Lastly, it supports elastic GPU capacity, increasing the number
    of GPUs during idle times and reducing the number of GPUs as the load grows dynamically,
    thereby utilizing idle GPUs effectively. The performance of Gandiva is demonstrated
    on production clusters at Microsoft. AntMan (Xiao et al., [2020](#bib.bib129))
    is a production solution for distributed DL clusters at Alibaba. It analyzes the
    cause of GPU underutilization in distributed DL clusters for production use in
    three aspects: hardware, cluster scheduling, and job behavior. Exploiting the
    profiles of fluctuating resource demands from distributed training jobs, AntMan
    co-designs the cluster scheduler and distributed DL framework with dynamic scaling
    mechanisms for GPU resources during job execution. This approach ensures jobs’
    service-level objectives (SLOs) in large-scale clusters while enhancing cluster
    utilization through opportunistic scheduling. Leveraging the analysis of the production
    trace at Alibaba, Fragmentation Gradient Descent (FGD) (Weng et al., [2023](#bib.bib126))
    addresses severe GPU fragmentation in large clusters. FGD minimizes GPU fragmentation
    growth through task packing to achieve maximum GPU allocation rates. TGS (Wu et al.,
    [2023](#bib.bib127)) provides transparent GPU sharing at OS layer for distributed
    DL tasks in production clusters of containers. TGS addresses challenges of the
    lack of application profiling knowledge and the potential oversubscription of
    GPU memory during the sharing of GPU resources. It tackles the first challenge
    by monitoring and controlling the rate of sending GPU kernels to the GPU for each
    container adaptively, aiming to maximize the rate of opportunistic jobs while
    not affecting that of production jobs. It tackles the second challenge by unifying
    GPU memory and host memory in a single address space via CUDA unified-memory allocation (cud,
    [2024b](#bib.bib5)) that enables both performance isolation and transparency of
    GPU memory allocation. Oversubscribed memory of opportunistic jobs is evicted
    to the host memory automatically, ensuring the performance of production jobs.
    Strati et al. (Strati et al., [2024](#bib.bib103)) suggest that DNN workloads
    have numerous data-dependent operators with distinct computation and memory requirements.
    These individual operators can saturate the GPU computation units or memory bandwidth
    but often leave other resources underutilized. To address the issue of imbalanced
    utilization of GPU and other resources, they propose a fine-grained and interference-aware
    GPU allocator, named Orion, to co-schedule GPU kernels based on the computation
    and memory profiles of DNN workloads to optimize overall resource utilization.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C2]: Reducing the latency of GPU context switching for distributed
    DL workloads, which include offloading and loading of models and data |'
  prefs: []
  type: TYPE_TB
- en: '$\bullet$ Context switching: Some work utilizes fast context switching to reduce
    GPU latency, tackling Challenge [[C2]](#S3.SS1.SSS1.tab2 "3.1.1\. GPU sharing:
    ‣ 3.1\. Resource Allocation for Distributed Training ‣ 3\. Resource Allocation
    ‣ Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep
    Learning: A Survey"). GPU context switching refers to a GPU switching between
    processes when it is executing multiple training jobs or tasks in parallel or
    sequence. Salus (Yu and Chowdhury, [2020](#bib.bib139)) achieves low switching
    latency via a fine-grained GPU sharing policy that exposes two GPU sharing primitives:
    fast job switching and memory sharing. The former enables rapid preemption and
    efficient time sharing for the currently active DL job on a GPU, whereas the latter
    packs smaller distributed DL tasks on the same device to ensure high memory utilization
    and prevent memory fragmentation. In contrast, PipeSwitch (Bai et al., [2020](#bib.bib11))
    supports fast-context switching for pipelines of distributed DL jobs. PipeSwitch
    optimizes the context switching overhead through model-aware grouping for pipelines
    and proactive allocating of GPU memory. The model-aware grouping of layers aims
    to minimize the overhead of transferring the model between CPUs and GPUs during
    context switching. The proactive allocation of GPU memory for standby workers
    before it should be active expedites the speed of context switching. To prevent
    job interference, PipeSwitch enforces process-level isolation, by initialing a
    new separate process for each active-worker task. To minimize the overhead of
    loading an application from the memory pool to a GPU, DistMind (Jin et al., [2024](#bib.bib54))
    exposes the abstractions of the GPU pool and memory pool and incorporates three-stage
    pipelining, cache-aware load balancing, and DNN-aware sharding in the GPU scheduler
    to achieve low application loading overhead and high GPU efficiency. G-Safe (Pavlidakis
    et al., [2024](#bib.bib87)) focuses on the safety problem of GPU sharing in multi-tenant
    environments. It constrains the GPU kernels of each application to stay within
    the memory partition allocated to them during context switching.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C3]: Ensuring that the intermediately defined performance goal
    used for guiding the resource allocation strategy leads to a straightforward improvement
    in the actual performance goal when estimating the performance of distributed
    DL jobs in the GPU-allocation strategy. |'
  prefs: []
  type: TYPE_TB
- en: '$\bullet$ Performance estimating: Some works employ the performance-estimate-guided
    approach to enhance GPU-resource allocation, tackling Challenge [[C3]](#S3.SS1.SSS1.tab3
    "3.1.1\. GPU sharing: ‣ 3.1\. Resource Allocation for Distributed Training ‣ 3\.
    Resource Allocation ‣ Resource Allocation and Workload Scheduling for Large-Scale
    Distributed Deep Learning: A Survey"). Both the performance goal and performance-estimation
    method can vary in these approaches. To illustrate Challenge [[C3]](#S3.SS1.SSS1.tab3
    "3.1.1\. GPU sharing: ‣ 3.1\. Resource Allocation for Distributed Training ‣ 3\.
    Resource Allocation ‣ Resource Allocation and Workload Scheduling for Large-Scale
    Distributed Deep Learning: A Survey"), when a strategy aims to reduce the average
    job completion time but uses GPU utilization as an intermediate performance estimate,
    it must ensure that a higher GPU utilization leads to a reduced average job completion
    time. Optimus (Peng et al., [2018](#bib.bib88)) introduces a dynamic allocation
    algorithm based on marginal gains, estimating the remaining execution time of
    a distributed DL task. In this greedy policy, a job with a larger marginal gain
    will be allocated a higher quota of GPU resources. Harmony (Bao et al., [2019](#bib.bib12))
    uses a deep reinforcement learning (DRL) algorithm to place distributed DL jobs
    on GPU resources that lead to the minimum training or inference time. The learning
    rewards for unseen placements are guided by historical allocation samples. Horus (Yeung
    et al., [2021](#bib.bib136)) builds a model to predict GPU utilization of heterogeneous
    distributed DL tasks from computation graph features. It identifies GPU utilization
    as a general proxy metric for making optimal placement decisions. GPARS (Wang
    et al., [2024](#bib.bib122)) leverages spatiotemporal correlations among jobs
    and adopts graph attention networks for precise job duration prediction. It designs
    a dynamic objective function to allocate suitable GPU types for newly submitted
    jobs.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C4]: Determining the timing and quota for expanding resources
    in elastic distributed training, which requires monitoring runtime performance
    statistics. |'
  prefs: []
  type: TYPE_TB
- en: '$\bullet$ Elastic training: Elastic training, which involves expanding and
    shrinking resource capacity dynamically, is an important strategy to improve resource
    utilization and save costs for distributed DL in the cloud environment. Many studies
    tackle Challenge [[C4]](#S3.SS1.SSS1.tab4 "3.1.1\. GPU sharing: ‣ 3.1\. Resource
    Allocation for Distributed Training ‣ 3\. Resource Allocation ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey")
    and focus on elastic GPU memory allocation. For example, Pollux (Qiao et al.,
    [2021](#bib.bib89)) adjusts GPU resources available to distributed DL jobs dynamically,
    aiming to maximize the overall training goodput within the cluster. To improve
    the efficiency of GPU memory sharing, Zico (Lim et al., [2021](#bib.bib67)) monitors
    the memory-usage patterns of individual distributed DL jobs by tracking computational
    progress during training. Based on the monitoring statistics, Zico allocates and
    deallocates memory among concurrent jobs automatically, ensuring no exceeding
    of the memory budget. AFS (Hwang et al., [2021](#bib.bib48)) points out that handling
    future jobs requires proactive preparation of resources based on current share
    calculations. When the GPU scheduler estimates that the GPU contention will be
    heavy in the future, it allocates more resources to long-lasting jobs; otherwise
    it allocates more resources to short jobs. EasyScale (Li et al., [2023a](#bib.bib58))
    utilizes a thread abstraction called EasyScaleThread to preserve consistent training
    accuracy when the number of workers changes in data-parallel training and proposes
    intra-job and inter-job GPU schedulers to scale in or out GPUs for workers dynamically.
    The intra-job scheduler proposes online GPU allocation proposals to the inter-job
    scheduler to maximize distributed training throughput of a specific distributed
    DL job, and the inter-job scheduler approves or declines proposals based on marginal
    speedup and workload balancing considerations. EasyScale can improve GPU utilization
    in heterogeneous GPU clusters with such two-level elastic GPU scheduling. In contrast,
    some studies focus on elastic container resources. For instance, FlowCon (Mao
    et al., [2023](#bib.bib77)) introduces a container placement strategy based on
    growth efficiency and dynamic resource configuration for elastic allocation and
    withdrawal of resources during runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C5]: Improving GPU utilization for batches of multiple hyperparameter
    tuning jobs, which have mostly homogeneous workloads among different jobs to improve
    overall training throughput. |'
  prefs: []
  type: TYPE_TB
- en: '$\bullet$ Hyperparameter tuning: Hyperparameter tuning workloads represent
    a batch of distributed jobs with highly similar workload characteristics, which
    can thus be leveraged for resource allocation. Several studies explore strategies
    for improving GPU utilization during hyperparameter tuning in distributed DL clusters,
    tackling Challenge [[C5]](#S3.SS1.SSS1.tab5 "3.1.1\. GPU sharing: ‣ 3.1\. Resource
    Allocation for Distributed Training ‣ 3\. Resource Allocation ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey").
    Fluid (Yu et al., [2021a](#bib.bib140)) is a distributed DL hyperparameter tuning
    execution engine that abstracts the hyperparameter tuning process as a sequence
    of trial groups. It employs a water-filling approach to expedite the hyperparameter
    tuning process to enhance GPU utilization. Titan (Gao et al., [2022](#bib.bib32))
    adopts a heuristic approach by consolidating multiple fine-tuning workloads into
    one, which is particularly advantageous considering that multiple fine-tuning
    workloads often share the same model parameters. DISC (Liu et al., [2022d](#bib.bib70))
    leverages adaptive scaling to adjust the size of GPU time slices occupied by hyperparameter-tuning
    jobs at runtime. This dynamic allocation of GPU time slices for each hyperparameter
    tuning job is based on its potential to create a steep increase in the model accuracy.
    Hydro (Hu et al., [2023](#bib.bib44)) addresses cluster-wise resource utilization
    and tuning efficiency by incorporating a heterogeneity-aware allocation strategy.
    This method extends the resources of hyperparameter-tuning workloads by interleaving
    them with pipeline-enabled large-model training tasks. By effectively utilizing
    idle time intervals on each node caused by the gaps between the forward and backward
    processing of micro-batches, Hydro enhances overall resource utilization and tuning
    efficiency in large-scale distributed DL clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C6]: Network bandwidth allocation for distributed DL must be
    based on sufficient knowledge of the workload and coordination of bandwidth resource
    across the application and network layers. |'
  prefs: []
  type: TYPE_TB
- en: '3.1.2\. Network bandwidth sharing:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In large-scale distributed environments, where communication is often the performance
    bottleneck, network bandwidth is another significant factor determining the efficiency
    of distributed training. To tackle Challenge [[C6]](#S3.SS1.SSS1.tab6 "3.1.1\.
    GPU sharing: ‣ 3.1\. Resource Allocation for Distributed Training ‣ 3\. Resource
    Allocation ‣ Resource Allocation and Workload Scheduling for Large-Scale Distributed
    Deep Learning: A Survey"), network layers, e.g., the transport layer, usually
    work collaboratively with the application layer, and the network bandwidth allocator
    be implemented on either layer based on the level of tasks required to allocate
    network bandwidth.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Job: Some work focuses on network bandwidth sharing for multiple
    distributed training jobs. For instance, Liquid (Gu et al., [2022](#bib.bib39))
    proposes a computational and communication-resource-estimation algorithm and a
    network-efficient job-placement strategy for distributed training jobs. The resource-estimation
    algorithm models resource requirements of distributed training jobs, including
    GPU computing power, GPU memory, and network bandwidth requirements. The job-placement
    strategy assigns distributed training jobs to a cluster of computing nodes and
    containers, finding a best-fit job placement solution that satisfies the estimated
    computational and communication-resource requirements and exhibits less GPU fragmentation
    and network communication cost across containers.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Gradient block task: A distributed training job can be break down
    into multiple training tasks of gradient blocks. Some work focuses on network
    bandwidth sharing at the granularity of the gradient block task. For instance,
    Prophet (Zhang et al., [2021](#bib.bib146)) groups into certain gradient blocks
    based on the profiled time interval and models the distributed training time in
    terms of the network bandwidth and order of network transfers of gradient blocks.
    Based on this model, Prophet searches for an optimal order of network transfers
    of gradient blocks, aiming to minimize the distributed training time. This optimal
    order of gradient block transfers optimizes both the network bandwidth sharing
    among gradient blocks and the overlapping between network transfers and GPU computation.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Coflow: A coflow is an abstraction several network flows related
    to a specific communication task, e.g., several or a fraction of gradient transfers,
    and is usually scheduled on the transport layer. Some work focuses on network
    bandwidth sharing for coflows. For instance, Parrot (Li et al., [2020a](#bib.bib62))
    perceives the communication pattern of a distributed training job as a series
    of dependent coflows and estimates the remaining processing time of distributed
    training jobs based on the amount of information carried per coflow. Parrot allocates
    network bandwidth to active coflows of concurrent jobs within the cluster, so
    that the effective completion time of coflows of the job with a shorter remaining
    processing time has a higher priority to be minimized.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C7]: Satisfying SLOs, such as latency and throughput, for distributed
    inference jobs of various complexity within specific resource constraints. |'
  prefs: []
  type: TYPE_TB
- en: 3.2\. Resource Allocation for Distributed Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In contrast to distributed training that caters to long-term offline workloads,
    distributed inference typically demands real-time execution with more stringent
    requirements on latency and accuracy. This difference in demands requires resource
    allocation solutions to address the distinct characteristics of inference workloads
    effectively. In the distributed inference process, GPU sharing is the focus of
    research, which primarily faces Challenge [[C7]](#S3.SS1.SSS2.tab1 "3.1.2\. Network
    bandwidth sharing: ‣ 3.1\. Resource Allocation for Distributed Training ‣ 3\.
    Resource Allocation ‣ Resource Allocation and Workload Scheduling for Large-Scale
    Distributed Deep Learning: A Survey"). To tackle this challenge, various resource
    allocation methods can be divided into three major categories: spatial, temporal,
    and hybrid sharing. In the context of multiple distributed DL jobs, the spatial
    sharing of GPUs involves the sharing of GPU space partitions while the temporal
    sharing involves the sharing of computation time slices of an entire GPU. Hybrid
    approaches combine techniques from these two categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Spatial Sharing: Many existing works exploit spatial sharing of GPUs
    to optimize the performance of distributed inference tasks. GSLICE (Dhakal et al.,
    [2020](#bib.bib23)) introduces an inference system that achieves safe and efficient
    GPU sharing through spatial GPU multiplexing systematically. It utilizes MPS (Nvi,
    [2024b](#bib.bib4)), a GPU spatial-multiplexing framework with virtualization,
    to handle various inference requests. iGniter (Xu et al., [2022](#bib.bib130))
    employs an inference performance model to calculate an appropriate batch size
    and the lower bound of allocated GPU resources. Subsequently, it allocates GPU
    resources for each inference workload by employing a greedy approach to identify
    the placement GPU devices that can achieve minimal performance interference. The
    SLO-aware ML Inference Framework (Cho et al., [2022](#bib.bib20)) designs a resource
    auto-scaling strategy in the cloud by leveraging rich and precise workload-specific
    metrics, with a special consideration of the heterogeneity in the GPU computational
    capability. This effective and elastic management of resources ensures meeting
    the SLO for diverse inference workloads in the cloud. Tackling the problem that
    large models may not be deployed on a single GPU, AlpaServe (Li et al., [2023c](#bib.bib64))
    utilizes queuing theory to mathematically verify the benefits of model parallelism
    and searches for a partitioning strategy that minimizes the stage imbalance for
    inter-operator model parallelism.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Temporal Sharing: Recent temporal-sharing approaches designed for
    specific distributed inference systems have shown improvements in GPU utilization,
    especially in cloud environments shared by numerous tenants. Nexus (Shen et al.,
    [2019](#bib.bib97)) employs a heuristic approach to select requests for co-location
    on the same GPU. Initially, it determines the most suitable batch size to meet
    throughput and SLO requirements for the existing inference workloads. Subsequently,
    Nexus identifies all possible combinations within a GPU’s duty cycle on a single
    GPU in a best-fit manner, maximizing utilization without violating latency requirements.
    Focusing on inference services in the cloud, INFaaS (Romero et al., [2021](#bib.bib92))
    addresses the problem of co-location interference arising from shared hardware
    resources. It allocates available resources to interfered instances through workload
    migration or virtual-machine-level scaling, aiming to reduce monetary costs through
    GPU sharing while meeting latency requirements via virtual-machine-level scaling.
    Cocktail (Gunasekaran et al., [2022](#bib.bib40)) scales the virtual machine resources
    for various inference models in the cloud automatically and proactively based
    on the predicted workload and popularity of these models. This approach enhances
    the efficiency of resource allocation in distributed DL inference systems with
    a specific set of supported inference models.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Hybrid Sharing: Several works study the hybrid GPU sharing approaches,
    considering both spatial and temporal sharing. Gpulet (Choi et al., [2022](#bib.bib21))
    supports spatial sharing of GPUs via the abstraction of virtual GPUs that are
    split partitions derived from physical GPUs. Given allocated virtual GPU resources,
    Gpulet supports temporal sharing by scheduling the batch sizes of inference jobs
    of multiple tenants, with a goal to guarantee the SLO. This hybrid design enables
    cost-effective cloud-resource allocation for the inference of numerous heterogeneous
    DL models. FaST-GShare (Gu et al., [2023](#bib.bib38)) utilizes spatial and temporal
    sharing of GPUs to maximize inference function throughput in the Function-as-a-Service
    serverless architecture for distributed DL. It supports auto-scaling of inference
    resources in the cloud based on the profiling of function throughput and resource
    allocation, maximizing GPU utilization while ensuring the SLO.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$ Fine-grained and elastic GPU allocation strategies are critical for
    improving GPU utilization. Coarse-grained or even exclusive-access GPU allocation
    for individual distributed DL jobs is common in small clusters but can lead to
    extremely low GPU utilization in the data center environment. Fine-grained GPU
    allocation strategies for diverse distributed training and inference workloads,
    which share GPU computational resources for multiple jobs and subtasks, are crucial
    for improving GPU utilization, reducing memory fragmentation, and ensuring performance
    isolation. However, as resource requirements can fluctuate during the long-term
    distributed training process, elastic GPU allocation strategies are also important
    for fully utilizing GPU resources while maintaining SLO from the cloud providers’
    perspective. Both strategy approaches require the support of virtualization technologies
    and a deep understanding of the workload characteristics of distributed DL. Moreover,
    the latter approach also requires knowing the runtime workload performance, which
    can be achieved by performance monitoring and dynamic adaptation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ High-performance large-scale distributed DL requires the orchestration
    of efficient allocation of GPU and network resources. The allocation of network
    resources can frequently be overlooked as a bottleneck for efficient resource
    utilization in distributed DL. Many resource allocation strategies of distributed
    DL focus on addressing computation issues, such as low utilization, load imbalance,
    and long queuing delays. However, with the increase of the cluster scale, the
    complexity of GPU network connections increases exponentially, and lack of consideration
    to efficient network-resource allocation can result in significant low job-execution
    performance of large-scale distributed DL. Efficient network bandwidth allocation
    strategies can alleviate communication contention. Fully utilizing resources of
    both GPU and network bandwidth leads to enhanced overall performance of distributed
    training and inference on a large scale.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Heterogeneity in resources and workloads is a significant consideration
    for effective resource allocation strategies for distributed DL. Heterogeneous
    resources and workloads are pervasive in the data center environment, which has
    large-scale resources and numerous tenants. On the one hand, Computing nodes and
    networks in various specifications and configurations introduce resource heterogeneity,
    and the heterogeneity that affects the performance of distributed DL most is in
    the heterogeneity in the GPU computation capacity and network protocol, bandwidth,
    and topology. Lack of consideration for resource heterogeneity can either underestimate
    the resource capacity of come resources and cause resource underutilization or
    overestimate the resource capacity of some other resources and cause resource
    contention. On the other hand, distributed DL workload characteristics of different
    tenants over different processing stages of the job in different periods can also
    be heterogeneous. Lack of consideration for heterogeneous workloads can cause
    inaccurate estimation of workload performance, which results in inferior resource
    allocation decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. Studies on Workload Scheduling Strategies for Large-Scale Distributed
    Training
  prefs: []
  type: TYPE_NORMAL
- en: '|    Category | Ref. | Year | Highlight |'
  prefs: []
  type: TYPE_TB
- en: '|    Training Scheduling ([4.1](#S4.SS1 "4.1\. Distributed Training Scheduling
    ‣ 4\. Workload Scheduling ‣ Resource Allocation and Workload Scheduling for Large-Scale
    Distributed Deep Learning: A Survey")) | Throughput: Job Level [[C8]](#S4.SS1.SSS1.tab1
    "4.1.1\. Throughput ‣ 4.1\. Distributed Training Scheduling ‣ 4\. Workload Scheduling
    ‣ Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep
    Learning: A Survey") ([4.1.1](#S4.SS1.SSS1 "4.1.1\. Throughput ‣ 4.1\. Distributed
    Training Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation and Workload
    Scheduling for Large-Scale Distributed Deep Learning: A Survey")) | Tiresias (Gu
    et al., [2019](#bib.bib37))  [C8(1)] | 2019 | Using LAS algorithm to prioritize
    jobs based on their service, a metric defined as the multiplication of requested
    GPU resources and execution time. |'
  prefs: []
  type: TYPE_TB
- en: '| OSDL (Wang et al., [2022a](#bib.bib118)) [C8(2)] | 2022 | Designing job-placement
    and scheduling algorithms in hybrid networks with OCS and EPS. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Heet (Mo et al., [2024](#bib.bib79))  [C8(2)] | 2024 | Measuring scaling
    efficiency on heterogeneous nodes and uses a price function to balance scaling
    and scheduling efficiency. |'
  prefs: []
  type: TYPE_TB
- en: '|  | FfDL (Jayaram et al., [2019](#bib.bib51))  [C8(2)] | 2019 | Using the
    operating lessons from the industry practice to guide the balance dependability
    with scalability, elasticity, flexibility, and efficiency. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Philly (Jeon et al., [2019](#bib.bib52))  [C8(2)] | 2019 | Correlating
    scheduler logs with logs from individual jobs and conducting a thorough analysis
    about the impact of gang scheduling and locality constraints on the queuing delay
    and job runtime. |'
  prefs: []
  type: TYPE_TB
- en: '|  | E-LAS (Sultana et al., [2020](#bib.bib104))  [C8(2)] | 2020 | Using real-time
    epoch progress rates specific to distributed training jobs, as well as services
    obtained from the temporal and spatial domains, to guide scheduling decisions
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | CASSINI (Rajasekaran et al., [2024](#bib.bib91))  [C8(2)] | 2024 | Introducing
    a circle geometric abstraction to model communication workload patterns and shifting
    the angles of the circles to interleave workloads of different jobs on the same
    network link. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Liu et al. (Liu et al., [2024b](#bib.bib69))  [C8(2)] | 2024 | Proportionally
    assigning job workloads on heterogeneous clusters for load balancing and high
    throughput. |'
  prefs: []
  type: TYPE_TB
- en: '|  | AutoSched (Gao et al., [2024b](#bib.bib35))  [C8(2)] | 2024 | Generating
    simulated workload trace to search for the best framework configuration for existing
    distributed training schedulers. |'
  prefs: []
  type: TYPE_TB
- en: '|  | SMD (Yu et al., [2021b](#bib.bib138)) [C8(3)] | 2021 | Allowing multiple
    jobs to compete for the communication bandwidth. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sched² (Luan et al., [2019](#bib.bib76)) [C8(3)] | 2019 | Using DRL to
    perform smart locality-aware scheduling of distributed training jobs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | MLFS (Wang et al., [2020b](#bib.bib120))  [C8(3)] | 2020 | Leveraging
    the data from the heuristic scheduling method for training a DRL model and making
    decisions on job scheduling using this trained DRL model automatically. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Yang et al. (Yang et al., [2023a](#bib.bib131)) | 2023 | Utilizing a trainable
    performance model to guide the exploration of DRL and adaptively scheduling all-reduce
    communication workloads. |'
  prefs: []
  type: TYPE_TB
- en: '| Throughput: Pipeline Level [[C9]](#S4.SS1.SSS1.tab2 "4.1.1\. Throughput ‣
    4.1\. Distributed Training Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey")
    ([4.1.1](#S4.SS1.SSS1 "4.1.1\. Throughput ‣ 4.1\. Distributed Training Scheduling
    ‣ 4\. Workload Scheduling ‣ Resource Allocation and Workload Scheduling for Large-Scale
    Distributed Deep Learning: A Survey")) | GPipe (Huang et al., [2019](#bib.bib47))
    | 2019 | Distributing layer-wise model partitions across multiple GPUs and splitting
    mini-batches into micro-batches for pipelining execution. |'
  prefs: []
  type: TYPE_TB
- en: '| PipeDream (Narayanan et al., [2019](#bib.bib80)) | 2019 | Using a heuristic
    model to determine the workload partitioned on each worker to balance workloads
    and minimize communication overheads under various model and hardware constraints.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | AutoPipe (Liu et al., [2022c](#bib.bib73)) | 2022 | An heuristic-based
    adaptive method to achieve balanced model partitioning. |'
  prefs: []
  type: TYPE_TB
- en: '|  | HetPipe (Park et al., [2020](#bib.bib86)) | 2020 | Integrating pipeline
    parallelism with data parallelism in heterogeneous GPU clusters. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Piper (Tarnawski et al., [2021](#bib.bib111)) | 2021 | A fine-grained
    pipeline workload partitioning scheme integrating various parallelism modes, including
    the data, layer-wise model, and tensor-wise model parallelism |'
  prefs: []
  type: TYPE_TB
- en: '|  | MG_WFBP (Shi et al., [2021](#bib.bib99)) | 2021 | Separating the computation
    of backpropagation into subtasks bounded by merged-gradient layers and overlapping
    it with the communication of model synchronization in data-parallel training.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | DeAR (Zhang et al., [2023a](#bib.bib144)) | 2023 | Decoupling the all-reduce
    primitive into two continuous operations, which enables overlapping communication
    tasks with feedforward tasks and reducing the communication overhead of model
    synchronization in data-parallel training. |'
  prefs: []
  type: TYPE_TB
- en: '|  | ScheMoE (Shi et al., [2024](#bib.bib100)) | 2024 | Partitioning input
    tokens to smaller tensors in all-to-all communications to overlap communication
    and computation in MoE models training. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Chimera (Li and Hoefler, [2021](#bib.bib60)) | 2021 | Integrating bidirectional
    pipelines improve 1F1B stage execution order to reduce pipeline stall and memory
    overhead. |'
  prefs: []
  type: TYPE_TB
- en: '|  | OOO BackProp (Oh et al., [2022](#bib.bib84)) | 2022 | Leveraging gradient
    computation dependencies to reorder stage executions in the pipeline, prioritizing
    critical gradient computations. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hanayo (Liu et al., [2023](#bib.bib74)) | 2023 | Running multiple waves
    of stages in a pipeline to reduce pipeline stall with low memory overhead. |'
  prefs: []
  type: TYPE_TB
- en: '|  | MixPipe (Zhang et al., [2023b](#bib.bib145)) | 2023 | Bidirectional pipelines
    for synchronous data-parallel training, with a mixed scheduling of 1F1B and 2F1B
    to balance memory and pipeline stall. |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdaPipe (Sun et al., [2024](#bib.bib105)) | 2024 | Adaptive recomputation
    for different stages in a pipeline to maximize saved recomputation cost within
    memory limits. |'
  prefs: []
  type: TYPE_TB
- en: '| Throughput: Network Flow Level [[C10]](#S4.SS1.SSS1.tab3 "4.1.1\. Throughput
    ‣ 4.1\. Distributed Training Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey")
    ([4.1.1](#S4.SS1.SSS1 "4.1.1\. Throughput ‣ 4.1\. Distributed Training Scheduling
    ‣ 4\. Workload Scheduling ‣ Resource Allocation and Workload Scheduling for Large-Scale
    Distributed Deep Learning: A Survey")) | JPAS (Zhou et al., [2020](#bib.bib150))
    | 2020 | Using a simple greedy mechanism to order all DDL jobs periodically. |'
  prefs: []
  type: TYPE_TB
- en: '| Geryon (Wang et al., [2020a](#bib.bib123)) | 2020 | Employing multiple flows
    with varying priorities to transfer parameters of different urgency levels. |'
  prefs: []
  type: TYPE_TB
- en: '|  | TensorExpress (Kang et al., [2020](#bib.bib55)) | 2020 | Enables each
    switch to transmit tensor packets according to their priority using multiple queues.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Beamer (He et al., [2021](#bib.bib42)) | 2021 | Reducing the SCT by considering
    stage information in its scheduling approach. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Tereis (Chen et al., [2023](#bib.bib16)) | 2023 | Exploring the utilization
    of idle GPU computational resources during data transmission periods. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Mercury (Duan et al., [2023](#bib.bib25)) | 2023 | Working on data packet
    to shift priority scheduling to the transport layer. |'
  prefs: []
  type: TYPE_TB
- en: '| Cost Efficiency [[C11]](#S4.SS1.SSS1.tab4 "4.1.1\. Throughput ‣ 4.1\. Distributed
    Training Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation and Workload
    Scheduling for Large-Scale Distributed Deep Learning: A Survey") ([4.1.2](#S4.SS1.SSS2
    "4.1.2\. Cost efficiency ‣ 4.1\. Distributed Training Scheduling ‣ 4\. Workload
    Scheduling ‣ Resource Allocation and Workload Scheduling for Large-Scale Distributed
    Deep Learning: A Survey")) | Cynthia (Zheng et al., [2019](#bib.bib149)) | 2019
    | Providing predictable distributed training performance and reducing the training
    budget. |'
  prefs: []
  type: TYPE_TB
- en: '| FC² (Ta, [2019](#bib.bib107)) | 2019 | A scheduler that recommends cost-effective
    cloud-resource allocations for distributed training tasks with a PS. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Jahani (Jahani et al., [2019](#bib.bib49)) | 2019 | Modeling the scheduling
    process as a MILP problem to reduce the leasing cost in a global manner while
    maintaining the job latency. |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPOEO (Wang et al., [2022b](#bib.bib119)) | 2022 | Saving power in GPU
    data centers and using a customized scheduler to orchestrate jobs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | STS (Filippini et al., [2023](#bib.bib31)) | 2023 | Exploiting the probability
    distribution of early termination and adapting the resource assignment during
    the execution of the jobs to minimize the expected energy cost |'
  prefs: []
  type: TYPE_TB
- en: '| Deadline Guarantee [[C12]](#S4.SS1.SSS2.tab1 "4.1.2\. Cost efficiency ‣ 4.1\.
    Distributed Training Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey")
    ([4.1.3](#S4.SS1.SSS3 "4.1.3\. Deadline Guarantee ‣ 4.1\. Distributed Training
    Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation and Workload Scheduling
    for Large-Scale Distributed Deep Learning: A Survey")) | GENIE (Chen et al., [2020](#bib.bib18))
    | 2020 | Proposing a prediction model derived from lightweight profiling to estimate
    the processing rate and response latency for diverse DL workloads. |'
  prefs: []
  type: TYPE_TB
- en: '| Chronus (Gao et al., [2021](#bib.bib33)) | 2021 | Providing deadline guarantee
    for SLO jobs and maximizing the performance of best-effort jobs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hydra (Yang et al., [2023b](#bib.bib132)) | 2023 | Adopting a sampling
    approach that exploits the inherent iterative periodicity of DL jobs to estimate
    job completion times accurately on heterogeneous GPUs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | UniSched (Gao et al., [2024a](#bib.bib34)) | 2024 | Jointly optimize job
    profiling, job scheduling, and resource allocation to guarantee deadline. |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 4\. Workload Scheduling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In large-scale GPU clusters with complex network connections, scheduling distributed
    DL workloads effectively is critical for ensuring the high performance of task
    execution, optimal hardware utilization, and achievement of various scheduling
    objectives. Training and inference stages of distributed DL are widely recognized
    as particularly computation and communication-intensive (Ye et al., [2024b](#bib.bib135)).
    The following section studies workload scheduling strategies on training and inference
    workloads and focuses on providing efficient communication or overlapping computational
    and communication tasks for overall efficiency in large-scale distributed DL.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Distributed Training Scheduling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Efficient workload scheduling strategies are crucial for distributed training
    workloads, especially in large-scale settings with large data, models, and device
    scales. Large-scale distributed training involves iteratively executing massive
    computational tasks for feedforward and backpropagation calculation and communication
    tasks for data flowing and model synchronization. It is a long-term and computation-intensive
    process that requires efficient scheduling strategies to improve execution parallelism
    and completion time and meet various performance goals. In this subsection, we
    survey workload scheduling strategies of large-scale distributed training with
    various performance goals and scheduling granularity levels. Table [4](#S3.T4
    "Table 4 ‣ 3.3\. Discussion ‣ 3\. Resource Allocation ‣ Resource Allocation and
    Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey") summarizes
    these strategies, which are categorized by various performance goals, including
    throughput, cost efficiency, and deadline guarantee goals, while the strategies
    focusing on distributed training throughput are further classified into three
    categories based on the scheduling granularity: the job, pipeline, and network
    flow.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1\. Throughput
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The throughput of distributed DL refers to the speed at which jobs or tasks
    are completed or the amount of work accomplished per unit of time. It is one of
    the most critical performance goals of distributed training scheduling (Ye et al.,
    [2024b](#bib.bib135)) and is determined synthetically by various factors, including
    resource utilization, parallelism level, and communication overhead. Workload
    scheduling strategies usually work on the job, pipeline, and network flow levels
    to achieve high throughput.
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C8]: (1) Online scheduling of distributed DL jobs whose arrival
    and completion times are unpredictable to achieve high throughput; (2) Resource-aware
    and workload-aware scheduling of distributed DL jobs in complicated, heterogeneous,
    or opaque resource structures; (3) Efficiently solving complex distributed DL
    workload scheduling problems with various resource constraints and workloads.
    |'
  prefs: []
  type: TYPE_TB
- en: '$\bullet$ Job-level scheduling. Scheduling distributed training at the job
    level, which involves the reordering job execution priorities and the placement
    of jobs on GPUs, is one of the most common and effective scheduling approaches (Gu
    et al., [2019](#bib.bib37); Jayaram et al., [2019](#bib.bib51)). Job-level scheduling
    for distributed training workloads faces several challenges as stated in Challenge
    [[C8]](#S4.SS1.SSS1.tab1 "4.1.1\. Throughput ‣ 4.1\. Distributed Training Scheduling
    ‣ 4\. Workload Scheduling ‣ Resource Allocation and Workload Scheduling for Large-Scale
    Distributed Deep Learning: A Survey"), which include several aspects: online scheduling,
    resource-aware scheduling, and complexity. For Challenge [C8(1)] about online
    scheduling, on the one hand, the unpredictable job arrival time requires a prompt
    online scheduling decision for each job upon its arrival, which may trigger significant
    preemption overhead if the system allows preemptive scheduling. On the other hand,
    the complex workload characteristics and resource topology make it hard to predict
    the job completion time accurately, and an inaccurate estimate can impede the
    scheduling algorithm from achieving high throughput. For Challenge [C8(2)] about
    resource-aware scheduling, the distributed DL workload scheduler should match
    workloads with large-scale resources, especially when the network topology of
    GPUs and nodes is complicated, heterogeneous, and sometimes even opaque and unobservable,
    e.g., in a multi-available-zone cloud environment. For Challenge [C8(3)] about
    scheduling complexity, the complexity of the job-placement problem can increase
    exponentially with the scale of the cluster with various resource constraints
    and workloads, requiring efficient and practical algorithms to find the optimal
    scheduling solution.'
  prefs: []
  type: TYPE_NORMAL
- en: Some studies refine the job priority algorithm to tackle the preemption problem
    of online job scheduling. For example, Tiresias (Gu et al., [2019](#bib.bib37))
    draws inspiration from the classic Multi-Level Feedback Queue (MLFQ) algorithm (Chowdhury
    and Stoica, [2015](#bib.bib22)) and develops a priority discretization approach
    to mitigate issues related to frequent preemption. In addition, Tiresias uses
    a Least-Attained-Service (LAS) algorithm to prioritize jobs based on their service
    levels, which are quantified by the product of requested GPU resources and execution
    time, to avoid scheduling starvation.
  prefs: []
  type: TYPE_NORMAL
- en: Some studies utilize resource topology-aware and workload-aware scheduling algorithms
    to improve performance estimation. For resource topology-aware solutions, OSDL (Wang
    et al., [2022a](#bib.bib118)) designs algorithms for job placing and scheduling
    of distributed training jobs in hybrid networks with optical circuit switching
    (OCS) and electrical packet switching (EPS). The job placing algorithm utilizes
    the hybrid network topology information to use lightpaths reasonably, and the
    job scheduling algorithm jointly optimizes bandwidth requests of distributed training
    jobs in the OCS and EPS domains. Heet (Mo et al., [2024](#bib.bib79)) proposes
    a 3D collaborative filtering method to accurately measure the scaling efficiency
    of all elastic configurations on heterogeneous nodes, substantially reducing profiling
    overhead. Meanwhile, Heet utilizes a price function to effectively balance scaling
    efficiency and scheduling efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: For workload-aware solutions, FfDL (Jayaram et al., [2019](#bib.bib51)), an
    open-source scheduling platform developed by IBM, incorporates operational insights
    from industry practices to strike a balance between dependability and scalability,
    while maintaining elasticity, flexibility, and efficiency. In a related study,
    Philly (Jeon et al., [2019](#bib.bib52)) performs a comprehensive analysis by
    correlating logs of the scheduler with logs of individual jobs, examining the
    impact of gang scheduling and locality constraints on queuing delay and job completion
    time. Drawing on insights from this analysis, Philly advocates relaxing locality
    constraints to enhance job time efficiency. Unlike the above methods, which rely
    on job completion time estimates or prior knowledge, E-LAS (Sultana et al., [2020](#bib.bib104))
    utilizes real-time epoch progress rates specific to distributed training jobs,
    combined with service metrics derived from temporal and spatial domains, to inform
    scheduling decisions. E-LAS surpasses Tiresias in training throughput by reducing
    the average completion time for distributed training jobs. CASSINI (Rajasekaran
    et al., [2024](#bib.bib91)) is a network-workload-aware distributed training job
    scheduler that uses a geometric circle abstraction with angular rotations to represent
    time shifts for communication workload patterns. It schedules different time shifts
    to distribute communication workloads on network links, interleaves communication
    workloads on the same network link, and reduces job completion time. Liu et al. (Liu
    et al., [2024b](#bib.bib69)) leverage proportional workload assignment on a heterogeneous
    GPU cluster to maximize distributed training throughput and minimize job completion
    time. To reduce the scheduling computational complexity, they propose constructing
    the sparsification of feasible solutions through sampling, which can significantly
    decrease the decision-making latency.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to common workload schedulers, which schedule distributed training
    workloads, some studies explore various configurations of existing workload schedulers
    to find the best configuration for specific workloads. For example, AutoSched (Gao
    et al., [2024b](#bib.bib35)) develops a workload generation engine to produce
    training workloads that can reveal future trace patterns, which facilitates accurate
    and efficient configuration tuning of distributed training workload schedulers.
    With the generated workload trace, AutoSched searches for the best configuration
    via a learnable causal model. AutoSched is supposed to be a general configuration-turning
    framework for various off-the-shelf distributed training schedulers, including
    Tiresias.
  prefs: []
  type: TYPE_NORMAL
- en: Several methods tackle the scheduling complexity by modeling the scheduling
    problem as an optimization problem and applying dynamic programming or DRL algorithms
    to solve the problem efficiently. SMD (Yu et al., [2021b](#bib.bib138)) presents
    a resource-scheduling analytical model that accommodates multiple jobs competing
    for communication bandwidth. This model treats the scheduling problem as a non-convex
    integer non-linear program with bin-packing constraints. SMD introduces an $\epsilon$-approximation
    algorithm for its resolution, termed the sum-of-ratios multi dimensional knapsack
    decomposition. Sched² (Luan et al., [2019](#bib.bib76)) utilizes DRL to schedule
    distributed training jobs with a locality-aware approach. This method tries to
    understand both the locality sensitivity of jobs and the fragmentation condition
    of clusters comprehensively within the entire learning stack. Through this heightened
    awareness, the DRL model adjusts its scheduling decisions dynamically and adaptively,
    responding effectively to the varying locality sensitivities of individual jobs
    and the evolving state of cluster fragmentation. MLFS (Wang et al., [2020b](#bib.bib120))
    employs data from heuristic scheduling methods to train a DRL model and subsequently
    uses this model to make informed decisions about job scheduling autonomously.
    Yang et al. (Yang et al., [2023a](#bib.bib131)) propose a meta-learning-based
    DRL method to improve the job completion time by adaptively scheduling all-reduce
    communication workloads in data-parallel training. To address the issue of massive
    samples in DRL and improve DRL efficiency, the proposed method trains a performance
    model to predict the training time and guide the DRL exploration strategy into
    an effective search space.
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C9]: Partitioning workloads for load balancing across different
    workers and optimizing the execution order to reduce pipeline stall and memory
    overhead in pipeline-level scheduling for distributed training workloads. |'
  prefs: []
  type: TYPE_TB
- en: '$\bullet$ Pipeline-level scheduling. In the pipeline parallelism mode of distributed
    training, pipeline-level scheduling divides training mini-batches into micro-batches
    and manages the sequential processing of micro-batch tasks within a pipeline architecture.
    This level of scheduling is widely adopted by large-model distributed training
    jobs. This scheduling approach orchestrates computational and communication tasks
    across various stages in a pre-defined execution order and aims to improve their
    execution parallelism. As a result, the execution of computational and communication
    tasks of the same or different stages overlap, which increases the overall pipeline
    throughput. This approach primarily faces Challenge [[C9]](#S4.SS1.SSS1.tab2 "4.1.1\.
    Throughput ‣ 4.1\. Distributed Training Scheduling ‣ 4\. Workload Scheduling ‣
    Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning:
    A Survey"). The pipeline stall refers to the phenomenon of a faster stage halting
    to wait for dependent slower stages to catch up, which can lead to low pipeline
    utilization and high memory overhead. The memory overhead is the space required
    to retain the results of the feedforward phase in the memory for the later calculation
    of the backpropagation phase in each micro-batch.'
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline parallelism is the state-of-the-art approach for large-model training.
    GPipe (Huang et al., [2019](#bib.bib47)), a pioneer in utilizing pipeline parallelism
    to train large models, distributes layer-wise model partitions across multiple
    GPUs and splits mini-batches into micro-batches for pipelining execution. It reduces
    the pipeline memory overhead by recomputing the activations of the feedforward
    phase again in the backpropagation phase. This library can achieve nearly linear
    convergence speedups and offer the flexibility to scale to various DNN models
    of immense sizes efficiently. However, GPipe assumes a partitioned model for pipelining
    is readily available or specified manually by users and does not design a model
    partitioning scheme for load balance.
  prefs: []
  type: TYPE_NORMAL
- en: To design an efficient model partitioning scheme for pipeline-level scheduling,
    some studies focus on balancing workloads across workers with hardware constraints.
    PipeDream (Narayanan et al., [2019](#bib.bib80)) builds a heuristic model to determine
    the workload to be partitioned on each worker to balance workloads and minimize
    communication overheads. The model considers various constraints, including the
    model scale, training iteration, device memory capacity, hardware topology and
    bandwidth, and number of workers, and the decision result relies on inputs from
    a short profiling run. To evenly distribute workload among worker, PipeDream also
    integrates data parallelism with pipeline parallelism at certain stages. AutoPipe (Liu
    et al., [2022c](#bib.bib73)) introduces an adaptive method to achieve balanced
    partitioning. It first generates a relatively balanced model partition scheme
    through dynamic programming. It then refines the scheme using the heuristic that
    pipeline completion time can be reduced by moving certain stages in the pipeline’s
    critical execution path forward or backward in the timeline. However, both PipeDream
    and AutoPipe focus only on the homogeneous GPU setting. To address pipeline load
    balancing in heterogeneous GPU clusters, HetPipe (Park et al., [2020](#bib.bib86))
    partitions large DNN models to minimize the maximum completion time of the partitions
    within heterogeneous GPU memory bounds of multiple virtual workers in the pipeline.
    To reduce the communication overhead of fully synchronous pipeline parallelism,
    HetPipe introduces a wave-synchronous-parallel approach to allow bounded model
    staleness within a wave of micro-batches but guarantees convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some studies focus on fine-grained workload pipelining schemes to maximize
    pipeline throughput. For instance, Piper (Tarnawski et al., [2021](#bib.bib111))
    focuses on fine-grained model partitioning, while MG_WFBP (Shi et al., [2021](#bib.bib99)),
    DeAR (Zhang et al., [2023a](#bib.bib144)) and ScheMoE (Shi et al., [2024](#bib.bib100))
    focus on fine-grained overlapping of computational and communication tasks. For
    fine-grained model partitioning, Piper (Tarnawski et al., [2021](#bib.bib111))
    supports tensor-wise model parallelism in the model partitioning scheme for pipeline
    scheduling, which is not supported in prior work, in addition to data parallelism
    and layer-wise model parallelism. It applies a two-level dynamic programming algorithm
    to search for the optimal partitioning of a DNN model to maximize pipeline throughput
    within memory constraints. With increased search space, Piper can find high-quality
    parallelism configurations with high pipeline throughput. For fine-grained computation
    and communication overlapping, MG_WFBP (Shi et al., [2021](#bib.bib99)) divides
    the calculation of a backpropagation task into numerous subtasks separated by
    merged-gradient layers, which stand as trigger points for model synchronization
    in data-parallel training. As a result, the communication of model synchronization
    in a subtask can overlap with the computation of backpropagation in a subsequent
    subtask. In contrast, DeAR (Zhang et al., [2023a](#bib.bib144)) decouples the
    all-reduce primitive into two continuous operations: reduce-scatter and all-gather.
    This decouple enables overlapping communication tasks of the previous stage with
    feedforward tasks of the next stage in the pipeline execution, reducing the communication
    overhead of model synchronization in data-parallel training. Focusing on the distributed
    training of mixture-of-experts models, which has the communication bottleneck
    caused by the all-to-all collective communication, ScheMoE (Shi et al., [2024](#bib.bib100))
    pipelines all-to-all communications with expert computations by virtually partitioning
    input tokens to multiple smaller tensors to increase the chance of task overlapping.'
  prefs: []
  type: TYPE_NORMAL
- en: Some studies focus on optimizing the pipeline execution order to reduce pipeline
    stall. For example, Chimera (Li and Hoefler, [2021](#bib.bib60)) applies bidirectional
    pipelines that are composed of two pipelines executing stages in reserve directions
    in a one-forward-one-backward (1F1B) manner. Computational tasks of different
    micro-batches are mostly overlapped on different workers and the resultant bidirectional
    pipelines execute in a compacter manner than in PipeDream. Chimera also builds
    a model for determining the optimal number of pipeline stages and number of replicated
    pipelines, whose values rely on empirical results as inputs. Out-Of-Order (OOO)
    BackProp (Oh et al., [2022](#bib.bib84)) leverages gradient computation dependencies
    to reorder stage executions in the pipeline to maximize GPU-resource utilization.
    In data-parallel training, OOO reorders the sequence of gradient computations
    to maximize the overlap between computation and parameter communication. In pipeline-parallel
    training, it prioritizes critical gradient computations to minimize pipeline stall.
  prefs: []
  type: TYPE_NORMAL
- en: Some studies focus on reducing GPU memory consumption and recomputation cost
    for pipeline execution. On the one hand, though the bidirectional pipeline approach
    of Chimera can achieve low pipeline stall, it has multiple model replicas in two
    pipelines, which requires large GPU memory consumption. Hanayo (Liu et al., [2023](#bib.bib74))
    mitigates the issue of excessive memory consumption by running multiple waves
    of forward and backward stages in a pipeline to reduce pipeline stall while not
    increasing GPU memory consumption. MixPipe (Zhang et al., [2023b](#bib.bib145)),
    another bidirectional pipeline approach for synchronous data-parallel training,
    regulates a flexible number of micro-batches injected into the bidirectional pipelines
    to balance pipeline and device utilization. MixPipe also features a mixed scheduling
    of 1F1B and 2F1B to balance memory usage and pipeline stall. On the other hand,
    though the recomputation strategy for the backward stage in the pipeline can relieve
    memory consumption, its cost can be non-negligible. To balance memory saving and
    computation cost in recomputation, AdaPipe (Sun et al., [2024](#bib.bib105)) models
    the memory and time cost of different recomputation strategies and introduces
    an adaptive recomputation mechanism to allow different recomputation strategies,
    e.g., partial and full recomputation, for different stages in a pipeline. AdaPipe
    achieves maximum saved recomputation cost within memory limits.
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C10]: Scheduling network flows at different granularity level
    to increase bandwidth utilization, network latency, and network congestion for
    distributed DL workloads. |'
  prefs: []
  type: TYPE_TB
- en: '$\bullet$ Network-flow-level scheduling. Efficient network flow scheduling
    determines the transmission priority of data packets, network flows, and coflows
    related to distributed DL jobs, aiming to significantly increase network bandwidth
    utilization, reduce network latency, and avoid network congestion, as stated in
    Challenge [[C10]](#S4.SS1.SSS1.tab3 "4.1.1\. Throughput ‣ 4.1\. Distributed Training
    Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation and Workload Scheduling
    for Large-Scale Distributed Deep Learning: A Survey"). Network flow scheduling
    can work at various granularity levels, including the job, coflow, and data packet
    levels.'
  prefs: []
  type: TYPE_NORMAL
- en: Some studies focus on the job level. JPAS (Zhou et al., [2020](#bib.bib150))
    implements a straightforward greedy mechanism to organize all distributed training
    jobs periodically. This approach enables each host machine to prioritize its network
    flows according to the established job order, delegating the task of flow scheduling
    and rate allocation to the underlying priority-enabled networks. Tereis (Chen
    et al., [2023](#bib.bib16)) explores the utilization of idle GPU computational
    resources during data transmission periods. It predicts the completion time for
    a distributed DL job and its corresponding data transmission time, allowing for
    the simultaneous packaging of two jobs on the same GPU. This ensures that one
    job is completed before the other concludes its data transfer.
  prefs: []
  type: TYPE_NORMAL
- en: Some studies focus on the coflow level. Geryon (Wang et al., [2020a](#bib.bib123))
    employs multiple flows with varying priorities to transfer parameters of different
    urgency levels. This approach coordinates multiple PSs effectively and gives precedence
    to urgent parameter transfers across the entire network fabric. Beamer (He et al.,
    [2021](#bib.bib42)) focuses on reducing the stage-completion time (SCT) by considering
    stage information in its scheduling approach. It proposes a stage-aware coflow-scheduling
    method to minimize the average SCT.
  prefs: []
  type: TYPE_NORMAL
- en: Some other studies focus on the data packet level. To address in-network delays,
    such as queuing delays, TensorExpress (Kang et al., [2020](#bib.bib55)) shifts
    priority scheduling to the transport layer, focusing on the packet granularity.
    It enables each switch to transmit tensor packets according to their priorities
    using multiple queues. This method ensures that high-priority data packets are
    handled efficiently to minimize delays. Similarly, Mercury (Duan et al., [2023](#bib.bib25))
    transmits packets with the highest priority in the Mercury buffer first. Additionally,
    Mercury incorporates immediate aggregation at the transport layer, enabling full
    overlapping of gradient push-and-pull operations. This approach not only streamlines
    data flow but also maximizes the efficiency of network resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C11]: Jointly optimizing energy consumption, monetary cost,
    and throughput for distributed DL workloads with awareness of cloud resources
    and policies from the perspectives of cloud service providers or service users.
    |'
  prefs: []
  type: TYPE_TB
- en: 4.1.2\. Cost efficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The cost-efficiency objective of distributed training scheduling aims to minimize
    operational costs while ensuring optimal performance for distributed training
    workloads, especially in the cloud environment. It primarily faces Challenge [[C11]](#S4.SS1.SSS1.tab4
    "4.1.1\. Throughput ‣ 4.1\. Distributed Training Scheduling ‣ 4\. Workload Scheduling
    ‣ Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep
    Learning: A Survey") and focuses on a balance between resource utilization, energy
    consumption, and monetary expenditures in the scheduling decisions. Cynthia (Zheng
    et al., [2019](#bib.bib149)) offers predictable distributed training performance
    while reducing the training budget. This scheduler identifies the optimal resource
    type and maintains training throughput effectively, thereby minimizing monetary
    costs. Similar to Cynthia, FC² (Ta, [2019](#bib.bib107)) is a scheduler that recommends
    cost-effective cloud resource allocations for parameter servers in distributed
    training tasks. It prioritizes instances with the largest network bandwidth within
    the budget to circumvent communication bottlenecks. Furthermore, it introduces
    a heuristic named Scale-Opt for determining worker instances, ensuring job throughput,
    and maximizing cost savings. Jahani (Jahani et al., [2019](#bib.bib49)) considers
    computing nodes with varying numbers of GPUs as distinct virtual machines. The
    scheduling process is modeled as a mixed-integer linear programming (MILP) problem,
    aiming to reduce leasing costs globally while maintaining job latency. GPOEO (Wang
    et al., [2022b](#bib.bib119)) achieves significant power savings for training
    workloads. It can be integrated into GPU data centers easily, utilizing a customized
    scheduler to manage job orchestration. STS (Filippini et al., [2023](#bib.bib31))
    optimizes the scheduling of distributed training jobs from the perspective of
    cloud service providers operating data centers. It leverages the probability distribution
    of early job termination to adapt resource assignments during job execution, with
    the aim of minimizing the expected energy cost.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C12]: Accurately estimating job completion or remaining times
    based on workload monitoring statistics in distributed DL workload scheduling
    to guarantee deadlines in the cloud environment. |'
  prefs: []
  type: TYPE_TB
- en: 4.1.3\. Deadline Guarantee
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Deadline-guaranteed scheduling focuses on ensuring the completion of distributed
    DL jobs before a specified deadline for jobs whose timing is a crucial consideration.
    This performance goal is more common in the cloud environment, where cloud providers
    can elastically scale resources for distributed training workloads to guarantee
    the SLO for cloud users. Achieving this performance goal primarily faces Challenge
    [[C12]](#S4.SS1.SSS2.tab1 "4.1.2\. Cost efficiency ‣ 4.1\. Distributed Training
    Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation and Workload Scheduling
    for Large-Scale Distributed Deep Learning: A Survey"). GENIE (Chen et al., [2020](#bib.bib18)),
    a trailblazing deadline-aware scheduler for distributed training workloads, explores
    the key factors that impact the performance of distributed DL tasks. It introduces
    a predictive model based on lightweight profiling, enabling an accurate estimation
    of the processing rate and response latency for a variety of distributed DL workloads.
    However, a significant limitation of GENIE is that it is unable to handle mixed
    workloads that include both deadline-sensitive tasks and best-effort tasks simultaneously (Ye
    et al., [2024b](#bib.bib135)). Chronus (Gao et al., [2021](#bib.bib33)), an end-to-end
    scheduling system, meets SLOs by guaranteeing deadlines for SLO-aware jobs while
    also enhancing the performance of best-effort jobs. This dual-focused strategy
    enables Chronus to manage a wide range of workload requirements. By extending
    these studies, Hydra (Yang et al., [2023b](#bib.bib132)) emerges as a dynamic
    and multifaceted scheduler to tackle various scheduling challenges, including
    adhering to deadlines and reducing job completion times. Hydra introduces an sampling
    approach leveraging the iterative periodicity inherent in distributed DL jobs.
    This technique enables precise estimation of job completion times in heterogeneous
    GPU environments, thereby improving efficiency and effectiveness of scheduling
    for various distributed DL workloads. In contrast to other work that usually optimizes
    a specific scheduling stage to guarantee deadline for distributed training jobs,
    UniSched (Gao et al., [2024a](#bib.bib34)) adopts a mixed integer linear programming
    framework to jointly optimize job profiling, job scheduling, and resource allocation
    to satisfy various scheduling objectives, including the deadline SLO and latency.
    Two key components support the optimization of UniSched: an estimator for estimating
    job completion time and a selector for selecting jobs and allocating resources.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5\. Studies on Workload Scheduling Strategies for Large-Scale Distributed
    Inference
  prefs: []
  type: TYPE_NORMAL
- en: '|    Category | Ref. | Year | Highlight |'
  prefs: []
  type: TYPE_TB
- en: '|    Inference Scheduling ([4.2](#S4.SS2 "4.2\. Distributed Inference Scheduling
    ‣ 4\. Workload Scheduling ‣ Resource Allocation and Workload Scheduling for Large-Scale
    Distributed Deep Learning: A Survey")) | Latency and Cost Efficiency [[C13]](#S4.SS2.tab1
    "4.2\. Distributed Inference Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey")
    ([4.2.1](#S4.SS2.SSS1 "4.2.1\. Latency and cost efficiency ‣ 4.2\. Distributed
    Inference Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation and Workload
    Scheduling for Large-Scale Distributed Deep Learning: A Survey")) | Sniper (Liu
    et al., [2022a](#bib.bib71)) | 2022 | Using non-invasive performance characterization
    networks based on neural network similarity (NNS) to predict the inference time
    of DNNs accurately. |'
  prefs: []
  type: TYPE_TB
- en: '| Ace-Sniper (Liu et al., [2024a](#bib.bib72)) | 2024 | Including both hardware
    and software platform information in the resource abstraction to tackle heterogeneous
    hardware and platforms for distributed inference. |'
  prefs: []
  type: TYPE_TB
- en: '|  | AP² (Shi et al., [2023](#bib.bib98)) | 2023 | Minimizing distributed inference
    latency in 6G mobile communication systems with communications, heterogeneous
    devices, and task dependency constraints. |'
  prefs: []
  type: TYPE_TB
- en: '|  | AutoDeep (Li et al., [2020b](#bib.bib63)) | 2020 | Leveraging Bayesian
    Optimization and DRL to unearth the optimal cloud configuration and device placement
    with limited search time adaptively. |'
  prefs: []
  type: TYPE_TB
- en: '|  | HexGen (Jiang et al., [2023](#bib.bib53)) | 2024 | Applying asymmetric
    tensor-wise and layer-wise partitioning for pipeline-parallel inference to minimize
    communication and computation costs over heterogeneous GPUs. |'
  prefs: []
  type: TYPE_TB
- en: '| Throughput [[C14]](#S4.SS2.SSS1.tab1 "4.2.1\. Latency and cost efficiency
    ‣ 4.2\. Distributed Inference Scheduling ‣ 4\. Workload Scheduling ‣ Resource
    Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning:
    A Survey") ([4.2.2](#S4.SS2.SSS2 "4.2.2\. Throughput ‣ 4.2\. Distributed Inference
    Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation and Workload Scheduling
    for Large-Scale Distributed Deep Learning: A Survey")) | Rafiki (Wang et al.,
    [2018](#bib.bib124)) | 2018 | Using a practical AIMD algorithm to adjust inference
    batch size. |'
  prefs: []
  type: TYPE_TB
- en: '| Nanily (Tang et al., [2019](#bib.bib109)) | 2019 | Deriving the corresponding
    batch size so that the inference completion time is equal to or close to the maximum
    remaining time. |'
  prefs: []
  type: TYPE_TB
- en: '|  | RRL (Qin et al., [2019](#bib.bib90)) | 2019 | Focusing on optimizing parallel
    configurations at different levels. |'
  prefs: []
  type: TYPE_TB
- en: '|  | IRIS (Ferikoglou et al., [2023](#bib.bib29)) | 2023 | Adaptively adjusting
    the number of inference threads or containers based on predicted QPS to increase
    computation resource utilization in the cloud. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Morphling (Wang et al., [2021](#bib.bib121)) | 2021 | Adapting the meta-model
    to a new inference service by sampling a small number of configurations and using
    it to find the optimal one. |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 4.2\. Distributed Inference Scheduling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The scheduling of distributed inference workloads on available GPUs to meet
    various performance requirements is critical for the application of distributed
    DL models, especially as online services. Distinct from distributed training workloads,
    which are typically iterative, long-term, and resource-intensive, distributed
    inference workloads exhibit another set of characteristics: one-round, short-term,
    and lightweight (Ye et al., [2024b](#bib.bib135); Tang et al., [2023b](#bib.bib108)).
    In correspondence with such workload characteristic differences, the scheduling
    of distributed inference workloads also focuses on latency in addition to cost
    efficiency and throughput. Table [5](#S4.T5 "Table 5 ‣ 4.1.3\. Deadline Guarantee
    ‣ 4.1\. Distributed Training Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey")
    summarizes these distributed inference-scheduling strategies, focusing on various
    performance goals, including latency, cost efficiency, and throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C13]: Profiling workload characteristics of distributed inference
    jobs and scheduling them in low-latency and cost-efficient manners with an awareness
    of resource budgets. |'
  prefs: []
  type: TYPE_TB
- en: 4.2.1\. Latency and cost efficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Scheduling distributed inference jobs faces Challenge [[C13]](#S4.SS2.tab1
    "4.2\. Distributed Inference Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation
    and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey").
    The inference latency refers to the time it takes to make a prediction given an
    inference query. To maintain satisfactory latency, distributed inference schedulers
    are designed to scale resources proactively in response to request density and
    to reorder execution sequences strategically at the job level. For example, Sniper (Liu
    et al., [2022a](#bib.bib71)) stands out as a self-updating cloud-edge collaborative
    inference scheduling system with a focus on time awareness. It abstracts heterogeneous
    hardware resources and employs a non-invasive performance characterization model
    to predict the inference time of DNNs accurately based on neural network similarity.
    This system achieves a stable increase in throughput successfully even in dynamic
    cloud-edge environments, demonstrating its effectiveness and robustness in optimizing
    the distributed inference scheduling. Ace-Sniper (Liu et al., [2024a](#bib.bib72))
    extends Sniper by including software platform information in the resource abstraction,
    such as the CUDA and PyTorch library, to tackle heterogeneous hardware and platforms
    for distributed inference. Distributed inference latency is more of a concern
    in wireless networks, where communications are usually unstable and devices are
    heterogeneous. AP² (Shi et al., [2023](#bib.bib98)) aims to minimize distributed
    inference latency in 6G mobile communication systems with communications, heterogeneous
    devices, and task dependency constraints. It estimates task completion time on
    different devices based on profiling results and adopts a genetic algorithm (Haldurai
    et al., [2016](#bib.bib41)) to optimize the task arrangement for minimized inference
    latency while maintaining system reliability.'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, cost efficiency is another critical factor for distributed inference,
    especially when used in cloud services. AutoDeep (Li et al., [2020b](#bib.bib63))
    automates cloud deployment for real-time online DNN inference, focusing on minimizing
    costs while satisfying latency constraints. To achieve this, AutoDeep utilizes
    Bayesian optimization combined with DRL, which enables the adaptive discovery
    of the optimal cloud configuration and device placement and reduces the required
    searching time significantly. Through this method, AutoDeep achieves a trade-off
    between operational costs and latency in DNN inference workloads efficiently.
    HexGen (Jiang et al., [2023](#bib.bib53)) improves distributed inference cost
    efficiency for large generative models over heterogeneous GPU devices. It applies
    asymmetric tensor-wise and layer-wise model partitioning for pipeline-parallel
    inference and aims to minimize communication and computation costs with heterogeneous
    GPU memory constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Latency and cost efficiency are recognized as interdependent objective in the
    inference system design. Improving one objective may inadvertently compromise
    the other if the solution is not designed meticulously, which motivates researchers
    to develop scheduling systems that optimizes both objectives simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: '|   Challenge [C14]: Scheduling many distributed inference jobs with diverse
    workload characteristics to improve the inference throughput in the cloud. |'
  prefs: []
  type: TYPE_TB
- en: 4.2.2\. Throughput
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Scheduling batches of distributed inference jobs in the cloud also faces Challenge
    [[C14]](#S4.SS2.SSS1.tab1 "4.2.1\. Latency and cost efficiency ‣ 4.2\. Distributed
    Inference Scheduling ‣ 4\. Workload Scheduling ‣ Resource Allocation and Workload
    Scheduling for Large-Scale Distributed Deep Learning: A Survey"). To tackle this
    challenge, researchers typically refine the scheduling system for distributed
    inference workloads to enhance throughput through batch execution and configuration
    adjustments.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Batch execution: Batching inference has been identified as an efficient
    method to enhance resource utilization and reduce scheduling overhead (Ye et al.,
    [2024b](#bib.bib135)). Various schedulers incorporate heuristic methods to fine-tune
    the batch size for the optimal performance. For instance, Rafiki (Wang et al.,
    [2018](#bib.bib124)) employs a practical Additive-Increase Multiplicative-Decrease
    (AIMD) algorithm to adjust the inference batch size dynamically. This approach
    allows for responsive adaptation to varying workload conditions. Nanily (Tang
    et al., [2019](#bib.bib109)) establishes an upper limit on the batch size by calculating
    the maximum remaining time for a request, which is determined by subtracting the
    minimum queuing time of available resources from the remaining time. It then computes
    an appropriate batch size such that the inference completion time equals to or
    approximates this maximum remaining time.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Configuration adjustment: In addition to the batch-execution approach,
    certain schedulers employ end-to-end configuration tuning to enhance distributed
    inference throughput. RRL (Qin et al., [2019](#bib.bib90)) emphasizes the optimization
    of parallel configurations at various levels, including inter-request-level and
    intra-request-level parallelisms. This optimization significantly reduces the
    overall system latency and improves the throughput. In cloud environments, distributed
    inference throughput is significantly affected by client queries per second (QPS)
    and the number of parallel workers in the inference system. IRIS (Ferikoglou et al.,
    [2023](#bib.bib29)) adaptively adjusts the parallelism level based on the online
    inference QPS predicted by a model pre-trained with monitoring profiles in an
    offline phase. IRIS integrates the parallelism level scheduling algorithm into
    the container orchestration platform, increasing overall computational resource
    utilization and throughput for distributed inference within the cluster. Morphling (Wang
    et al., [2021](#bib.bib121)), on the other hand, presents a rapid and near-optimal
    auto-configuration framework designed specifically for cloud-native model serving.
    This framework adapts to new inference services by sampling a limited set of configurations
    and then employs a meta-model to identify the most optimal configuration. This
    strategy allows Morphling to adjust quickly and efficiently to various service
    requirements while maintaining high system performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$ Fine-grained workload ordering and overlapping is key to scheduling
    large-scale distributed DL workloads in various parallelism modes. Job-level scheduling
    is important for online instant workloads and offline batch workloads when the
    distributed DL workloads are relatively lightweight in the multi-tenant data center
    environment. However, as the volumes of models and resources increase rapidly,
    pipeline-level scheduling for large models in large-scale clusters, a scheduling
    approach orthogonal to job-level scheduling, is essential for contemporary distributed
    training. Though obeying different training procedures, data-parallel and model-parallel
    training can both leverage the pipeline to optimize the execution order and maximize
    the overlapping of different processing stages, including computation-computation,
    computation-communication, and communication-communication overlapping. In practice,
    people pipelines the workloads of hybrid training parallelism modes for greater
    training throughput.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Solving complex distributed DL workload scheduling problems typically
    requires DRL. Distributed DL workload scheduling can be a complex problem, especially
    in large-scale data centers with various resource and performance goal constraints.
    Firstly, DRL can quickly adapt to constantly changing distributed DL environments
    and workloads by optimizing the policy through trial and error. Secondly, DRL
    can internally train DNNs for policy and value decisions to efficiently explore
    the vast search space of large-scale distributed DL workload scheduling. Thirdly,
    DRL can make real-time decisions for distributed DL workload scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Though throughput is essential for distributed DL workload scheduling,
    cost efficiency is an increasing concern. As the energy and monetary cost of distributed
    training and inference increases exponentially with large datasets and models,
    cost efficiency has become a decisive factor when deploying a training or inference
    process in the cloud from both providers’ and users’ perspectives. On the one
    hand, cloud providers must measure the cost of scheduling dynamic distributed
    DL workloads on diverse resources and design a competitive cost model for distributed
    training and inference services. On the other hand, cloud users need to estimate
    the cost of distributed training or inference based on the cost model and strike
    a balance between cost and other performance goals, such as throughput.
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Distributed Training of LLMs: A Case Study'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recently, with the tremendous success of the application of LLMs (Brown et al.,
    [2020](#bib.bib14); Thoppilan et al., [2022](#bib.bib114); Touvron et al., [2023a](#bib.bib115),
    [b](#bib.bib116); et al., [2023](#bib.bib27)) in various domains, such as NLP (Zhao
    et al., [2023b](#bib.bib147)), programming (Nijkamp et al., [2023](#bib.bib83)),
    finance (Huang et al., [2023](#bib.bib46)), and medicine (Thirunavukarasu et al.,
    [2023](#bib.bib113)), distributed training and fine-tuning LLMs efficiently have
    become a heated and important topic for researchers in the fields of computer
    science, artificial intelligence, and communications. As contemporary LLMs are
    in ultra-large sizes with up to hundreds of billions of parameters, training them
    typically requires hundreds of billions of tokens in the training dataset, hundreds
    of GPUs, and tens of days (Zhao et al., [2023b](#bib.bib147)). Efficient resource
    allocation and workload scheduling distributed DL strategies that can scale well
    to large data centers and workloads are critical for LLM training. This section
    examines several real cases of training LLMs in existing literature (Narayanan
    et al., [2021](#bib.bib82); Liao et al., [2021](#bib.bib66); Yuan et al., [2022](#bib.bib141);
    Athlur et al., [2022](#bib.bib8); Smith et al., [2022](#bib.bib102); Ryabinin
    et al., [2023](#bib.bib93); Jang et al., [2023](#bib.bib50)) to uncover insights
    and practical considerations for applying these distributed DL framework strategies
    in a large-scale setting.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ What are the important considerations for allocating resources across
    multiple data centers for LLM training? As LLM training requires a large volume
    of computational resources, the resources available in a single data center may
    not be able to support an LLM training job. Collaborative training of LLMs across
    multiple geologically apart data centers, , which form a computational power network
    that share information and resources, has become a common practice (Yuan et al.,
    [2022](#bib.bib141)) and faced several new challenges. Firstly, compared to distributed
    training within a single data center, resource allocation across data centers
    faces significant challenges due to the heterogeneity in various computational
    and communication resources. Secondly, with many tenants across multiple data
    centers, performance isolation enabled by various virtual technologies must be
    ensured to prevent performance interference between different tenants and workloads.
    Thirdly, fault tolerance and data security are major concerns when considering
    the network transfer of the model and data to other data centers. The former concern
    will be discussed soon while the latter is solved mainly at the distributed DL
    algorithm level via federated learning (Liu et al., [2022b](#bib.bib68)), which
    is not a focus of this survey.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ How to efficiently allocate resources for LLM training on a heterogeneous
    computational power network? Tackling heterogeneous resources in a computational
    power network, the resource allocator needs global knowledge about the resource
    capacity, pricing, and other specifications of all data centers (Liao et al.,
    [2021](#bib.bib66)). The computational power network should also monitor real-time
    resource usage status and workload profiles within each data center. While leveraging
    both global and local resource information of the computational power network,
    the resource allocator considers user-specific and workload-specific requirements,
    such as the geological preference, price and completion time constraint, cost
    of transferring the training model and dataset, and training performance estimate.
    Based on these factors, the resource allocator can distribute black-boxed resources
    efficiently for LLM training within the computational power network with heterogeneous
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ How crucial is pipeline parallelism for LLM training? Pipeline parallelism
    is essential for LLM training. As indicated in (Narayanan et al., [2021](#bib.bib82)),
    by diminishing the impact of the communication volume and worker idle time during
    pipeline flushes, heuristic pipeline parallelism proves effective in practice
    with trillion-scale LLMs on more than 3,000 GPU. In contrast to layer-slicing
    parallelism, multiple-layer-slicing pipeline parallelism only communicates end-of-layer
    activations and gradients, which can be 300$times$ smaller in the communication
    volume in a 2.2-billion-parameter example (Athlur et al., [2022](#bib.bib8)).
    It is a common practice to use what is known as 3D parallelism (Smith et al.,
    [2022](#bib.bib102); Zeng et al., [2023](#bib.bib142)) for LLM training, which
    combines data, pipeline, and layer-slicing parallelisms, to maximize the training
    throughput.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ How crucial is fault-tolerant scheduling for LLM training? Given the
    involvement of a large number of workers in prolonged training sessions for LLMs,
    ensuring fault-tolerance is of utmost importance for resilient scheduling. Frequent
    failure in devices or networks can potentially block the training process, degrade
    the convergence performance, and necessitate redundant restarting of failed tasks
    and pipelines. SWARM parallelism (Ryabinin et al., [2023](#bib.bib93)) incorporates
    the dynamic membership of unstable workers into account for fault-tolerant pipeline
    scheduling. This dynamic fault-tolerant pipeline scheduling allows rerouting a
    task from a disconnected worker to other workers and ensures continuous task execution
    in case of worker failure in the pipeline. According to Oobleck (Jang et al.,
    [2023](#bib.bib50)), a failed pipeline can be recovered by using pipeline replicas
    and templates swiftly. We can instantiate some logically equivalent pipeline replicas,
    which possess replicated model states. Additionally, we define pipeline templates,
    which include information about the number of workers and stages in the pipeline,
    as well as the mapping of stages to GPUs. Once a pipeline failure occurs, a new
    pipeline can be restored based on the pipeline template and replicas instantly.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Conclusion and Outlook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1\. Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the explosive increase in the volume of data, models, and resources, efficient
    framework strategies, including resource allocation and workload scheduling, are
    crucial for distributed DL. This survey systematically investigates up-to-date
    efficient resource allocation and workload scheduling framework strategies for
    large-scale distributed DL. The discussion covers topics focusing on various resource
    types, scheduling granularity levels, and performance goals during the training
    and inference processes of distributed DL. We highlight the critical challenges
    for each topic and introduce the corresponding solutions. To illustrate the practical
    application of these framework strategies in real scenarios, we use a case study
    on distributed LLM training, typically with tens of billions of parameters on
    hundreds of GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Outlook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the emergence of LLMs trained on ultra-large datasets, an increasing number
    of ultra-large GPU data centers are in use or on construction schedule. Distributed
    DL framework strategies targeting large-scale settings with ultra-large data,
    models, and clusters are deemed a future research trend in this domain. Compared
    to traditional distributed DL, large-scale distributed DL has new characteristics
    and poses new challenges for resource allocation and workload scheduling. We discuss
    these characteristics and challenges pertaining to the large scale as a hint of
    future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1\. Multi-data center collaborative learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many large technology corporations and research organizations are constructing
    computational power networks consisting of multiple geographically distributed
    GPU data centers. Promoting new computing paradigms for contemporary large-scale
    distributed DL, the computational power network enables the sharing and coordination
    of ultra-large computational resources across multiple data centers for large
    workloads. However, compared to distributed DL within a single data center, the
    computational power network case presents various resource allocation and workload
    scheduling challenges, including higher resource heterogeneity, higher communication
    overhead, and tighter requirements for fault tolerance and data security. Overcoming
    these challenges requires scalable algorithms that work efficiently in large-scale
    environments with various constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2\. Resource and workload heterogeneity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the frequent upgrades of hardware devices for distributed DL, data centers
    commonly have heterogeneous computational and communication resources with various
    capacities in storage, computation speed, network bandwidth and latency, and energy
    and pricing costs. Distributed DL workloads also exhibit heterogeneity in various
    aspects, such as dataset distribution, model complexity, distributed training
    parallelism modes, model synchronization mechanisms, and training dynamics. A
    promising research direction is dynamic resource allocation to adjust resources
    based on resource availability across heterogeneous environments. Another is adaptive
    workload scheduling, which matches the dynamic nature of changing workloads during
    prolonged training and inference processes to continually optimize performance.
    Solving these scheduling problems with resource and workload heterogeneity also
    requires efficient optimization methods, such as DRL.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3\. Pipeline execution for large-model training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pipelining has been applied to overlap various computational and communication
    workloads in large-model training. However, the optimization of pipeline execution
    for various training parallelism modes of large models has yet to be sufficiently
    explored. An example is related to adaptive pipeline scheduling. During the long-term
    execution of large-model training with dynamic workloads, the pipeline execution
    plan that initially balanced workloads can lead to significant workload skew among
    the workers. Fine-grained adaptive pipeline scheduling that dynamically adjusts
    the granularity of pipeline stages and rebalancing workloads can reduce pipeline
    stall throughout the training process. Another example is related to hierarchical
    pipeline scheduling. To isolate the negative influence of the pipeline stall,
    the global and multi-level local workload schedulers can use multiple pipelines
    hierarchically within a computing node, within a rack, within a data center, and
    across data centers.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.4\. Resilient distributed DL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Failure in devices and tasks are always in company with distributed frameworks,
    and its influence is non-negligible in large-scale environments. Resilient distributed
    DL framework strategies that can tolerate various failures become important for
    large-scale resource allocation and workload scheduling. When optimizing distributed
    DL framework strategies, the solution should consider GPU failures, network disruptions,
    and storage issues that can interrupt the distributed DL process and decay performance.
    Resource allocation strategies could proactively allocate redundant resources
    based on device failure considerations to tolerate failures. Workload scheduling
    strategies could replicate datasets and tasks to tolerate task failures or conduct
    checkpoints to reduce recovery overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.5\. Orchestration with distributed DL algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: By being aware of the mechanisms of distributed DL algorithms, distributed DL
    framework strategies can be optimized for and orchestrated with them. For example,
    the workload scheduler can accurately estimate the communication overhead and
    effectively optimize the scheduling solution by orchestrating lossless or lossy
    compression technologies for gradient compression and model synchronization mechanisms
    for data-parallel training and federating learning. The resource allocator can
    promptly and dynamically adjust corresponding resources for distributed DL workloads
    by being aware of the adaptive policies of distributed DL algorithms. Distributed
    DL algorithms can also be jointly optimized with distributed DL framework strategies
    for their best configurations and adaptive policies.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.6\. Orchestration with distributed DL infrastructures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Modern distributed DL infrastructures usually apply virtualization technologies
    and programmable network devices to extend resource capacities. On the one hand,
    virtualization technologies change resource capacities and pricing costs of computational
    and communication devices. On the other hand, programmable network devices, such
    as programmable switches, extend the capability of network devices by integrating
    limited computational power. They often work to optimize distributed DL-aware
    network traffic, such as in-network aggregation for distributed gradients. These
    infrastructure technologies allow framework strategies to be optimized for various
    performance goals other than throughput, such as cost efficiency, performance
    isolation, and network congestion. Resource allocation strategies can leverage
    virtualization technologies to extend the allocation capability elastically, and
    workload scheduling strategies should consider virtualization performance and
    in-network aggregation when determining an optimal scheduling solution.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cud (2024a) 2024a. *CUDA*. [https://developer.nvidia.com/cuda-zone](https://developer.nvidia.com/cuda-zone)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nvi (2024a) 2024a. NVIDIA multi-instance GPU. [https://www.nvidia.com/en-us/technologies/multi-instance-gpu/](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nvi (2024b) 2024b. NVIDIA Multi-process service. [https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cud (2024b) 2024b. Unified Memory for CUDA. [https://developer.nvidia.com/blog/unified-memory-cuda-beginners/](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Al-Garadi et al. (2020) Mohammed Ali Al-Garadi, Amr Mohamed, Abdulla Khalid
    Al-Ali, Xiaojiang Du, Ihsan Ali, and Mohsen Guizani. 2020. A Survey of Machine
    and Deep Learning Methods for Internet of Things (IoT) Security. *IEEE Communications
    Surveys & Tutorials* 22, 3 (2020), 1646–1685.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amerini et al. (2019) Irene Amerini, Chang-Tsun Li, and Roberto Caldelli. 2019.
    Social network identification through image classification with CNN. *IEEE access*
    7 (2019), 35264–35273.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Athlur et al. (2022) Sanjith Athlur, Nitika Saran, Muthian Sivathanu, Ramachandran
    Ramjee, and Nipun Kwatra. 2022. Varuna: Scalable, Low-Cost Training of Massive
    Deep Learning Models. In *Proceedings of the Seventeenth European Conference on
    Computer Systems* (Rennes, France). New York, NY, USA, 472–487.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baccour et al. (2022) Emna Baccour, Naram Mhaisen, Alaa Awad Abdellatif, Aiman
    Erbad, Amr Mohamed, Mounir Hamdi, and Mohsen Guizani. 2022. Pervasive AI for IoT
    Applications: A Survey on Resource-Efficient Distributed Artificial Intelligence.
    *IEEE Communications Surveys & Tutorials* 24, 4 (2022), 2366–2418.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio.
    2015. Neural machine translation by jointly learning to align and translate. In
    *3rd International Conference on Learning Representations, ICLR 2015*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2020) Zhihao Bai, Zhen Zhang, Yibo Zhu, and Xin Jin. 2020. PipeSwitch:
    Fast pipelined context switching for deep learning applications. In *14th USENIX
    Symposium on Operating Systems Design and Implementation (OSDI 20)*. 499–514.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bao et al. (2019) Yixin Bao, Yanghua Peng, and Chuan Wu. 2019. Deep Learning-based
    Job Placement in Distributed Machine Learning Clusters. In *IEEE INFOCOM 2019
    - IEEE Conference on Computer Communications*. 505–513.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bi et al. (2022) Renwan Bi, Jinbo Xiong, Youliang Tian, Qi Li, and Kim-Kwang Raymond
    Choo. 2022. Achieving lightweight and privacy-preserving object detection for
    connected autonomous vehicles. *IEEE Internet of Things Journal* 10, 3 (2022),
    2314–2329.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems* 33 (2020), 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2023) Xuanyu Cao, Tamer Başar, Suhas Diggavi, Yonina C Eldar, Khaled B
    Letaief, H Vincent Poor, and Junshan Zhang. 2023. Communication-efficient distributed
    learning: An overview. *IEEE journal on selected areas in communications* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Chen Chen, Sen Wang, Yingwen Chen, and Jianchen Han. 2023.
    Tereis: A Package-Based Scheduling in Deep Learning Systems. In *2022 IEEE 28th
    International Conference on Parallel and Distributed Systems (ICPADS)*. IEEE,
    867–874.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mingzhe Chen, Nir Shlezinger, H Vincent Poor, Yonina C Eldar,
    and Shuguang Cui. 2021. Communication-efficient federated learning. *Proceedings
    of the National Academy of Sciences* 118, 17 (2021), e2024789118.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020) Zhaoyun Chen, Wei Quan, Mei Wen, Jianbin Fang, Jie Yu, Chunyuan
    Zhang, and Lei Luo. 2020. Deep Learning Research and Development Platform: Characterizing
    and Scheduling with QoS Guarantees on GPU Clusters. *IEEE Transactions on Parallel
    and Distributed Systems* 31, 1 (2020), 34–50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2023) Runxiang Cheng, Chris Cai, Selman Yilmaz, Rahul Mitra, Malay
    Bag, Mrinmoy Ghosh, and Tianyin Xu. 2023. Towards GPU Memory Efficiency for Distributed
    Training at Scale. In *Proceedings of the 2023 ACM Symposium on Cloud Computing*.
    281–297.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. (2022) Junguk Cho, Diman Zad Tootaghaj, Lianjie Cao, and Puneet Sharma.
    2022. Sla-driven ml inference framework for clouds with heterogeneous accelerators.
    *Proceedings of Machine Learning and Systems* 4 (2022), 20–32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choi et al. (2022) Seungbeom Choi, Sunho Lee, Yeonjae Kim, Jongse Park, Youngjin
    Kwon, and Jaehyuk Huh. 2022. Serving heterogeneous machine learning models on
    Multi-GPU servers with Spatio-Temporal sharing. In *2022 USENIX Annual Technical
    Conference (USENIX ATC 22)*. 199–216.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chowdhury and Stoica (2015) Mosharaf Chowdhury and Ion Stoica. 2015. Efficient
    coflow scheduling without prior knowledge. *ACM SIGCOMM Computer Communication
    Review* 45, 4 (2015), 393–406.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dhakal et al. (2020) Aditya Dhakal, Sameer G Kulkarni, and KK Ramakrishnan.
    2020. Gslice: controlled spatial sharing of gpus for a scalable inference platform.
    In *Proceedings of the 11th ACM Symposium on Cloud Computing*. 492–506.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An Image is Worth 16x16 Words:
    Transformers for Image Recognition at Scale. In *International Conference on Learning
    Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duan et al. (2023) Qingyang Duan, Chao Peng, Zeqin Wang, Yuedong Xu, Shaoteng
    Liu, Jun Wu, and John CS Lui. 2023. Accelerating Distributed DNN Training via
    Transport Layer Scheduling. *IEEE Transactions on Parallel and Distributed Systems*
    34, 5 (2023), 1650–1666.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duchi et al. (2011) John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive
    subgradient methods for online learning and stochastic optimization. *Journal
    of machine learning research* 12, 7 (2011).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'et al. (2023) Aakanksha Chowdhery et al. 2023. PaLM: Scaling Language Modeling
    with Pathways. *Journal of Machine Learning Research* 24, 240 (2023), 1–113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2020) Qi Feng, Debiao He, Zhe Liu, Huaqun Wang, and Kim-Kwang Raymond
    Choo. 2020. SecureNLP: A system for multi-party privacy-preserving natural language
    processing. *IEEE Transactions on Information Forensics and Security* 15 (2020),
    3709–3721.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ferikoglou et al. (2023) Aggelos Ferikoglou, Panos Chrysomeris, Achilleas Tzenetopoulos,
    Manolis Katsaragakis, Dimosthenis Masouros, and Dimitrios Soudris. 2023. IRIS:
    Interference and Resource Aware Predictive Orchestration for ML Inference Serving.
    In *2023 IEEE 16th International Conference on Cloud Computing (CLOUD)*. IEEE,
    1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ferrand et al. (2020) Paul Ferrand, Alexis Decurninge, and Maxime Guillaud.
    2020. DNN-based localization from channel estimates: Feature design and experimental
    results. In *GLOBECOM 2020-2020 IEEE Global Communications Conference*. IEEE,
    1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filippini et al. (2023) Federica Filippini, Jonatha Anselmi, Danilo Ardagna,
    and Bruno Gaujal. 2023. A Stochastic Approach for Scheduling AI Training Jobs
    in GPU-based Systems. *IEEE Transactions on Cloud Computing* (2023), 1–17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2022) Wei Gao, Peng Sun, Yonggang Wen, and Tianwei Zhang. 2022.
    Titan: a scheduler for foundation model fine-tuning workloads. In *Proceedings
    of the 13th Symposium on Cloud Computing*. 348–354.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2021) Wei Gao, Zhisheng Ye, Peng Sun, Yonggang Wen, and Tianwei
    Zhang. 2021. Chronus: A novel deadline-aware scheduler for deep learning training
    jobs. In *Proceedings of the ACM Symposium on Cloud Computing*. 609–623.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2024a) Wei Gao, Zhisheng Ye, Peng Sun, Tianwei Zhang, and Yonggang
    Wen. 2024a. UniSched: A Unified Scheduler for Deep Learning Training Jobs With
    Different User Demands. *IEEE Trans. Comput.* 73, 6 (2024), 1500–1515.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2024b) Wei Gao, Xu Zhang, Shan Huang, Shangwei Guo, Peng Sun, Yonggang
    Wen, and Tianwei Zhang. 2024b. AutoSched: An Adaptive Self-configured Framework
    for Scheduling Deep Learning Training Workloads. In *ACM International Conference
    on Supercomputing (ICS)*. 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gomez et al. (2022) Aidan N. Gomez, Oscar Key, Kuba Perlin, Stephen Gou, Nick
    Frosst, Jeff Dean, and Yarin Gal. 2022. Interlocking Backpropagation: Improving
    Depthwise Model-Parallelism. *J. Mach. Learn. Res.* 23, 1 (jan 2022), 1–28.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2019) Juncheng Gu, Mosharaf Chowdhury, KangG. Shin, Yibo Zhu, Myeongjae
    Jeon, Junjie Qian, HongqiangHarry Liu, and Chuanxiong Guo. 2019. Tiresias: A GPU
    Cluster Manager for Distributed Deep Learning. *Networked Systems Design and Implementation,Networked
    Systems Design and Implementation* (Jan 2019), 485–500.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2023) Jianfeng Gu, Yichao Zhu, Puxuan Wang, Mohak Chadha, and Michael
    Gerndt. 2023. FaST-GShare: Enabling efficient spatio-temporal GPU sharing in serverless
    computing for deep learning inference. In *Proceedings of the 52nd International
    Conference on Parallel Processing*. 635–644.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2022) Rong Gu, Yuquan Chen, Shuai Liu, Haipeng Dai, Guihai Chen,
    Kai Zhang, Yang Che, and Yihua Huang. 2022. Liquid: Intelligent resource estimation
    and network-efficient scheduling for deep learning jobs on distributed gpu clusters.
    *IEEE Transactions on Parallel and Distributed Systems* 33, 11 (2022), 2808–2820.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gunasekaran et al. (2022) Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth
    Thinakaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R Das. 2022. Cocktail:
    A multidimensional optimization for model serving in cloud. In *19th USENIX Symposium
    on Networked Systems Design and Implementation (NSDI 22)*. 1041–1057.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haldurai et al. (2016) Lingaraj Haldurai, T Madhubala, and R Rajalakshmi. 2016.
    A study on genetic algorithm and its applications. *International Journal of computer
    sciences and Engineering* 4, 10 (2016), 139.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2021) Yihong He, Weibo Cai, Pan Zhou, Gang Sun, Shouxi Luo, Hongfang
    Yu, and Mohsen Guizani. 2021. Beamer: Stage-aware coflow scheduling to accelerate
    hyper-parameter tuning in deep learning clusters. *IEEE Transactions on Network
    and Service Management* 19, 2 (2021), 1083–1097.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2021) Qinghao Hu, Peng Sun, Shengen Yan, Yonggang Wen, and Tianwei
    Zhang. 2021. Characterization and prediction of deep learning workloads in large-scale
    gpu datacenters. In *Proceedings of the International Conference for High Performance
    Computing, Networking, Storage and Analysis*. 1–15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2023) Qinghao Hu, Zhisheng Ye, Meng Zhang, Qiaoling Chen, Peng Sun,
    Yonggang Wen, and Tianwei Zhang. 2023. Hydro: Surrogate-Based Hyperparameter Tuning
    Service in Datacenters. In *17th USENIX Symposium on Operating Systems Design
    and Implementation (OSDI 23)*. 757–777.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hua et al. (2019) Yuxiu Hua, Zhifeng Zhao, Rongpeng Li, Xianfu Chen, Zhiming
    Liu, and Honggang Zhang. 2019. Deep learning with long short-term memory for time
    series prediction. *IEEE Communications Magazine* 57, 6 (2019), 114–119.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Allen H Huang, Hui Wang, and Yi Yang. 2023. FinBERT: A
    large language model for extracting information from financial text. *Contemporary
    Accounting Research* 40, 2 (2023), 806–841.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat,
    Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al.
    2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism.
    *Advances in neural information processing systems* 32 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hwang et al. (2021) Changho Hwang, Taehyun Kim, Sunghyun Kim, Jinwoo Shin, and
    KyoungSoo Park. 2021. Elastic resource sharing for distributed deep learning.
    In *18th USENIX Symposium on Networked Systems Design and Implementation (NSDI
    21)*. 721–739.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jahani et al. (2019) Arezoo Jahani, Marco Lattuada, Michele Ciavotta, Danilo
    Ardagna, Edoardo Amaldi, and Li Zhang. 2019. Optimizing on-demand GPUs in the
    Cloud for Deep Learning Applications Training. In *2019 4th International Conference
    on Computing, Communications and Security (ICCCS)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jang et al. (2023) Insu Jang, Zhenning Yang, Zhen Zhang, Xin Jin, and Mosharaf
    Chowdhury. 2023. Oobleck: Resilient Distributed Training of Large Models Using
    Pipeline Templates. In *Proceedings of the 29th Symposium on Operating Systems
    Principles* (Koblenz, Germany). New York, NY, USA, 382–395.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jayaram et al. (2019) K. R. Jayaram, Vinod Muthusamy, Parijat Dube, Vatche
    Ishakian, Chen Wang, Benjamin Herta, Scott Boag, Diana Arroyo, Asser Tantawi,
    Archit Verma, Falk Pollok, and Rania Khalaf. 2019. FfDL: A Flexible Multi-tenant
    Deep Learning Platform. In *Proceedings of the 20th International Middleware Conference*.
    82–95.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeon et al. (2019) Myeongjae Jeon, Shivaram Venkataraman, Amar Phanishayee,
    Junjie Qian, Wencong Xiao, and Fan Yang. 2019. Analysis of Large-ScaleMulti-TenantGPU
    clusters for DNN training workloads. In *2019 USENIX Annual Technical Conference
    (USENIX ATC 19)*. 947–960.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2023) Youhe Jiang, Ran Yan, Xiaozhe Yao, Beidi Chen, and Binhang
    Yuan. 2023. Hexgen: Generative inference of foundation model over heterogeneous
    decentralized environment. *arXiv preprint arXiv:2311.11514* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. (2024) Xin Jin, Zhihao Bai, Zhen Zhang, Yibo Zhu, Yinmin Zhong,
    and Xuanzhe Liu. 2024. DistMind: Efficient Resource Disaggregation for Deep Learning
    Workloads. *IEEE/ACM Transactions on Networking* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. (2020) Minkoo Kang, Gyeongsik Yang, Yeonho Yoo, and Chuck Yoo.
    2020. TensorExpress: In-network communication scheduling for distributed deep
    learning. In *2020 IEEE 13th international conference on cloud computing (CLOUD)*.
    IEEE, 25–27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method
    for Stochastic Optimization. In *3rd International Conference on Learning Representations,
    ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Jiamin Li, Hong Xu, Yibo Zhu, Zherui Liu, Chuanxiong Guo,
    and Cong Wang. 2023b. Lyra: Elastic scheduling for deep learning clusters. In
    *Proceedings of the Eighteenth European Conference on Computer Systems*. 835–850.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Mingzhen Li, Wencong Xiao, Hailong Yang, Biao Sun, Hanyu
    Zhao, Shiru Ren, Zhongzhi Luan, Xianyan Jia, Yi Liu, Yong Li, et al. 2023a. EasyScale:
    Elastic Training with Consistent Accuracy and Improved Utilization on GPUs. In
    *Proceedings of the International Conference for High Performance Computing, Networking,
    Storage and Analysis*. 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2014) Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. 2014.
    Efficient mini-batch training for stochastic optimization. In *Proceedings of
    the 20th ACM SIGKDD international conference on Knowledge discovery and data mining*.
    661–670.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Hoefler (2021) Shigang Li and Torsten Hoefler. 2021. Chimera: efficiently
    training large-scale neural networks with bidirectional pipelines. In *Proceedings
    of the International Conference for High Performance Computing, Networking, Storage
    and Analysis*. 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020c) Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter
    Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and
    Soumith Chintala. 2020c. PyTorch Distributed: Experiences on Accelerating Data
    Parallel Training. *Proc. VLDB Endow.* 13, 12 (aug 2020), 3005–3018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020a) Wenxin Li, Sheng Chen, Keqiu Li, Heng Qi, Renhai Xu, and Song
    Zhang. 2020a. Efficient online scheduling for coflow-aware machine learning clusters.
    *IEEE Transactions on Cloud Computing* 10, 4 (2020), 2564–2579.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020b) Yang Li, Zhenhua Han, Quanlu Zhang, Zhenhua Li, and Haisheng
    Tan. 2020b. Automating cloud deployment for deep learning inference of real-time
    online services. In *IEEE INFOCOM 2020-IEEE Conference on Computer Communications*.
    IEEE, 1668–1677.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023c) Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying
    Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E Gonzalez, et al.
    2023c. AlpaServe: Statistical multiplexing with model parallelism for deep learning
    serving. In *17th USENIX Symposium on Operating Systems Design and Implementation
    (OSDI 23)*. 663–679.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2024) Feng Liang, Zhen Zhang, Haifeng Lu, Victor C. M. Leung,
    Yanyi Guo, and Xiping Hu. 2024. Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey. arXiv:2404.06114 [cs.DC]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao et al. (2021) Heng Liao, Jiajin Tu, Jing Xia, Hu Liu, Xiping Zhou, Honghui
    Yuan, and Yuxing Hu. 2021. Ascend: a Scalable and Unified Architecture for Ubiquitous
    Deep Neural Network Computing : Industry Track Paper. In *2021 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA)*. 789–801.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lim et al. (2021) Gangmuk Lim, Jeongseob Ahn, Wencong Xiao, Youngjin Kwon,
    and Myeongjae Jeon. 2021. Zico: Efficient GPU Memory Sharing for Concurrent DNN
    Training. In *2021 USENIX Annual Technical Conference (USENIX ATC 21)*. 161–175.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022b) Ji Liu, Jizhou Huang, Yang Zhou, Xuhong Li, Shilei Ji, Haoyi
    Xiong, and Dejing Dou. 2022b. From distributed machine learning to federated learning:
    A survey. *Knowledge and Information Systems* 64, 4 (2022), 885–917.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2024b) Kaiyang Liu, Jingrong Wang, Zhiming Huang, and Jianping Pan.
    2024b. Sampling-Based Multi-Job Placement for Heterogeneous Deep Learning Clusters.
    *IEEE Transactions on Parallel and Distributed Systems* (2024).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022d) Liu Liu, Jian Yu, and Zhijun Ding. 2022d. Adaptive and Efficient
    GPU Time Sharing for Hyperparameter Tuning in Cloud. In *Proceedings of the 51st
    International Conference on Parallel Processing*. 1–11.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022a) Weihong Liu, Jiawei Geng, Zongwei Zhu, Jing Cao, and Zirui
    Lian. 2022a. Sniper: Cloud-Edge Collaborative Inference Scheduling with Neural
    Network Similarity Modeling. In *Proceedings of the 59th ACM/IEEE Design Automation
    Conference* (San Francisco, California) *(DAC ’22)*. New York, NY, USA, 505–510.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024a) Weihong Liu, Jiawei Geng, Zongwei Zhu, Yang Zhao, Cheng
    Ji, Changlong Li, Zirui Lian, and Xuehai Zhou. 2024a. Ace-Sniper: Cloud–Edge Collaborative
    Scheduling Framework With DNN Inference Latency Modeling on Heterogeneous Devices.
    *IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems*
    43, 2 (2024), 534–547.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022c) Weijie Liu, Zhiquan Lai, Shengwei Li, Yabo Duan, Keshi Ge,
    and Dongsheng Li. 2022c. AutoPipe: A fast pipeline parallelism approach with balanced
    partitioning and micro-batch slicing. In *2022 IEEE International Conference on
    Cluster Computing (CLUSTER)*. IEEE, 301–312.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Ziming Liu, Shenggan Cheng, Haotian Zhou, and Yang You. 2023.
    Hanayo: Harnessing Wave-like Pipeline Parallelism for Enhanced Large Model Training
    Efficiency. In *Proceedings of the International Conference for High Performance
    Computing, Networking, Storage and Analysis*. 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lotfollahi et al. (2020) Mohammad Lotfollahi, Mahdi Jafari Siavoshani, Ramin
    Shirali Hossein Zade, and Mohammdsadegh Saberian. 2020. Deep packet: A novel approach
    for encrypted traffic classification using deep learning. *Soft Computing* 24,
    3 (2020), 1999–2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luan et al. (2019) Yunteng Luan, Xukun Chen, Hanyu Zhao, Zhi Yang, and Yafei
    Dai. 2019. SCHED²: Scheduling Deep Learning Training via Deep Reinforcement Learning.
    In *2019 IEEE Global Communications Conference (GLOBECOM)*. IEEE, 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mao et al. (2023) Ying Mao, Vaishali Sharma, Wenjia Zheng, Long Cheng, Qiang
    Guan, and Ang Li. 2023. Elastic Resource Management for Deep Learning Applications
    in a Container Cluster. *IEEE Transactions on Cloud Computing* 11, 2 (2023), 2204–2216.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mayer and Jacobsen (2020) Ruben Mayer and Hans-Arno Jacobsen. 2020. Scalable
    deep learning on distributed infrastructures: Challenges, techniques, and tools.
    *ACM Computing Surveys (CSUR)* 53, 1 (2020), 1–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mo et al. (2024) Zizhao Mo, Huanle Xu, and Chengzhong Xu. 2024. Heet: Accelerating
    Elastic Training in Heterogeneous Deep Learning Clusters. In *Proceedings of the
    29th ACM International Conference on Architectural Support for Programming Languages
    and Operating Systems, Volume 2*. 499–513.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Narayanan et al. (2019) Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek
    Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia.
    2019. PipeDream: Generalized pipeline parallelism for DNN training. In *Proceedings
    of the 27th ACM Symposium on Operating Systems Principles*. 1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narayanan et al. (2020) Deepak Narayanan, Keshav Santhanam, Fiodar Kazhamiaka,
    Amar Phanishayee, and Matei Zaharia. 2020. Heterogeneity-Aware cluster scheduling
    policies for deep learning workloads. In *14th USENIX Symposium on Operating Systems
    Design and Implementation (OSDI 20)*. 481–498.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narayanan et al. (2021) Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick
    LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,
    Julie Bernauer, Bryan Catanzaro, et al. 2021. Efficient large-scale language model
    training on gpu clusters using megatron-lm. In *Proceedings of the International
    Conference for High Performance Computing, Networking, Storage and Analysis*.
    1–15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nijkamp et al. (2023) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. CodeGen: An Open
    Large Language Model for Code with Multi-Turn Program Synthesis. In *The Eleventh
    International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oh et al. (2022) Hyungjun Oh, Junyeol Lee, Hyeongju Kim, and Jiwon Seo. 2022.
    Out-of-order backprop: An effective scheduling technique for deep learning. In
    *Proceedings of the Seventeenth European Conference on Computer Systems*. 435–452.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ouyang et al. (2021) Shuo Ouyang, Dezun Dong, Yemao Xu, and Liquan Xiao. 2021.
    Communication optimization strategies for distributed deep neural network training:
    A survey. *J. Parallel and Distrib. Comput.* 149 (2021), 52–65.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2020) Jay H Park, Gyeongchan Yun, M Yi Chang, Nguyen T Nguyen,
    Seungmin Lee, Jaesik Choi, Sam H Noh, and Young-ri Choi. 2020. HetPipe: Enabling
    large DNN training on (whimpy) heterogeneous GPU clusters through integration
    of pipelined model parallelism and data parallelism. In *2020 USENIX Annual Technical
    Conference (USENIX ATC 20)*. 307–321.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pavlidakis et al. (2024) Manos Pavlidakis, Giorgos Vasiliadis, Stelios Mavridis,
    Anargyros Argyros, Antony Chazapis, and Angelos Bilas. 2024. G-Safe: Safe GPU
    Sharing in Multi-Tenant Environments. *arXiv preprint arXiv:2401.09290* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2018) Yanghua Peng, Yixin Bao, Yangrui Chen, Chuan Wu, and Chuanxiong
    Guo. 2018. Optimus: an efficient dynamic resource scheduler for deep learning
    clusters. In *Proceedings of the Thirteenth EuroSys Conference*. 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiao et al. (2021) Aurick Qiao, Sang Keun Choe, Suhas Jayaram Subramanya, Willie
    Neiswanger, Qirong Ho, Hao Zhang, Gregory R. Ganger, and Eric P. Xing. 2021. Pollux:
    Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning. In *15th USENIX
    Symposium on Operating Systems Design and Implementation (OSDI 21)*. 1–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2019) Heyang Qin, Syed Zawad, Yanqi Zhou, Lei Yang, Dongfang Zhao,
    and Feng Yan. 2019. Swift machine learning model serving scheduling: a region
    based reinforcement learning approach. In *Proceedings of the International Conference
    for High Performance Computing, Networking, Storage and Analysis*. 1–23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rajasekaran et al. (2024) Sudarsanan Rajasekaran, Manya Ghobadi, and Aditya
    Akella. 2024. CASSINI:Network-Aware Job Scheduling in Machine Learning Clusters.
    In *21st USENIX Symposium on Networked Systems Design and Implementation (NSDI
    24)*. 1403–1420.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Romero et al. (2021) Francisco Romero, Qian Li, Neeraja J Yadwadkar, and Christos
    Kozyrakis. 2021. INFaaS: Automated model-less inference serving. In *2021 USENIX
    Annual Technical Conference (USENIX ATC 21)*. 397–411.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ryabinin et al. (2023) Max Ryabinin, Tim Dettmers, Michael Diskin, and Alexander
    Borzunov. 2023. SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient.
    In *Proceedings of the 40th International Conference on Machine Learning* (Honolulu,
    Hawaii, USA). JMLR, 29416–29440.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sagduyu et al. (2023) Yalin E Sagduyu, Sennur Ulukus, and Aylin Yener. 2023.
    Task-oriented communications for nextG: End-to-end deep learning and AI security
    aspects. *IEEE Wireless Communications* 30, 3 (2023), 52–60.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saikia et al. (2022) Prajwalita Saikia, Sudip Biswas, Keshav Singh, and Chih-Peng
    Li. 2022. Signal Detection in GSM-Based In-Band Full-Duplex Communication Using
    DNN. *IEEE Transactions on Vehicular Technology* 72, 2 (2022), 2661–2666.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Say (2021) Buser Say. 2021. A unified framework for planning with learned neural
    network transition models. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 35\. 5016–5024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2019) Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu
    Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. 2019. Nexus:
    A GPU cluster engine for accelerating DNN-based video analysis. In *Proceedings
    of the 27th ACM Symposium on Operating Systems Principles*. 322–337.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2023) Hongjian Shi, Weichu Zheng, Zifei Liu, Ruhui Ma, and Haibing
    Guan. 2023. Automatic pipeline parallelism: A parallel inference framework for
    deep learning applications in 6G mobile communication systems. *IEEE Journal on
    Selected Areas in Communications* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2021) Shaohuai Shi, Xiaowen Chu, and Bo Li. 2021. MG-WFBP: Merging
    gradients wisely for efficient communication in distributed deep learning. *IEEE
    Transactions on Parallel and Distributed Systems* 32, 8 (2021), 1903–1917.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2024) Shaohuai Shi, Xinglin Pan, Qiang Wang, Chengjian Liu, Xiaozhe
    Ren, Zhongzhe Hu, Yu Yang, Bo Li, and Xiaowen Chu. 2024. ScheMoE: An Extensible
    Mixture-of-Experts Distributed Training System with Tasks Scheduling. In *Proceedings
    of the Nineteenth European Conference on Computer Systems*. 236–249.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2020) Yuanming Shi, Kai Yang, Tao Jiang, Jun Zhang, and Khaled B
    Letaief. 2020. Communication-efficient edge AI: Algorithms and systems. *IEEE
    Communications Surveys & Tutorials* 22, 4 (2020), 2167–2191.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smith et al. (2022) Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley,
    Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
    Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train megatron-turing
    nlg 530b, a large-scale generative language model. *arXiv preprint arXiv:2201.11990*
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Strati et al. (2024) Foteini Strati, Xianzhe Ma, and Ana Klimovic. 2024. Orion:
    Interference-aware, Fine-grained GPU Sharing for ML Applications. In *Proceedings
    of the Nineteenth European Conference on Computer Systems*. 1075–1092.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sultana et al. (2020) Abeda Sultana, Li Chen, Fei Xu, and Xu Yuan. 2020. E-LAS:
    Design and Analysis of Completion-Time Agnostic Scheduling for Distributed Deep
    Learning Cluster. In *49th International Conference on Parallel Processing - ICPP*.
    1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2024) Zhenbo Sun, Huanqi Cao, Yuanwei Wang, Guanyu Feng, Shengqi
    Chen, Haojie Wang, and Wenguang Chen. 2024. AdaPipe: Optimizing Pipeline Parallelism
    with Adaptive Recomputation and Partitioning. In *Proceedings of the 29th ACM
    International Conference on Architectural Support for Programming Languages and
    Operating Systems, Volume 3* (¡conf-loc¿, ¡city¿La Jolla¡/city¿, ¡state¿CA¡/state¿,
    ¡country¿USA¡/country¿, ¡/conf-loc¿) *(ASPLOS ’24)*. Association for Computing
    Machinery, New York, NY, USA, 86–100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2013) Ilya Sutskever, James Martens, George Dahl, and Geoffrey
    Hinton. 2013. On the importance of initialization and momentum in deep learning.
    In *Proceedings of the 30th International Conference on Machine Learning*, Vol. 28.
    PMLR, Atlanta, Georgia, USA, 1139–1147.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ta (2019) Nguyen Binh Duong Ta. 2019. $FC^{2}$: cloud-based cluster provisioning
    for distributed machine learning. *Cluster Computing* (Dec 2019), 1299–1315.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2023b) Shujiong Tang, Yue Yu, Hui Wang, Guiliang Wang, Wuhui Chen,
    Zenglin Xu, Song Guo, and Wen Gao. 2023b. A Survey on Scheduling Techniques in
    Computing and Network Convergence. *IEEE Communications Surveys & Tutorials* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2019) Xuehai Tang, Peng Wang, Qiuyang Liu, Wang Wang, and Jizhong
    Han. 2019. Nanily: A QoS-Aware Scheduling for DNN Inference Workload in Clouds.
    In *2019 IEEE 21st International Conference on High Performance Computing and
    Communications; IEEE 17th International Conference on Smart City; IEEE 5th International
    Conference on Data Science and Systems (HPCC/SmartCity/DSS)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2023a) Zhenheng Tang, Shaohuai Shi, Xiaowen Chu, Wei Wang, and
    Bo Li. 2023a. Communication-efficient distributed deep learning: A comprehensive
    survey. *arXiv preprint arXiv:2003.06307* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tarnawski et al. (2021) Jakub M Tarnawski, Deepak Narayanan, and Amar Phanishayee.
    2021. Piper: Multidimensional planner for dnn parallelization. *Advances in Neural
    Information Processing Systems* 34 (2021), 24829–24840.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thieme et al. (2023) Alexander H Thieme, Yuanning Zheng, Gautam Machiraju, Chris
    Sadee, Mirja Mittermaier, Maximilian Gertler, Jorge L Salinas, Krithika Srinivasan,
    Prashnna Gyawali, Francisco Carrillo-Perez, et al. 2023. A deep-learning algorithm
    to classify skin lesions from mpox virus infection. *Nature medicine* 29, 3 (2023),
    738–747.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023.
    Large language models in medicine. *Nature medicine* 29, 8 (2023), 1930–1940.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
    Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,
    Yu Du, et al. 2022. Lamda: Language models for dialog applications. *arXiv preprint
    arXiv:2201.08239* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vu et al. (2020) Ly Vu, Quang Uy Nguyen, Diep N Nguyen, Dinh Thai Hoang, Eryk
    Dutkiewicz, et al. 2020. Learning latent representation for IoT anomaly detection.
    *IEEE Transactions on Cybernetics* 52, 5 (2020), 3769–3782.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022a) Cen Wang, Noboru Yoshikane, Filippos Balasis, and Takehiro
    Tsuritani. 2022a. OSDL: Dedicated optical slice provisioning in support of distributed
    deep learning. *Computer Networks* 214 (2022), 109191.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022b) Farui Wang, Weizhe Zhang, Shichao Lai, Meng Hao, and Zheng
    Wang. 2022b. Dynamic GPU Energy Optimization for Machine Learning Training Workloads.
    *IEEE Transactions on Parallel and Distributed Systems* (Jan 2022), 1–1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020b) Haoyu Wang, Zetian Liu, and Haiying Shen. 2020b. Job scheduling
    for large-scale machine learning clusters. In *Proceedings of the 16th International
    Conference on emerging Networking EXperiments and Technologies*. 108–120.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Luping Wang, Lingyun Yang, Yinghao Yu, Wei Wang, Bo Li,
    Xianchao Sun, Jian He, and Liping Zhang. 2021. Morphling: fast, near-optimal auto-configuration
    for cloud-native model serving. In *Proceedings of the ACM Symposium on Cloud
    Computing*. 639–653.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024) Sheng Wang, Shiping Chen, and Yumei Shi. 2024. GPARS: Graph
    predictive algorithm for efficient resource scheduling in heterogeneous GPU clusters.
    *Future Generation Computer Systems* 152 (2024), 127–137.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020a) Shuai Wang, Dan Li, and Jinkun Geng. 2020a. Geryon: Accelerating
    distributed CNN training by network-level flow scheduling. In *IEEE INFOCOM 2020-IEEE
    Conference on Computer Communications*. IEEE, 1678–1687.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Wei Wang, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen,
    Teck Khim Ng, Beng Chin Ooi, Jie Shao, and Moaz Reyad. 2018. Rafiki: machine learning
    as an analytics service system. *Proceedings of the VLDB Endowment* (Oct 2018),
    128–140.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weng et al. (2022) Qizhen Weng, Wencong Xiao, Yinghao Yu, Wei Wang, Cheng Wang,
    Jian He, Yong Li, Liping Zhang, Wei Lin, and Yu Ding. 2022. MLaaS in the wild:
    Workload analysis and scheduling in Large-Scale heterogeneous GPU clusters. In
    *19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)*.
    945–960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weng et al. (2023) Qizhen Weng, Lingyun Yang, Yinghao Yu, Wei Wang, Xiaochuan
    Tang, Guodong Yang, and Liping Zhang. 2023. Beware of Fragmentation: Scheduling
    GPU-Sharing Workloads with Fragmentation Gradient Descent. In *2023 USENIX Annual
    Technical Conference (USENIX ATC 23)*. 995–1008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2023) Bingyang Wu, Zili Zhang, Zhihao Bai, Xuanzhe Liu, and Xin Jin.
    2023. Transparent GPU Sharing in Container Clouds for Deep Learning Workloads.
    In *20th USENIX Symposium on Networked Systems Design and Implementation (NSDI
    23)*. 69–85.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2018) Wencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian
    Sivathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu
    Zhang, et al. 2018. Gandiva: Introspective cluster scheduling for deep learning.
    In *13th USENIX Symposium on Operating Systems Design and Implementation (OSDI
    18)*. 595–610.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2020) Wencong Xiao, Shiru Ren, Yong Li, Yang Zhang, Pengyang Hou,
    Zhi Li, Yihui Feng, Wei Lin, and Yangqing Jia. 2020. AntMan: Dynamic scaling on
    GPU clusters for deep learning. In *14th USENIX Symposium on Operating Systems
    Design and Implementation (OSDI 20)*. 533–548.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2022) Fei Xu, Jianian Xu, Jiabin Chen, Li Chen, Ruitao Shang, Zhi
    Zhou, and Fangming Liu. 2022. igniter: Interference-aware gpu resource provisioning
    for predictable dnn inference in the cloud. *IEEE Transactions on Parallel and
    Distributed Systems* 34, 3 (2022), 812–827.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023a) Jin Yang, Liang Bao, Wenjing Liu, Rong Yang, and Chase Q.
    Wu. 2023a. On a Meta Learning-Based Scheduler for Deep Learning Clusters. *IEEE
    Transactions on Cloud Computing* 11, 4 (2023), 3631–3642.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023b) Zichao Yang, Heng Wu, Yuanjia Xu, Yuewen Wu, Hua Zhong,
    and Wenbo Zhang. 2023b. Hydra: Deadline-Aware and Efficiency-Oriented Scheduling
    for Deep Learning Jobs on Heterogeneous GPUs. *IEEE Trans. Comput.* 72, 8 (2023),
    2224–2236.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Jiangchao Yao, Shengyu Zhang, Yang Yao, Feng Wang, Jianxin
    Ma, Jianwei Zhang, Yunfei Chu, Luo Ji, Kunyang Jia, Tao Shen, et al. 2022. Edge-cloud
    polarization and collaboration: A comprehensive survey for ai. *IEEE Transactions
    on Knowledge and Data Engineering* 35, 7 (2022), 6866–6886.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2024a) Zhisheng Ye, Wei Gao, Qinghao Hu, Peng Sun, Xiaolin Wang,
    Yingwei Luo, Tianwei Zhang, and Yonggang Wen. 2024a. Deep Learning Workload Scheduling
    in GPU Datacenters: A Survey. *ACM Comput. Surv.* 56, 6 (jan 2024), 1–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2024b) Zhisheng Ye, Wei Gao, Qinghao Hu, Peng Sun, Xiaolin Wang,
    Yingwei Luo, Tianwei Zhang, and Yonggang Wen. 2024b. Deep Learning Workload Scheduling
    in GPU Datacenters: A Survey. *Comput. Surveys* 56, 6 (2024), 1–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yeung et al. (2021) Gingfung Yeung, Damian Borowiec, Renyu Yang, Adrian Friday,
    Richard Harper, and Peter Garraghan. 2021. Horus: Interference-aware and prediction-based
    scheduling in deep learning systems. *IEEE Transactions on Parallel and Distributed
    Systems* 33, 1 (2021), 88–100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2023) Enda Yu, Dezun Dong, and Xiangke Liao. 2023. Communication
    Optimization Algorithms for Distributed Deep Learning Systems: A Survey. *IEEE
    Transactions on Parallel and Distributed Systems* 34, 12 (2023), 3294–3308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2021b) Menglu Yu, Chuan Wu, Bo Ji, and Jia Liu. 2021b. A Sum-of-Ratios
    Multi-Dimensional-Knapsack Decomposition for DNN Resource Scheduling. In *IEEE
    INFOCOM 2021 - IEEE Conference on Computer Communications*. 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu and Chowdhury (2020) Peifeng Yu and Mosharaf Chowdhury. 2020. Salus: Fine-grained
    gpu sharing primitives for deep learning applications. In *Proceedings of the
    3rd MLSys Conference*. 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2021a) Peifeng Yu, Jiachen Liu, and Mosharaf Chowdhury. 2021a. Fluid:
    Resource-aware Hyperparameter Tuning Engine. In *Proceedings of Machine Learning
    and Systems*, Vol. 3\. 502–516.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2022) Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri
    Dao, Beidi Chen, Percy S Liang, Christopher Re, and Ce Zhang. 2022. Decentralized
    training of foundation models in heterogeneous environments. *Advances in Neural
    Information Processing Systems* 35 (2022), 25464–25477.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2023) Fanlong Zeng, Wensheng Gan, Yongheng Wang, and S Yu Philip.
    2023. Distributed training of large language models. In *2023 IEEE 29th International
    Conference on Parallel and Distributed Systems (ICPADS)*. IEEE, 840–847.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022) Cheng Zhang, Wanshou Jiang, Yuan Zhang, Wei Wang, Qing Zhao,
    and Chenjie Wang. 2022. Transformer and CNN hybrid deep neural network for semantic
    segmentation of very-high-resolution remote sensing imagery. *IEEE Transactions
    on Geoscience and Remote Sensing* 60 (2022), 1–20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023a) Lin Zhang, Shaohuai Shi, Xiaowen Chu, Wei Wang, Bo Li,
    and Chengjian Liu. 2023a. DeAR: Accelerating Distributed Deep Learning with Fine-Grained
    All-Reduce Pipelining. In *2023 IEEE 43rd International Conference on Distributed
    Computing Systems (ICDCS)*. IEEE, 142–153.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023b) Weigang Zhang, Biyu Zhou, Xuehai Tang, Zhaoxing Wang,
    and Songlin Hu. 2023b. MixPipe: Efficient Bidirectional Pipeline Parallelism for
    Training Large-Scale Models. In *2023 60th ACM/IEEE Design Automation Conference
    (DAC)*. IEEE, 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021) Zhenwei Zhang, Qiang Qi, Ruitao Shang, Li Chen, and Fei
    Xu. 2021. Prophet: Speeding up distributed dnn training with predictable communication
    scheduling. In *Proceedings of the 50th International Conference on Parallel Processing*.
    1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2023b) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan
    Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li,
    Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023b. A Survey
    of Large Language Models. arXiv:2303.18223 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2023a) Yihao Zhao, Xin Liu, Shufan Liu, Xiang Li, Yibo Zhu, Gang
    Huang, Xuanzhe Liu, and Xin Jin. 2023a. Muxflow: Efficient and safe gpu sharing
    in large-scale production deep learning clusters. *arXiv preprint arXiv:2303.13803*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2019) Haoyue Zheng, Fei Xu, Li Chen, Zhi Zhou, and Fangming Liu.
    2019. Cynthia: Cost-efficient cloud resource provisioning for predictable distributed
    deep neural network training. In *Proceedings of the 48th International Conference
    on Parallel Processing*. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2020) Pan Zhou, Xinshu He, Shouxi Luo, Hongfang Yu, and Gang Sun.
    2020. JPAS: Job-progress-aware flow scheduling for deep learning clusters. *Journal
    of Network and Computer Applications* (May 2020), 102590.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2019) Zhi Zhou, Xu Chen, En Li, Liekang Zeng, Ke Luo, and Junshan
    Zhang. 2019. Edge intelligence: Paving the last mile of artificial intelligence
    with edge computing. *Proc. IEEE* 107, 8 (2019), 1738–1762.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
